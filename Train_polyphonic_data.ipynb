{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_polyphonic_data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMHQ1NUHsBKC5mlF6W3EpyR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Train_polyphonic_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "TGpH9G_kAwmy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "COo0mYvKAMmc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install partitura\n",
        "! pip install git+https://github.com/CPJKU/partitura.git@develop\n",
        "import partitura\n",
        "import statistics\n",
        "\n",
        "\n",
        "\n",
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataloader - Set the dataset"
      ],
      "metadata": {
        "id": "de_kI138A7ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DATA = \"AI-MA_project/pr_polyphonic\"\n",
        "\n",
        "batch_size = 1 \n",
        "workers = 0"
      ],
      "metadata": {
        "id": "GZhTj8KDA8Yf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse"
      ],
      "metadata": {
        "id": "VIEk3O1r0PAZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_load = 1000\n",
        "\n",
        "class MusicDataset_polyphonic(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "        piece_lengths = [\"2_voice\",\"4_voice\"]\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for v_len_idx in piece_lengths:\n",
        "            if v_len_idx == \"4_voice\":\n",
        "                for iLabel in range(len(labels)):\n",
        "                    if iLabel == 4:   \n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, v_len_idx , self.labels[iLabel], \"*.pkl\")))       \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                                loaded_obj = pickle.load(f)  \n",
        "\n",
        "                                #print(\"loaded_obj before:\",loaded_obj.shape)\n",
        "\n",
        "                                pitch, t_len = loaded_obj.shape\n",
        "                                if t_len > max_len_load:\n",
        "                                  loaded_obj = loaded_obj[:,:max_len_load]\n",
        "                            \n",
        "\n",
        "                                #print(\"loaded_obj after:\",loaded_obj.shape)\n",
        "\n",
        "                                voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "\n",
        "                                len_list.append(len(loaded_obj.T))\n",
        "                                file_names_list.append(name[-7:-4])\n",
        "                                nbr_voices_list.append(4)                        \n",
        "\n",
        "                        self.pr_dict[self.labels[iLabel]] += voice_files\n",
        "                        self.pr_dict[\"length\"] +=  len_list\n",
        "                        self.pr_dict[\"nbr_voices\"] +=  nbr_voices_list\n",
        "                        self.pr_dict[\"name\"] += file_names_list\n",
        "                   \n",
        "                    else:\n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA,v_len_idx, self.labels[iLabel], \"*.pkl\"))) \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: \n",
        "                                  loaded_obj = pickle.load(f)\n",
        "\n",
        "                                  pitch, t_len = loaded_obj.shape\n",
        "                                  if t_len > max_len_load:\n",
        "                                    loaded_obj = loaded_obj[:,:max_len_load]\n",
        "\n",
        "\n",
        "                                  voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "                        self.pr_dict[self.labels[iLabel]] += voice_files\n",
        "\n",
        "\n",
        "            if v_len_idx == \"2_voice\":\n",
        "                for iLabel in range(len(labels)):\n",
        "                    if iLabel == 4:   \n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, v_len_idx , self.labels[iLabel], \"*.pkl\")))   \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f:\n",
        "                                loaded_obj = pickle.load(f)  \n",
        "\n",
        "                                pitch, t_len = loaded_obj.shape\n",
        "                                if t_len > max_len_load:\n",
        "                                  loaded_obj = loaded_obj[:,:max_len_load]\n",
        "\n",
        "                                voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "                                len_list.append(len(loaded_obj.T))\n",
        "                                file_names_list.append(name[-7:-4])\n",
        "                                nbr_voices_list.append(2)\n",
        "                              \n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files\n",
        "                        self.pr_dict[\"length\"] = len_list\n",
        "                        self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                        self.pr_dict[\"name\"] = file_names_list\n",
        "    \n",
        "                    else:\n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA,v_len_idx, self.labels[iLabel], \"*.pkl\"))) \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: \n",
        "                                  loaded_obj = pickle.load(f)   \n",
        "\n",
        "                                  pitch, t_len = loaded_obj.shape\n",
        "                                  if t_len > max_len_load:\n",
        "                                    loaded_obj = loaded_obj[:,:max_len_load]\n",
        "\n",
        "                                  voice_files.append(sparse.csr_matrix(loaded_obj))  \n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files\n",
        "                    \n",
        "    def __len__(self):\n",
        "        file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, \"2_voice\" , self.labels[4], \"*.pkl\")))\n",
        "        file_names_4 = sorted(glob.glob(os.path.join(PATH_TO_DATA, \"4_voice\" , self.labels[4], \"*.pkl\")))\n",
        "        return len(file_names_2) + len(file_names_4) \n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        for key,value in self.pr_dict.items():\n",
        "          v0 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_0\"][idx]).T)\n",
        "          v1 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_1\"][idx]).T)\n",
        "          \n",
        "          if self.pr_dict[\"nbr_voices\"][idx] == 4:       ### -168 WORKS ONLY IN THIS CASE BC 168 SAMPLES OF LEN(2) ARE LOADED FIRST\n",
        "              v2 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_2\"][idx-168]).T)\n",
        "              v3 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_3\"][idx-168]).T)\n",
        "          if self.pr_dict[\"nbr_voices\"][idx] == 2:\n",
        "              v2 = torch.zeros(v1.shape)\n",
        "              v3 = torch.zeros(v1.shape)\n",
        "          \n",
        "          v_all = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_all\"][idx]).T)\n",
        "          length = self.pr_dict[\"length\"][idx]\n",
        "          nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "          file_name = self.pr_dict[\"name\"][idx]\n",
        "          voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "        return (voices, length, nbr_voices, file_name)"
      ],
      "metadata": {
        "id": "RR_q4F28BE1t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = MusicDataset_polyphonic(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 1:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices,all_voices.shape)\n",
        "\n",
        "\n",
        "### try sparse matrix - convert when using to matrix - in loader only sparse matrix\n",
        "## plot the size for pieces ->  (polyphonic pieces longer / modern pieces)"
      ],
      "metadata": {
        "id": "HFMMT5ydBeDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad012aab-4746-4319-d4a3-e327bfffb644"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "002 tensor([2]) torch.Size([1, 1000, 88, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define U-Net"
      ],
      "metadata": {
        "id": "fGlQ8KsIkl2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "wD23kpffkfKy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Network\n"
      ],
      "metadata": {
        "id": "GXc03KGJkfwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        #self.loss = nn.CrossEntropyLoss(reduction=\"sum\")                       # use weight parameters maybe take 1/88   \n",
        "        #weight_tensor = torch.tensor([1,1,1,4],dtype=torch.double).to(device)\n",
        "        #self.loss = nn.CrossEntropyLoss(weight=weight_tensor,reduction='mean',ignore_index=99)     \n",
        "\n",
        "    \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                      ### squeeze output here before returning                                       \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        #print(\"voices[:,:,:,-1]\", voices[:,:,:,-1].shape,voices.shape, nbr_voices)\n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "        #print(\"score_0\",score_0.shape,score_0)\n",
        "        if torch.any(torch.isnan(score_0)) or torch.any(torch.isinf(score_0)) :\n",
        "          print('score_0 invalid detected at iteration ')\n",
        "          print(torch.any(torch.isnan(score_0)),torch.any(torch.isinf(score_0)))\n",
        "          \n",
        "\n",
        "        if torch.any(torch.isnan(score_1)) or torch.any(torch.isinf(score_1)) :\n",
        "          print('score_1 invalid detected at iteration ')\n",
        "\n",
        "        if torch.any(torch.isnan(score_2)) or torch.any(torch.isinf(score_2)) :\n",
        "          print('score_2 invalid detected at iteration ')\n",
        "\n",
        "        if torch.any(torch.isnan(score_3)) or torch.any(torch.isinf(score_3)) :\n",
        "          print('score_3 invalid detected at iteration ')\n",
        " \n",
        "        weight_v0 = voices[:,:,:,0].squeeze().sum()\n",
        "        weight_v1 = voices[:,:,:,1].squeeze().sum()\n",
        "        weight_v2 = voices[:,:,:,2].squeeze().sum()\n",
        "        weight_v3 = voices[:,:,:,3].squeeze().sum()\n",
        "\n",
        "        stack_tensors_gt = torch.swapaxes(torch.swapaxes(voices[:,:,:,:4].squeeze(), 0, 2), 1,2)\n",
        "        #stack_tensors_gt = torch.stack([v0, v1, v2, v3], dim=0)\n",
        "        \n",
        "        \n",
        "\n",
        "        stack_gt = torch.argmax(stack_tensors_gt,axis=0)\n",
        "\n",
        "        \n",
        "        weight_tensor = torch.stack([weight_v0/weight_v0,weight_v0/weight_v0,weight_v0/weight_v0,weight_v0/weight_v0])\n",
        "        criterion_weighted = nn.CrossEntropyLoss(weight=weight_tensor,reduction='mean',ignore_index=99)  \n",
        "        stack_pred = torch.stack([score_0, score_1, score_2, score_3], dim=0).softmax(dim=0)[None, :]\n",
        "        mask_pred = torch.squeeze(voices[:,:,:,-1])== 0\n",
        "        stack_gt[mask_pred] = 99\n",
        "        loss = criterion_weighted(stack_pred, stack_gt[None,:])\n",
        "\n",
        "        if torch.any(torch.isnan(stack_pred)) or torch.any(torch.isinf(stack_pred)) :\n",
        "          print('invalid input detected at iteration ')\n",
        "\n",
        "        return loss   \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "\n",
        "        else:\n",
        "            sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "            prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            \n",
        "            mask_pred = np.squeeze(sentences)== 0\n",
        "            v_pred_argm[mask_pred] = -1\n",
        "\n",
        "            return v_pred_argm        "
      ],
      "metadata": {
        "id": "nO4PtE0kknDd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "monophonic = True\n",
        "his = start_experiment(1, 0.0001, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, \"CNN\", learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MDeP7tqWmJGb",
        "outputId": "3bf31929-d6b6-4100-94b7-2507121796ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmonophonic = True\\nhis = start_experiment(1, 0.0001, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, \"CNN\", learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Training Process"
      ],
      "metadata": {
        "id": "QyIAxKS-k4m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "UrErJAevk2Sg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tryining Loop"
      ],
      "metadata": {
        "id": "u2R-p3BnlZdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### versuch hier mal nur für 4 voices zu trainieren\n",
        "\n",
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "                \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            if nbr_voices == 4:\n",
        "              voices = voices.to(device).float()\n",
        "              optimizer.zero_grad()\n",
        "              loss = model.forward(voices, lens, nbr_voices)             \n",
        "              print(\"loss:\",loss)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              loss_sum += loss.item()    \n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "        print(\"Epoch: {}, Train Loss: {}\".format(i_epoch,train_loss)) \n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "50qWCIMNk7ZL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "metadata": {
        "id": "-NUWBUvqk_No"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter choice"
      ],
      "metadata": {
        "id": "-bFYYY_6l5hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MusicNetwork\n",
        "epochs = 2\n",
        "lr = 0.001 # was 0.001    #0.00001\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "metadata": {
        "id": "qlFVa23Ql5yz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the experiment"
      ],
      "metadata": {
        "id": "Bq16m1D9lXHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO3ZxWbzlBis",
        "outputId": "3b560d40-312d-4570-8139-84793326b45a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataloader 199 val_dataloader 36\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Training on device: cpu\n",
            "monophonic set to: True\n",
            "loss: tensor(1.3763, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.4422, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3561, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3814, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3862, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3760, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3587, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3112, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3098, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2798, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2809, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2789, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2253, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1541, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2656, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1438, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1457, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1089, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1193, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1929, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1396, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1609, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0918, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1488, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1796, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0774, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1326, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9826, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0358, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0406, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2372, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1010, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1829, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2012, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1060, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2552, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1418, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1598, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2695, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0574, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3026, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2229, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0269, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9408, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1232, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0418, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0893, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1914, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0023, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9815, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0401, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9219, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0677, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1200, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0796, grad_fn=<NllLoss2DBackward0>)\n",
            "Epoch: 1, Train Loss: 0.3233506430932625\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.9789, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0547, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9782, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0982, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0349, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1296, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0789, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9215, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0059, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0429, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1153, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1103, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0011, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0604, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9862, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9508, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0342, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0173, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1148, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0101, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9496, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9502, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0039, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0149, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9889, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0275, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9883, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9652, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8848, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9686, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1405, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9381, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0702, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1630, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9297, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1939, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9469, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9443, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0115, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9690, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0952, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0421, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8221, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8975, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0247, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9662, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9775, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1791, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8587, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9199, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0112, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8066, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0163, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0664, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9600, grad_fn=<NllLoss2DBackward0>)\n",
            "Epoch: 2, Train Loss: 0.2784750141091083\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load trained model"
      ],
      "metadata": {
        "id": "UoqAYzPnnj1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSlYUsRUmALB",
        "outputId": "736c0849-3088-4dba-9396-b0e32a82b2f2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f0c867eb250>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRtGR86j1DYB",
        "outputId": "56b4677e-ea25-40c1-b4a7-a6b7a3cac870"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture  \n",
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch2.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "yFBgUAmNntEo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dic with key:filename, val: part_obj  "
      ],
      "metadata": {
        "id": "iIXfywU8n3f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "Hu2g0G8jozVf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "\n",
        "path_parts = \"AI-MA_project/polyphonic_new\"\n",
        "part_dic = {}\n",
        "\n",
        "#### create a list with all filenames in the right order ####\n",
        "file_names_part = []\n",
        "\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    file_names_part.append(filename[9:12])\n",
        "print(file_names_part)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k29NpOgtn3np",
        "outputId": "146ef339-0a0e-4c9e-aa8e-89ba5bac0982"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['000', '001', '002', '003', '004', '005', '006', '007', '008', '009', '010', '011', '012', '013', '014', '015', '016', '018', '019', '020', '021', '022', '023', '024', '025', '026', '027', '028', '029', '030', '031', '032', '033', '034', '035', '036', '037', '039', '040', '041', '042', '043', '044', '045', '046', '047', '048', '049', '050', '051', '052', '053', '054', '055', '056', '057', '058', '059', '060', '061', '062', '063', '064', '065', '066', '067', '068', '069', '070', '071', '072', '073', '074', '075', '076', '077', '078', '079', '080', '081', '082', '083', '084', '085', '087', '088', '089', '090', '091', '092', '093', '094', '095', '096', '097', '098', '099', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy_polyphonic_separate(model, train_dataloader, part_dic):\n",
        "\n",
        "    unitl_len_idx = max_len_load\n",
        "\n",
        "    path_parts = \"AI-MA_project/polyphonic_new\"\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict_two = {'0': [], '1': [] }\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "                                      \n",
        "                #print(\"nbr_voices:\",nbr_voices)\n",
        "        if idx not in [0, 7, 21, 32, 35]: \n",
        "\n",
        "            if nbr_voices == 4:\n",
        "                file_name = file_name[0]\n",
        "                filename_part = os.path.join(path_parts, \"part_file\" + file_name + \".mid\")\n",
        "                part = partitura.load_score_midi(filename_part)\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "                part_3 = part[3]\n",
        "                \n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)              \n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]   \n",
        "\n",
        "                        if end_first <= unitl_len_idx:\n",
        "                          pitch_first = pitch_list[i]\n",
        "                          pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                          truth_list = [label for i in range(len(pred_list_first))]\n",
        "              \n",
        "                          result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                          # do majority vote if not all predictions are for same voice\n",
        "                          if result == False:\n",
        "                              major, major_idx = torch.mode(pred_list_first,0)\n",
        "                              major = major.numpy().tolist()\n",
        "                              pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "                        if len(pred_list_first)==0:\n",
        "                          print(\"ATTENTION:\", len(pred_list_first),str(label) )\n",
        "                          print(\"index:\", idx)\n",
        "                          print(\"------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                  counting = 0\n",
        "                  for j in range(len(total_predictions_dict[i])):\n",
        "                      if total_predictions_dict[i][j][0] == gt:         \n",
        "                        counting +=1\n",
        "                  count_dict_2[i].append(counting)\n",
        "\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                \n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "\n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "\n",
        "            if nbr_voices == 2:\n",
        "                file_name = file_name[0]\n",
        "                filename_part = os.path.join(path_parts, \"part_file\" + file_name + \".mid\")\n",
        "                part = partitura.load_score_midi(filename_part)\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                \n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                \n",
        "                list_of_note_arrays_2 = [note_array_0,note_array_1]\n",
        "\n",
        "                \n",
        "                ground_truth_label_list_2 = [0,1]              \n",
        "                total_predictions_dict_2 = {'0': [], '1': [] }\n",
        "                total_truth_dict_2 = {'0': [], '1': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays_2):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list_2[el_note_arr]\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]   \n",
        "\n",
        "                        if end_first <= unitl_len_idx:\n",
        "                          pitch_first = pitch_list[i]\n",
        "                          pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                          truth_list = [label for i in range(len(pred_list_first))]\n",
        "              \n",
        "                          result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                          # do majority vote if not all predictions are for same voice\n",
        "                          if result == False:\n",
        "                              major, major_idx = torch.mode(pred_list_first,0)\n",
        "                              major = major.numpy().tolist()\n",
        "                              pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        total_predictions_dict_2[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict_2[str(label)].append(truth_list)\n",
        "\n",
        "                        #print(\"pred_list\",str(label),len(pred_list_first),pred_list_first)\n",
        "                        if len(pred_list_first)==0:\n",
        "                          print(\"ATTENTION:\", len(pred_list_first),str(label) )\n",
        "                          print(\"index:\", idx)\n",
        "                          print(\"------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "                count_dict_22 = {'0': [], '1': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict_2.keys()):\n",
        "                  print(\"enumerate(total_predictions_dict_2.keys()):\",gt,i)\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict_2.keys()):\n",
        "                  counting = 0\n",
        "                  for j in range(len(total_predictions_dict_2[i])):\n",
        "                      if total_predictions_dict_2[i][j][0] == gt:       \n",
        "                        counting +=1\n",
        "                  count_dict_22[i].append(counting)\n",
        "\n",
        "                \n",
        "                acc_0_2 = count_dict_22[\"0\"][0]/len(total_predictions_dict_2[\"0\"])\n",
        "                acc_1_2 = count_dict_22[\"1\"][0]/len(total_predictions_dict_2[\"1\"])\n",
        "                \n",
        "                print(\"acc_0_2, sample {}:\".format(idx),acc_0_2)\n",
        "                print(\"acc_1_2, sample {}:\".format(idx),acc_1_2)\n",
        "                \n",
        "                acc_score_dict_two[\"0\"].append(acc_0_2)\n",
        "                acc_score_dict_two[\"1\"].append(acc_1_2)\n",
        "                \n",
        "                \n",
        "    return total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict_two[\"0\"]), statistics.mean(acc_score_dict_two[\"1\"]), statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "WqSAyLg6-3HF"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_pred , dict_gt, acc_0_2, acc_1_2, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_polyphonic_separate(model,val_dataloader,part_dic=file_names_part)\n",
        "\n",
        "print(\"acc_0_2:\",acc_0_2)\n",
        "print(\"acc_1_2:\",acc_1_2)\n",
        "print(\"acc_0:\",acc_0)\n",
        "print(\"acc_1:\",acc_1)\n",
        "print(\"acc_2:\",acc_2)\n",
        "print(\"acc_3:\",acc_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TY74JqWntaX1",
        "outputId": "0a792c6d-7c8d-4df5-8c1d-2d89a6311723"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=78 velocity=64 time=228\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=69 velocity=64 time=78\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=66 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=73 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=69 velocity=64 time=48\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=239\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:419: UserWarning: pitch spelling\n",
            "  warnings.warn(\"pitch spelling\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:115: UserWarning: voice estimation\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: create_part\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add time sigs and measures\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: tie notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: find tuplets\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: done create_part\n",
            "  part_name=part_names.get(part_nr, None),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enumerate(total_predictions_dict_2.keys()): 0 0\n",
            "enumerate(total_predictions_dict_2.keys()): 1 1\n",
            "acc_0_2, sample 1: 0.9894039735099338\n",
            "acc_1_2, sample 1: 0.8663303909205549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=78 velocity=64 time=108\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=119\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enumerate(total_predictions_dict_2.keys()): 0 0\n",
            "enumerate(total_predictions_dict_2.keys()): 1 1\n",
            "acc_0_2, sample 2: 0.9416342412451362\n",
            "acc_1_2, sample 2: 0.003537735849056604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=4 note=44 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=4 note=42 velocity=64 time=90\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=4 note=37 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=4 note=37 velocity=64 time=90\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: UserWarning: voice estimation\n",
            "  app.launch_new_instance()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0, sample 3: 0.9967105263157895\n",
            "acc 1, sample 3: 0.9922779922779923\n",
            "acc 2, sample 3: 0.9765258215962441\n",
            "acc 3, sample 3: 0.9732142857142857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=67 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=67 velocity=64 time=120\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=75 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=479\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=2 note=58 velocity=64 time=240\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=2 note=58 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=359\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-f0c15122b6a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict_pred\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdict_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_0_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_1_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_0\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0macc_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_accuracy_polyphonic_separate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpart_dic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_names_part\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"acc_0_2:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_0_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"acc_1_2:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_1_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"acc_0:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-153-2c0f25c7f3c0>\u001b[0m in \u001b[0;36mevaluate_accuracy_polyphonic_separate\u001b[0;34m(model, train_dataloader, part_dic)\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0mmonophonic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonophonic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mground_truth_label_list_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mel_note_arr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-13bea14f730c>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentences, sentences_len, monophonic)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonophonic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# Compute the outputs from the linear units.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mscores_comb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmonophonic\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-13bea14f730c>\u001b[0m in \u001b[0;36mcompute_outputs\u001b[0;34m(self, sentences, sentences_len)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m           \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m                      \u001b[0;31m### squeeze output here before returning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-69b725fc1feb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdown\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble_conv_downs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdown\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble_conv_downs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mconcat_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy mixed all voices (old)"
      ],
      "metadata": {
        "id": "u2frgy_KLUDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy_polyphonic(model, train_dataloader, part_dic):\n",
        "\n",
        "    unitl_len_idx = max_len_load\n",
        "    path_parts = \"AI-MA_project/polyphonic_new\"\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "                                      \n",
        "                #print(\"nbr_voices:\",nbr_voices)\n",
        "        if idx >30 and idx not in [0, 7, 21, 32, 35]: \n",
        "            #if nbr_voices == 2:\n",
        "                \n",
        "                file_name = file_name[0]\n",
        "                filename_part = os.path.join(path_parts, \"part_file\" + file_name + \".mid\")\n",
        "                part = partitura.load_score_midi(filename_part)\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                \n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                \n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1]\n",
        "\n",
        "                if len(part)== 4:\n",
        "                    part_2 = part[2]\n",
        "                    note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    \n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]   \n",
        "\n",
        "                        if end_first <= unitl_len_idx:\n",
        "                          #print(\"stop\",end_first)\n",
        "                          pitch_first = pitch_list[i]\n",
        "                          pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                          truth_list = [label for i in range(len(pred_list_first))]\n",
        "              \n",
        "                          result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                          # do majority vote if not all predictions are for same voice\n",
        "                          if result == False:\n",
        "                              major, major_idx = torch.mode(pred_list_first,0)\n",
        "                              major = major.numpy().tolist()\n",
        "                              pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "                        #print(\"pred_list\",str(label),len(pred_list_first),pred_list_first)\n",
        "                        if len(pred_list_first)==0:\n",
        "                          print(\"ATTENTION:\", len(pred_list_first),str(label) )\n",
        "                          print(\"index:\", idx)\n",
        "                          print(\"------------------------------------------------------------------------\")\n",
        "                          print(\"total_predictions_dict\",str(label),len(pred_list_first),pred_list_first)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                  counting = 0\n",
        "                  for j in range(len(total_predictions_dict[i])):\n",
        "                      if total_predictions_dict[i][j][0] == gt:         \n",
        "                        counting +=1\n",
        "                  count_dict_2[i].append(counting)\n",
        "\n",
        "\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                \n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                \n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                    acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                    print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                    acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                \n",
        "\n",
        "                \n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "\n",
        "    return total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "yvS8dZJaoxkq"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_pred , dict_gt, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_polyphonic(model,val_dataloader,part_dic=file_names_part)\n",
        "\n",
        "\n",
        "print(\"acc_0:\",acc_0)\n",
        "print(\"acc_1:\",acc_1)\n",
        "print(\"acc_2:\",acc_2)\n",
        "print(\"acc_3:\",acc_3)"
      ],
      "metadata": {
        "id": "8KzCauC6_FsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7tL0aaP711g5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}