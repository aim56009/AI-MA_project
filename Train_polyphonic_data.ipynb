{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_polyphonic_data.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Train_polyphonic_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "TGpH9G_kAwmy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "COo0mYvKAMmc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install partitura\n",
        "!pip install git+https://github.com/CPJKU/partitura.git@develop\n",
        "import partitura\n",
        "import statistics\n",
        "import matplotlib\n",
        "from scipy import sparse\n",
        "\n",
        "\n",
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataloader - Set the dataset"
      ],
      "metadata": {
        "id": "de_kI138A7ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DATA = \"AI-MA_project/pr_polyphonic\"\n",
        "\n",
        "batch_size = 1 \n",
        "workers = 0"
      ],
      "metadata": {
        "id": "GZhTj8KDA8Yf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_load = 5000\n",
        "\n",
        "class MusicDataset_polyphonic(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "        piece_lengths = [\"2_voice\",\"4_voice\"]\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        counter_files_v2 = 0\n",
        "\n",
        "        for v_len_idx in piece_lengths:\n",
        "            if v_len_idx == \"4_voice\":\n",
        "                for iLabel in range(len(labels)):\n",
        "                    if iLabel == 4:   \n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, v_len_idx , self.labels[iLabel], \"*.pkl\")))       \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einfÃ¼hren damit das funktioniert\n",
        "                                loaded_obj = pickle.load(f)  \n",
        "\n",
        "                                #print(\"loaded_obj before:\",loaded_obj.shape)\n",
        "\n",
        "                                pitch, t_len = loaded_obj.shape\n",
        "                                if t_len > max_len_load:\n",
        "                                  loaded_obj = loaded_obj[:,:max_len_load]\n",
        "                            \n",
        "\n",
        "                                #print(\"loaded_obj after:\",loaded_obj.shape)\n",
        "\n",
        "                                voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "\n",
        "                                len_list.append(len(loaded_obj.T))\n",
        "                                file_names_list.append(name[-7:-4])\n",
        "                                nbr_voices_list.append(4)                        \n",
        "\n",
        "                        self.pr_dict[self.labels[iLabel]] += voice_files\n",
        "                        self.pr_dict[\"length\"] +=  len_list\n",
        "                        self.pr_dict[\"nbr_voices\"] +=  nbr_voices_list\n",
        "                        self.pr_dict[\"name\"] += file_names_list\n",
        "                   \n",
        "                    else:\n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA,v_len_idx, self.labels[iLabel], \"*.pkl\"))) \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: \n",
        "                                  loaded_obj = pickle.load(f)\n",
        "\n",
        "                                  pitch, t_len = loaded_obj.shape\n",
        "                                  if t_len > max_len_load:\n",
        "                                    loaded_obj = loaded_obj[:,:max_len_load]\n",
        "\n",
        "\n",
        "                                  voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "                        self.pr_dict[self.labels[iLabel]] += voice_files\n",
        "\n",
        "\n",
        "            if v_len_idx == \"2_voice\":\n",
        "              #if counter_files_v2 < len(sorted(glob.glob(os.path.join(PATH_TO_DATA, \"4_voice\" , self.labels[4], \"*.pkl\")))):     ############### load as many 2 voice pieces as 4 voice pieces    ##########\n",
        "                #counter_files_v2 +=1\n",
        "                for iLabel in range(len(labels)):\n",
        "                    if iLabel == 4:   \n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, v_len_idx , self.labels[iLabel], \"*.pkl\")))   \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f:\n",
        "                                loaded_obj = pickle.load(f)  \n",
        "\n",
        "                                pitch, t_len = loaded_obj.shape\n",
        "                                if t_len > max_len_load:\n",
        "                                  loaded_obj = loaded_obj[:,:max_len_load]\n",
        "\n",
        "                                voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "                                len_list.append(len(loaded_obj.T))\n",
        "                                file_names_list.append(name[-7:-4])\n",
        "                                nbr_voices_list.append(2)\n",
        "                              \n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files\n",
        "                        self.pr_dict[\"length\"] = len_list\n",
        "                        self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                        self.pr_dict[\"name\"] = file_names_list\n",
        "    \n",
        "                    else:\n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA,v_len_idx, self.labels[iLabel], \"*.pkl\"))) \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: \n",
        "                                  loaded_obj = pickle.load(f)   \n",
        "\n",
        "                                  pitch, t_len = loaded_obj.shape\n",
        "                                  if t_len > max_len_load:\n",
        "                                    loaded_obj = loaded_obj[:,:max_len_load]\n",
        "\n",
        "                                  voice_files.append(sparse.csr_matrix(loaded_obj))  \n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files\n",
        "                    \n",
        "    def __len__(self):\n",
        "        file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, \"2_voice\" , self.labels[4], \"*.pkl\")))\n",
        "        file_names_4 = sorted(glob.glob(os.path.join(PATH_TO_DATA, \"4_voice\" , self.labels[4], \"*.pkl\")))\n",
        "        return len(file_names_2) + len(file_names_4) \n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        for key,value in self.pr_dict.items():\n",
        "          v0 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_0\"][idx]).T)\n",
        "          v1 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_1\"][idx]).T)\n",
        "          \n",
        "          if self.pr_dict[\"nbr_voices\"][idx] == 4:       ### -168 WORKS ONLY IN THIS CASE BC 168 SAMPLES OF LEN(2) ARE LOADED FIRST\n",
        "              v2 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_2\"][idx-168]).T)\n",
        "              v3 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_3\"][idx-168]).T)\n",
        "          if self.pr_dict[\"nbr_voices\"][idx] == 2:\n",
        "              v2 = torch.zeros(v1.shape)\n",
        "              v3 = torch.zeros(v1.shape)\n",
        "          \n",
        "          v_all = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_all\"][idx]).T)\n",
        "          length = self.pr_dict[\"length\"][idx]\n",
        "          nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "          file_name = self.pr_dict[\"name\"][idx]\n",
        "          voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "        return (voices, length, nbr_voices, file_name)"
      ],
      "metadata": {
        "id": "RR_q4F28BE1t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots\n"
      ],
      "metadata": {
        "id": "_iGHmvhijcEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "dataset = MusicDataset_polyphonic(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 168:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices,all_voices.shape)\n",
        "        break\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HFMMT5ydBeDO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3a0e6ffb-fa1d-432d-8db9-ab965172fc97"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndataset = MusicDataset_polyphonic(PATH_TO_DATA)\\nloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\\n\\nfor i, sample_batched in enumerate(loader):\\n    if i == 168:\\n        all_voices, length, nbr_voices, file_name = sample_batched\\n        print(file_name[0],nbr_voices,all_voices.shape)\\n        break\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 168:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices,all_voices.shape)\n",
        "        break\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "m6jxjpWJcD16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ff76f5d5-e615-4ba2-d686-c697af81d4d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor i, sample_batched in enumerate(loader):\\n    if i == 168:\\n        all_voices, length, nbr_voices, file_name = sample_batched\\n        print(file_name[0],nbr_voices,all_voices.shape)\\n        break\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "pianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\n",
        "pianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\n",
        "pianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\n",
        "pianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\n",
        "pianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\n",
        "\n",
        "colors_0 = 'none red'.split()\n",
        "colors_1 = 'none green'.split()\n",
        "colors_2 = 'none blue'.split()\n",
        "colors_3 = 'black orange'.split()\n",
        "\n",
        "cmap_0 = matplotlib.colors.ListedColormap(colors_0, name='colors', N=None)\n",
        "cmap_1 = matplotlib.colors.ListedColormap(colors_1, name='colors', N=None)\n",
        "cmap_2 = matplotlib.colors.ListedColormap(colors_2, name='colors', N=None)\n",
        "cmap_3 = matplotlib.colors.ListedColormap(colors_3, name='colors', N=None)\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(20, 10))\n",
        "\n",
        "ax.imshow(pianoroll_3[3000:4000,:], origin=\"lower\", cmap=cmap_3, interpolation='nearest', aspect='auto')\n",
        "ax.imshow(pianoroll_2[3000:4000,:], origin=\"lower\", cmap=cmap_2, interpolation='nearest', aspect='auto')\n",
        "ax.imshow(pianoroll_1[3000:4000,:], origin=\"lower\", cmap=cmap_1, interpolation='nearest', aspect='auto')\n",
        "ax.imshow(pianoroll_0[3000:4000,:], origin=\"lower\", cmap=cmap_0, interpolation='nearest', aspect='auto')\n",
        "ax.set_xlabel(f'Time ({\"beat\"}s/{12})')\n",
        "ax.set_ylabel('Piano key')\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NOV7MCxhSud7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8d5c8c25-e55f-4a55-b2ee-1d1b274b177f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\\npianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\\npianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\\npianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\\npianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\\n\\ncolors_0 = \\'none red\\'.split()\\ncolors_1 = \\'none green\\'.split()\\ncolors_2 = \\'none blue\\'.split()\\ncolors_3 = \\'black orange\\'.split()\\n\\ncmap_0 = matplotlib.colors.ListedColormap(colors_0, name=\\'colors\\', N=None)\\ncmap_1 = matplotlib.colors.ListedColormap(colors_1, name=\\'colors\\', N=None)\\ncmap_2 = matplotlib.colors.ListedColormap(colors_2, name=\\'colors\\', N=None)\\ncmap_3 = matplotlib.colors.ListedColormap(colors_3, name=\\'colors\\', N=None)\\n\\nfig, ax = plt.subplots(1, figsize=(20, 10))\\n\\nax.imshow(pianoroll_3[3000:4000,:], origin=\"lower\", cmap=cmap_3, interpolation=\\'nearest\\', aspect=\\'auto\\')\\nax.imshow(pianoroll_2[3000:4000,:], origin=\"lower\", cmap=cmap_2, interpolation=\\'nearest\\', aspect=\\'auto\\')\\nax.imshow(pianoroll_1[3000:4000,:], origin=\"lower\", cmap=cmap_1, interpolation=\\'nearest\\', aspect=\\'auto\\')\\nax.imshow(pianoroll_0[3000:4000,:], origin=\"lower\", cmap=cmap_0, interpolation=\\'nearest\\', aspect=\\'auto\\')\\nax.set_xlabel(f\\'Time ({\"beat\"}s/{12})\\')\\nax.set_ylabel(\\'Piano key\\')\\nplt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define U-Net"
      ],
      "metadata": {
        "id": "fGlQ8KsIkl2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "\n",
        "        #y = x\n",
        "        concat_layers = concat_layers[::-1]\n",
        "\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x#, y"
      ],
      "metadata": {
        "id": "wD23kpffkfKy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Network\n"
      ],
      "metadata": {
        "id": "GXc03KGJkfwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "\n",
        "        self.input_unet = int(np.floor(max_len_load/2/2/2/2)*1024*5)\n",
        "        #self.clf_head = nn.Linear(self.input_unet, 4)                         ################   4 bc max nbr voices ==4 in our dataset\n",
        "        #self.clf_head_gap = nn.Linear(1024, 128)\n",
        "        #self.clf_head_gap_2 = nn.Linear(128, 4)\n",
        "        self.indicator = 0\n",
        "\n",
        "    \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          #out, down_state = self.cnn(sentences)\n",
        "\n",
        "          out = self.cnn(sentences)\n",
        "\n",
        "          #self.down_state = down_state\n",
        "\n",
        "          ############### try implementing GAP ###############\n",
        "          #gap = nn.AdaptiveAvgPool2d((1,1))\n",
        "          #input = down_state\n",
        "          #gap_reduced = gap(input).squeeze()\n",
        "          #out_clf_head = self.clf_head_gap_2(self.clf_head_gap(gap_reduced))\n",
        "          ######################################################\n",
        "\n",
        "          return out#, out_clf_head  \n",
        "                 \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        #scores_comb, out_clf_head = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "        #clf_score = out_clf_head.softmax(dim=0)\n",
        "        #clf_label = nbr_voices[0]-1\n",
        "        #define_clf_loss = nn.CrossEntropyLoss(reduction='mean')\n",
        "        #loss_clf = define_clf_loss(clf_score, clf_label)\n",
        "\n",
        "\n",
        "        weight_v0 = voices[:,:,:,0].squeeze().sum()\n",
        "        weight_v1 = voices[:,:,:,1].squeeze().sum()\n",
        "        weight_v2 = voices[:,:,:,2].squeeze().sum()\n",
        "        weight_v3 = voices[:,:,:,3].squeeze().sum()\n",
        "\n",
        "        stack_tensors_gt = torch.swapaxes(torch.swapaxes(voices[:,:,:,:4].squeeze(), 0, 2), 1,2)\n",
        "        stack_gt = torch.argmax(stack_tensors_gt,axis=0)\n",
        "\n",
        "        \n",
        "        weight_tensor = torch.stack([weight_v0/weight_v0,weight_v0/weight_v0,weight_v0/weight_v0,weight_v0/weight_v0])\n",
        "        criterion_weighted = nn.CrossEntropyLoss(weight=weight_tensor,reduction='mean',ignore_index=99)  \n",
        "        stack_pred = torch.stack([score_0, score_1, score_2, score_3], dim=0).softmax(dim=0)[None, :]\n",
        "        mask_pred = torch.squeeze(voices[:,:,:,-1])== 0\n",
        "        stack_gt[mask_pred] = 99\n",
        "        loss = criterion_weighted(stack_pred, stack_gt[None,:])\n",
        "\n",
        "        if torch.any(torch.isnan(stack_pred)) or torch.any(torch.isinf(stack_pred)) :\n",
        "          print('invalid input detected at iteration ')\n",
        "\n",
        "        return loss #+ loss_clf\n",
        "        #return loss_clf\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "        #scores_comb, out_clf_head = self.compute_outputs(sentences, sentences_len)\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "        sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "        prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "        \n",
        "        mask_pred = np.squeeze(sentences)== 0\n",
        "        v_pred_argm[mask_pred] = -1\n",
        "\n",
        "        ### only output if clf is not none ####       ####!!!!!!!!!NNEEEED TO TEST AGAIN !!!!\n",
        "        #clf_score = out_clf_head.softmax(dim=0)\n",
        "        #pred_clf = torch.argmax(clf_score,axis=0)              \n",
        "\n",
        "        return v_pred_argm #, pred_clf   "
      ],
      "metadata": {
        "id": "nO4PtE0kknDd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "monophonic = True\n",
        "his = start_experiment(1, 0.001, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, \"CNN\", learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MDeP7tqWmJGb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3d502843-41a2-4849-e53e-fb544f7e55d4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmonophonic = True\\nhis = start_experiment(1, 0.001, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, \"CNN\", learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Training Process"
      ],
      "metadata": {
        "id": "QyIAxKS-k4m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    #scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "    scheduler = None\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "UrErJAevk2Sg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tryining Loop"
      ],
      "metadata": {
        "id": "u2R-p3BnlZdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### versuch hier mal nur fÃ¼r 4 voices zu trainieren\n",
        "\n",
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    history = []\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        loss_sum_clf = 0\n",
        "        model.train()\n",
        "                \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            if nbr_voices == 4:\n",
        "            #if idx < 100:\n",
        "              voices = voices.to(device).float()\n",
        "              optimizer.zero_grad()\n",
        "              loss = model.forward(voices, lens, nbr_voices)\n",
        "              if idx%50==0:\n",
        "                print(\"sample {} / {}\".format(idx,len(train_dataloader)))           \n",
        "                #print(\"loss:\",loss)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              loss_sum += loss.item()  \n",
        "              history.append(loss.detach().cpu().numpy())\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "        print(\"Epoch: {}, Train Loss: {}\".format(i_epoch,train_loss)) \n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "        torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "50qWCIMNk7ZL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "metadata": {
        "id": "-NUWBUvqk_No"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter choice"
      ],
      "metadata": {
        "id": "-bFYYY_6l5hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MusicNetwork\n",
        "epochs = 5\n",
        "lr = 1e-4 \n",
        "momentum = 0.9\n",
        "decay = 0 #1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\" # with clf head: \"CNN_CLF\"\n",
        "\n",
        "monophonic = True"
      ],
      "metadata": {
        "id": "qlFVa23Ql5yz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the experiment"
      ],
      "metadata": {
        "id": "Bq16m1D9lXHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "metadata": {
        "id": "kO3ZxWbzlBis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his)\n",
        "plt.title(\"Loss over Samples\")\n",
        "plt.xlabel(\"Samples\")\n",
        "plt.ylabel(\"Loss\")"
      ],
      "metadata": {
        "id": "cazo64DlQQH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load trained model"
      ],
      "metadata": {
        "id": "UoqAYzPnnj1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "#train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "metadata": {
        "id": "qTx4tg3mLisA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader"
      ],
      "metadata": {
        "id": "dSlYUsRUmALB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_dataloader)"
      ],
      "metadata": {
        "id": "ZRtGR86j1DYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture  \n",
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch5.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "yFBgUAmNntEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dic with key:filename, val: part_obj  "
      ],
      "metadata": {
        "id": "iIXfywU8n3f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "\n",
        "path_parts = \"AI-MA_project/polyphonic_new\"\n",
        "part_dic = {}\n",
        "\n",
        "#### create a list with all filenames in the right order ####\n",
        "file_names_part = []\n",
        "\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    file_names_part.append(filename[9:12])\n",
        "print(file_names_part)"
      ],
      "metadata": {
        "id": "k29NpOgtn3np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute polyphonic accuracy"
      ],
      "metadata": {
        "id": "JO7EESFlO6rz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## long code "
      ],
      "metadata": {
        "id": "9bbgDkpYzyal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy_polyphonic_separate(model, train_dataloader, part_dic,print_predictions=False):\n",
        "\n",
        "    unitl_len_idx = max_len_load\n",
        "\n",
        "    path_parts = \"AI-MA_project/polyphonic_new\"\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict_new = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict_two = {'0': [], '1': [] }\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "        #print(\"nbr_voices:\",nbr_voices)\n",
        "        if idx not in [3, 14, 20, 21, 23, 31, 34, 35, 41, 45] and nbr_voices==4:   \n",
        "            print(\"idx:\",idx)\n",
        "            if nbr_voices == 4:\n",
        "                file_name = file_name[0]\n",
        "                filename_part = os.path.join(path_parts, \"part_file\" + file_name + \".mid\")\n",
        "                part = partitura.load_score_midi(filename_part)\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "                part_3 = part[3]\n",
        "                \n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)              \n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]    \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        #prediction, prediction_clf = model.predict(voices[:,:,:,-1], lens, monophonic) \n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "                        #print(\"prediction number of voices:\",prediction_clf+1)\n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]   \n",
        "\n",
        "                        if end_first <= unitl_len_idx:\n",
        "                          pitch_first = pitch_list[i]\n",
        "                          pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                          truth_list = [label for i in range(len(pred_list_first))]\n",
        "              \n",
        "                          result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                          # do majority vote if not all predictions are for same voice\n",
        "                          if result == False:\n",
        "                              major, major_idx = torch.mode(pred_list_first,0)\n",
        "                              major = major.numpy().tolist()\n",
        "                              pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "                        if len(pred_list_first)==0:\n",
        "                          print(\"ATTENTION:\", len(pred_list_first),str(label) )\n",
        "                          print(\"index:\", idx)\n",
        "                          print(\"------------------------------------------------------------------------\")\n",
        "\n",
        "                if print_predictions == True:\n",
        "                    #################### print pr ####################\n",
        "                    fig_size_1 = (10, 5)\n",
        "\n",
        "                    prediction_0 = np.where(prediction ==0, 1, 0)\n",
        "                    prediction_1 = np.where(prediction ==1, 1, 0)\n",
        "                    prediction_2 = np.where(prediction ==2, 1, 0)\n",
        "                    prediction_3 = np.where(prediction ==3, 1, 0)\n",
        "\n",
        "                    ####### print predicte pr #######                  \n",
        "                    colors_0 = 'black red'.split()\n",
        "                    colors_1 = 'none green'.split()  \n",
        "                    colors_2 = 'none blue'.split() \n",
        "                    colors_3 = 'none orange'.split()                \n",
        "                    cmap_0 = matplotlib.colors.ListedColormap(colors_0, name='colors', N=None)        \n",
        "                    cmap_1 = matplotlib.colors.ListedColormap(colors_1, name='colors', N=None)\n",
        "                    cmap_2 = matplotlib.colors.ListedColormap(colors_2, name='colors', N=None)\n",
        "                    cmap_3 = matplotlib.colors.ListedColormap(colors_3, name='colors', N=None)\n",
        "\n",
        "                    fig, ax = plt.subplots(1, figsize=fig_size_1)\n",
        "\n",
        "                    ax.imshow(prediction_0.T, origin=\"lower\", cmap=cmap_0, interpolation='nearest', aspect='auto') \n",
        "                    ax.imshow(prediction_1.T, origin=\"lower\", cmap=cmap_1, interpolation='nearest', aspect='auto')   \n",
        "                    ax.imshow(prediction_2.T, origin=\"lower\", cmap=cmap_2, interpolation='nearest', aspect='auto')\n",
        "                    ax.imshow(prediction_3.T, origin=\"lower\", cmap=cmap_3, interpolation='nearest', aspect='auto')\n",
        "\n",
        "                    ax.set_title(\"PREDICTED: red-v0, green-v1, blue-v2, orange-v3\")                \n",
        "                    ax.set_xlabel(f'Time ({\"beat\"}s/{12})')\n",
        "                    ax.set_ylabel('Piano key')\n",
        "                    plt.show()\n",
        "\n",
        "                    ####### print original pr #######\n",
        "                    onset_beat = partitura.utils.ensure_notearray(part)['onset_beat'][-1]\n",
        "                    duration_beat = partitura.utils.ensure_notearray(part)['duration_beat'][-1]\n",
        "                    beat_all = onset_beat + duration_beat\n",
        "                    pr_zero = partitura.utils.compute_pianoroll(part_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "                    pr_zero = pr_zero.toarray()\n",
        "                    pr_one = partitura.utils.compute_pianoroll(part_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "                    pr_one = pr_one.toarray()\n",
        "                    pr_two = partitura.utils.compute_pianoroll(part_2, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "                    pr_two = pr_two.toarray()\n",
        "                    pr_three = partitura.utils.compute_pianoroll(part_3, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "                    pr_three = pr_three.toarray()\n",
        "\n",
        "                    fig, ax = plt.subplots(1, figsize=fig_size_1)\n",
        "                    ax.imshow(pr_zero[:,:unitl_len_idx], origin=\"lower\", cmap=cmap_0, interpolation='nearest', aspect='auto')\n",
        "                    ax.imshow(pr_one[:,:unitl_len_idx], origin=\"lower\", cmap=cmap_1, interpolation='nearest', aspect='auto')\n",
        "                    ax.imshow(pr_two[:,:unitl_len_idx], origin=\"lower\", cmap=cmap_2, interpolation='nearest', aspect='auto')\n",
        "                    ax.imshow(pr_three[:,:unitl_len_idx], origin=\"lower\", cmap=cmap_3, interpolation='nearest', aspect='auto')\n",
        "                    \n",
        "                    \n",
        "                        \n",
        "                    ax.set_title(\"ORIGINAL: red-v0, green-v1 ,blue-v2, orange-v3\")                \n",
        "                    ax.set_xlabel(f'Time ({\"beat\"}s/{12})')\n",
        "                    ax.set_ylabel('Piano key' )\n",
        "                    plt.show()\n",
        "\n",
        "                \n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:         \n",
        "                            counting +=1\n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                \n",
        "                \n",
        "                ########## COMPARE HERE EACH PREDICTION ALWAYS WITH ALL VOICES !!!   ##########\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                intermed_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                  intermed_dict = {'0': [], '1': [], '2': [], '3': [] } \n",
        "                  for j in range(len(total_predictions_dict[i])):\n",
        "                    for idx_tr, tr in enumerate([0,1,2,3]):\n",
        "                      if total_predictions_dict[i][j][0] == tr:\n",
        "                        intermed_dict[list(intermed_dict.keys())[idx_tr]].append(1)\n",
        "                  sum_0 = np.array([intermed_dict[\"0\"]]).sum()\n",
        "                  sum_1 = np.array([intermed_dict[\"1\"]]).sum()\n",
        "                  sum_2 = np.array([intermed_dict[\"2\"]]).sum()\n",
        "                  sum_3 = np.array([intermed_dict[\"3\"]]).sum()\n",
        "                  count_dict_2[list(count_dict_2.keys())[gt]] = intermed_dict\n",
        "\n",
        "\n",
        "                hit_list_0 = [np.array([count_dict_2[\"0\"][\"0\"]]).sum(),np.array([count_dict_2[\"0\"][\"1\"]]).sum(),np.array([count_dict_2[\"0\"][\"2\"]]).sum(),np.array([count_dict_2[\"0\"][\"3\"]]).sum() ]\n",
        "                hit_list_1 = [np.array([count_dict_2[\"1\"][\"0\"]]).sum(),np.array([count_dict_2[\"1\"][\"1\"]]).sum(),np.array([count_dict_2[\"1\"][\"2\"]]).sum(),np.array([count_dict_2[\"1\"][\"3\"]]).sum() ]\n",
        "                hit_list_2 = [np.array([count_dict_2[\"2\"][\"0\"]]).sum(),np.array([count_dict_2[\"2\"][\"1\"]]).sum(),np.array([count_dict_2[\"2\"][\"2\"]]).sum(),np.array([count_dict_2[\"2\"][\"3\"]]).sum() ]\n",
        "                hit_list_3 = [np.array([count_dict_2[\"3\"][\"0\"]]).sum(),np.array([count_dict_2[\"3\"][\"1\"]]).sum(),np.array([count_dict_2[\"3\"][\"2\"]]).sum(),np.array([count_dict_2[\"3\"][\"3\"]]).sum() ]  \n",
        "\n",
        "                hit_list_temp_0 = hit_list_0.copy()\n",
        "                hit_list_temp_1 = hit_list_1.copy()\n",
        "                hit_list_temp_2 = hit_list_2.copy()\n",
        "                hit_list_temp_3 = hit_list_3.copy()\n",
        "                \n",
        "                old_max_hist_list = [hit_list_0, hit_list_1, hit_list_2, hit_list_3]\n",
        "\n",
        "                for i in range(4):\n",
        "                    max_0 = np.max(hit_list_temp_0)\n",
        "                    max_idx_0 = np.argmax(hit_list_temp_0)\n",
        "                    max_1 = np.max(hit_list_temp_1)\n",
        "                    max_idx_1 = np.argmax(hit_list_temp_1)\n",
        "                    max_2 = np.max(hit_list_temp_2)\n",
        "                    max_idx_2 = np.argmax(hit_list_temp_2)\n",
        "                    max_3 = np.max(hit_list_temp_3)\n",
        "                    max_idx_3 = np.argmax(hit_list_temp_3)\n",
        "\n",
        "                    max_list = [max_0, max_1, max_2, max_3]\n",
        "                    max_idx_list = [max_idx_0, max_idx_1, max_idx_2, max_idx_3]\n",
        "                    max_hit_list = [hit_list_temp_0, hit_list_temp_1, hit_list_temp_2, hit_list_temp_3]\n",
        "\n",
        "                    sorted_idx_max_list = np.argsort(max_list)\n",
        "\n",
        "                    max_value_max_list = max_list[sorted_idx_max_list[-1]]\n",
        "                    max_value_max_idx_list = sorted_idx_max_list[-1]\n",
        "\n",
        "                    biggest_list = old_max_hist_list[max_value_max_idx_list]\n",
        "                    max_prediction_idx = biggest_list.index(max_value_max_list)\n",
        "\n",
        "\n",
        "                    biggest_list_new = max_hit_list[max_value_max_idx_list]\n",
        "                    max_prediction_idx_new = biggest_list_new.index(max_value_max_list)\n",
        "\n",
        "                    max_pred_acc = max_value_max_list/len(total_predictions_dict[list(total_predictions_dict.keys())[max_value_max_idx_list]])\n",
        "                    print(\"maximal overlap between gt voice {} and pred {}, ACC {}:\".format(max_value_max_idx_list, max_prediction_idx, max_pred_acc))\n",
        "\n",
        "                    hit_list_temp_0.pop(max_prediction_idx_new)\n",
        "                    hit_list_temp_1.pop(max_prediction_idx_new)\n",
        "                    hit_list_temp_2.pop(max_prediction_idx_new)\n",
        "                    hit_list_temp_3.pop(max_prediction_idx_new)\n",
        "\n",
        "                    acc_score_dict_new[list(acc_score_dict_new.keys())[max_value_max_idx_list]].append(max_pred_acc)\n",
        "\n",
        "\n",
        "                \n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "\n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "\n",
        "            if nbr_voices == 2:\n",
        "                file_name = file_name[0]\n",
        "                filename_part = os.path.join(path_parts, \"part_file\" + file_name + \".mid\")\n",
        "                part = partitura.load_score_midi(filename_part)\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                \n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                \n",
        "                list_of_note_arrays_2 = [note_array_0,note_array_1]\n",
        "\n",
        "                \n",
        "                ground_truth_label_list_2 = [0,1]              \n",
        "                total_predictions_dict_2 = {'0': [], '1': [] }\n",
        "                total_truth_dict_2 = {'0': [], '1': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays_2):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        #prediction, prediction_clf = model.predict(voices[:,:,:,-1], lens, monophonic)\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)   \n",
        "                        label = ground_truth_label_list_2[el_note_arr]\n",
        "                        #print(\"prediction number of voices:\",prediction_clf+1)\n",
        "\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]   \n",
        "\n",
        "                        if end_first <= unitl_len_idx:\n",
        "                          pitch_first = pitch_list[i]\n",
        "                          pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                          truth_list = [label for i in range(len(pred_list_first))]\n",
        "              \n",
        "                          result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                          # do majority vote if not all predictions are for same voice\n",
        "                          if result == False:\n",
        "                              major, major_idx = torch.mode(pred_list_first,0)\n",
        "                              major = major.numpy().tolist()\n",
        "                              pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        total_predictions_dict_2[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict_2[str(label)].append(truth_list)\n",
        "\n",
        "                        if len(pred_list_first)==0:\n",
        "                          print(\"ATTENTION:\", len(pred_list_first),str(label) )\n",
        "                          print(\"index:\", idx)\n",
        "                          print(\"------------------------------------------------------------------------\")\n",
        "\n",
        "                if print_predictions == True:\n",
        "                    #################### print pr ####################\n",
        "                    fig_size_1 = (10, 5)\n",
        "                    prediction_0 = np.where(prediction ==0, 1, 0)\n",
        "                    prediction_1 = np.where(prediction ==1, 1, 0)\n",
        "                    prediction_2 = np.where(prediction ==2, 1, 0)\n",
        "                    prediction_3 = np.where(prediction ==3, 1, 0)\n",
        "\n",
        "                    ####### print predicte pr #######                  \n",
        "                    colors_0 = 'black red'.split()\n",
        "                    colors_1 = 'none green'.split()  \n",
        "                    colors_2 = 'none blue'.split() \n",
        "                    colors_3 = 'none orange'.split()                \n",
        "                    cmap_0 = matplotlib.colors.ListedColormap(colors_0, name='colors', N=None)        \n",
        "                    cmap_1 = matplotlib.colors.ListedColormap(colors_1, name='colors', N=None)\n",
        "                    cmap_2 = matplotlib.colors.ListedColormap(colors_2, name='colors', N=None)\n",
        "                    cmap_3 = matplotlib.colors.ListedColormap(colors_3, name='colors', N=None)\n",
        "\n",
        "                    fig, ax = plt.subplots(1, figsize=fig_size_1)\n",
        "\n",
        "                    ax.imshow(prediction_0.T, origin=\"lower\", cmap=cmap_0, interpolation='nearest', aspect='auto') \n",
        "                    ax.imshow(prediction_1.T, origin=\"lower\", cmap=cmap_1, interpolation='nearest', aspect='auto')   \n",
        "                    ax.imshow(prediction_2.T, origin=\"lower\", cmap=cmap_2, interpolation='nearest', aspect='auto')\n",
        "                    ax.imshow(prediction_3.T, origin=\"lower\", cmap=cmap_3, interpolation='nearest', aspect='auto')\n",
        "                    ax.set_title(\"PREDICTED: 2 piece red-v0, green-v1, blue-v2, orange-v3\")                \n",
        "                    ax.set_xlabel(f'Time ({\"beat\"}s/{12})')\n",
        "                    ax.set_ylabel('Piano key')\n",
        "                    plt.show()\n",
        "\n",
        "\n",
        "                    ####### print original pr #######\n",
        "                    onset_beat = partitura.utils.ensure_notearray(part)['onset_beat'][-1]\n",
        "                    duration_beat = partitura.utils.ensure_notearray(part)['duration_beat'][-1]\n",
        "                    beat_all = onset_beat + duration_beat\n",
        "                    pr_zero = partitura.utils.compute_pianoroll(part_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "                    pr_zero = pr_zero.toarray()\n",
        "                    pr_one = partitura.utils.compute_pianoroll(part_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "                    pr_one = pr_one.toarray()\n",
        "\n",
        "                    fig, ax = plt.subplots(1, figsize=fig_size_1)\n",
        "                    ax.imshow(pr_zero[:,:unitl_len_idx], origin=\"lower\", cmap=cmap_0, interpolation='nearest', aspect='auto')\n",
        "                    ax.imshow(pr_one[:,:unitl_len_idx], origin=\"lower\", cmap=cmap_1, interpolation='nearest', aspect='auto') \n",
        "                    ax.set_title(\"ORIGINAL: 2 piece red-v0, green-v1\")                \n",
        "                    ax.set_xlabel(f'Time ({\"beat\"}s/{12})')\n",
        "                    ax.set_ylabel('Piano key')\n",
        "                    plt.show()\n",
        "\n",
        "\n",
        "                ########## COMPARE HERE EACH PREDICTION ALWAYS WITH ALL VOICES !!!   ##########\n",
        "                count_dict_22 = {'0': [], '1': [] }\n",
        "                intermed_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict_2.keys()):\n",
        "                  intermed_dict = {'0': [], '1': [], '2': [], '3': [] } \n",
        "                  for j in range(len(total_predictions_dict_2[i])):\n",
        "                    for idx_tr, tr in enumerate([0,1,2,3]):\n",
        "                      if total_predictions_dict_2[i][j][0] == tr:\n",
        "                        intermed_dict[list(intermed_dict.keys())[idx_tr]].append(1)\n",
        "                  sum_0 = np.array([intermed_dict[\"0\"]]).sum()\n",
        "                  sum_1 = np.array([intermed_dict[\"1\"]]).sum()\n",
        "                  sum_2 = np.array([intermed_dict[\"2\"]]).sum()\n",
        "                  sum_3 = np.array([intermed_dict[\"3\"]]).sum()\n",
        "                  count_dict_22[list(count_dict_22.keys())[gt]] = intermed_dict\n",
        "\n",
        "                \"\"\"\n",
        "                hit_list_0 = [np.array([count_dict_22[\"0\"][\"0\"]]).sum(),np.array([count_dict_22[\"0\"][\"1\"]]).sum(),np.array([count_dict_22[\"0\"][\"2\"]]).sum(),np.array([count_dict_22[\"0\"][\"3\"]]).sum() ]\n",
        "                hit_list_1 = [np.array([count_dict_22[\"1\"][\"0\"]]).sum(),np.array([count_dict_22[\"1\"][\"1\"]]).sum(),np.array([count_dict_22[\"1\"][\"2\"]]).sum(),np.array([count_dict_22[\"1\"][\"3\"]]).sum() ]  \n",
        "                max_0 = np.max(hit_list_0)\n",
        "                max_idx_0 = np.argmax(hit_list_0)\n",
        "                max_1 = np.max(hit_list_1)\n",
        "                max_idx_1 = np.argmax(hit_list_1)\n",
        "\n",
        "                if max_0 >= max_1:\n",
        "                    acc_0_2 = max_0/len(total_predictions_dict_2[\"0\"])\n",
        "                    print(\"maximal overlap voice 0 and :\",max_idx_0)\n",
        "                    if max_idx_0 == max_idx_1:\n",
        "                        acc_1_2 = np.argsort(hit_list_1)[-2]/len(total_predictions_dict_2[\"1\"])\n",
        "                        print(\"maximal overlap voice 1 and :\",np.argsort(hit_list_1)[-2])\n",
        "                    else:\n",
        "                        acc_1_2 = max_1/len(total_predictions_dict_2[\"1\"])\n",
        "                        print(\"maximal overlap voice 1 and :\",max_idx_1)\n",
        "                    \n",
        "                if max_1 > max_0:\n",
        "                    if max_idx_0 == max_idx_1:\n",
        "                        acc_0_2 = np.argsort(hit_list_0)[-2]/len(total_predictions_dict_2[\"0\"])\n",
        "                        print(\"maximal overlap voice 0 and :\",np.argsort(hit_list_0)[-2])\n",
        "                    else:\n",
        "                        acc_0_2 = max_0/len(total_predictions_dict_2[\"0\"])\n",
        "                        print(\"maximal overlap voice 0 and :\",max_idx_0)\n",
        "                    \n",
        "                    acc_1_2 = max_1/len(total_predictions_dict_2[\"1\"])\n",
        "                    print(\"maximal overlap voice 1 and :\",max_idx_1)\n",
        "\n",
        "\n",
        "                print(\"acc_0_2, sample {}:\".format(idx),acc_0_2)\n",
        "                print(\"acc_1_2, sample {}:\".format(idx),acc_1_2)\n",
        "\n",
        "                acc_score_dict_two[\"0\"].append(acc_0_2)\n",
        "                acc_score_dict_two[\"1\"].append(acc_1_2)\n",
        "                \"\"\"\n",
        "                hit_list_0 = [np.array([count_dict_22[\"0\"][\"0\"]]).sum(),np.array([count_dict_22[\"0\"][\"1\"]]).sum(),np.array([count_dict_22[\"0\"][\"2\"]]).sum(),np.array([count_dict_22[\"0\"][\"3\"]]).sum() ]\n",
        "                hit_list_1 = [np.array([count_dict_22[\"1\"][\"0\"]]).sum(),np.array([count_dict_22[\"1\"][\"1\"]]).sum(),np.array([count_dict_22[\"1\"][\"2\"]]).sum(),np.array([count_dict_22[\"1\"][\"3\"]]).sum() ]  \n",
        "                \n",
        "                hit_list_temp_0 = hit_list_0.copy()\n",
        "                hit_list_temp_1 = hit_list_1.copy()\n",
        "\n",
        "                old_max_hist_list = [hit_list_0, hit_list_1]\n",
        "\n",
        "\n",
        "                #print(\"hit_list_0\",hit_list_0)\n",
        "                #print(\"hit_list_1\",hit_list_1)\n",
        "\n",
        "                for i in range(2):\n",
        "                    max_0 = np.max(hit_list_temp_0)\n",
        "                    max_idx_0 = np.argmax(hit_list_temp_0)\n",
        "                    max_1 = np.max(hit_list_temp_1)\n",
        "                    max_idx_1 = np.argmax(hit_list_temp_1)\n",
        "\n",
        "                    max_list = [max_0, max_1]\n",
        "                    max_idx_list = [max_idx_0, max_idx_1]\n",
        "                    max_hit_list = [hit_list_temp_0, hit_list_temp_1]\n",
        "                    sorted_idx_max_list = np.argsort(max_list)\n",
        "\n",
        "                    max_value_max_list = max_list[sorted_idx_max_list[-1]]\n",
        "                    max_value_max_idx_list = sorted_idx_max_list[-1]\n",
        "\n",
        "                    biggest_list = old_max_hist_list[max_value_max_idx_list]\n",
        "                    max_prediction_idx = biggest_list.index(max_value_max_list)\n",
        "\n",
        "                    biggest_list_new = max_hit_list[max_value_max_idx_list]\n",
        "                    max_prediction_idx_new = biggest_list_new.index(max_value_max_list)\n",
        "\n",
        "                    max_pred_acc = max_value_max_list/len(total_predictions_dict_2[list(total_predictions_dict_2.keys())[max_value_max_idx_list]])\n",
        "                    print(\"maximal overlap between gt voice {} and pred {}, ACC {}:\".format(max_value_max_idx_list, max_prediction_idx, max_pred_acc))\n",
        "\n",
        "                    hit_list_temp_0.pop(max_prediction_idx_new)\n",
        "                    hit_list_temp_1.pop(max_prediction_idx_new)\n",
        "\n",
        "\n",
        "                    acc_score_dict_two[list(acc_score_dict_two.keys())[max_value_max_idx_list]].append(max_pred_acc)\n",
        "\n",
        "\n",
        "                \n",
        "    #return statistics.mean(acc_score_dict_new[\"0\"]), statistics.mean(acc_score_dict_new[\"1\"]), statistics.mean(acc_score_dict_new[\"2\"]), statistics.mean(acc_score_dict_new[\"3\"]), total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict_two[\"0\"]), statistics.mean(acc_score_dict_two[\"1\"]), statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])\n",
        "    return statistics.mean(acc_score_dict_new[\"0\"]), statistics.mean(acc_score_dict_new[\"1\"]), statistics.mean(acc_score_dict_new[\"2\"]), statistics.mean(acc_score_dict_new[\"3\"]), total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict_two[\"0\"]), statistics.mean(acc_score_dict_two[\"1\"])#, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "WqSAyLg6-3HF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluate"
      ],
      "metadata": {
        "id": "qOuYprRaz0uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#acc_0_new, acc_1_new, acc_2_new, acc_3_new, dict_pred , dict_gt, acc_0_2, acc_1_2, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_polyphonic_separate(model,val_dataloader,part_dic=file_names_part,print_predictions=True)\n",
        "acc_0_new, acc_1_new, acc_2_new, acc_3_new, dict_pred , dict_gt, acc_0_2, acc_1_2 = evaluate_accuracy_polyphonic_separate(model,val_dataloader,part_dic=file_names_part,print_predictions=True)\n",
        "\n",
        "print(\"acc_0_2:\",acc_0_2)\n",
        "print(\"acc_1_2:\",acc_1_2)\n",
        "#print(\"acc_0:\",acc_0)\n",
        "#print(\"acc_1:\",acc_1)\n",
        "#print(\"acc_2:\",acc_2)\n",
        "#print(\"acc_3:\",acc_3)\n",
        "print(\"acc_0_new:\",acc_0_new)\n",
        "print(\"acc_1_new:\",acc_1_new)\n",
        "print(\"acc_2_new:\",acc_2_new)\n",
        "print(\"acc_3_new:\",acc_3_new)"
      ],
      "metadata": {
        "id": "Ku1I666FYFQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy HMM"
      ],
      "metadata": {
        "id": "5SnKMmSczusv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## calculate mc leod pr helper fct"
      ],
      "metadata": {
        "id": "QaBIQSrrz7EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mcleod_pr (file_name, sentences,nbr_voices):\n",
        "    path = \"AI-MA_project/polyphonic_new\"\n",
        "    fullname = os.path.join(path, \"part_file\"+ file_name +\".mid\")\n",
        "\n",
        "\n",
        "    part = partitura.load_score_midi(fullname)\n",
        "    onset_beat = partitura.utils.ensure_notearray(part)['onset_beat'][-1]\n",
        "    duration_beat = partitura.utils.ensure_notearray(part)['duration_beat'][-1]\n",
        "    beat_all = onset_beat + duration_beat\n",
        "\n",
        "\n",
        "    ### apply hmms method ### \n",
        "    part_hmm = partitura.load_score_midi(fullname,part_voice_assign_mode=2)\n",
        "    voice_info = partitura.utils.note_array_from_part(part_hmm)[\"voice\"]\n",
        "    \n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(voice_info==1)\n",
        "    pos_1 = np.where(voice_info==2)\n",
        "    \n",
        "    if nbr_voices ==4:\n",
        "        pos_2 = np.where(voice_info==3)\n",
        "        pos_3 = np.where(voice_info==4)\n",
        "\n",
        "    note_array_all = partitura.utils.ensure_notearray(part_hmm)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    note_array_0= partitura.utils.ensure_notearray(part_hmm)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part_hmm)[pos_1]\n",
        "    \n",
        "    if nbr_voices==4:\n",
        "        note_array_2 = partitura.utils.ensure_notearray(part_hmm)[pos_2]\n",
        "        note_array_3 = partitura.utils.ensure_notearray(part_hmm)[pos_3]\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    onset_beat_0 = note_array_all['onset_beat'][-1]\n",
        "    duration_beat_0 = note_array_all['duration_beat'][-1]\n",
        "    beat_0 = onset_beat_0 + duration_beat_0\n",
        "    \n",
        "\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "    pr_zero = pr_zero.toarray()[:,:max_len_load]\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "    pr_one = pr_one.toarray()[:,:max_len_load]\n",
        "\n",
        "\n",
        "    if nbr_voices==4:\n",
        "\n",
        "        pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "        pr_two = pr_two.toarray()[:,:max_len_load]\n",
        "\n",
        "        pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "        pr_three = pr_three.toarray()[:,:max_len_load]\n",
        "    else:\n",
        "        pr_two = np.zeros(pr_one.shape)\n",
        "        pr_three = np.zeros(pr_one.shape)\n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0)) \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "4eSGsh2jzwkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluate_mc_leod_fugues\n"
      ],
      "metadata": {
        "id": "x_N5SRy50AL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mc_leod_fugues(train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [] }\n",
        "    acc_score_dict_four = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "        if  idx <34 and idx not in [0,1,3,4,9,10,11,12,13,14,16,19,20,21,22,23,24,26,27,31,32,33,34]:\n",
        "\n",
        "                file_name = file_name[0]\n",
        "                filename_part = os.path.join(path_parts, \"part_file\" + file_name + \".mid\")\n",
        "                part = partitura.load_score_midi(filename_part)\n",
        "\n",
        "                #file_name = file_name[0]\n",
        "                #part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                \n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                \n",
        "                list_of_note_arrays = [note_array_0,note_array_1]\n",
        "\n",
        "\n",
        "                if len(part) == 4:\n",
        "                    part_2 = part[2]\n",
        "                    note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "           \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "          \n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################                   \n",
        "                    prediction = calculate_mcleod_pr(file_name,voices[:,:,:,-1],nbr_voices) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]                     ### check if an why this is sometimes empty\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                        if len(pred_list_first)==0:\n",
        "                          print(\"ATTENTION:\", len(pred_list_first),str(label) )\n",
        "                          print(\"index:\", idx)\n",
        "                          print(\"------------------------------------------------------------------------\")\n",
        "                          break\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                if len(list_of_note_arrays)==2:\n",
        "                        acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                        acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                        \n",
        "                        print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                        print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                        \n",
        "                        acc_score_dict[\"0\"].append(acc_0)\n",
        "                        acc_score_dict[\"1\"].append(acc_1)\n",
        "                \n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                        acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                        print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                        acc_score_dict_four[\"0\"].append(acc_0)\n",
        "\n",
        "                        acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                        print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                        acc_score_dict_four[\"1\"].append(acc_1)\n",
        "\n",
        "                        acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                        print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                        acc_score_dict_four[\"2\"].append(acc_2)\n",
        "\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict_four[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict_four[\"0\"]), statistics.mean(acc_score_dict_four[\"1\"]),statistics.mean(acc_score_dict_four[\"2\"]),statistics.mean(acc_score_dict_four[\"3\"])   "
      ],
      "metadata": {
        "id": "9__7RhLMz_SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evaluate\n"
      ],
      "metadata": {
        "id": "GIS9GiEA0FNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_pred , acc_score_dict, acc_0 , acc_1, acc_0_four, acc_1_four, acc_2_four, acc_3_four = evaluate_mc_leod_fugues(val_dataloader,part_dic=file_names_part,F1=False)\n",
        "print(\"accuracy two voice pieces:\",acc_0 , acc_1)\n",
        "print(\"accuracy four voice pieces:\",acc_0_four, acc_1_four, acc_2_four, acc_3_four)"
      ],
      "metadata": {
        "id": "v23DHoaV0G05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy Chew"
      ],
      "metadata": {
        "id": "Gka9nSXbz4Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zuBjfU5Fz5gI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}