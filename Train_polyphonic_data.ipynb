{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_polyphonic_data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNEQl4rXkEVgpGVre2FALHA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Train_polyphonic_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "TGpH9G_kAwmy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "COo0mYvKAMmc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install partitura\n",
        "! pip install git+https://github.com/CPJKU/partitura.git@develop\n",
        "import partitura\n",
        "import statistics\n",
        "\n",
        "\n",
        "\n",
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataloader - Set the dataset"
      ],
      "metadata": {
        "id": "de_kI138A7ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DATA = \"AI-MA_project/pr_polyphonic\"\n",
        "\n",
        "batch_size = 1 \n",
        "workers = 0"
      ],
      "metadata": {
        "id": "GZhTj8KDA8Yf"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse"
      ],
      "metadata": {
        "id": "VIEk3O1r0PAZ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MusicDataset_polyphonic(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "        piece_lengths = [\"2_voice\",\"4_voice\"]\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for v_len_idx in piece_lengths:\n",
        "            if v_len_idx == \"4_voice\":\n",
        "                for iLabel in range(len(labels)):\n",
        "                    if iLabel == 4:   \n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, v_len_idx , self.labels[iLabel], \"*.pkl\")))       \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                                loaded_obj = pickle.load(f)  \n",
        "\n",
        "                                voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "\n",
        "                                len_list.append(len(loaded_obj.T))\n",
        "                                file_names_list.append(name[-7:-4])\n",
        "                                nbr_voices_list.append(4)                        \n",
        "\n",
        "                        self.pr_dict[self.labels[iLabel]] += voice_files\n",
        "                        self.pr_dict[\"length\"] +=  len_list\n",
        "                        self.pr_dict[\"nbr_voices\"] +=  nbr_voices_list\n",
        "                        self.pr_dict[\"name\"] += file_names_list\n",
        "                   \n",
        "                    else:\n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA,v_len_idx, self.labels[iLabel], \"*.pkl\"))) \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: \n",
        "                                  loaded_obj = pickle.load(f)     \n",
        "                                  voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "                        self.pr_dict[self.labels[iLabel]] += voice_files\n",
        "\n",
        "\n",
        "            if v_len_idx == \"2_voice\":\n",
        "                for iLabel in range(len(labels)):\n",
        "                    if iLabel == 4:   \n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, v_len_idx , self.labels[iLabel], \"*.pkl\")))   \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f:\n",
        "                                loaded_obj = pickle.load(f)  \n",
        "                                voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "                                len_list.append(len(loaded_obj.T))\n",
        "                                file_names_list.append(name[-7:-4])\n",
        "                                nbr_voices_list.append(2)\n",
        "                              \n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files\n",
        "                        self.pr_dict[\"length\"] = len_list\n",
        "                        self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                        self.pr_dict[\"name\"] = file_names_list\n",
        "    \n",
        "                    else:\n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA,v_len_idx, self.labels[iLabel], \"*.pkl\"))) \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: \n",
        "                                  loaded_obj = pickle.load(f)   \n",
        "                                  voice_files.append(sparse.csr_matrix(loaded_obj))  \n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files\n",
        "                    \n",
        "    def __len__(self):\n",
        "        file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, \"2_voice\" , self.labels[4], \"*.pkl\")))\n",
        "        file_names_4 = sorted(glob.glob(os.path.join(PATH_TO_DATA, \"4_voice\" , self.labels[4], \"*.pkl\")))\n",
        "        return len(file_names_2) + len(file_names_4) \n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        for key,value in self.pr_dict.items():\n",
        "          v0 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_0\"][idx]).T)\n",
        "          v1 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_1\"][idx]).T)\n",
        "          \n",
        "          if self.pr_dict[\"nbr_voices\"][idx] == 4:       ### -168 WORKS ONLY IN THIS CASE BC 168 SAMPLES OF LEN(2) ARE LOADED FIRST\n",
        "              v2 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_2\"][idx-168]).T)\n",
        "              v3 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_3\"][idx-168]).T)\n",
        "          if self.pr_dict[\"nbr_voices\"][idx] == 2:\n",
        "              v2 = torch.zeros(v1.shape)\n",
        "              v3 = torch.zeros(v1.shape)\n",
        "          \n",
        "          v_all = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_all\"][idx]).T)\n",
        "          length = self.pr_dict[\"length\"][idx]\n",
        "          nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "          file_name = self.pr_dict[\"name\"][idx]\n",
        "          voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "        return (voices, length, nbr_voices, file_name)"
      ],
      "metadata": {
        "id": "RR_q4F28BE1t"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "dataset = MusicDataset_polyphonic(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 1:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices,all_voices.shape)\n",
        "\"\"\"\n",
        "\n",
        "### try sparse matrix - convert when using to matrix - in loader only sparse matrix\n",
        "## plot the size for pieces ->  (polyphonic pieces longer / modern pieces)"
      ],
      "metadata": {
        "id": "HFMMT5ydBeDO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "bf65f3ac-9d8c-493a-8477-3efcc3894e42"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndataset = MusicDataset_polyphonic(PATH_TO_DATA)\\nloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\\n\\nfor i, sample_batched in enumerate(loader):\\n    if i == 1:\\n        all_voices, length, nbr_voices, file_name = sample_batched\\n        print(file_name[0],nbr_voices,all_voices.shape)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define U-Net"
      ],
      "metadata": {
        "id": "fGlQ8KsIkl2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "wD23kpffkfKy"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Network\n"
      ],
      "metadata": {
        "id": "GXc03KGJkfwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        #self.loss = nn.CrossEntropyLoss(reduction=\"sum\")                       # use weight parameters maybe take 1/88   \n",
        "        #weight_tensor = torch.tensor([1,1,1,4],dtype=torch.double).to(device)\n",
        "        #self.loss = nn.CrossEntropyLoss(weight=weight_tensor,reduction='mean',ignore_index=99)     \n",
        "\n",
        "    \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                      ### squeeze output here before returning                                       \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        #print(\"voices[:,:,:,-1]\", voices[:,:,:,-1].shape,voices.shape, nbr_voices)\n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()    ### v3 is automatically tensor containing only 0 bc dataloader does this for pieces with 3 voices\n",
        "        \n",
        "        weight_v0 = v0.sum()\n",
        "        weight_v1 = v1.sum()\n",
        "        weight_v2 = v2.sum()\n",
        "        weight_v3 = v3.sum()\n",
        "\n",
        "        stack_tensors_gt = torch.stack([v0, v1, v2, v3], dim=0)\n",
        "        stack_gt = torch.argmax(stack_tensors_gt,axis=0)\n",
        "\n",
        "\n",
        "        weight_tensor = torch.stack([weight_v0/weight_v0,weight_v0/weight_v0,weight_v0/weight_v0,5*weight_v0/weight_v0])\n",
        "        criterion_weighted = nn.CrossEntropyLoss(weight=weight_tensor,reduction='mean',ignore_index=99)        \n",
        "        stack_pred = torch.stack([score_0, score_1, score_2, score_3], dim=0).softmax(dim=0)[None, :]\n",
        "        mask_pred = torch.squeeze(voices[:,:,:,-1])== 0\n",
        "        stack_gt[mask_pred] = 99\n",
        "\n",
        "        loss = criterion_weighted(stack_pred, stack_gt[None,:])\n",
        "\n",
        "        #print(\"loss:\",loss)\n",
        "        return loss   \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "\n",
        "        else:\n",
        "            sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "            prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            \n",
        "            mask_pred = np.squeeze(sentences)== 0\n",
        "            v_pred_argm[mask_pred] = -1\n",
        "\n",
        "            return v_pred_argm        "
      ],
      "metadata": {
        "id": "nO4PtE0kknDd"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#monophonic = True\n",
        "#his = start_experiment(1, 0.0001, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, \"CNN\", learn_all)\n"
      ],
      "metadata": {
        "id": "MDeP7tqWmJGb"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Training Process"
      ],
      "metadata": {
        "id": "QyIAxKS-k4m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "UrErJAevk2Sg"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tryining Loop"
      ],
      "metadata": {
        "id": "u2R-p3BnlZdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### versuch hier mal nur für 4 voices zu trainieren\n",
        "\n",
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "                \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            if nbr_voices == 4:\n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "        print(\"Epoch: {}, Train Loss: {}\".format(i_epoch,train_loss)) \n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "50qWCIMNk7ZL"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "metadata": {
        "id": "-NUWBUvqk_No"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter choice"
      ],
      "metadata": {
        "id": "-bFYYY_6l5hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MusicNetwork\n",
        "epochs = 10\n",
        "lr = 0.001 # was 0.001    #0.00001\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "metadata": {
        "id": "qlFVa23Ql5yz"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the experiment"
      ],
      "metadata": {
        "id": "Bq16m1D9lXHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO3ZxWbzlBis",
        "outputId": "f3ba6a34-d670-4043-fb88-44c0ecdddcb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataloader 199 val_dataloader 36\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Training on device: cpu\n",
            "monophonic set to: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load trained model"
      ],
      "metadata": {
        "id": "UoqAYzPnnj1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader "
      ],
      "metadata": {
        "id": "dSlYUsRUmALB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture  \n",
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch10.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "yFBgUAmNntEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dic with key:filename, val: part_obj  "
      ],
      "metadata": {
        "id": "iIXfywU8n3f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "Hu2g0G8jozVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "\n",
        "path_parts = \"AI-MA_project/polyphonic\"\n",
        "part_dic = {}\n",
        "\n",
        "#### create a list with all filenames in the right order ####\n",
        "file_names_part = []\n",
        "\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    print(filename)\n",
        "    file_names_part.append(filename[3:7])\n",
        "print(file_names_part)\n",
        "\n",
        "#### create a list with all part objects in the right order ####\n",
        "part_list = []\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    fullname = os.path.join(path_parts, filename)\n",
        "    part = partitura.load_score_midi(fullname)\n",
        "    part_list.append(part)\n",
        "print(part_list)\n",
        "\n",
        "#### create a dict with keys:filenames , values: part object ####\n",
        "for i in range(len(file_names_part)):\n",
        "    part_dic[file_names_part[i]] = part_list[i]\n",
        "\n",
        "print(part_dic.keys(),part_dic.values())"
      ],
      "metadata": {
        "id": "k29NpOgtn3np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yvS8dZJaoxkq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}