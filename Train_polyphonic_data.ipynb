{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_polyphonic_data.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Train_polyphonic_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "TGpH9G_kAwmy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "COo0mYvKAMmc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install partitura\n",
        "! pip install git+https://github.com/CPJKU/partitura.git@develop\n",
        "import partitura\n",
        "import statistics\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "\n",
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataloader - Set the dataset"
      ],
      "metadata": {
        "id": "de_kI138A7ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DATA = \"AI-MA_project/pr_polyphonic\"\n",
        "\n",
        "batch_size = 1 \n",
        "workers = 0"
      ],
      "metadata": {
        "id": "GZhTj8KDA8Yf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse"
      ],
      "metadata": {
        "id": "VIEk3O1r0PAZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_load = 3000\n",
        "\n",
        "class MusicDataset_polyphonic(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "        piece_lengths = [\"2_voice\",\"4_voice\"]\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        counter_files_v2 = 0\n",
        "\n",
        "        for v_len_idx in piece_lengths:\n",
        "            if v_len_idx == \"4_voice\":\n",
        "                for iLabel in range(len(labels)):\n",
        "                    if iLabel == 4:   \n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, v_len_idx , self.labels[iLabel], \"*.pkl\")))       \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                                loaded_obj = pickle.load(f)  \n",
        "\n",
        "                                #print(\"loaded_obj before:\",loaded_obj.shape)\n",
        "\n",
        "                                pitch, t_len = loaded_obj.shape\n",
        "                                if t_len > max_len_load:\n",
        "                                  loaded_obj = loaded_obj[:,:max_len_load]\n",
        "                            \n",
        "\n",
        "                                #print(\"loaded_obj after:\",loaded_obj.shape)\n",
        "\n",
        "                                voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "\n",
        "                                len_list.append(len(loaded_obj.T))\n",
        "                                file_names_list.append(name[-7:-4])\n",
        "                                nbr_voices_list.append(4)                        \n",
        "\n",
        "                        self.pr_dict[self.labels[iLabel]] += voice_files\n",
        "                        self.pr_dict[\"length\"] +=  len_list\n",
        "                        self.pr_dict[\"nbr_voices\"] +=  nbr_voices_list\n",
        "                        self.pr_dict[\"name\"] += file_names_list\n",
        "                   \n",
        "                    else:\n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA,v_len_idx, self.labels[iLabel], \"*.pkl\"))) \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: \n",
        "                                  loaded_obj = pickle.load(f)\n",
        "\n",
        "                                  pitch, t_len = loaded_obj.shape\n",
        "                                  if t_len > max_len_load:\n",
        "                                    loaded_obj = loaded_obj[:,:max_len_load]\n",
        "\n",
        "\n",
        "                                  voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "                        self.pr_dict[self.labels[iLabel]] += voice_files\n",
        "\n",
        "\n",
        "            if v_len_idx == \"2_voice\":\n",
        "              #if counter_files_v2 < len(sorted(glob.glob(os.path.join(PATH_TO_DATA, \"4_voice\" , self.labels[4], \"*.pkl\")))):     ############### load as many 2 voice pieces as 4 voice pieces    ##########\n",
        "                #counter_files_v2 +=1\n",
        "                for iLabel in range(len(labels)):\n",
        "                    if iLabel == 4:   \n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, v_len_idx , self.labels[iLabel], \"*.pkl\")))   \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f:\n",
        "                                loaded_obj = pickle.load(f)  \n",
        "\n",
        "                                pitch, t_len = loaded_obj.shape\n",
        "                                if t_len > max_len_load:\n",
        "                                  loaded_obj = loaded_obj[:,:max_len_load]\n",
        "\n",
        "                                voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "                                len_list.append(len(loaded_obj.T))\n",
        "                                file_names_list.append(name[-7:-4])\n",
        "                                nbr_voices_list.append(2)\n",
        "                              \n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files\n",
        "                        self.pr_dict[\"length\"] = len_list\n",
        "                        self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                        self.pr_dict[\"name\"] = file_names_list\n",
        "    \n",
        "                    else:\n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA,v_len_idx, self.labels[iLabel], \"*.pkl\"))) \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: \n",
        "                                  loaded_obj = pickle.load(f)   \n",
        "\n",
        "                                  pitch, t_len = loaded_obj.shape\n",
        "                                  if t_len > max_len_load:\n",
        "                                    loaded_obj = loaded_obj[:,:max_len_load]\n",
        "\n",
        "                                  voice_files.append(sparse.csr_matrix(loaded_obj))  \n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files\n",
        "                    \n",
        "    def __len__(self):\n",
        "        file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, \"2_voice\" , self.labels[4], \"*.pkl\")))\n",
        "        file_names_4 = sorted(glob.glob(os.path.join(PATH_TO_DATA, \"4_voice\" , self.labels[4], \"*.pkl\")))\n",
        "        return len(file_names_2) + len(file_names_4) \n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        for key,value in self.pr_dict.items():\n",
        "          v0 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_0\"][idx]).T)\n",
        "          v1 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_1\"][idx]).T)\n",
        "          \n",
        "          if self.pr_dict[\"nbr_voices\"][idx] == 4:       ### -168 WORKS ONLY IN THIS CASE BC 168 SAMPLES OF LEN(2) ARE LOADED FIRST\n",
        "              v2 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_2\"][idx-168]).T)\n",
        "              v3 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_3\"][idx-168]).T)\n",
        "          if self.pr_dict[\"nbr_voices\"][idx] == 2:\n",
        "              v2 = torch.zeros(v1.shape)\n",
        "              v3 = torch.zeros(v1.shape)\n",
        "          \n",
        "          v_all = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_all\"][idx]).T)\n",
        "          length = self.pr_dict[\"length\"][idx]\n",
        "          nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "          file_name = self.pr_dict[\"name\"][idx]\n",
        "          voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "        return (voices, length, nbr_voices, file_name)"
      ],
      "metadata": {
        "id": "RR_q4F28BE1t"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots\n"
      ],
      "metadata": {
        "id": "_iGHmvhijcEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "dataset = MusicDataset_polyphonic(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 168:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices,all_voices.shape)\n",
        "        break\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HFMMT5ydBeDO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "432dec3e-370d-4db6-9fe8-404289ab6a41"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndataset = MusicDataset_polyphonic(PATH_TO_DATA)\\nloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\\n\\nfor i, sample_batched in enumerate(loader):\\n    if i == 168:\\n        all_voices, length, nbr_voices, file_name = sample_batched\\n        print(file_name[0],nbr_voices,all_voices.shape)\\n        break\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 168:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices,all_voices.shape)\n",
        "        break\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "m6jxjpWJcD16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "79f22b42-b4f9-42f2-d38f-65d172125404"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor i, sample_batched in enumerate(loader):\\n    if i == 168:\\n        all_voices, length, nbr_voices, file_name = sample_batched\\n        print(file_name[0],nbr_voices,all_voices.shape)\\n        break\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "pianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\n",
        "pianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\n",
        "pianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\n",
        "pianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\n",
        "pianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\n",
        "\n",
        "colors_0 = 'none red'.split()\n",
        "colors_1 = 'none green'.split()\n",
        "colors_2 = 'none blue'.split()\n",
        "colors_3 = 'black orange'.split()\n",
        "\n",
        "cmap_0 = matplotlib.colors.ListedColormap(colors_0, name='colors', N=None)\n",
        "cmap_1 = matplotlib.colors.ListedColormap(colors_1, name='colors', N=None)\n",
        "cmap_2 = matplotlib.colors.ListedColormap(colors_2, name='colors', N=None)\n",
        "cmap_3 = matplotlib.colors.ListedColormap(colors_3, name='colors', N=None)\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(20, 10))\n",
        "\n",
        "ax.imshow(pianoroll_3[3000:4000,:], origin=\"lower\", cmap=cmap_3, interpolation='nearest', aspect='auto')\n",
        "ax.imshow(pianoroll_2[3000:4000,:], origin=\"lower\", cmap=cmap_2, interpolation='nearest', aspect='auto')\n",
        "ax.imshow(pianoroll_1[3000:4000,:], origin=\"lower\", cmap=cmap_1, interpolation='nearest', aspect='auto')\n",
        "ax.imshow(pianoroll_0[3000:4000,:], origin=\"lower\", cmap=cmap_0, interpolation='nearest', aspect='auto')\n",
        "ax.set_xlabel(f'Time ({\"beat\"}s/{12})')\n",
        "ax.set_ylabel('Piano key')\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NOV7MCxhSud7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d568a0a1-45d3-4bf1-9dac-a164fa1b5b7d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\\npianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\\npianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\\npianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\\npianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\\n\\ncolors_0 = \\'none red\\'.split()\\ncolors_1 = \\'none green\\'.split()\\ncolors_2 = \\'none blue\\'.split()\\ncolors_3 = \\'black orange\\'.split()\\n\\ncmap_0 = matplotlib.colors.ListedColormap(colors_0, name=\\'colors\\', N=None)\\ncmap_1 = matplotlib.colors.ListedColormap(colors_1, name=\\'colors\\', N=None)\\ncmap_2 = matplotlib.colors.ListedColormap(colors_2, name=\\'colors\\', N=None)\\ncmap_3 = matplotlib.colors.ListedColormap(colors_3, name=\\'colors\\', N=None)\\n\\nfig, ax = plt.subplots(1, figsize=(20, 10))\\n\\nax.imshow(pianoroll_3[3000:4000,:], origin=\"lower\", cmap=cmap_3, interpolation=\\'nearest\\', aspect=\\'auto\\')\\nax.imshow(pianoroll_2[3000:4000,:], origin=\"lower\", cmap=cmap_2, interpolation=\\'nearest\\', aspect=\\'auto\\')\\nax.imshow(pianoroll_1[3000:4000,:], origin=\"lower\", cmap=cmap_1, interpolation=\\'nearest\\', aspect=\\'auto\\')\\nax.imshow(pianoroll_0[3000:4000,:], origin=\"lower\", cmap=cmap_0, interpolation=\\'nearest\\', aspect=\\'auto\\')\\nax.set_xlabel(f\\'Time ({\"beat\"}s/{12})\\')\\nax.set_ylabel(\\'Piano key\\')\\nplt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define U-Net"
      ],
      "metadata": {
        "id": "fGlQ8KsIkl2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        y = x\n",
        "        concat_layers = concat_layers[::-1]\n",
        "\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x, y"
      ],
      "metadata": {
        "id": "wD23kpffkfKy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Network\n"
      ],
      "metadata": {
        "id": "GXc03KGJkfwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "\n",
        "        self.input_unet = int(np.floor(max_len_load/2/2/2/2)*1024*5)\n",
        "        self.clf_head = nn.Linear(self.input_unet, 4)                         ################   4 bc max nbr voices ==4 in our dataset\n",
        "        self.clf_head_gap = nn.Linear(1024, 128)\n",
        "        self.clf_head_gap_2 = nn.Linear(128, 4)\n",
        "        self.indicator = 0\n",
        "\n",
        "    \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out, down_state = self.cnn(sentences)\n",
        "\n",
        "          self.down_state = down_state\n",
        "\n",
        "          ############### try implementing GAP ###############\n",
        "          #print(\"down_state\",down_state.shape)\n",
        "          gap = nn.AdaptiveAvgPool2d((1,1))\n",
        "          input = down_state\n",
        "          gap_reduced = gap(input).squeeze()\n",
        "          #print(gap_reduced.shape)\n",
        "          out_clf_head = self.clf_head_gap_2(self.clf_head_gap(gap_reduced))\n",
        "          #print(\"self.clf_head_gap(gap_reduced)\",self.clf_head_gap(gap_reduced))\n",
        "          ######################################################\n",
        "\n",
        "          return out, out_clf_head  \n",
        "          \"\"\"\n",
        "          if self.input_unet == torch.flatten(down_state).shape[0]:\n",
        "            self.indicator = 1\n",
        "            out_clf_head = self.clf_head(torch.flatten(down_state))\n",
        "            return out, out_clf_head                \n",
        "          else:      ### in case input is smaller than 5000\n",
        "            self.indicator = 0\n",
        "            return out, None        \n",
        "          \"\"\"                              \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb, out_clf_head = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "        #if self.indicator ==0:\n",
        "        #  loss_clf = 0\n",
        "        #else:\n",
        "        clf_score = out_clf_head.softmax(dim=0)\n",
        "        clf_label = nbr_voices[0]-1\n",
        "        define_clf_loss = nn.CrossEntropyLoss(reduction='mean')\n",
        "        loss_clf = define_clf_loss(clf_score, clf_label)\n",
        "\n",
        "        print(\"loss_clf\",loss_clf)\n",
        " \n",
        "\n",
        "        weight_v0 = voices[:,:,:,0].squeeze().sum()\n",
        "        weight_v1 = voices[:,:,:,1].squeeze().sum()\n",
        "        weight_v2 = voices[:,:,:,2].squeeze().sum()\n",
        "        weight_v3 = voices[:,:,:,3].squeeze().sum()\n",
        "\n",
        "        stack_tensors_gt = torch.swapaxes(torch.swapaxes(voices[:,:,:,:4].squeeze(), 0, 2), 1,2)\n",
        "        stack_gt = torch.argmax(stack_tensors_gt,axis=0)\n",
        "\n",
        "        \n",
        "        weight_tensor = torch.stack([weight_v0/weight_v0,weight_v0/weight_v0,weight_v0/weight_v0,weight_v0/weight_v0])\n",
        "        criterion_weighted = nn.CrossEntropyLoss(weight=weight_tensor,reduction='mean',ignore_index=99)  \n",
        "        stack_pred = torch.stack([score_0, score_1, score_2, score_3], dim=0).softmax(dim=0)[None, :]\n",
        "        mask_pred = torch.squeeze(voices[:,:,:,-1])== 0\n",
        "        stack_gt[mask_pred] = 99\n",
        "        loss = criterion_weighted(stack_pred, stack_gt[None,:])\n",
        "\n",
        "        if torch.any(torch.isnan(stack_pred)) or torch.any(torch.isinf(stack_pred)) :\n",
        "          print('invalid input detected at iteration ')\n",
        "\n",
        "        return loss + loss_clf\n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "        scores_comb, out_clf_head = self.compute_outputs(sentences, sentences_len)\n",
        "        sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "        prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "        \n",
        "        mask_pred = np.squeeze(sentences)== 0\n",
        "        v_pred_argm[mask_pred] = -1\n",
        "\n",
        "        ### only output if clf is not none ####       ####!!!!!!!!!NNEEEED TO TEST AGAIN !!!!\n",
        "        #if out_clf_head != None:\n",
        "        clf_score = out_clf_head.softmax(dim=0)\n",
        "        pred_clf = torch.argmax(clf_score,axis=0)              \n",
        "        #else:      \n",
        "        #  pred_clf = 0\n",
        "                \n",
        "        #clf_score = out_clf_head.softmax(dim=0)\n",
        "        #pred_clf = torch.argmax(clf_score,axis=0)\n",
        "\n",
        "        return v_pred_argm, pred_clf   "
      ],
      "metadata": {
        "id": "nO4PtE0kknDd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "monophonic = True\n",
        "his = start_experiment(1, 0.001, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, \"CNN\", learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MDeP7tqWmJGb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0fdd98f-3052-44f3-d618-02eefd9b27c0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmonophonic = True\\nhis = start_experiment(1, 0.001, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, \"CNN\", learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Training Process"
      ],
      "metadata": {
        "id": "QyIAxKS-k4m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "UrErJAevk2Sg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tryining Loop"
      ],
      "metadata": {
        "id": "u2R-p3BnlZdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### versuch hier mal nur für 4 voices zu trainieren\n",
        "\n",
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        loss_sum_clf = 0\n",
        "        model.train()\n",
        "                \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            #if nbr_voices == 4:\n",
        "            if idx < 80:\n",
        "              voices = voices.to(device).float()\n",
        "              optimizer.zero_grad()\n",
        "              loss = model.forward(voices, lens, nbr_voices)\n",
        "              print(\"sample {} / {}\".format(idx,len(train_dataloader)))           \n",
        "              print(\"loss:\",loss)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              loss_sum += loss.item()  \n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "        print(\"Epoch: {}, Train Loss: {}\".format(i_epoch,train_loss)) \n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "50qWCIMNk7ZL"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "metadata": {
        "id": "-NUWBUvqk_No"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter choice"
      ],
      "metadata": {
        "id": "-bFYYY_6l5hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MusicNetwork\n",
        "epochs = 1\n",
        "lr = 0.001 # was 0.001    #0.00001\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\" # with clf head: \"CNN_CLF\"\n",
        "\n",
        "monophonic = True"
      ],
      "metadata": {
        "id": "qlFVa23Ql5yz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the experiment"
      ],
      "metadata": {
        "id": "Bq16m1D9lXHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "metadata": {
        "id": "kO3ZxWbzlBis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e40b49-3316-47c1-f819-db3645483d5b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataloader 260 val_dataloader 46\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Training on device: cpu\n",
            "loss_clf tensor(1.3876, grad_fn=<NllLossBackward0>)\n",
            "sample 0 / 260\n",
            "loss: tensor(2.7568, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.5730, grad_fn=<NllLossBackward0>)\n",
            "sample 1 / 260\n",
            "loss: tensor(2.9309, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.5118, grad_fn=<NllLossBackward0>)\n",
            "sample 2 / 260\n",
            "loss: tensor(2.8507, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.3419, grad_fn=<NllLossBackward0>)\n",
            "sample 3 / 260\n",
            "loss: tensor(2.6070, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.1361, grad_fn=<NllLossBackward0>)\n",
            "sample 4 / 260\n",
            "loss: tensor(2.3968, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.9757, grad_fn=<NllLossBackward0>)\n",
            "sample 5 / 260\n",
            "loss: tensor(2.2268, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.5663, grad_fn=<NllLossBackward0>)\n",
            "sample 6 / 260\n",
            "loss: tensor(2.9637, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.8350, grad_fn=<NllLossBackward0>)\n",
            "sample 7 / 260\n",
            "loss: tensor(2.0343, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.8048, grad_fn=<NllLossBackward0>)\n",
            "sample 8 / 260\n",
            "loss: tensor(2.0210, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7838, grad_fn=<NllLossBackward0>)\n",
            "sample 9 / 260\n",
            "loss: tensor(1.9613, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7072, grad_fn=<NllLossBackward0>)\n",
            "sample 10 / 260\n",
            "loss: tensor(3.0790, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7199, grad_fn=<NllLossBackward0>)\n",
            "sample 11 / 260\n",
            "loss: tensor(3.0660, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7235, grad_fn=<NllLossBackward0>)\n",
            "sample 12 / 260\n",
            "loss: tensor(3.1004, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7521, grad_fn=<NllLossBackward0>)\n",
            "sample 13 / 260\n",
            "loss: tensor(1.8944, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7318, grad_fn=<NllLossBackward0>)\n",
            "sample 14 / 260\n",
            "loss: tensor(3.0955, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7337, grad_fn=<NllLossBackward0>)\n",
            "sample 15 / 260\n",
            "loss: tensor(3.0807, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7481, grad_fn=<NllLossBackward0>)\n",
            "sample 16 / 260\n",
            "loss: tensor(1.8029, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7362, grad_fn=<NllLossBackward0>)\n",
            "sample 17 / 260\n",
            "loss: tensor(3.0587, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7476, grad_fn=<NllLossBackward0>)\n",
            "sample 18 / 260\n",
            "loss: tensor(1.8169, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7471, grad_fn=<NllLossBackward0>)\n",
            "sample 19 / 260\n",
            "loss: tensor(1.7668, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7472, grad_fn=<NllLossBackward0>)\n",
            "sample 20 / 260\n",
            "loss: tensor(1.7646, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7381, grad_fn=<NllLossBackward0>)\n",
            "sample 21 / 260\n",
            "loss: tensor(3.1224, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7390, grad_fn=<NllLossBackward0>)\n",
            "sample 22 / 260\n",
            "loss: tensor(3.1053, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7395, grad_fn=<NllLossBackward0>)\n",
            "sample 23 / 260\n",
            "loss: tensor(3.0082, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7458, grad_fn=<NllLossBackward0>)\n",
            "sample 24 / 260\n",
            "loss: tensor(1.8329, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7465, grad_fn=<NllLossBackward0>)\n",
            "sample 25 / 260\n",
            "loss: tensor(1.7628, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7457, grad_fn=<NllLossBackward0>)\n",
            "sample 26 / 260\n",
            "loss: tensor(1.6617, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7399, grad_fn=<NllLossBackward0>)\n",
            "sample 27 / 260\n",
            "loss: tensor(3.1055, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7397, grad_fn=<NllLossBackward0>)\n",
            "sample 28 / 260\n",
            "loss: tensor(3.0409, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7457, grad_fn=<NllLossBackward0>)\n",
            "sample 29 / 260\n",
            "loss: tensor(1.7267, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7399, grad_fn=<NllLossBackward0>)\n",
            "sample 30 / 260\n",
            "loss: tensor(3.0726, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7394, grad_fn=<NllLossBackward0>)\n",
            "sample 31 / 260\n",
            "loss: tensor(3.0226, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7397, grad_fn=<NllLossBackward0>)\n",
            "sample 32 / 260\n",
            "loss: tensor(3.0760, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7471, grad_fn=<NllLossBackward0>)\n",
            "sample 33 / 260\n",
            "loss: tensor(1.6763, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7396, grad_fn=<NllLossBackward0>)\n",
            "sample 34 / 260\n",
            "loss: tensor(2.9763, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7464, grad_fn=<NllLossBackward0>)\n",
            "sample 35 / 260\n",
            "loss: tensor(1.5841, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7390, grad_fn=<NllLossBackward0>)\n",
            "sample 36 / 260\n",
            "loss: tensor(3.0574, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7385, grad_fn=<NllLossBackward0>)\n",
            "sample 37 / 260\n",
            "loss: tensor(2.9917, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7464, grad_fn=<NllLossBackward0>)\n",
            "sample 38 / 260\n",
            "loss: tensor(1.8864, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7384, grad_fn=<NllLossBackward0>)\n",
            "sample 39 / 260\n",
            "loss: tensor(2.9746, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7466, grad_fn=<NllLossBackward0>)\n",
            "sample 40 / 260\n",
            "loss: tensor(1.6980, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7466, grad_fn=<NllLossBackward0>)\n",
            "sample 41 / 260\n",
            "loss: tensor(1.7060, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7376, grad_fn=<NllLossBackward0>)\n",
            "sample 42 / 260\n",
            "loss: tensor(3.0035, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7471, grad_fn=<NllLossBackward0>)\n",
            "sample 43 / 260\n",
            "loss: tensor(1.6453, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7370, grad_fn=<NllLossBackward0>)\n",
            "sample 44 / 260\n",
            "loss: tensor(3.0150, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7472, grad_fn=<NllLossBackward0>)\n",
            "sample 45 / 260\n",
            "loss: tensor(1.6270, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7366, grad_fn=<NllLossBackward0>)\n",
            "sample 46 / 260\n",
            "loss: tensor(3.0201, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7362, grad_fn=<NllLossBackward0>)\n",
            "sample 47 / 260\n",
            "loss: tensor(3.0020, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7478, grad_fn=<NllLossBackward0>)\n",
            "sample 48 / 260\n",
            "loss: tensor(1.6069, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7492, grad_fn=<NllLossBackward0>)\n",
            "sample 49 / 260\n",
            "loss: tensor(1.7069, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7484, grad_fn=<NllLossBackward0>)\n",
            "sample 50 / 260\n",
            "loss: tensor(1.5531, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7345, grad_fn=<NllLossBackward0>)\n",
            "sample 51 / 260\n",
            "loss: tensor(2.9350, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7338, grad_fn=<NllLossBackward0>)\n",
            "sample 52 / 260\n",
            "loss: tensor(3.0215, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7495, grad_fn=<NllLossBackward0>)\n",
            "sample 53 / 260\n",
            "loss: tensor(1.6700, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7495, grad_fn=<NllLossBackward0>)\n",
            "sample 54 / 260\n",
            "loss: tensor(1.6514, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7505, grad_fn=<NllLossBackward0>)\n",
            "sample 55 / 260\n",
            "loss: tensor(1.6628, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7325, grad_fn=<NllLossBackward0>)\n",
            "sample 56 / 260\n",
            "loss: tensor(3.0748, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7508, grad_fn=<NllLossBackward0>)\n",
            "sample 57 / 260\n",
            "loss: tensor(1.6414, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7329, grad_fn=<NllLossBackward0>)\n",
            "sample 58 / 260\n",
            "loss: tensor(3.1131, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7310, grad_fn=<NllLossBackward0>)\n",
            "sample 59 / 260\n",
            "loss: tensor(3.0831, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7512, grad_fn=<NllLossBackward0>)\n",
            "sample 60 / 260\n",
            "loss: tensor(1.6186, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7269, grad_fn=<NllLossBackward0>)\n",
            "sample 61 / 260\n",
            "loss: tensor(3.0534, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7258, grad_fn=<NllLossBackward0>)\n",
            "sample 62 / 260\n",
            "loss: tensor(3.0103, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7233, grad_fn=<NllLossBackward0>)\n",
            "sample 63 / 260\n",
            "loss: tensor(3.0719, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7176, grad_fn=<NllLossBackward0>)\n",
            "sample 64 / 260\n",
            "loss: tensor(3.0279, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7603, grad_fn=<NllLossBackward0>)\n",
            "sample 65 / 260\n",
            "loss: tensor(1.7533, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7649, grad_fn=<NllLossBackward0>)\n",
            "sample 66 / 260\n",
            "loss: tensor(1.7664, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7655, grad_fn=<NllLossBackward0>)\n",
            "sample 67 / 260\n",
            "loss: tensor(1.7383, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.7010, grad_fn=<NllLossBackward0>)\n",
            "sample 68 / 260\n",
            "loss: tensor(2.9444, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.6940, grad_fn=<NllLossBackward0>)\n",
            "sample 69 / 260\n",
            "loss: tensor(3.0112, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7816, grad_fn=<NllLossBackward0>)\n",
            "sample 70 / 260\n",
            "loss: tensor(1.6265, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7820, grad_fn=<NllLossBackward0>)\n",
            "sample 71 / 260\n",
            "loss: tensor(1.6834, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7868, grad_fn=<NllLossBackward0>)\n",
            "sample 72 / 260\n",
            "loss: tensor(1.6610, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.6833, grad_fn=<NllLossBackward0>)\n",
            "sample 73 / 260\n",
            "loss: tensor(3.1276, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.6750, grad_fn=<NllLossBackward0>)\n",
            "sample 74 / 260\n",
            "loss: tensor(3.0880, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7872, grad_fn=<NllLossBackward0>)\n",
            "sample 75 / 260\n",
            "loss: tensor(1.9293, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.6561, grad_fn=<NllLossBackward0>)\n",
            "sample 76 / 260\n",
            "loss: tensor(3.1412, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.8095, grad_fn=<NllLossBackward0>)\n",
            "sample 77 / 260\n",
            "loss: tensor(1.6489, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.8051, grad_fn=<NllLossBackward0>)\n",
            "sample 78 / 260\n",
            "loss: tensor(1.6664, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.8071, grad_fn=<NllLossBackward0>)\n",
            "sample 79 / 260\n",
            "loss: tensor(1.7244, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7984, grad_fn=<NllLossBackward0>)\n",
            "sample 80 / 260\n",
            "loss: tensor(1.5978, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.6595, grad_fn=<NllLossBackward0>)\n",
            "sample 81 / 260\n",
            "loss: tensor(3.0419, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.6663, grad_fn=<NllLossBackward0>)\n",
            "sample 82 / 260\n",
            "loss: tensor(3.0047, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7936, grad_fn=<NllLossBackward0>)\n",
            "sample 83 / 260\n",
            "loss: tensor(1.7170, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.7917, grad_fn=<NllLossBackward0>)\n",
            "sample 84 / 260\n",
            "loss: tensor(1.8740, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.6671, grad_fn=<NllLossBackward0>)\n",
            "sample 85 / 260\n",
            "loss: tensor(3.0249, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.6592, grad_fn=<NllLossBackward0>)\n",
            "sample 86 / 260\n",
            "loss: tensor(2.9439, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.6483, grad_fn=<NllLossBackward0>)\n",
            "sample 87 / 260\n",
            "loss: tensor(2.9176, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.8133, grad_fn=<NllLossBackward0>)\n",
            "sample 88 / 260\n",
            "loss: tensor(1.6582, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.8188, grad_fn=<NllLossBackward0>)\n",
            "sample 89 / 260\n",
            "loss: tensor(1.7499, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.6057, grad_fn=<NllLossBackward0>)\n",
            "sample 90 / 260\n",
            "loss: tensor(2.9173, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.8489, grad_fn=<NllLossBackward0>)\n",
            "sample 91 / 260\n",
            "loss: tensor(1.7627, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.5640, grad_fn=<NllLossBackward0>)\n",
            "sample 92 / 260\n",
            "loss: tensor(2.8866, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.5352, grad_fn=<NllLossBackward0>)\n",
            "sample 93 / 260\n",
            "loss: tensor(2.9135, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.4865, grad_fn=<NllLossBackward0>)\n",
            "sample 94 / 260\n",
            "loss: tensor(2.8429, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.3961, grad_fn=<NllLossBackward0>)\n",
            "sample 95 / 260\n",
            "loss: tensor(2.7512, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.3081, grad_fn=<NllLossBackward0>)\n",
            "sample 96 / 260\n",
            "loss: tensor(2.6953, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.1629, grad_fn=<NllLossBackward0>)\n",
            "sample 97 / 260\n",
            "loss: tensor(1.9580, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.2189, grad_fn=<NllLossBackward0>)\n",
            "sample 98 / 260\n",
            "loss: tensor(2.0456, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.2358, grad_fn=<NllLossBackward0>)\n",
            "sample 99 / 260\n",
            "loss: tensor(2.1410, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.1240, grad_fn=<NllLossBackward0>)\n",
            "sample 100 / 260\n",
            "loss: tensor(2.4731, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.2399, grad_fn=<NllLossBackward0>)\n",
            "sample 101 / 260\n",
            "loss: tensor(2.1856, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.2337, grad_fn=<NllLossBackward0>)\n",
            "sample 102 / 260\n",
            "loss: tensor(2.1430, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.2126, grad_fn=<NllLossBackward0>)\n",
            "sample 103 / 260\n",
            "loss: tensor(2.1180, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.1785, grad_fn=<NllLossBackward0>)\n",
            "sample 104 / 260\n",
            "loss: tensor(2.4601, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.1718, grad_fn=<NllLossBackward0>)\n",
            "sample 105 / 260\n",
            "loss: tensor(2.0077, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.2114, grad_fn=<NllLossBackward0>)\n",
            "sample 106 / 260\n",
            "loss: tensor(2.4141, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.1446, grad_fn=<NllLossBackward0>)\n",
            "sample 107 / 260\n",
            "loss: tensor(1.9969, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.1291, grad_fn=<NllLossBackward0>)\n",
            "sample 108 / 260\n",
            "loss: tensor(2.0705, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.1028, grad_fn=<NllLossBackward0>)\n",
            "sample 109 / 260\n",
            "loss: tensor(1.9889, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.0714, grad_fn=<NllLossBackward0>)\n",
            "sample 110 / 260\n",
            "loss: tensor(1.9415, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.0336, grad_fn=<NllLossBackward0>)\n",
            "sample 111 / 260\n",
            "loss: tensor(1.8216, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.9813, grad_fn=<NllLossBackward0>)\n",
            "sample 112 / 260\n",
            "loss: tensor(1.8280, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.4552, grad_fn=<NllLossBackward0>)\n",
            "sample 113 / 260\n",
            "loss: tensor(2.7458, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.4885, grad_fn=<NllLossBackward0>)\n",
            "sample 114 / 260\n",
            "loss: tensor(2.8207, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.5006, grad_fn=<NllLossBackward0>)\n",
            "sample 115 / 260\n",
            "loss: tensor(2.8619, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.4984, grad_fn=<NllLossBackward0>)\n",
            "sample 116 / 260\n",
            "loss: tensor(2.7953, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.4856, grad_fn=<NllLossBackward0>)\n",
            "sample 117 / 260\n",
            "loss: tensor(2.8083, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(1.4582, grad_fn=<NllLossBackward0>)\n",
            "sample 118 / 260\n",
            "loss: tensor(2.8504, grad_fn=<AddBackward0>)\n",
            "loss_clf tensor(0.9523, grad_fn=<NllLossBackward0>)\n",
            "sample 119 / 260\n",
            "loss: tensor(1.8547, grad_fn=<AddBackward0>)\n",
            "Epoch: 1, Train Loss: 1.1050728027637189\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load trained model"
      ],
      "metadata": {
        "id": "UoqAYzPnnj1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "#train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "metadata": {
        "id": "qTx4tg3mLisA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader"
      ],
      "metadata": {
        "id": "dSlYUsRUmALB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2198e4a-369c-49ca-dc02-a113652cca12"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f99f1b83990>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_dataloader)"
      ],
      "metadata": {
        "id": "ZRtGR86j1DYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60f0e427-7912-4bc6-f9ff-8e5d1823fd07"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture  \n",
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch1.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "yFBgUAmNntEo"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dic with key:filename, val: part_obj  "
      ],
      "metadata": {
        "id": "iIXfywU8n3f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "\n",
        "path_parts = \"AI-MA_project/polyphonic_new\"\n",
        "part_dic = {}\n",
        "\n",
        "#### create a list with all filenames in the right order ####\n",
        "file_names_part = []\n",
        "\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    file_names_part.append(filename[9:12])\n",
        "print(file_names_part)"
      ],
      "metadata": {
        "id": "k29NpOgtn3np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea41325-da7d-4ea5-d319-569a1fea89e2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['000', '001', '002', '003', '004', '005', '006', '007', '008', '009', '010', '011', '012', '013', '014', '015', '016', '018', '019', '020', '021', '022', '023', '024', '025', '026', '027', '028', '029', '030', '031', '032', '033', '034', '035', '036', '037', '039', '040', '041', '042', '043', '044', '045', '046', '047', '048', '049', '050', '051', '052', '053', '054', '055', '056', '057', '058', '059', '060', '061', '062', '063', '064', '065', '066', '067', '068', '069', '070', '071', '072', '073', '074', '075', '076', '077', '078', '079', '080', '081', '082', '083', '084', '085', '087', '088', '089', '090', '091', '092', '093', '094', '095', '096', '097', '098', '099', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute polyphonic accuracy"
      ],
      "metadata": {
        "id": "JO7EESFlO6rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy_polyphonic_separate(model, train_dataloader, part_dic,print_predictions=False):\n",
        "\n",
        "    unitl_len_idx = max_len_load\n",
        "\n",
        "    path_parts = \"AI-MA_project/polyphonic_new\"\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict_two = {'0': [], '1': [] }\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "        #print(\"nbr_voices:\",nbr_voices)\n",
        "        if idx not in [0, 3, 7, 8, 15, 21, 25, 32, 35]:   \n",
        "            print(\"idx:\",idx)\n",
        "            if nbr_voices == 4:\n",
        "                file_name = file_name[0]\n",
        "                filename_part = os.path.join(path_parts, \"part_file\" + file_name + \".mid\")\n",
        "                part = partitura.load_score_midi(filename_part)\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "                part_3 = part[3]\n",
        "                \n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)              \n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction, prediction_clf = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "                        print(\"prediction number of voices:\",prediction_clf+1)\n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]   \n",
        "\n",
        "                        if end_first <= unitl_len_idx:\n",
        "                          pitch_first = pitch_list[i]\n",
        "                          pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                          truth_list = [label for i in range(len(pred_list_first))]\n",
        "              \n",
        "                          result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                          # do majority vote if not all predictions are for same voice\n",
        "                          if result == False:\n",
        "                              major, major_idx = torch.mode(pred_list_first,0)\n",
        "                              major = major.numpy().tolist()\n",
        "                              pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "                        if len(pred_list_first)==0:\n",
        "                          print(\"ATTENTION:\", len(pred_list_first),str(label) )\n",
        "                          print(\"index:\", idx)\n",
        "                          print(\"------------------------------------------------------------------------\")\n",
        "\n",
        "                if print_predictions == True:\n",
        "                    #################### print pr ####################\n",
        "                    fig_size_1 = (10, 5)\n",
        "\n",
        "                    prediction_0 = np.where(prediction ==0, 1, 0)\n",
        "                    prediction_1 = np.where(prediction ==1, 1, 0)\n",
        "                    prediction_2 = np.where(prediction ==2, 1, 0)\n",
        "                    prediction_3 = np.where(prediction ==3, 1, 0)\n",
        "\n",
        "                    ####### print predicte pr #######                  \n",
        "                    colors_0 = 'black red'.split()\n",
        "                    colors_1 = 'none green'.split()  \n",
        "                    colors_2 = 'none blue'.split() \n",
        "                    colors_3 = 'none orange'.split()                \n",
        "                    cmap_0 = matplotlib.colors.ListedColormap(colors_0, name='colors', N=None)        \n",
        "                    cmap_1 = matplotlib.colors.ListedColormap(colors_1, name='colors', N=None)\n",
        "                    cmap_2 = matplotlib.colors.ListedColormap(colors_2, name='colors', N=None)\n",
        "                    cmap_3 = matplotlib.colors.ListedColormap(colors_3, name='colors', N=None)\n",
        "\n",
        "                    fig, ax = plt.subplots(1, figsize=fig_size_1)\n",
        "\n",
        "                    ax.imshow(prediction_0.T, origin=\"lower\", cmap=cmap_0, interpolation='nearest', aspect='auto') \n",
        "                    ax.imshow(prediction_1.T, origin=\"lower\", cmap=cmap_1, interpolation='nearest', aspect='auto')   \n",
        "                    ax.imshow(prediction_2.T, origin=\"lower\", cmap=cmap_2, interpolation='nearest', aspect='auto')\n",
        "                    ax.imshow(prediction_3.T, origin=\"lower\", cmap=cmap_3, interpolation='nearest', aspect='auto')\n",
        "\n",
        "                    ax.set_title(\"PREDICTED: red-v0, green-v1, blue-v2, orange-v3\")                \n",
        "                    ax.set_xlabel(f'Time ({\"beat\"}s/{12})')\n",
        "                    ax.set_ylabel('Piano key')\n",
        "                    plt.show()\n",
        "\n",
        "                    ####### print original pr #######\n",
        "                    onset_beat = partitura.utils.ensure_notearray(part)['onset_beat'][-1]\n",
        "                    duration_beat = partitura.utils.ensure_notearray(part)['duration_beat'][-1]\n",
        "                    beat_all = onset_beat + duration_beat\n",
        "                    pr_zero = partitura.utils.compute_pianoroll(part_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "                    pr_zero = pr_zero.toarray()\n",
        "                    pr_one = partitura.utils.compute_pianoroll(part_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "                    pr_one = pr_one.toarray()\n",
        "                    pr_two = partitura.utils.compute_pianoroll(part_2, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "                    pr_two = pr_two.toarray()\n",
        "                    pr_three = partitura.utils.compute_pianoroll(part_3, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "                    pr_three = pr_three.toarray()\n",
        "\n",
        "                    fig, ax = plt.subplots(1, figsize=fig_size_1)\n",
        "                    ax.imshow(pr_zero[:,:unitl_len_idx], origin=\"lower\", cmap=cmap_0, interpolation='nearest', aspect='auto')\n",
        "                    ax.imshow(pr_one[:,:unitl_len_idx], origin=\"lower\", cmap=cmap_1, interpolation='nearest', aspect='auto')\n",
        "                    ax.imshow(pr_two[:,:unitl_len_idx], origin=\"lower\", cmap=cmap_2, interpolation='nearest', aspect='auto')\n",
        "                    ax.imshow(pr_three[:,:unitl_len_idx], origin=\"lower\", cmap=cmap_3, interpolation='nearest', aspect='auto')\n",
        "                    \n",
        "                    \n",
        "                        \n",
        "                    ax.set_title(\"ORIGINAL: red-v0, green-v1 ,blue-v2, orange-v3\")                \n",
        "                    ax.set_xlabel(f'Time ({\"beat\"}s/{12})')\n",
        "                    ax.set_ylabel('Piano key' )\n",
        "                    plt.show()\n",
        "\n",
        "                \n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:         \n",
        "                            counting +=1\n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                \n",
        "                \n",
        "                ########## COMPARE HERE EACH PREDICTION ALWAYS WITH ALL VOICES !!!   ##########\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                intermed_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                  intermed_dict = {'0': [], '1': [], '2': [], '3': [] } \n",
        "                  for j in range(len(total_predictions_dict[i])):\n",
        "                    for idx_tr, tr in enumerate([0,1,2,3]):\n",
        "                      if total_predictions_dict[i][j][0] == tr:\n",
        "                        intermed_dict[list(intermed_dict.keys())[idx_tr]].append(1)\n",
        "                  sum_0 = np.array([intermed_dict[\"0\"]]).sum()\n",
        "                  sum_1 = np.array([intermed_dict[\"1\"]]).sum()\n",
        "                  sum_2 = np.array([intermed_dict[\"2\"]]).sum()\n",
        "                  sum_3 = np.array([intermed_dict[\"3\"]]).sum()\n",
        "                  count_dict_2[list(count_dict_2.keys())[gt]] = intermed_dict\n",
        "\n",
        "\n",
        "                hit_list_0 = [np.array([count_dict_2[\"0\"][\"0\"]]).sum(),np.array([count_dict_2[\"0\"][\"1\"]]).sum(),np.array([count_dict_2[\"0\"][\"2\"]]).sum(),np.array([count_dict_2[\"0\"][\"3\"]]).sum() ]\n",
        "                hit_list_1 = [np.array([count_dict_2[\"1\"][\"0\"]]).sum(),np.array([count_dict_2[\"1\"][\"1\"]]).sum(),np.array([count_dict_2[\"1\"][\"2\"]]).sum(),np.array([count_dict_2[\"1\"][\"3\"]]).sum() ]\n",
        "                hit_list_2 = [np.array([count_dict_2[\"2\"][\"0\"]]).sum(),np.array([count_dict_2[\"2\"][\"1\"]]).sum(),np.array([count_dict_2[\"2\"][\"2\"]]).sum(),np.array([count_dict_2[\"2\"][\"3\"]]).sum() ]\n",
        "                hit_list_3 = [np.array([count_dict_2[\"3\"][\"0\"]]).sum(),np.array([count_dict_2[\"3\"][\"1\"]]).sum(),np.array([count_dict_2[\"3\"][\"2\"]]).sum(),np.array([count_dict_2[\"3\"][\"3\"]]).sum() ]  \n",
        "\n",
        "                hit_list_temp_0 = hit_list_0.copy()\n",
        "                hit_list_temp_1 = hit_list_1.copy()\n",
        "                hit_list_temp_2 = hit_list_2.copy()\n",
        "                hit_list_temp_3 = hit_list_3.copy()\n",
        "                \n",
        "                old_max_hist_list = [hit_list_0, hit_list_1, hit_list_2, hit_list_3]\n",
        "\n",
        "                for i in range(4):\n",
        "                    max_0 = np.max(hit_list_temp_0)\n",
        "                    max_idx_0 = np.argmax(hit_list_temp_0)\n",
        "                    max_1 = np.max(hit_list_temp_1)\n",
        "                    max_idx_1 = np.argmax(hit_list_temp_1)\n",
        "                    max_2 = np.max(hit_list_temp_2)\n",
        "                    max_idx_2 = np.argmax(hit_list_temp_2)\n",
        "                    max_3 = np.max(hit_list_temp_3)\n",
        "                    max_idx_3 = np.argmax(hit_list_temp_3)\n",
        "\n",
        "                    max_list = [max_0, max_1, max_2, max_3]\n",
        "                    max_idx_list = [max_idx_0, max_idx_1, max_idx_2, max_idx_3]\n",
        "                    max_hit_list = [hit_list_temp_0, hit_list_temp_1, hit_list_temp_2, hit_list_temp_3]\n",
        "\n",
        "                    sorted_idx_max_list = np.argsort(max_list)\n",
        "\n",
        "                    max_value_max_list = max_list[sorted_idx_max_list[-1]]\n",
        "                    max_value_max_idx_list = sorted_idx_max_list[-1]\n",
        "\n",
        "                    biggest_list = old_max_hist_list[max_value_max_idx_list]\n",
        "                    max_prediction_idx = biggest_list.index(max_value_max_list)\n",
        "\n",
        "\n",
        "                    biggest_list_new = max_hit_list[max_value_max_idx_list]\n",
        "                    max_prediction_idx_new = biggest_list_new.index(max_value_max_list)\n",
        "\n",
        "                    max_pred_acc = max_value_max_list/len(total_predictions_dict[list(total_predictions_dict.keys())[max_value_max_idx_list]])\n",
        "                    print(\"maximal overlap between gt voice {} and pred {}, ACC {}:\".format(max_value_max_idx_list, max_prediction_idx, max_pred_acc))\n",
        "\n",
        "                    #print(\"hit_list_0\",hit_list_temp_0)\n",
        "                    #print(\"hit_list_1\",hit_list_temp_1)\n",
        "                    #print(\"hit_list_2\",hit_list_temp_2)\n",
        "                    #print(\"hit_list_3\",hit_list_temp_3)\n",
        "                    #print(\"len total:\",len(total_predictions_dict[\"0\"]),len(total_predictions_dict[\"1\"]),len(total_predictions_dict[\"2\"]),len(total_predictions_dict[\"3\"] ))\n",
        "\n",
        "                    hit_list_temp_0.pop(max_prediction_idx_new)\n",
        "                    hit_list_temp_1.pop(max_prediction_idx_new)\n",
        "                    hit_list_temp_2.pop(max_prediction_idx_new)\n",
        "                    hit_list_temp_3.pop(max_prediction_idx_new)\n",
        "\n",
        "\n",
        "                \n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "\n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "\n",
        "            if nbr_voices == 2:\n",
        "                file_name = file_name[0]\n",
        "                filename_part = os.path.join(path_parts, \"part_file\" + file_name + \".mid\")\n",
        "                part = partitura.load_score_midi(filename_part)\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                \n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                \n",
        "                list_of_note_arrays_2 = [note_array_0,note_array_1]\n",
        "\n",
        "                \n",
        "                ground_truth_label_list_2 = [0,1]              \n",
        "                total_predictions_dict_2 = {'0': [], '1': [] }\n",
        "                total_truth_dict_2 = {'0': [], '1': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays_2):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction, prediction_clf = model.predict(voices[:,:,:,-1], lens, monophonic)   \n",
        "                        label = ground_truth_label_list_2[el_note_arr]\n",
        "                        print(\"prediction number of voices:\",prediction_clf+1)\n",
        "\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]   \n",
        "\n",
        "                        if end_first <= unitl_len_idx:\n",
        "                          pitch_first = pitch_list[i]\n",
        "                          pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                          truth_list = [label for i in range(len(pred_list_first))]\n",
        "              \n",
        "                          result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                          # do majority vote if not all predictions are for same voice\n",
        "                          if result == False:\n",
        "                              major, major_idx = torch.mode(pred_list_first,0)\n",
        "                              major = major.numpy().tolist()\n",
        "                              pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        total_predictions_dict_2[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict_2[str(label)].append(truth_list)\n",
        "\n",
        "                        if len(pred_list_first)==0:\n",
        "                          print(\"ATTENTION:\", len(pred_list_first),str(label) )\n",
        "                          print(\"index:\", idx)\n",
        "                          print(\"------------------------------------------------------------------------\")\n",
        "\n",
        "                if print_predictions == True:\n",
        "                    #################### print pr ####################\n",
        "                    fig_size_1 = (10, 5)\n",
        "                    prediction_0 = np.where(prediction ==0, 1, 0)\n",
        "                    prediction_1 = np.where(prediction ==1, 1, 0)\n",
        "                    prediction_2 = np.where(prediction ==2, 1, 0)\n",
        "                    prediction_3 = np.where(prediction ==3, 1, 0)\n",
        "\n",
        "                    ####### print predicte pr #######                  \n",
        "                    colors_0 = 'black red'.split()\n",
        "                    colors_1 = 'none green'.split()  \n",
        "                    colors_2 = 'none blue'.split() \n",
        "                    colors_3 = 'none orange'.split()                \n",
        "                    cmap_0 = matplotlib.colors.ListedColormap(colors_0, name='colors', N=None)        \n",
        "                    cmap_1 = matplotlib.colors.ListedColormap(colors_1, name='colors', N=None)\n",
        "                    cmap_2 = matplotlib.colors.ListedColormap(colors_2, name='colors', N=None)\n",
        "                    cmap_3 = matplotlib.colors.ListedColormap(colors_3, name='colors', N=None)\n",
        "\n",
        "                    fig, ax = plt.subplots(1, figsize=fig_size_1)\n",
        "\n",
        "                    ax.imshow(prediction_0.T, origin=\"lower\", cmap=cmap_0, interpolation='nearest', aspect='auto') \n",
        "                    ax.imshow(prediction_1.T, origin=\"lower\", cmap=cmap_1, interpolation='nearest', aspect='auto')   \n",
        "                    ax.imshow(prediction_2.T, origin=\"lower\", cmap=cmap_2, interpolation='nearest', aspect='auto')\n",
        "                    ax.imshow(prediction_3.T, origin=\"lower\", cmap=cmap_3, interpolation='nearest', aspect='auto')\n",
        "                    ax.set_title(\"PREDICTED: 2 piece red-v0, green-v1, blue-v2, orange-v3\")                \n",
        "                    ax.set_xlabel(f'Time ({\"beat\"}s/{12})')\n",
        "                    ax.set_ylabel('Piano key')\n",
        "                    plt.show()\n",
        "\n",
        "\n",
        "                    ####### print original pr #######\n",
        "                    onset_beat = partitura.utils.ensure_notearray(part)['onset_beat'][-1]\n",
        "                    duration_beat = partitura.utils.ensure_notearray(part)['duration_beat'][-1]\n",
        "                    beat_all = onset_beat + duration_beat\n",
        "                    pr_zero = partitura.utils.compute_pianoroll(part_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "                    pr_zero = pr_zero.toarray()\n",
        "                    pr_one = partitura.utils.compute_pianoroll(part_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_all)\n",
        "                    pr_one = pr_one.toarray()\n",
        "\n",
        "                    fig, ax = plt.subplots(1, figsize=fig_size_1)\n",
        "                    ax.imshow(pr_zero[:,:unitl_len_idx], origin=\"lower\", cmap=cmap_0, interpolation='nearest', aspect='auto')\n",
        "                    ax.imshow(pr_one[:,:unitl_len_idx], origin=\"lower\", cmap=cmap_1, interpolation='nearest', aspect='auto') \n",
        "                    ax.set_title(\"ORIGINAL: 2 piece red-v0, green-v1\")                \n",
        "                    ax.set_xlabel(f'Time ({\"beat\"}s/{12})')\n",
        "                    ax.set_ylabel('Piano key')\n",
        "                    plt.show()\n",
        "\n",
        "\n",
        "                ########## COMPARE HERE EACH PREDICTION ALWAYS WITH ALL VOICES !!!   ##########\n",
        "                count_dict_22 = {'0': [], '1': [] }\n",
        "                intermed_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict_2.keys()):\n",
        "                  intermed_dict = {'0': [], '1': [], '2': [], '3': [] } \n",
        "                  for j in range(len(total_predictions_dict_2[i])):\n",
        "                    for idx_tr, tr in enumerate([0,1,2,3]):\n",
        "                      if total_predictions_dict_2[i][j][0] == tr:\n",
        "                        intermed_dict[list(intermed_dict.keys())[idx_tr]].append(1)\n",
        "                  sum_0 = np.array([intermed_dict[\"0\"]]).sum()\n",
        "                  sum_1 = np.array([intermed_dict[\"1\"]]).sum()\n",
        "                  sum_2 = np.array([intermed_dict[\"2\"]]).sum()\n",
        "                  sum_3 = np.array([intermed_dict[\"3\"]]).sum()\n",
        "                  count_dict_22[list(count_dict_22.keys())[gt]] = intermed_dict\n",
        "\n",
        "\n",
        "                hit_list_0 = [np.array([count_dict_22[\"0\"][\"0\"]]).sum(),np.array([count_dict_22[\"0\"][\"1\"]]).sum(),np.array([count_dict_22[\"0\"][\"2\"]]).sum(),np.array([count_dict_22[\"0\"][\"3\"]]).sum() ]\n",
        "                hit_list_1 = [np.array([count_dict_22[\"1\"][\"0\"]]).sum(),np.array([count_dict_22[\"1\"][\"1\"]]).sum(),np.array([count_dict_22[\"1\"][\"2\"]]).sum(),np.array([count_dict_22[\"1\"][\"3\"]]).sum() ]  \n",
        "                max_0 = np.max(hit_list_0)\n",
        "                max_idx_0 = np.argmax(hit_list_0)\n",
        "                max_1 = np.max(hit_list_1)\n",
        "                max_idx_1 = np.argmax(hit_list_1)\n",
        "\n",
        "                if max_0 >= max_1:\n",
        "                    acc_0_2 = max_0/len(total_predictions_dict_2[\"0\"])\n",
        "                    print(\"maximal overlap voice 0 and :\",max_idx_0)\n",
        "                    if max_idx_0 == max_idx_1:\n",
        "                        acc_1_2 = np.argsort(hit_list_1)[-2]/len(total_predictions_dict_2[\"1\"])\n",
        "                        print(\"maximal overlap voice 1 and :\",np.argsort(hit_list_1)[-2])\n",
        "                    else:\n",
        "                        acc_1_2 = max_1/len(total_predictions_dict_2[\"1\"])\n",
        "                        print(\"maximal overlap voice 1 and :\",max_idx_1)\n",
        "                    \n",
        "                if max_1 > max_0:\n",
        "                    if max_idx_0 == max_idx_1:\n",
        "                        acc_0_2 = np.argsort(hit_list_0)[-2]/len(total_predictions_dict_2[\"0\"])\n",
        "                        print(\"maximal overlap voice 0 and :\",np.argsort(hit_list_0)[-2])\n",
        "                    else:\n",
        "                        acc_0_2 = max_0/len(total_predictions_dict_2[\"0\"])\n",
        "                        print(\"maximal overlap voice 0 and :\",max_idx_0)\n",
        "                    \n",
        "                    acc_1_2 = max_1/len(total_predictions_dict_2[\"1\"])\n",
        "                    print(\"maximal overlap voice 1 and :\",max_idx_1)\n",
        "\n",
        "\n",
        "                print(\"acc_0_2, sample {}:\".format(idx),acc_0_2)\n",
        "                print(\"acc_1_2, sample {}:\".format(idx),acc_1_2)\n",
        "\n",
        "                acc_score_dict_two[\"0\"].append(acc_0_2)\n",
        "                acc_score_dict_two[\"1\"].append(acc_1_2)\n",
        "\n",
        "    return total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict_two[\"0\"]), statistics.mean(acc_score_dict_two[\"1\"]), statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "WqSAyLg6-3HF"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_pred , dict_gt, acc_0_2, acc_1_2, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_polyphonic_separate(model,val_dataloader,part_dic=file_names_part,print_predictions=True)\n",
        "\n",
        "print(\"acc_0_2:\",acc_0_2)\n",
        "print(\"acc_1_2:\",acc_1_2)\n",
        "print(\"acc_0:\",acc_0)\n",
        "print(\"acc_1:\",acc_1)\n",
        "print(\"acc_2:\",acc_2)\n",
        "print(\"acc_3:\",acc_3)"
      ],
      "metadata": {
        "id": "Ku1I666FYFQk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "517041ba-fc63-4c5a-b1cb-42605f6eb7e9"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=239\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=2 note=61 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=2 note=63 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:419: UserWarning: pitch spelling\n",
            "  warnings.warn(\"pitch spelling\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: voice estimation\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: create_part\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add time sigs and measures\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: tie notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: find tuplets\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: done create_part\n",
            "  part_name=part_names.get(part_nr, None),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction number of voices: tensor(2)\n",
            "prediction number of voices: tensor(2)\n",
            "prediction number of voices: tensor(2)\n",
            "prediction number of voices: tensor(2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maximal overlap between gt voice 0 and pred 0, ACC 0.972568578553616:\n",
            "hit_list_0 [390, 11, 0.0, 0.0]\n",
            "hit_list_1 [80, 267, 0.0, 0.0]\n",
            "hit_list_2 [27, 262, 0.0, 0.0]\n",
            "hit_list_3 [0.0, 344, 0.0, 0.0]\n",
            "len total: 401 347 289 344\n",
            "maximal overlap between gt voice 3 and pred 1, ACC 1.0:\n",
            "hit_list_0 [11, 0.0, 0.0]\n",
            "hit_list_1 [267, 0.0, 0.0]\n",
            "hit_list_2 [262, 0.0, 0.0]\n",
            "hit_list_3 [344, 0.0, 0.0]\n",
            "len total: 401 347 289 344\n",
            "maximal overlap between gt voice 3 and pred 0, ACC 0.0:\n",
            "hit_list_0 [0.0, 0.0]\n",
            "hit_list_1 [0.0, 0.0]\n",
            "hit_list_2 [0.0, 0.0]\n",
            "hit_list_3 [0.0, 0.0]\n",
            "len total: 401 347 289 344\n",
            "maximal overlap between gt voice 3 and pred 0, ACC 0.0:\n",
            "hit_list_0 [0.0]\n",
            "hit_list_1 [0.0]\n",
            "hit_list_2 [0.0]\n",
            "hit_list_3 [0.0]\n",
            "len total: 401 347 289 344\n",
            "acc 0, sample 1: 0.972568578553616\n",
            "acc 1, sample 1: 0.7694524495677233\n",
            "acc 2, sample 1: 0.0\n",
            "acc 3, sample 1: 0.0\n",
            "idx: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=299\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction number of voices: tensor(2)\n",
            "prediction number of voices: tensor(2)\n",
            "prediction number of voices: tensor(2)\n",
            "prediction number of voices: tensor(2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maximal overlap between gt voice 0 and pred 0, ACC 0.9701754385964912:\n",
            "hit_list_0 [553, 17, 0.0, 0.0]\n",
            "hit_list_1 [195, 375, 0.0, 0.0]\n",
            "hit_list_2 [21, 550, 0.0, 0.0]\n",
            "hit_list_3 [0.0, 381, 0.0, 0.0]\n",
            "len total: 570 570 571 381\n",
            "maximal overlap between gt voice 2 and pred 1, ACC 0.9632224168126094:\n",
            "hit_list_0 [17, 0.0, 0.0]\n",
            "hit_list_1 [375, 0.0, 0.0]\n",
            "hit_list_2 [550, 0.0, 0.0]\n",
            "hit_list_3 [381, 0.0, 0.0]\n",
            "len total: 570 570 571 381\n",
            "maximal overlap between gt voice 3 and pred 0, ACC 0.0:\n",
            "hit_list_0 [0.0, 0.0]\n",
            "hit_list_1 [0.0, 0.0]\n",
            "hit_list_2 [0.0, 0.0]\n",
            "hit_list_3 [0.0, 0.0]\n",
            "len total: 570 570 571 381\n",
            "maximal overlap between gt voice 3 and pred 0, ACC 0.0:\n",
            "hit_list_0 [0.0]\n",
            "hit_list_1 [0.0]\n",
            "hit_list_2 [0.0]\n",
            "hit_list_3 [0.0]\n",
            "len total: 570 570 571 381\n",
            "acc 0, sample 2: 0.9701754385964912\n",
            "acc 1, sample 2: 0.6578947368421053\n",
            "acc 2, sample 2: 0.0\n",
            "acc 3, sample 2: 0.0\n",
            "idx: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=81 velocity=64 time=18\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=90 velocity=64 time=48\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=179\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=83 velocity=64 time=18\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=2 note=68 velocity=64 time=48\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction number of voices: tensor(2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-8aa7cc93601e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict_pred\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdict_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_0_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_1_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_0\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0macc_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_accuracy_polyphonic_separate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpart_dic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_names_part\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_predictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"acc_0_2:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_0_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"acc_1_2:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_1_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"acc_0:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-124-86ff093ba961>\u001b[0m in \u001b[0;36mevaluate_accuracy_polyphonic_separate\u001b[0;34m(model, train_dataloader, part_dic, print_predictions)\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0mmonophonic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                         \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonophonic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mground_truth_label_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mel_note_arr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction number of voices:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction_clf\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-907559ed6a20>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentences, sentences_len, monophonic)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonophonic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mscores_comb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_clf_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0msum_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores_comb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-907559ed6a20>\u001b[0m in \u001b[0;36mcompute_outputs\u001b[0;34m(self, sentences, sentences_len)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m           \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m           \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdown_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdown_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-88ccce604c53>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mconcatenated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdouble_conv_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1441\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy mixed all voices (old)"
      ],
      "metadata": {
        "id": "u2frgy_KLUDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy_polyphonic(model, train_dataloader, part_dic):\n",
        "\n",
        "    unitl_len_idx = max_len_load\n",
        "    path_parts = \"AI-MA_project/polyphonic_new\"\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "                                      \n",
        "                #print(\"nbr_voices:\",nbr_voices)\n",
        "        if idx >30 and idx not in [0, 7, 21, 32, 35]: \n",
        "            #if nbr_voices == 2:\n",
        "                \n",
        "                file_name = file_name[0]\n",
        "                filename_part = os.path.join(path_parts, \"part_file\" + file_name + \".mid\")\n",
        "                part = partitura.load_score_midi(filename_part)\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                \n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                \n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1]\n",
        "\n",
        "                if len(part)== 4:\n",
        "                    part_2 = part[2]\n",
        "                    note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    \n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction, prediction_clf = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "                        print(\"prediction clf:\",prediction_clf)\n",
        "\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]   \n",
        "\n",
        "                        if end_first <= unitl_len_idx:\n",
        "                          #print(\"stop\",end_first)\n",
        "                          pitch_first = pitch_list[i]\n",
        "                          pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                          truth_list = [label for i in range(len(pred_list_first))]\n",
        "              \n",
        "                          result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                          # do majority vote if not all predictions are for same voice\n",
        "                          if result == False:\n",
        "                              major, major_idx = torch.mode(pred_list_first,0)\n",
        "                              major = major.numpy().tolist()\n",
        "                              pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "                        #print(\"pred_list\",str(label),len(pred_list_first),pred_list_first)\n",
        "                        if len(pred_list_first)==0:\n",
        "                          print(\"ATTENTION:\", len(pred_list_first),str(label) )\n",
        "                          print(\"index:\", idx)\n",
        "                          print(\"------------------------------------------------------------------------\")\n",
        "                          print(\"total_predictions_dict\",str(label),len(pred_list_first),pred_list_first)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                  counting = 0\n",
        "                  for j in range(len(total_predictions_dict[i])):\n",
        "                      if total_predictions_dict[i][j][0] == gt:         \n",
        "                        counting +=1\n",
        "                  count_dict_2[i].append(counting)\n",
        "\n",
        "\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                \n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                \n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                    acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                    print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                    acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                \n",
        "\n",
        "                \n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "\n",
        "    return total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "yvS8dZJaoxkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_pred , dict_gt, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_polyphonic(model,val_dataloader,part_dic=file_names_part)\n",
        "\n",
        "\n",
        "print(\"acc_0:\",acc_0)\n",
        "print(\"acc_1:\",acc_1)\n",
        "print(\"acc_2:\",acc_2)\n",
        "print(\"acc_3:\",acc_3)"
      ],
      "metadata": {
        "id": "8KzCauC6_FsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7tL0aaP711g5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}