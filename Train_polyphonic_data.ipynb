{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_polyphonic_data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2znd9C8+gb0EYafD6bKbX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Train_polyphonic_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "TGpH9G_kAwmy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "COo0mYvKAMmc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install partitura\n",
        "! pip install git+https://github.com/CPJKU/partitura.git@develop\n",
        "import partitura\n",
        "import statistics\n",
        "\n",
        "\n",
        "\n",
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataloader - Set the dataset"
      ],
      "metadata": {
        "id": "de_kI138A7ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DATA = \"AI-MA_project/pr_polyphonic\"\n",
        "\n",
        "batch_size = 1 \n",
        "workers = 0"
      ],
      "metadata": {
        "id": "GZhTj8KDA8Yf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse"
      ],
      "metadata": {
        "id": "VIEk3O1r0PAZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_load = 100\n",
        "\n",
        "class MusicDataset_polyphonic(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "        piece_lengths = [\"2_voice\",\"4_voice\"]\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for v_len_idx in piece_lengths:\n",
        "            if v_len_idx == \"4_voice\":\n",
        "                for iLabel in range(len(labels)):\n",
        "                    if iLabel == 4:   \n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, v_len_idx , self.labels[iLabel], \"*.pkl\")))       \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                                loaded_obj = pickle.load(f)  \n",
        "\n",
        "                                #print(\"loaded_obj before:\",loaded_obj.shape)\n",
        "\n",
        "                                pitch, t_len = loaded_obj.shape\n",
        "                                if t_len > max_len_load:\n",
        "                                  loaded_obj = loaded_obj[:,:max_len_load]\n",
        "                            \n",
        "\n",
        "                                #print(\"loaded_obj after:\",loaded_obj.shape)\n",
        "\n",
        "                                voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "\n",
        "                                len_list.append(len(loaded_obj.T))\n",
        "                                file_names_list.append(name[-7:-4])\n",
        "                                nbr_voices_list.append(4)                        \n",
        "\n",
        "                        self.pr_dict[self.labels[iLabel]] += voice_files\n",
        "                        self.pr_dict[\"length\"] +=  len_list\n",
        "                        self.pr_dict[\"nbr_voices\"] +=  nbr_voices_list\n",
        "                        self.pr_dict[\"name\"] += file_names_list\n",
        "                   \n",
        "                    else:\n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA,v_len_idx, self.labels[iLabel], \"*.pkl\"))) \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: \n",
        "                                  loaded_obj = pickle.load(f)\n",
        "\n",
        "                                  pitch, t_len = loaded_obj.shape\n",
        "                                  if t_len > max_len_load:\n",
        "                                    loaded_obj = loaded_obj[:,:max_len_load]\n",
        "\n",
        "\n",
        "                                  voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "                        self.pr_dict[self.labels[iLabel]] += voice_files\n",
        "\n",
        "\n",
        "            if v_len_idx == \"2_voice\":\n",
        "                for iLabel in range(len(labels)):\n",
        "                    if iLabel == 4:   \n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, v_len_idx , self.labels[iLabel], \"*.pkl\")))   \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f:\n",
        "                                loaded_obj = pickle.load(f)  \n",
        "\n",
        "                                pitch, t_len = loaded_obj.shape\n",
        "                                if t_len > max_len_load:\n",
        "                                  loaded_obj = loaded_obj[:,:max_len_load]\n",
        "\n",
        "                                voice_files.append(sparse.csr_matrix(loaded_obj))\n",
        "                                len_list.append(len(loaded_obj.T))\n",
        "                                file_names_list.append(name[-7:-4])\n",
        "                                nbr_voices_list.append(2)\n",
        "                              \n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files\n",
        "                        self.pr_dict[\"length\"] = len_list\n",
        "                        self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                        self.pr_dict[\"name\"] = file_names_list\n",
        "    \n",
        "                    else:\n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA,v_len_idx, self.labels[iLabel], \"*.pkl\"))) \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: \n",
        "                                  loaded_obj = pickle.load(f)   \n",
        "\n",
        "                                  pitch, t_len = loaded_obj.shape\n",
        "                                  if t_len > max_len_load:\n",
        "                                    loaded_obj = loaded_obj[:,:max_len_load]\n",
        "\n",
        "                                  voice_files.append(sparse.csr_matrix(loaded_obj))  \n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files\n",
        "                    \n",
        "    def __len__(self):\n",
        "        file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, \"2_voice\" , self.labels[4], \"*.pkl\")))\n",
        "        file_names_4 = sorted(glob.glob(os.path.join(PATH_TO_DATA, \"4_voice\" , self.labels[4], \"*.pkl\")))\n",
        "        return len(file_names_2) + len(file_names_4) \n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        for key,value in self.pr_dict.items():\n",
        "          v0 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_0\"][idx]).T)\n",
        "          v1 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_1\"][idx]).T)\n",
        "          \n",
        "          if self.pr_dict[\"nbr_voices\"][idx] == 4:       ### -168 WORKS ONLY IN THIS CASE BC 168 SAMPLES OF LEN(2) ARE LOADED FIRST\n",
        "              v2 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_2\"][idx-168]).T)\n",
        "              v3 = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_3\"][idx-168]).T)\n",
        "          if self.pr_dict[\"nbr_voices\"][idx] == 2:\n",
        "              v2 = torch.zeros(v1.shape)\n",
        "              v3 = torch.zeros(v1.shape)\n",
        "          \n",
        "          v_all = torch.tensor(sparse.csr_matrix.todense(self.pr_dict[\"voice_all\"][idx]).T)\n",
        "          length = self.pr_dict[\"length\"][idx]\n",
        "          nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "          file_name = self.pr_dict[\"name\"][idx]\n",
        "          voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "        return (voices, length, nbr_voices, file_name)"
      ],
      "metadata": {
        "id": "RR_q4F28BE1t"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = MusicDataset_polyphonic(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 1:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices,all_voices.shape)\n",
        "\n",
        "\n",
        "### try sparse matrix - convert when using to matrix - in loader only sparse matrix\n",
        "## plot the size for pieces ->  (polyphonic pieces longer / modern pieces)"
      ],
      "metadata": {
        "id": "HFMMT5ydBeDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5287b451-c6d4-4c50-f5c4-710f78542f22"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "002 tensor([2]) torch.Size([1, 100, 88, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define U-Net"
      ],
      "metadata": {
        "id": "fGlQ8KsIkl2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "wD23kpffkfKy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Network\n"
      ],
      "metadata": {
        "id": "GXc03KGJkfwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        #self.loss = nn.CrossEntropyLoss(reduction=\"sum\")                       # use weight parameters maybe take 1/88   \n",
        "        #weight_tensor = torch.tensor([1,1,1,4],dtype=torch.double).to(device)\n",
        "        #self.loss = nn.CrossEntropyLoss(weight=weight_tensor,reduction='mean',ignore_index=99)     \n",
        "\n",
        "    \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                      ### squeeze output here before returning                                       \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        #print(\"voices[:,:,:,-1]\", voices[:,:,:,-1].shape,voices.shape, nbr_voices)\n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        " \n",
        "        weight_v0 = voices[:,:,:,0].squeeze().sum()\n",
        "        weight_v1 = voices[:,:,:,1].squeeze().sum()\n",
        "        weight_v2 = voices[:,:,:,2].squeeze().sum()\n",
        "        weight_v3 = voices[:,:,:,3].squeeze().sum()\n",
        "\n",
        "        stack_tensors_gt = torch.swapaxes(torch.swapaxes(voices[:,:,:,:4].squeeze(), 0, 2), 1,2)\n",
        "        #stack_tensors_gt = torch.stack([v0, v1, v2, v3], dim=0)\n",
        "        \n",
        "        \n",
        "\n",
        "        stack_gt = torch.argmax(stack_tensors_gt,axis=0)\n",
        "\n",
        "        \n",
        "        weight_tensor = torch.stack([weight_v0/weight_v0,weight_v0/weight_v0,weight_v0/weight_v0,weight_v0/weight_v0])\n",
        "        criterion_weighted = nn.CrossEntropyLoss(weight=weight_tensor,reduction='mean',ignore_index=99)  \n",
        "        stack_pred = torch.stack([score_0, score_1, score_2, score_3], dim=0).softmax(dim=0)[None, :]\n",
        "        mask_pred = torch.squeeze(voices[:,:,:,-1])== 0\n",
        "        stack_gt[mask_pred] = 99\n",
        "        loss = criterion_weighted(stack_pred, stack_gt[None,:])\n",
        "\n",
        "        if torch.any(torch.isnan(stack_pred)) or torch.any(torch.isinf(stack_pred)) :\n",
        "          print('invalid input detected at iteration ')\n",
        "\n",
        "        return loss   \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "\n",
        "        else:\n",
        "            sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "            prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            \n",
        "            mask_pred = np.squeeze(sentences)== 0\n",
        "            v_pred_argm[mask_pred] = -1\n",
        "\n",
        "            return v_pred_argm        "
      ],
      "metadata": {
        "id": "nO4PtE0kknDd"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "monophonic = True\n",
        "his = start_experiment(1, 0.0001, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, \"CNN\", learn_all)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDeP7tqWmJGb",
        "outputId": "6a484661-b4c0-426e-df8b-dba797f965a2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataloader 199 val_dataloader 36\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Training on device: cpu\n",
            "monophonic set to: True\n",
            "loss: tensor(1.4198, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3992, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.4430, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3794, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3724, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3964, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3823, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3876, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3792, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.4008, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "Epoch: 1, Train Loss: nan\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Training Process"
      ],
      "metadata": {
        "id": "QyIAxKS-k4m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "UrErJAevk2Sg"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tryining Loop"
      ],
      "metadata": {
        "id": "u2R-p3BnlZdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### versuch hier mal nur für 4 voices zu trainieren\n",
        "\n",
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "                \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            if nbr_voices == 4:\n",
        "              voices = voices.to(device).float()\n",
        "              optimizer.zero_grad()\n",
        "              loss = model.forward(voices, lens, nbr_voices)             \n",
        "              print(\"loss:\",loss)\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "              loss_sum += loss.item()    \n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "        print(\"Epoch: {}, Train Loss: {}\".format(i_epoch,train_loss)) \n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "50qWCIMNk7ZL"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "metadata": {
        "id": "-NUWBUvqk_No"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter choice"
      ],
      "metadata": {
        "id": "-bFYYY_6l5hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MusicNetwork\n",
        "epochs = 1\n",
        "lr = 0.001 # was 0.001    #0.00001\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "metadata": {
        "id": "qlFVa23Ql5yz"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the experiment"
      ],
      "metadata": {
        "id": "Bq16m1D9lXHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO3ZxWbzlBis",
        "outputId": "4c7cfac1-9d58-4e2d-8a3d-23803ebcc073"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataloader 199 val_dataloader 36\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Training on device: cpu\n",
            "monophonic set to: True\n",
            "loss: tensor(1.3597, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.4494, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3042, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3881, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.4005, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3560, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3658, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.4153, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.4114, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3708, grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "invalid input detected at iteration \n",
            "loss: tensor(nan, grad_fn=<NllLoss2DBackward0>)\n",
            "Epoch: 1, Train Loss: nan\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load trained model"
      ],
      "metadata": {
        "id": "UoqAYzPnnj1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader "
      ],
      "metadata": {
        "id": "dSlYUsRUmALB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture  \n",
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch1.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "yFBgUAmNntEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dic with key:filename, val: part_obj  "
      ],
      "metadata": {
        "id": "iIXfywU8n3f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "Hu2g0G8jozVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "\n",
        "path_parts = \"AI-MA_project/polyphonic_new\"\n",
        "part_dic = {}\n",
        "\n",
        "#### create a list with all filenames in the right order ####\n",
        "file_names_part = []\n",
        "\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    file_names_part.append(filename[9:12])\n",
        "print(file_names_part)"
      ],
      "metadata": {
        "id": "k29NpOgtn3np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy_polyphonic(model, train_dataloader, part_dic):\n",
        "\n",
        "    path_parts = \"AI-MA_project/polyphonic_new\"\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "                                      \n",
        "                #print(\"nbr_voices:\",nbr_voices)\n",
        "            #if idx == 0 or idx==2:\n",
        "            if nbr_voices == 4:\n",
        "                \n",
        "\n",
        "                file_name = file_name[0]\n",
        "\n",
        "                #print(\"file_name loader:\",file_name)\n",
        "\n",
        "                filename_part = os.path.join(path_parts, \"part_file\" + file_name + \".mid\")\n",
        "\n",
        "                #print(\"filename_part:\",filename_part)\n",
        "                part = partitura.load_score_midi(filename_part)\n",
        "\n",
        "                #part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "                if len(part)== 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "             \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                  counting = 0\n",
        "                  for j in range(len(total_predictions_dict[i])):\n",
        "                      if total_predictions_dict[i][j][0] == gt:                     \n",
        "                        counting +=1  \n",
        "                  count_dict_2[i].append(counting)\n",
        "\n",
        "                print(\"acc v3 test\",count_dict_2[\"3\"][0],len(total_predictions_dict[\"3\"]),len(total_predictions_dict[\"0\"]) )\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                    acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                    print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                    acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                \n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "\n",
        "    return total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "yvS8dZJaoxkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_pred , dict_gt, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_polyphonic(model,val_dataloader,part_dic=file_names_part)"
      ],
      "metadata": {
        "id": "3wkyuQcFtVAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RlVVCWmqt0Jk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}