{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_polyphonic_data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP9DdJUxV112duLGG16h8TZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Train_polyphonic_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "TGpH9G_kAwmy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "COo0mYvKAMmc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install partitura\n",
        "! pip install git+https://github.com/CPJKU/partitura.git@develop\n",
        "import partitura\n",
        "import statistics\n",
        "\n",
        "\n",
        "\n",
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataloader - Set the dataset"
      ],
      "metadata": {
        "id": "de_kI138A7ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DATA = \"AI-MA_project/pr_polyphonic\"\n",
        "\n",
        "batch_size = 1 \n",
        "workers = 0"
      ],
      "metadata": {
        "id": "GZhTj8KDA8Yf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voice_files = np.array([])\n",
        "np.append(voice_files,0)"
      ],
      "metadata": {
        "id": "VIEk3O1r0PAZ",
        "outputId": "1d676b74-9004-4975-e580-adab13ee41c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MusicDataset_polyphonic(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        piece_lengths = [\"2_voice\",\"4_voice\"]\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "\n",
        "\n",
        "        for v_len_idx in piece_lengths:\n",
        "            if v_len_idx == \"4_voice\":\n",
        "                for iLabel in range(len(labels)):\n",
        "                    if iLabel == 4:   \n",
        "                        voice_files = np.array([])\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, v_len_idx , self.labels[iLabel], \"*.pkl\")))       \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                                loaded_obj = pickle.load(f)  \n",
        "                                voice_files.append(loaded_obj) \n",
        "                                len_list.append(len(loaded_obj.T))\n",
        "                                file_names_list.append(name[-7:-4])\n",
        "                                nbr_voices_list.append(4)                        \n",
        "\n",
        "                                \n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                        self.pr_dict[\"length\"] = len_list\n",
        "                        self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                        self.pr_dict[\"name\"] = file_names_list\n",
        "                        print(len(self.pr_dict[\"name\"]))\n",
        "                   \n",
        "                    else:\n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA,v_len_idx, self.labels[iLabel], \"*.pkl\"))) \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: \n",
        "                                  loaded_obj = pickle.load(f)     \n",
        "                                  voice_files.append(loaded_obj)\n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "            if v_len_idx == \"2_voice\":\n",
        "                for iLabel in range(len(labels)):\n",
        "                    if iLabel == 4:   \n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, v_len_idx , self.labels[iLabel], \"*.pkl\")))   \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f:\n",
        "                                loaded_obj = pickle.load(f)  \n",
        "                                voice_files.append(loaded_obj) \n",
        "                                len_list.append(len(loaded_obj.T))\n",
        "                                file_names_list.append(name[-7:-4])\n",
        "                                nbr_voices_list.append(2)\n",
        "                              \n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                        self.pr_dict[\"length\"] = len_list\n",
        "                        self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                        self.pr_dict[\"name\"] = file_names_list\n",
        "                        print(\"2\",len(self.pr_dict[\"name\"]))\n",
        "    \n",
        "                    else:\n",
        "                        voice_files = []\n",
        "                        file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA,v_len_idx, self.labels[iLabel], \"*.pkl\"))) \n",
        "                        for name in file_names:\n",
        "                            with open(name ,'rb') as f: \n",
        "                                  loaded_obj = pickle.load(f)     \n",
        "                                  voice_files.append(loaded_obj)\n",
        "                        self.pr_dict[self.labels[iLabel]] = voice_files  \n",
        "                    \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "              out_list.append(self.pr_dict[key][idx]) \n",
        "        \n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 4:\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.tensor(out_list[3].T)\n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 2:\n",
        "            v2 = torch.zeros(v1.shape)\n",
        "            v3 = torch.zeros(v1.shape)\n",
        "\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "        nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "        file_name = self.pr_dict[\"name\"][idx]\n",
        "        voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "        return (voices, length, nbr_voices, file_name)"
      ],
      "metadata": {
        "id": "RR_q4F28BE1t"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = MusicDataset_polyphonic(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 1:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices)\n",
        "\n",
        "### try sparse matrix - convert when using to matrix - in loader only sparse matrix\n",
        "\n",
        "## plot the size for pieces ->  (polyphonic pieces longer / modern pieces)"
      ],
      "metadata": {
        "id": "HFMMT5ydBeDO",
        "outputId": "775c426d-8061-44ef-b512-eb5e2f378f90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_names 168 ['AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_001.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_002.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_003.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_004.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_005.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_007.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_008.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_009.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_012.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_014.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_015.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_016.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_018.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_020.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_021.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_023.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_024.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_025.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_027.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_029.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_031.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_032.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_033.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_034.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_035.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_040.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_041.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_045.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_046.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_048.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_049.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_052.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_053.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_055.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_056.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_057.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_059.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_060.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_063.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_064.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_066.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_067.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_068.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_071.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_072.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_074.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_075.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_076.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_077.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_078.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_079.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_080.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_082.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_083.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_085.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_088.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_089.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_090.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_092.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_093.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_095.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_097.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_100.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_101.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_102.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_103.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_104.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_107.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_108.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_110.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_112.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_113.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_114.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_116.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_117.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_118.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_119.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_120.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_121.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_125.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_127.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_128.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_129.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_130.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_132.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_133.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_134.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_137.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_138.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_141.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_143.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_144.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_145.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_146.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_147.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_148.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_150.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_151.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_152.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_153.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_154.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_155.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_156.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_158.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_159.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_162.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_164.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_165.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_166.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_167.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_168.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_169.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_170.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_172.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_173.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_175.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_176.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_179.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_180.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_181.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_185.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_186.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_187.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_189.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_190.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_191.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_193.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_194.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_195.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_196.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_197.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_198.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_199.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_201.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_202.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_203.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_205.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_206.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_207.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_208.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_209.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_210.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_211.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_213.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_216.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_217.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_220.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_221.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_224.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_226.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_227.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_233.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_234.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_236.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_239.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_241.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_243.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_245.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_246.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_247.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_249.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_251.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_252.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_253.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_255.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_256.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_257.pkl', 'AI-MA_project/pr_polyphonic/2_voice/voice_all/voice_all_259.pkl']\n",
            "2 168\n",
            "file_names 67 ['AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_000.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_006.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_010.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_011.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_013.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_019.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_022.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_026.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_028.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_030.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_036.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_039.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_043.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_044.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_047.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_050.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_051.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_054.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_058.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_061.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_062.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_065.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_069.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_084.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_087.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_091.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_094.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_105.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_106.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_109.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_115.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_123.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_124.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_126.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_131.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_135.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_136.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_139.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_140.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_142.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_149.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_157.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_160.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_161.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_163.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_174.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_177.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_183.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_188.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_212.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_214.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_218.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_219.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_223.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_225.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_230.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_232.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_235.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_237.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_238.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_242.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_244.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_248.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_250.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_254.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_258.pkl', 'AI-MA_project/pr_polyphonic/4_voice/voice_all/voice_all_261.pkl']\n",
            "235\n",
            "002 tensor([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "9bRDjwvhuenW",
        "outputId": "a8bfb091-62fb-46af-92bd-f142a07b5462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define U-Net"
      ],
      "metadata": {
        "id": "fGlQ8KsIkl2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "wD23kpffkfKy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Network\n"
      ],
      "metadata": {
        "id": "GXc03KGJkfwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        #self.loss = nn.CrossEntropyLoss(reduction=\"sum\")                       # use weight parameters maybe take 1/88   \n",
        "        #weight_tensor = torch.tensor([1,1,1,4],dtype=torch.double).to(device)\n",
        "        #self.loss = nn.CrossEntropyLoss(weight=weight_tensor,reduction='mean',ignore_index=99)     \n",
        "\n",
        "    \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                      ### squeeze output here before returning                                       \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        #print(\"voices[:,:,:,-1]\", voices[:,:,:,-1].shape,voices.shape, nbr_voices)\n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()    ### v3 is automatically tensor containing only 0 bc dataloader does this for pieces with 3 voices\n",
        "        \n",
        "        weight_v0 = v0.sum()\n",
        "        weight_v1 = v1.sum()\n",
        "        weight_v2 = v2.sum()\n",
        "        weight_v3 = v3.sum()\n",
        "\n",
        "        stack_tensors_gt = torch.stack([v0, v1, v2, v3], dim=0)\n",
        "        stack_gt = torch.argmax(stack_tensors_gt,axis=0)\n",
        "\n",
        "\n",
        "        weight_tensor = torch.stack([weight_v0/weight_v0,weight_v0/weight_v0,weight_v0/weight_v0,5*weight_v0/weight_v0])\n",
        "        criterion_weighted = nn.CrossEntropyLoss(weight=weight_tensor,reduction='mean',ignore_index=99)        \n",
        "        stack_pred = torch.stack([score_0, score_1, score_2, score_3], dim=0).softmax(dim=0)[None, :]\n",
        "        mask_pred = torch.squeeze(voices[:,:,:,-1])== 0\n",
        "        stack_gt[mask_pred] = 99\n",
        "\n",
        "        loss = criterion_weighted(stack_pred, stack_gt[None,:])\n",
        "\n",
        "        #print(\"loss:\",loss)\n",
        "        return loss   \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "\n",
        "        else:\n",
        "            sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "            prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            \n",
        "            mask_pred = np.squeeze(sentences)== 0\n",
        "            v_pred_argm[mask_pred] = -1\n",
        "\n",
        "            return v_pred_argm        "
      ],
      "metadata": {
        "id": "nO4PtE0kknDd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#monophonic = True\n",
        "#his = start_experiment(1, 0.0001, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, \"CNN\", learn_all)\n"
      ],
      "metadata": {
        "id": "MDeP7tqWmJGb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Training Process"
      ],
      "metadata": {
        "id": "QyIAxKS-k4m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "UrErJAevk2Sg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tryining Loop"
      ],
      "metadata": {
        "id": "u2R-p3BnlZdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### versuch hier mal nur für 4 voices zu trainieren\n",
        "\n",
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "                \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            #if nbr_voices == 3:\n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "        print(\"Epoch: {}, Train Loss: {}\".format(i_epoch,train_loss)) \n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "50qWCIMNk7ZL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "metadata": {
        "id": "-NUWBUvqk_No"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter choice"
      ],
      "metadata": {
        "id": "-bFYYY_6l5hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MusicNetwork\n",
        "epochs = 10\n",
        "lr = 0.001 # was 0.001    #0.00001\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "metadata": {
        "id": "qlFVa23Ql5yz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the experiment"
      ],
      "metadata": {
        "id": "Bq16m1D9lXHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "metadata": {
        "id": "kO3ZxWbzlBis",
        "outputId": "d204f71b-ccfe-47a9-8bb4-9203be0c4edd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5547, 1920, 3636, 17040, 3624, 6684, 8808, 2187, 3174, 4152, 5394, 12246, 2304, 13404, 1962, 7281, 6960, 9498, 3996, 3363, 6972, 29628, 3027, 13944, 3480, 4044, 6030, 13638, 2160, 14550, 6252, 16395, 5004, 3012, 37764, 2688, 22428, 4884, 4029, 2880, 22416, 2016, 19176, 2880, 3426, 3612, 4986, 1944, 1374, 7482, 1740, 4560, 606, 648, 6480, 1008, 16002, 4425, 11808, 2460, 912, 1728, 3738, 5160, 2736, 576, 6048, 7374, 7920, 9444, 5556, 3222, 792, 6480, 9690, 4302, 1086, 1104, 12768, 3300, 4740, 35724, 8370, 870, 7170, 5838, 7440, 4272, 8076, 7344, 977, 12786, 2448, 2148, 5412, 504, 5028, 96, 15072, 2568, 10754, 4044, 2796, 7632, 9924, 6708, 8268, 2604, 4986, 3516, 5028, 6912, 1752, 2028, 4386, 5916, 1056, 3456, 2964, 5520, 7494, 3288, 1020, 18420, 7014, 2016, 4320, 4764, 4032, 3708, 240, 4146, 576, 2124, 624, 1452, 17748, 2856, 3654, 7344, 3768, 3135, 28080, 6015, 5100, 9096, 2724, 6924, 6300, 5544, 3984, 10632, 3744, 1872, 13413, 12864, 13158, 26016, 1656, 2166, 12696, 12684, 180, 10434, 13416, 4248, 6114, 5028, 7794, 13380, 3888, 10278, 2904, 510, 4617, 14244, 9240, 6048, 996, 23904, 16080, 9294, 9936, 19890, 16236, 20394, 13086, 16368, 13644, 14094, 8856, 6264, 16422, 14790, 10188, 9798, 12438, 14520, 7212, 33432, 3189, 6696, 4608, 8826, 792, 15060, 7536, 12324, 9876, 25308, 7836, 14208, 6480, 1890, 6678, 3288, 7800, 3180, 31920, 17124, 9132, 34680, 9600, 5652, 19158, 8982, 8922, 11886, 10212, 11832, 3606, 1047, 9456, 15354, 8700]\n",
            "train_dataloader 56 val_dataloader 11\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Training on device: cpu\n",
            "monophonic set to: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load trained model"
      ],
      "metadata": {
        "id": "UoqAYzPnnj1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_polyphonic(PATH_TO_DATA) \n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader "
      ],
      "metadata": {
        "id": "dSlYUsRUmALB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture  \n",
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch10.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "yFBgUAmNntEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dic with key:filename, val: part_obj  "
      ],
      "metadata": {
        "id": "iIXfywU8n3f5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "Hu2g0G8jozVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture\n",
        "\n",
        "path_parts = \"AI-MA_project/polyphonic\"\n",
        "part_dic = {}\n",
        "\n",
        "#### create a list with all filenames in the right order ####\n",
        "file_names_part = []\n",
        "\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    print(filename)\n",
        "    file_names_part.append(filename[3:7])\n",
        "print(file_names_part)\n",
        "\n",
        "#### create a list with all part objects in the right order ####\n",
        "part_list = []\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    fullname = os.path.join(path_parts, filename)\n",
        "    part = partitura.load_score_midi(fullname)\n",
        "    part_list.append(part)\n",
        "print(part_list)\n",
        "\n",
        "#### create a dict with keys:filenames , values: part object ####\n",
        "for i in range(len(file_names_part)):\n",
        "    part_dic[file_names_part[i]] = part_list[i]\n",
        "\n",
        "print(part_dic.keys(),part_dic.values())"
      ],
      "metadata": {
        "id": "k29NpOgtn3np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yvS8dZJaoxkq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}