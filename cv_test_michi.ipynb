{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cv_test_michi.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNDvkN67pYkqVD7y8KV2qzL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/cv_test_michi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRR1tAqjl97k"
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "from networks import get_model\n",
        "from datasets import ImageDataset, Dataset, bbox_iou\n",
        "from visualizations import visualize_fms, visualize_predictions, visualize_seed_expansion\n",
        "from object_discovery import lost, detect_box, dino_seg\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD0GFcKUl-3x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6lEALVWl-6C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxnVG4vsl7nK"
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "from networks import get_model\n",
        "from datasets import ImageDataset, Dataset, bbox_iou\n",
        "from visualizations import visualize_fms, visualize_predictions, visualize_seed_expansion\n",
        "from object_discovery import lost, detect_box, dino_seg\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\"Visualize Self-Attention maps\")\n",
        "    parser.add_argument(\n",
        "        \"--arch\",\n",
        "        default=\"vit_small\",\n",
        "        type=str,\n",
        "        choices=[\n",
        "            \"vit_tiny\",\n",
        "            \"vit_small\",\n",
        "            \"vit_base\",\n",
        "            \"resnet50\",\n",
        "            \"vgg16_imagenet\",\n",
        "            \"resnet50_imagenet\",\n",
        "        ],\n",
        "        help=\"Model architecture.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--patch_size\", default=16, type=int, help=\"Patch resolution of the model.\"\n",
        "    )\n",
        "\n",
        "    # Use a dataset\n",
        "    parser.add_argument(\n",
        "        \"--dataset\",\n",
        "        default=\"VOC07\",\n",
        "        type=str,\n",
        "        choices=[None, \"VOC07\", \"VOC12\", \"COCO20k\"],\n",
        "        help=\"Dataset name.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--set\",\n",
        "        default=\"train\",\n",
        "        type=str,\n",
        "        choices=[\"val\", \"train\", \"trainval\", \"test\"],\n",
        "        help=\"Path of the image to load.\",\n",
        "    )\n",
        "    # Or use a single image\n",
        "    parser.add_argument(\n",
        "        \"--image_path\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"If want to apply only on one image, give file path.\",\n",
        "    )\n",
        "\n",
        "    # Folder used to output visualizations and \n",
        "    parser.add_argument(\n",
        "        \"--output_dir\", type=str, default=\"outputs\", help=\"Output directory to store predictions and visualizations.\"\n",
        "    )\n",
        "\n",
        "    # Evaluation setup\n",
        "    parser.add_argument(\"--no_hard\", action=\"store_true\", help=\"Only used in the case of the VOC_all setup (see the paper).\")\n",
        "    parser.add_argument(\"--no_evaluation\", action=\"store_true\", help=\"Compute the evaluation.\")\n",
        "    parser.add_argument(\"--save_predictions\", default=True, type=bool, help=\"Save predicted bouding boxes.\")\n",
        "\n",
        "    # Visualization\n",
        "    parser.add_argument(\n",
        "        \"--visualize\",\n",
        "        type=str,\n",
        "        choices=[\"fms\", \"seed_expansion\", \"pred\", None],\n",
        "        default=None,\n",
        "        help=\"Select the different type of visualizations.\",\n",
        "    )\n",
        "\n",
        "    # For ResNet dilation\n",
        "    parser.add_argument(\"--resnet_dilate\", type=int, default=2, help=\"Dilation level of the resnet model.\")\n",
        "\n",
        "    # LOST parameters\n",
        "    parser.add_argument(\n",
        "        \"--which_features\",\n",
        "        type=str,\n",
        "        default=\"k\",\n",
        "        choices=[\"k\", \"q\", \"v\"],\n",
        "        help=\"Which features to use\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--k_patches\",\n",
        "        type=int,\n",
        "        default=100,\n",
        "        help=\"Number of patches with the lowest degree considered.\"\n",
        "    )\n",
        "\n",
        "    # Use dino-seg proposed method\n",
        "    parser.add_argument(\"--dinoseg\", action=\"store_true\", help=\"Apply DINO-seg baseline.\")\n",
        "    parser.add_argument(\"--dinoseg_head\", type=int, default=4)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.image_path is not None:\n",
        "        args.save_predictions = False\n",
        "        args.no_evaluation = True\n",
        "        args.dataset = None\n",
        "\n",
        "    # -------------------------------------------------------------------------------------------------------\n",
        "    # Dataset\n",
        "\n",
        "    # If an image_path is given, apply the method only to the image\n",
        "    if args.image_path is not None:\n",
        "        dataset = ImageDataset(args.image_path)\n",
        "    else:\n",
        "        dataset = Dataset(args.dataset, args.set, args.no_hard)\n",
        "\n",
        "    # -------------------------------------------------------------------------------------------------------\n",
        "    # Model\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model = get_model(args.arch, args.patch_size, args.resnet_dilate, device)\n",
        "\n",
        "    # -------------------------------------------------------------------------------------------------------\n",
        "    # Directories\n",
        "    if args.image_path is None:\n",
        "        args.output_dir = os.path.join(args.output_dir, dataset.name)\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    # Naming\n",
        "    if args.dinoseg:\n",
        "        # Experiment with the baseline DINO-seg\n",
        "        if \"vit\" not in args.arch:\n",
        "            raise ValueError(\"DINO-seg can only be applied to tranformer networks.\")\n",
        "        exp_name = f\"{args.arch}-{args.patch_size}_dinoseg-head{args.dinoseg_head}\"\n",
        "    else:\n",
        "        # Experiment with LOST\n",
        "        exp_name = f\"LOST-{args.arch}\"\n",
        "        if \"resnet\" in args.arch:\n",
        "            exp_name += f\"dilate{args.resnet_dilate}\"\n",
        "        elif \"vit\" in args.arch:\n",
        "            exp_name += f\"{args.patch_size}_{args.which_features}\"\n",
        "\n",
        "    print(f\"Running LOST on the dataset {dataset.name} (exp: {exp_name})\")\n",
        "\n",
        "    # Visualization \n",
        "    if args.visualize:\n",
        "        vis_folder = f\"{args.output_dir}/visualizations/{exp_name}\"\n",
        "        os.makedirs(vis_folder, exist_ok=True)\n",
        "\n",
        "    # -------------------------------------------------------------------------------------------------------\n",
        "    # Loop over images\n",
        "    preds_dict = {}\n",
        "    cnt = 0\n",
        "    corloc = np.zeros(len(dataset.dataloader))\n",
        "    \n",
        "    pbar = tqdm(dataset.dataloader)\n",
        "    for im_id, inp in enumerate(pbar):\n",
        "\n",
        "        # ------------ IMAGE PROCESSING -------------------------------------------\n",
        "        img = inp[0]\n",
        "        init_image_size = img.shape\n",
        "\n",
        "        # Get the name of the image\n",
        "        im_name = dataset.get_image_name(inp[1])\n",
        "\n",
        "        # Pass in case of no gt boxes in the image\n",
        "        if im_name is None:\n",
        "            continue\n",
        "\n",
        "        # Padding the image with zeros to fit multiple of patch-size\n",
        "        size_im = (\n",
        "            img.shape[0],\n",
        "            int(np.ceil(img.shape[1] / args.patch_size) * args.patch_size),\n",
        "            int(np.ceil(img.shape[2] / args.patch_size) * args.patch_size),\n",
        "        )\n",
        "        paded = torch.zeros(size_im)\n",
        "        paded[:, : img.shape[1], : img.shape[2]] = img\n",
        "        img = paded\n",
        "\n",
        "        # Move to gpu\n",
        "        img = img.cuda(non_blocking=True)\n",
        "        # Size for transformers\n",
        "        w_featmap = img.shape[-2] // args.patch_size\n",
        "        h_featmap = img.shape[-1] // args.patch_size\n",
        "\n",
        "        # ------------ GROUND-TRUTH -------------------------------------------\n",
        "        if not args.no_evaluation:\n",
        "            gt_bbxs, gt_cls = dataset.extract_gt(inp[1], im_name)\n",
        "\n",
        "            if gt_bbxs is not None:\n",
        "                # Discard images with no gt annotations\n",
        "                # Happens only in the case of VOC07 and VOC12\n",
        "                if gt_bbxs.shape[0] == 0 and args.no_hard:\n",
        "                    continue\n",
        "\n",
        "        # ------------ EXTRACT FEATURES -------------------------------------------\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # ------------ FORWARD PASS -------------------------------------------\n",
        "            if \"vit\" in args.arch:\n",
        "                # Store the outputs of qkv layer from the last attention layer\n",
        "                feat_out = {}\n",
        "                def hook_fn_forward_qkv(module, input, output):\n",
        "                    feat_out[\"qkv\"] = output\n",
        "                model._modules[\"blocks\"][-1]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n",
        "\n",
        "                # Forward pass in the model\n",
        "                attentions = model.get_last_selfattention(img[None, :, :, :])\n",
        "\n",
        "                # Scaling factor\n",
        "                scales = [args.patch_size, args.patch_size]\n",
        "\n",
        "                # Dimensions\n",
        "                nb_im = attentions.shape[0]  # Batch size\n",
        "                nh = attentions.shape[1]  # Number of heads\n",
        "                nb_tokens = attentions.shape[2]  # Number of tokens\n",
        "\n",
        "                # Baseline: compute DINO segmentation technique proposed in the DINO paper\n",
        "                # and select the biggest component\n",
        "                if args.dinoseg:\n",
        "                    pred = dino_seg(attentions, (w_featmap, h_featmap), args.patch_size, head=args.dinoseg_head)\n",
        "                    pred = np.asarray(pred)\n",
        "                else:\n",
        "                    # Extract the qkv features of the last attention layer\n",
        "                    qkv = (\n",
        "                        feat_out[\"qkv\"]\n",
        "                        .reshape(nb_im, nb_tokens, 3, nh, -1 // nh)\n",
        "                        .permute(2, 0, 3, 1, 4)\n",
        "                    )\n",
        "                    q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "                    k = k.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n",
        "                    q = q.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n",
        "                    v = v.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n",
        "\n",
        "                    # Modality selection\n",
        "                    if args.which_features == \"k\":\n",
        "                        feats = k[:, 1:, :]\n",
        "                    elif args.which_features == \"q\":\n",
        "                        feats = q[:, 1:, :]\n",
        "                    elif args.which_features == \"v\":\n",
        "                        feats = v[:, 1:, :]\n",
        "\n",
        "            elif \"resnet\" in args.arch:\n",
        "                x = model.forward(img[None, :, :, :])\n",
        "                d, w_featmap, h_featmap = x.shape[1:]\n",
        "                feats = x.reshape((1, d, -1)).transpose(2, 1)\n",
        "                # Apply layernorm\n",
        "                layernorm = nn.LayerNorm(feats.size()[1:]).to(device)\n",
        "                feats = layernorm(feats)\n",
        "                # Scaling factor\n",
        "                scales = [\n",
        "                    float(img.shape[1]) / x.shape[2],\n",
        "                    float(img.shape[2]) / x.shape[3],\n",
        "                ]\n",
        "            elif \"vgg16\" in args.arch:\n",
        "                x = model.forward(img[None, :, :, :])\n",
        "                d, w_featmap, h_featmap = x.shape[1:]\n",
        "                feats = x.reshape((1, d, -1)).transpose(2, 1)\n",
        "                # Apply layernorm\n",
        "                layernorm = nn.LayerNorm(feats.size()[1:]).to(device)\n",
        "                feats = layernorm(feats)\n",
        "                # Scaling factor\n",
        "                scales = [\n",
        "                    float(img.shape[1]) / x.shape[2],\n",
        "                    float(img.shape[2]) / x.shape[3],\n",
        "                ]\n",
        "            else:\n",
        "                raise ValueError(\"Unknown model.\")\n",
        "\n",
        "        # ------------ Apply LOST -------------------------------------------\n",
        "        if not args.dinoseg:\n",
        "            pred, A, scores, seed = lost(\n",
        "                feats,\n",
        "                [w_featmap, h_featmap],\n",
        "                scales,\n",
        "                init_image_size,\n",
        "                k_patches=args.k_patches,\n",
        "            )\n",
        "\n",
        "            # ------------ Visualizations -------------------------------------------\n",
        "            if args.visualize == \"fms\":\n",
        "                visualize_fms(A.clone().cpu().numpy(), seed, scores, [w_featmap, h_featmap], scales, vis_folder, im_name)\n",
        "\n",
        "            elif args.visualize == \"seed_expansion\":\n",
        "                image = dataset.load_image(im_name)\n",
        "\n",
        "                # Before expansion\n",
        "                pred_seed, _ = detect_box(\n",
        "                    A[seed, :],\n",
        "                    seed,\n",
        "                    [w_featmap, h_featmap],\n",
        "                    scales=scales,\n",
        "                    initial_im_size=init_image_size[1:],\n",
        "                )\n",
        "                visualize_seed_expansion(image, pred, seed, pred_seed, scales, [w_featmap, h_featmap], vis_folder, im_name)\n",
        "\n",
        "            elif args.visualize == \"pred\":\n",
        "                image = dataset.load_image(im_name)\n",
        "                visualize_predictions(image, pred, seed, scales, [w_featmap, h_featmap], vis_folder, im_name)\n",
        "\n",
        "        # Save the prediction\n",
        "        preds_dict[im_name] = pred\n",
        "\n",
        "        # Evaluation\n",
        "        if args.no_evaluation:\n",
        "            continue\n",
        "\n",
        "        # Compare prediction to GT boxes\n",
        "        ious = bbox_iou(torch.from_numpy(pred), torch.from_numpy(gt_bbxs))\n",
        "\n",
        "        if torch.any(ious >= 0.5):\n",
        "            corloc[im_id] = 1\n",
        "\n",
        "        cnt += 1\n",
        "        if cnt % 50 == 0:\n",
        "            pbar.set_description(f\"Found {int(np.sum(corloc))}/{cnt}\")\n",
        "\n",
        "\n",
        "    # Save predicted bounding boxes\n",
        "    if args.save_predictions:\n",
        "        folder = f\"{args.output_dir}/{exp_name}\"\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "        filename = os.path.join(folder, \"preds.pkl\")\n",
        "        with open(filename, \"wb\") as f:\n",
        "            pickle.dump(preds_dict, f)\n",
        "        print(\"Predictions saved at %s\" % filename)\n",
        "\n",
        "    # Evaluate\n",
        "    if not args.no_evaluation:\n",
        "        print(f\"corloc: {100*np.sum(corloc)/cnt:.2f} ({int(np.sum(corloc))}/{cnt})\")\n",
        "        result_file = os.path.join(folder, 'results.txt')\n",
        "        with open(result_file, 'w') as f:\n",
        "            f.write('corloc,%.1f,,\\n'%(100*np.sum(corloc)/cnt))\n",
        "        print('File saved at %s'%result_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}