{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2799311b-c6fc-4a1e-d454-c07a0fe4a9ee"
      },
      "source": [
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI-MA_project'...\n",
            "remote: Enumerating objects: 1897, done.\u001b[K\n",
            "remote: Counting objects: 100% (1897/1897), done.\u001b[K\n",
            "remote: Compressing objects: 100% (141/141), done.\u001b[K\n",
            "remote: Total 1897 (delta 1810), reused 1794 (delta 1755), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1897/1897), 3.95 MiB | 5.92 MiB/s, done.\n",
            "Resolving deltas: 100% (1810/1810), done.\n",
            "Checking out files: 100% (1846/1846), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "batch_size = 1 \n",
        "PATH_TO_DATA = \"AI-MA_project/pianoroll\"\n",
        "workers = 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1-u8zYd-gyo"
      },
      "source": [
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))                             \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "            \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)                              \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                                        \n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):          \n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])                  \n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "\n",
        "        return (v0, v1, v2, v3, v_all, length)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "dataset = MusicDataset(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iybzKxeDjxe"
      },
      "source": [
        "# Model - RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1oi5Ey7Fzpb"
      },
      "source": [
        "class MusicRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim=128, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicRNN, self).__init__()\n",
        "\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "\n",
        "\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "\n",
        "        rnn_out ,_= self.rnn(sentences)\n",
        "        out_0 = self.top_layer_voice_0(rnn_out)\n",
        "        out_1 = self.top_layer_voice_1(rnn_out)\n",
        "        out_2 = self.top_layer_voice_2(rnn_out)\n",
        "        out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "        return out_0, out_1, out_2, out_3\n",
        "\n",
        "    def forward(self, sentences, v0, v1, v2, v3, sentences_len):                # maybe change sent -> v_all\n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_0, scores_1, scores_2, scores_3 = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_0.view(-1, self.n_out)\n",
        "        score_1  = scores_1.view(-1, self.n_out)\n",
        "        score_2  = scores_2.view(-1, self.n_out)\n",
        "        score_3  = scores_3.view(-1, self.n_out)\n",
        "\n",
        "        v0 = v0.squeeze()\n",
        "        v1 = v1.squeeze()\n",
        "        v2 = v2.squeeze()\n",
        "        v3 = v3.squeeze()\n",
        "        \n",
        "        loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) + self.loss(score_3, v3)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def predict(self, sentences, sentences_len):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_0, scores_1, scores_2, scores_3 = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
        "        predicted_0 = scores_0.argmax(dim=2)\n",
        "        predicted_1 = scores_1.argmax(dim=2)\n",
        "        predicted_2 = scores_2.argmax(dim=2)\n",
        "        predicted_3 = scores_3.argmax(dim=2)       \n",
        "        \n",
        "        return np.squeeze(predicted_0.cpu().numpy()), np.squeeze(predicted_1.cpu().numpy()), np.squeeze(predicted_2.cpu().numpy()), np.squeeze(predicted_3.cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Music - CNN\n"
      ],
      "metadata": {
        "id": "JNqxeacDwxNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList(\n",
        "            [self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList(\n",
        "            [nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2)\n",
        "             for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList(\n",
        "        [self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "XMdlm0_Vyyhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim=128, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                  #need to know input/output dim first\n",
        "        super(MusicCNN, self).__init__()\n",
        "\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        \n",
        "    \n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "\n",
        "        # Loss function that we will use during training.\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "\n",
        "        sentences = sentences[:,None]\n",
        "        out = self.cnn(sentences)\n",
        "        \n",
        "        out_0 = out[:,0,:]\n",
        "        out_1 = out[:,1,:]\n",
        "        out_2 = out[:,2,:]\n",
        "        out_3 = out[:,3,:]\n",
        "\n",
        "\n",
        "        return out_0, out_1, out_2, out_3\n",
        "\n",
        "\n",
        "    def forward(self, sentences, v0, v1, v2, v3, sentences_len):                # maybe change sent -> v_all\n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_0, scores_1, scores_2, scores_3 = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_0.view(-1, self.n_out)\n",
        "        score_1  = scores_1.view(-1, self.n_out)\n",
        "        score_2  = scores_2.view(-1, self.n_out)\n",
        "        score_3  = scores_3.view(-1, self.n_out)\n",
        "\n",
        "        v0 = v0.squeeze()\n",
        "        v1 = v1.squeeze()\n",
        "        v2 = v2.squeeze()\n",
        "        v3 = v3.squeeze()\n",
        "\n",
        "        \n",
        "        loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) + self.loss(score_3, v3)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def predict(self, sentences, sentences_len):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_0, scores_1, scores_2, scores_3 = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
        "        predicted_0 = scores_0.argmax(dim=2)\n",
        "        predicted_1 = scores_1.argmax(dim=2)\n",
        "        predicted_2 = scores_2.argmax(dim=2)\n",
        "        predicted_3 = scores_3.argmax(dim=2)       # prbl distr over 0-not there, 1-there   - argmax -> take higher probl of those 2\n",
        "        \n",
        "        return np.squeeze(predicted_0.cpu().numpy()), np.squeeze(predicted_1.cpu().numpy()), np.squeeze(predicted_2.cpu().numpy()), np.squeeze(predicted_3.cpu().numpy())"
      ],
      "metadata": {
        "id": "V5RzMSD4wwBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHgEe10xprjf",
        "outputId": "b57160a4-c30e-4972-83e0-68735e013efd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, learn_all)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and validation lenghts:  19 4\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Training on device: cuda\n",
            "Train Loss: 0.7341668261798754, Train Accuracy_0 : 0.9621377761297976, Train Accuracy_1 : 0.9386959689787694,Train Accuracy_2 : 0.9429976469267998, Train Accuracy_3 : 0.978083443867021\n",
            " Validation Accuracy_0 : 0.990396993527149, Validation Accuracy_1 : 0.957891688902642, Validation Accuracy_2 : 0.9451364539942749, Validation Accuracy_3 : 0.9898609841378104\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Train Loss: 0.32320253587548675, Train Accuracy_0 : 0.9905400813275957, Train Accuracy_1 : 0.9679828829881114,Train Accuracy_2 : 0.9685070770320333, Train Accuracy_3 : 0.9898391647681491\n",
            " Validation Accuracy_0 : 0.9904569995314624, Validation Accuracy_1 : 0.959232048107727, Validation Accuracy_2 : 0.9678902360245369, Validation Accuracy_3 : 0.9898696983051541\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Train Loss: 0.31018236437330293, Train Accuracy_0 : 0.9906215992653676, Train Accuracy_1 : 0.9681732103343044,Train Accuracy_2 : 0.969065677700777, Train Accuracy_3 : 0.9900491212323631\n",
            " Validation Accuracy_0 : 0.9905156854169844, Validation Accuracy_1 : 0.9513790993296469, Validation Accuracy_2 : 0.9717441691655689, Validation Accuracy_3 : 0.9898521276983465\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Train Loss: 0.3007882321938588, Train Accuracy_0 : 0.9905401825133443, Train Accuracy_1 : 0.9692025456222012,Train Accuracy_2 : 0.9713065299853983, Train Accuracy_3 : 0.9907165536982864\n",
            " Validation Accuracy_0 : 0.9905200578569953, Validation Accuracy_1 : 0.9627891966194453, Validation Accuracy_2 : 0.9732943915686314, Validation Accuracy_3 : 0.9898696983051541\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Train Loss: 0.2910765552923733, Train Accuracy_0 : 0.9905548572423288, Train Accuracy_1 : 0.970431247553216,Train Accuracy_2 : 0.9729274542295492, Train Accuracy_3 : 0.9915007712112002\n",
            " Validation Accuracy_0 : 0.9905381086433329, Validation Accuracy_1 : 0.9695509747090099, Validation Accuracy_2 : 0.9741772162964728, Validation Accuracy_3 : 0.9928155532857859\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Train Loss: 0.25420933201584245, Train Accuracy_0 : 0.9900946385487323, Train Accuracy_1 : 0.9717620008252569,Train Accuracy_2 : 0.9751971793398683, Train Accuracy_3 : 0.9943666428349097\n",
            " Validation Accuracy_0 : 0.9907067132881318, Validation Accuracy_1 : 0.972810470488987, Validation Accuracy_2 : 0.9750619871313407, Validation Accuracy_3 : 0.9925675879869237\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Train Loss: 0.2483946538862724, Train Accuracy_0 : 0.9906503446015457, Train Accuracy_1 : 0.9722938402389061,Train Accuracy_2 : 0.9759820523907419, Train Accuracy_3 : 0.9941089701514909\n",
            " Validation Accuracy_0 : 0.9908737955400437, Validation Accuracy_1 : 0.9729878615145795, Validation Accuracy_2 : 0.9758307381003957, Validation Accuracy_3 : 0.992578097733032\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Train Loss: 0.2448871837071161, Train Accuracy_0 : 0.990802836076327, Train Accuracy_1 : 0.9724205961828305,Train Accuracy_2 : 0.9764326405998285, Train Accuracy_3 : 0.9943705059746749\n",
            " Validation Accuracy_0 : 0.9910639927745272, Validation Accuracy_1 : 0.9730541622386736, Validation Accuracy_2 : 0.9763067832801838, Validation Accuracy_3 : 0.9925820019313173\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Train Loss: 0.2417890057475268, Train Accuracy_0 : 0.9910520001923008, Train Accuracy_1 : 0.9727297108447358,Train Accuracy_2 : 0.9766883606419527, Train Accuracy_3 : 0.9945991289061351\n",
            " Validation Accuracy_0 : 0.9912784624813178, Validation Accuracy_1 : 0.9731097339953871, Validation Accuracy_2 : 0.9766196396656824, Validation Accuracy_3 : 0.994853493123902\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Train Loss: 0.23877758392683493, Train Accuracy_0 : 0.9913392705733289, Train Accuracy_1 : 0.9729427320063333,Train Accuracy_2 : 0.9768681055804901, Train Accuracy_3 : 0.9946471570851377\n",
            " Validation Accuracy_0 : 0.9915633335702837, Validation Accuracy_1 : 0.9730128934013128, Validation Accuracy_2 : 0.9768828213219629, Validation Accuracy_3 : 0.9948269664037993\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITD9mYRfb9p-"
      },
      "source": [
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 128                                                            ### think about how to get the correct input=output size\n",
        "    ####model = MusicRNN(output_dim, hidden_dim, rnn_depth, cell_type)\n",
        "    model = MusicCNN(output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, epochs=50, val_dataloader=None, device=None, scheduler=None,):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    # Move model to GPU if needed\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        accuracy_v0_sum = 0\n",
        "        accuracy_v1_sum = 0\n",
        "        accuracy_v2_sum = 0\n",
        "        accuracy_v3_sum = 0\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        \n",
        "\n",
        "        for idx, (v0, v1, v2, v3, v_all, lens) in enumerate(train_dataloader):  \n",
        "\n",
        "            v0, v1, v2, v3, v_all = (v0.to(device).float(), v1.to(device).float(), v2.to(device).float(), v3.to(device).float(), v_all.to(device).float())\n",
        "            \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(v_all, v0, v1, v2, v3, lens)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()\n",
        "\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(v_all, lens)\n",
        "                \n",
        "\n",
        "                # compute the accuracy without considering the padding\n",
        "               \n",
        "                acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(v0).argmax(dim=1).cpu())\n",
        "                acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(v1).argmax(dim=1).cpu())\n",
        "                acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(v2).argmax(dim=1).cpu())\n",
        "                acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(v3).argmax(dim=1).cpu())\n",
        "                # normalize according to the number of sequences in the batch\n",
        "                accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "\n",
        "        train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "        train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "        train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "        train_accuracy_v3 = accuracy_v3_sum / len(train_dataloader)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "        history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "        history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "        history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "        \n",
        "        print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3)) \n",
        "\n",
        "\n",
        "        if val_dataloader is not None:\n",
        "            # Evaluate on the validation set\n",
        "            model.eval()\n",
        "            accuracy_v0_sum = 0\n",
        "            accuracy_v1_sum = 0\n",
        "            accuracy_v2_sum = 0\n",
        "            accuracy_v3_sum = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for v0, v1, v2, v3, v_all, lens in val_dataloader:\n",
        "\n",
        "                    v0, v1, v2, v3, v_all = (v0.to(device).float(), v1.to(device).float(), v2.to(device).float(), v3.to(device).float(), v_all.to(device).float())\n",
        "\n",
        "                    # Predict the model's output on a batch\n",
        "                    pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(v_all, lens)\n",
        "                        \n",
        "                        # compute the accuracy without considering the padding\n",
        "                    acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(v0).argmax(dim=1).cpu())\n",
        "                    acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(v1).argmax(dim=1).cpu())\n",
        "                    acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(v2).argmax(dim=1).cpu())\n",
        "                    acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(v3).argmax(dim=1).cpu())\n",
        "                        \n",
        "                        \n",
        "                        # normalize according to the number of sequences in the batch\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                # normalize according to the number of batches\n",
        "                val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                val_accuracy_v3 = accuracy_v3_sum / len(val_dataloader)\n",
        "\n",
        "            history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "            history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "            history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "            history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "            \n",
        "            print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNzPG7eeIEoO"
      },
      "source": [
        "#start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, learn_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "def start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay, learn_all):\n",
        "    \n",
        "\n",
        "    trainer = partial(train, epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset(PATH_TO_DATA)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        path_train, path_validation = sklearn.model_selection.train_test_split(PATH_TO_DATA, test_size=0.15, random_state=10,)\n",
        "\n",
        "        print(\"Train and validation lenghts: \", len(path_train), len(path_validation))\n",
        "        train_dataset = MusicDataset(path_train)\n",
        "        validation_dataset = MusicDataset(path_validation)\n",
        "        \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        _, history = trainer(train_dataloader, val_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicRNN\n",
        "epochs = 10\n",
        "lr = 0.001  # was 0.01\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 # was 1\n",
        "device = None #torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")            ---- if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"      # change later to True to see if learning works"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm"
      },
      "source": [
        "start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, learn_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B1TRcSBn0w2"
      },
      "source": [
        "# seems like there is still something wrong bc the accuracy doesnt change"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zH4kncO4O5-k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}