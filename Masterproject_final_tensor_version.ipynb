{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject_final_tensor_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "SsyC2uB0KfaT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a5d659-93f2-4e52-eb7f-b65a613d0b16"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install partitura\n",
        "import partitura"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: partitura in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: lark-parser in /usr/local/lib/python3.7/dist-packages (from partitura) (0.12.0)\n",
            "Requirement already satisfied: xmlschema in /usr/local/lib/python3.7/dist-packages (from partitura) (1.11.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.21.6)\n",
            "Requirement already satisfied: mido in /usr/local/lib/python3.7/dist-packages (from partitura) (1.2.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from partitura) (4.2.6)\n",
            "Requirement already satisfied: elementpath<3.0.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from xmlschema->partitura) (2.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "879d591b-d04c-4cea-a985-6af9d11360ba"
      },
      "source": [
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AI-MA_project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "batch_size = 1 \n",
        "PATH_TO_DATA = \"AI-MA_project/bach_pr_fugues\"\n",
        "#PATH_TO_DATA = \"AI-MA_project/pianoroll_88\"\n",
        "workers = 0"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1-u8zYd-gyo"
      },
      "source": [
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "    \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "  \n",
        "\n",
        "    def __getitem__(self, idx):          \n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])    \n",
        "\n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "\n",
        "\n",
        "        return (v0, v1, v2, v3, v_all, length)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_new(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        self.name_list = [\"1f01\",\"1f02\",\"1f03\",\"1f04\",\"1f05\",\"1f06\",\"1f07\",\"1f08\",\"1f09\",\"1f10\",\"1f11\",\"1f12\",\"1f13\",\"1f14\",\"1f15\",\"1f16\",\"1f17\",\"1f18\",\"1f19\",\"1f20\",\"1f21\",\"1f22\",\"1f23\",\"1f24\",\"2f01\",\"2f02\",\"2f03\",\"2f04\",\"2f05\",\"2f06\",\"2f07\",\"2f08\",\"2f09\",\"2f10\",\"2f11\",\"2f12\",\"2f13\",\"2f14\",\"2f15\",\"2f16\",\"2f17\",\"2f18\",\"2f19\",\"2f20\",\"2f21\",\"2f22\",\"2f23\",\"2f24\"]\n",
        "        self.name_list_voice_3 =  ['1f01', '1f05', '1f12', '1f14', '1f16', '1f17', '1f18', '1f23', '1f24', '2f02', '2f05', '2f07', '2f08', '2f09', '2f16', '2f17',  '2f22', '2f23']\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "\n",
        "                        file_names_list.append(name[-8:-4])\n",
        "\n",
        "                    if \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[49:53] + \".pkl\" in sorted(glob.glob(os.path.join(PATH_TO_DATA, \"voice_3\", \"*.pkl\"))):\n",
        "                        nbr_voices_list.append(4)\n",
        "                    else:\n",
        "                        nbr_voices_list.append(3)\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "\n",
        "\n",
        "            if iLabel == 3:  \n",
        "                voice_files = []\n",
        "                file_names_3 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[2], \"*.pkl\")))   \n",
        "                \n",
        "                ###### loop over all filnames in voices_2 and if an element there is not present in voices_3: append \"missing\" to the voice_files of label=3 => important bc. self.pr_dict[voice_3] has then len 42 and otherwise it would only have len 18  .. these \"missing\" el are not considered later in the dataloader (if len=3 is a diff case of get_idx)\n",
        "                for name in file_names_2:\n",
        "                    if name[45:49] in self.name_list_voice_3:\n",
        "                      correct_name_3 = \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[45:49] + \".pkl\"\n",
        "                      with open(correct_name_3 ,'rb') as f:  \n",
        "                            loaded_obj = pickle.load(f)  \n",
        "                            voice_files.append(loaded_obj)\n",
        "                    else:\n",
        "                      voice_files.append(\"missing\")\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                \n",
        "\n",
        "                \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                          loaded_obj = pickle.load(f)     \n",
        "                          voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 4:\n",
        "            for key, value in self.pr_dict.items():\n",
        "              out_list.append(self.pr_dict[key][idx])\n",
        "                              \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.tensor(out_list[3].T)\n",
        "            v_all = torch.tensor(out_list[4].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 3:\n",
        "\n",
        "            for key, value in self.pr_dict.items():\n",
        "                if key != \"voice_3\":\n",
        "                  out_list.append(self.pr_dict[key][idx]) \n",
        "            \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.zeros(v2.shape)\n",
        "            v_all = torch.tensor(out_list[3].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "            \n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        "
      ],
      "metadata": {
        "id": "3FxK6qr1FqIl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "#dataset = MusicDataset(PATH_TO_DATA)\n",
        "dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "#for i, sample_batched in enumerate(loader):\n",
        "#    all_voices, length, nbr_voices, file_name = sample_batched\n",
        "#    print(file_name[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LXsRYzQSUuQU",
        "outputId": "7cf8c143-d17c-4677-8a0a-d5203a6e81fd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor i, sample_batched in enumerate(loader):\\n    all_voices, length, nbr_voices = sample_batched\\n    if nbr_voices ==3:\\n      print(i,nbr_voices,all_voices.shape)\\n    else:\\n      print(i,nbr_voices)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "  if i ==10:\n",
        "    all_voices, length, nbr_voices, _ = sample_batched\n",
        "    all_voices_pr = all_voices[0,:,:,-1].numpy()\n",
        "    \n",
        "    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\n",
        "    print(note_array.shape)\n",
        "    print(note_array[:10])\n",
        "    print(note_array.dtype.names)\n",
        "\n",
        "    #print(i,nbr_voices,all_voices.shape)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "b4QCaMEi3nw7",
        "outputId": "554182b9-d3de-4c59-fcd4-803416cef0ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor i, sample_batched in enumerate(loader):\\n    all_voices, length, nbr_voices = sample_batched\\n    if nbr_voices ==3:\\n      print(i,nbr_voices,all_voices.shape)\\n    else:\\n      print(i,nbr_voices)\\n\\nfor i, sample_batched in enumerate(loader):\\n  if i ==10:\\n    all_voices, length, nbr_voices, _ = sample_batched\\n    all_voices_pr = all_voices[0,:,:,-1].numpy()\\n    \\n    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\\n    print(note_array.shape)\\n    print(note_array[:10])\\n    print(note_array.dtype.names)\\n\\n    #print(i,nbr_voices,all_voices.shape)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Music - Model\n"
      ],
      "metadata": {
        "id": "JNqxeacDwxNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UNET "
      ],
      "metadata": {
        "id": "QAIfIM69VHI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "XMdlm0_Vyyhc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")                       # use weight parameters maybe take 1/88       \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                      ### squeeze output here before returning                                       \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()\n",
        "\n",
        "\n",
        "\n",
        "        if nbr_voices==4:\n",
        "            loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) + self.loss(score_3, v3)           \n",
        "        else:\n",
        "            loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) \n",
        "        \n",
        "        return loss   #change also to matrix version\n",
        "        \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "            \n",
        "\n",
        "        else:\n",
        "            # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
        "            #predicted = scores_comb.argmax(dim=3)\n",
        "            #return np.squeeze(predicted.cpu().numpy())\n",
        "\n",
        "            sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "            prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            \n",
        "            mask_pred = np.squeeze(sentences)== 0\n",
        "            v_pred_argm[mask_pred] = -1\n",
        "\n",
        "            return v_pred_argm \n",
        "                       "
      ],
      "metadata": {
        "id": "CviiPTPOPW04"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"CNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(4, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "79cPe11WL6J0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "eb3e9b4f-6e6f-4955-99f7-86a4a3333e64"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"CNN\"\\nmonophonic = True\\nhis = start_experiment(4, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Define Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                                   ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]             \n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "                    ### before\n",
        "                    #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                    #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                    #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                    #for i in range(len(prediction[0,:])):\n",
        "                    #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                    #  accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                    #prediction = model.predict(voices, lens, monophonic)                    #for voice vise masking\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)         #for mixed voice masking        \n",
        "\n",
        "\n",
        "                    ## ground truth in shape 1280x88 -> mixed voice\n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    truth = v_ori_argm       \n",
        "\n",
        "                    # outsource accurcy to further down -> just a placeholder right now\n",
        "                    v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            #train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            #train_acc_list[3] = accuracy_sum_list[3] / 18                        ## bc only 18 pieces with len 3\n",
        "            #train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "            #history[\"train_loss\"].append(train_loss)\n",
        "            #history[\"train_acc\"].append(train_acc_list)\n",
        "            #print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "            \n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "                        voices = voices.to(device).float()\n",
        "                        ### before\n",
        "                        #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                        #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                        #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                        #for i in range(len(prediction[0,:])):\n",
        "                        #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                        #  val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    #val_acc_list[3] = val_accuracy_sum_list[3] / 18                         ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_accuracy_sum_list[4] / 2                          ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                #history[\"val_acc\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        #prediction = model.predict(voices, lens, monophonic)                #for voice vise masking\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)     # for masking with mixed voice\n",
        "\n",
        "\n",
        "\n",
        "                        ## ground truth in shape 1280x88 -> mixed voice\n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        truth = v_ori_argm       \n",
        "\n",
        "                        # outsource accurcy to further down -> just a placeholder right now\n",
        "                        v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "                    \n",
        "                history[\"val_acc\"].append(val_accuracy)\n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "    # save the model\n",
        "    #torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"RNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ge8pY70uHxF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "268cc554-cfef-4052-9793-7176ecfa5202"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"RNN\"\\nmonophonic = True\\nhis = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= [\"CNN\",\"RNN\"]\n",
        "monophonic_list = [True,False]\n",
        "\n",
        "for net in network_type:\n",
        "    for monophonic in monophonic_list: \n",
        "        print(\"network set to:\",net,\"monophnic:\",monophonic)\n",
        "        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Bs6-iNEBu8o",
        "outputId": "09a03ec8-d87b-422d-fa7e-df639e835b65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= [\"CNN\",\"RNN\"]\\nmonophonic_list = [True,False]\\n\\nfor net in network_type:\\n    for monophonic in monophonic_list: \\n        print(\"network set to:\",net,\"monophnic:\",monophonic)\\n        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        path_train, path_validation = sklearn.model_selection.train_test_split(PATH_TO_DATA, test_size=0.15, random_state=10,)\n",
        "\n",
        "        print(\"Train and validation lenghts: \", len(path_train), len(path_validation))\n",
        "        train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "        validation_dataset = MusicDataset_new(path_validation) #MusicDataset(path_validation)\n",
        "        \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicNetwork\n",
        "epochs = 10\n",
        "lr = 0.001  \n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Experiment"
      ],
      "metadata": {
        "id": "bdetlQP-LoRX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm",
        "outputId": "d9f6e8e9-615a-4e94-e276-54349e052695",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and validation lenghts:  23 5\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Training on device: cuda\n",
            "monophonic set to: True\n",
            "Train Loss: 2.753731329266618, Train Accuracy : 0.982860921865502\n",
            " Validation Accuracy : 1.9659270419888293\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Train Loss: 1.1512684385950973, Train Accuracy : 0.9838147409081252\n",
            " Validation Accuracy : 1.9679380963208148\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Train Loss: 1.0674631675569022, Train Accuracy : 0.9842201316676794\n",
            " Validation Accuracy : 1.9695958534452447\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Train Loss: 1.0029333437361367, Train Accuracy : 0.9861872829843429\n",
            " Validation Accuracy : 1.973282201728403\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Train Loss: 0.9322703110008705, Train Accuracy : 0.9868959269188547\n",
            " Validation Accuracy : 1.9737084842746002\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Train Loss: 0.7631077257598319, Train Accuracy : 0.9869889207467317\n",
            " Validation Accuracy : 1.9740597895900214\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Train Loss: 0.7291529476642609, Train Accuracy : 0.9871870870865085\n",
            " Validation Accuracy : 1.9742769807018052\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Train Loss: 0.709651546507347, Train Accuracy : 0.9871805816249977\n",
            " Validation Accuracy : 1.9743135881162415\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Train Loss: 0.6942860367821484, Train Accuracy : 0.9872666223690553\n",
            " Validation Accuracy : 1.9745251898077176\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Train Loss: 0.6802513432211992, Train Accuracy : 0.9874020252718873\n",
            " Validation Accuracy : 1.9747843212800245\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy evalutaion F-scores"
      ],
      "metadata": {
        "id": "sJbWsH72N2Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. create folder with part object of all pieces \n",
        "2. load a piece from dataloader with true labels, the mixed piece and the part object \n",
        "3. create notearray from part object\n",
        "4. take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "\n",
        "Output: pianoroll\n",
        "\n",
        "1 note in notearray could be mulitple bins\n",
        "\n",
        "take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "note start at same time with different pitch -> different notes\n",
        "\n",
        "for each note array find corresponding matrix -> \n",
        "\n",
        "\n",
        "if note is only composed by 1 bin: save indx of vocie -> save it to note array\n",
        "\n",
        "if more than 1: look what are idx that compose this note -> majority note -> save it for the note array (if its 50/50 take it random -> count how often this happens) \n",
        "\n",
        "\n",
        "with idx : in note_array find which note corresponds to what voice"
      ],
      "metadata": {
        "id": "CFClch37N6nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "metadata": {
        "id": "afYHFVNMlMnJ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch10.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4TAhTQcpmx8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c680ee-7a6d-400f-f86d-571d3baca62a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MusicNetwork(\n",
              "  (rnn): GRU(88, 300, num_layers=2, batch_first=True)\n",
              "  (cnn): UNET(\n",
              "    (double_conv_downs): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up_trans): ModuleList(\n",
              "      (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (double_conv_ups): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (max_pool_2x2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (final_conv): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (top_layer_voice_0): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_1): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_2): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_3): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (loss): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dic with key:filename, val: part_obj"
      ],
      "metadata": {
        "id": "5RVmMv6Q9CJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_parts = \"AI-MA_project/bach_fugues\"\n",
        "part_dic = {}\n",
        "\n",
        "#### create a list with all filenames in the right order ####\n",
        "file_names_part = []\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    file_names_part.append(filename[3:7])\n",
        "#print(file_names_part)\n",
        "\n",
        "#### create a list with all part objects in the right order ####\n",
        "part_list = []\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    fullname = os.path.join(path_parts, filename)\n",
        "    part = partitura.load_score_midi(fullname)\n",
        "    part_list.append(part)\n",
        "#print(part_list)\n",
        "\n",
        "#### create a dict with keys:filenames , values: part object ####\n",
        "for i in range(len(file_names_part)):\n",
        "      part_dic[file_names_part[i]] = part_list[i]"
      ],
      "metadata": {
        "id": "_XYM_KWu2qkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d7d236b-0ac9-4122-fe6a-fe7647edc351"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=76 velocity=64 time=30\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=78 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=119\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=419\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=72 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=43 velocity=64 time=360\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=299\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=60 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=77 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=75 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=80 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=73 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=68 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=71 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=59 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=56 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=58 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=61 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=60\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=55 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=389\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=2 note=49 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=67 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=66 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=359\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "part_dic.keys(),part_dic.values()"
      ],
      "metadata": {
        "id": "Tt3uHTJY9Ojj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce7aab82-df56-4f60-fa81-d3739b0541dd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dict_keys(['1f01', '1f02', '1f03', '1f04', '1f05', '1f06', '1f07', '1f08', '1f09', '1f10', '1f11', '1f12', '1f13', '1f14', '1f15', '1f16', '1f17', '1f18', '1f19', '1f20', '1f21', '1f22', '1f23', '1f24', '2f01', '2f02', '2f03', '2f04', '2f05', '2f06', '2f07', '2f08', '2f09', '2f10', '2f11', '2f12', '2f13', '2f14', '2f15', '2f16', '2f17', '2f18', '2f19', '2f20', '2f21', '2f22', '2f23', '2f24']),\n",
              " dict_values([[<partitura.score.Part object at 0x7f6d11fd1fd0>, <partitura.score.Part object at 0x7f6d11ee6e50>, <partitura.score.Part object at 0x7f6d11fe8390>, <partitura.score.Part object at 0x7f6d11daac90>], [<partitura.score.Part object at 0x7f6d1491dad0>, <partitura.score.Part object at 0x7f6d11b99850>, <partitura.score.Part object at 0x7f6d11ac3d10>], [<partitura.score.Part object at 0x7f6d117c2290>, <partitura.score.Part object at 0x7f6d118d4850>, <partitura.score.Part object at 0x7f6d115ad750>], [<partitura.score.Part object at 0x7f6d112923d0>, <partitura.score.Part object at 0x7f6d11059250>, <partitura.score.Part object at 0x7f6d10f0f950>, <partitura.score.Part object at 0x7f6d10dd2190>, <partitura.score.Part object at 0x7f6d10ce8d50>], [<partitura.score.Part object at 0x7f6d11906350>, <partitura.score.Part object at 0x7f6d10a6c0d0>, <partitura.score.Part object at 0x7f6d109f1ad0>, <partitura.score.Part object at 0x7f6d10b2fa10>], [<partitura.score.Part object at 0x7f6d11fa77d0>, <partitura.score.Part object at 0x7f6d108870d0>, <partitura.score.Part object at 0x7f6d107e8690>], [<partitura.score.Part object at 0x7f6d105e9e10>, <partitura.score.Part object at 0x7f6d10539090>, <partitura.score.Part object at 0x7f6d103a10d0>], [<partitura.score.Part object at 0x7f6d100e4cd0>, <partitura.score.Part object at 0x7f6d1000b050>, <partitura.score.Part object at 0x7f6a2fe31550>], [<partitura.score.Part object at 0x7f6d1069fe50>, <partitura.score.Part object at 0x7f6a2fb04090>, <partitura.score.Part object at 0x7f6a2fa9f090>], [<partitura.score.Part object at 0x7f6d100ebd50>, <partitura.score.Part object at 0x7f6a2f7e6850>], [<partitura.score.Part object at 0x7f6a2f5f78d0>, <partitura.score.Part object at 0x7f6a2f5adf50>, <partitura.score.Part object at 0x7f6a2f3d73d0>], [<partitura.score.Part object at 0x7f6a2f2b9950>, <partitura.score.Part object at 0x7f6a2f276050>, <partitura.score.Part object at 0x7f6a2f202590>, <partitura.score.Part object at 0x7f6a2fc7e1d0>], [<partitura.score.Part object at 0x7f6a2fc7dc10>, <partitura.score.Part object at 0x7f6a2f281450>, <partitura.score.Part object at 0x7f6a2eb22750>], [<partitura.score.Part object at 0x7f6a2e93d4d0>, <partitura.score.Part object at 0x7f6a2ece4a10>, <partitura.score.Part object at 0x7f6a2e975f50>, <partitura.score.Part object at 0x7f6a2e6414d0>], [<partitura.score.Part object at 0x7f6a2e274990>, <partitura.score.Part object at 0x7f6a2e4088d0>, <partitura.score.Part object at 0x7f6a2e061090>], [<partitura.score.Part object at 0x7f6a2e97c410>, <partitura.score.Part object at 0x7f6a2e51f810>, <partitura.score.Part object at 0x7f6a2dc5d5d0>, <partitura.score.Part object at 0x7f6a2dbf7a50>], [<partitura.score.Part object at 0x7f6a2d9d1590>, <partitura.score.Part object at 0x7f6a2d92a0d0>, <partitura.score.Part object at 0x7f6a2da09fd0>, <partitura.score.Part object at 0x7f6a2d818c90>], [<partitura.score.Part object at 0x7f6d102225d0>, <partitura.score.Part object at 0x7f6a2e930a90>, <partitura.score.Part object at 0x7f6a2d52c550>, <partitura.score.Part object at 0x7f6a2d438090>], [<partitura.score.Part object at 0x7f6a2d1ccc10>, <partitura.score.Part object at 0x7f6a2d204e50>, <partitura.score.Part object at 0x7f6a2d01f410>], [<partitura.score.Part object at 0x7f6a2cb99950>, <partitura.score.Part object at 0x7f6a2cc65050>, <partitura.score.Part object at 0x7f6a2c7db050>, <partitura.score.Part object at 0x7f6a2c6d3150>], [<partitura.score.Part object at 0x7f6a2cde5c90>, <partitura.score.Part object at 0x7f6a2c4776d0>, <partitura.score.Part object at 0x7f6a2c28bed0>], [<partitura.score.Part object at 0x7f6a2d23b0d0>, <partitura.score.Part object at 0x7f6a2bfe4490>, <partitura.score.Part object at 0x7f6a2c026ed0>, <partitura.score.Part object at 0x7f6a2bed5d10>, <partitura.score.Part object at 0x7f6a2be04f50>], [<partitura.score.Part object at 0x7f6a2bd7d890>, <partitura.score.Part object at 0x7f6a2bc2bcd0>, <partitura.score.Part object at 0x7f6a2bb9a710>, <partitura.score.Part object at 0x7f6a2ba85a10>], [<partitura.score.Part object at 0x7f6a2b7429d0>, <partitura.score.Part object at 0x7f6a2b7d60d0>, <partitura.score.Part object at 0x7f6a2b46c0d0>, <partitura.score.Part object at 0x7f6a2b25b050>], [<partitura.score.Part object at 0x7f6a2d1c1650>, <partitura.score.Part object at 0x7f6a2b957910>, <partitura.score.Part object at 0x7f6a2c0483d0>], [<partitura.score.Part object at 0x7f6a2ad140d0>, <partitura.score.Part object at 0x7f6a2ac49950>, <partitura.score.Part object at 0x7f6a2abf0cd0>, <partitura.score.Part object at 0x7f6a2ab25250>], [<partitura.score.Part object at 0x7f6a2b8f9a90>, <partitura.score.Part object at 0x7f6a2b742f50>, <partitura.score.Part object at 0x7f6a2ad66850>], [<partitura.score.Part object at 0x7f6a2a60e990>, <partitura.score.Part object at 0x7f6a2a698910>, <partitura.score.Part object at 0x7f6a2a2c0310>], [<partitura.score.Part object at 0x7f6a2a0be210>, <partitura.score.Part object at 0x7f6a29f1d090>, <partitura.score.Part object at 0x7f6a29ea9850>, <partitura.score.Part object at 0x7f6a29da74d0>], [<partitura.score.Part object at 0x7f6a2a6f8dd0>, <partitura.score.Part object at 0x7f6a29c75c10>, <partitura.score.Part object at 0x7f6a29aaaf90>], [<partitura.score.Part object at 0x7f6a2a6f6290>, <partitura.score.Part object at 0x7f6a298fcb90>, <partitura.score.Part object at 0x7f6a296cd290>, <partitura.score.Part object at 0x7f6a2961c150>], [<partitura.score.Part object at 0x7f6a294f7550>, <partitura.score.Part object at 0x7f6a2948c190>, <partitura.score.Part object at 0x7f6a29292a90>, <partitura.score.Part object at 0x7f6a29149150>], [<partitura.score.Part object at 0x7f6a29022dd0>, <partitura.score.Part object at 0x7f6a298a03d0>, <partitura.score.Part object at 0x7f6a2a04ee50>, <partitura.score.Part object at 0x7f6a28f3e510>], [<partitura.score.Part object at 0x7f6a28a37650>, <partitura.score.Part object at 0x7f6a289b1750>, <partitura.score.Part object at 0x7f6a287be0d0>], [<partitura.score.Part object at 0x7f6a294583d0>, <partitura.score.Part object at 0x7f6a2840c090>, <partitura.score.Part object at 0x7f6a28201090>], [<partitura.score.Part object at 0x7f6a280bea50>, <partitura.score.Part object at 0x7f6a27fc8390>, <partitura.score.Part object at 0x7f6a27d2e050>], [<partitura.score.Part object at 0x7f6a280bb1d0>, <partitura.score.Part object at 0x7f6a279c7f50>, <partitura.score.Part object at 0x7f6a27840890>], [<partitura.score.Part object at 0x7f6a280b6d10>, <partitura.score.Part object at 0x7f6a2902e910>, <partitura.score.Part object at 0x7f6a2724d790>], [<partitura.score.Part object at 0x7f6a275ced50>, <partitura.score.Part object at 0x7f6a26f30d90>, <partitura.score.Part object at 0x7f6a26e06e50>], [<partitura.score.Part object at 0x7f6a28a39d90>, <partitura.score.Part object at 0x7f6a26bd2050>, <partitura.score.Part object at 0x7f6a26a11310>, <partitura.score.Part object at 0x7f6a26869710>], [<partitura.score.Part object at 0x7f6a2901e150>, <partitura.score.Part object at 0x7f6a2a6eb450>, <partitura.score.Part object at 0x7f6a26388090>, <partitura.score.Part object at 0x7f6a262554d0>], [<partitura.score.Part object at 0x7f6a265ed790>, <partitura.score.Part object at 0x7f6a25d5ec50>, <partitura.score.Part object at 0x7f6a25b84350>], [<partitura.score.Part object at 0x7f6a27582410>, <partitura.score.Part object at 0x7f6a2662dfd0>, <partitura.score.Part object at 0x7f6a2579e310>], [<partitura.score.Part object at 0x7f6a255fc790>, <partitura.score.Part object at 0x7f6a28033090>, <partitura.score.Part object at 0x7f6a25458310>], [<partitura.score.Part object at 0x7f6a25213a90>, <partitura.score.Part object at 0x7f6a250dd090>, <partitura.score.Part object at 0x7f6a24f5f2d0>], [<partitura.score.Part object at 0x7f6a24bd54d0>, <partitura.score.Part object at 0x7f6a24d3b390>, <partitura.score.Part object at 0x7f6a24805cd0>, <partitura.score.Part object at 0x7f6a24657ad0>], [<partitura.score.Part object at 0x7f6a252ace10>, <partitura.score.Part object at 0x7f6a28d09f10>, <partitura.score.Part object at 0x7f6a24d71210>, <partitura.score.Part object at 0x7f6a242e7490>], [<partitura.score.Part object at 0x7f6a251671d0>, <partitura.score.Part object at 0x7f6a24aee310>, <partitura.score.Part object at 0x7f6a23e995d0>]]))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "geht gerade nur für monophonic True"
      ],
      "metadata": {
        "id": "v4TJGKiUs086"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#accuracy_sum_list = [0 for i in range(5)]\n",
        "\n",
        "def evaluate_accuracy_for_one(model, train_dataloader, part_dic):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "            if idx == 0:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                note_array_3 = part_3.note_array\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                ground_truth_label_list = [0,1,2,3]\n",
        "                \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    \n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    #print(\"old list\",pitch_list)\n",
        "                    pitch_list = [pitch_list[i]-21 for i in range(len(pitch_list))]   #pitch_list -20\n",
        "                    #print(\"new list\",pitch_list)\n",
        "\n",
        "                    note_idx_start = []\n",
        "                    note_idx_end = []\n",
        "\n",
        "                    for i in range(len(onset_beat)):\n",
        "                        onset = onset_beat[i]\n",
        "                        duration = duration_beat[i]\n",
        "\n",
        "                        note_range = 12 * (onset+duration)\n",
        "                        onset_pr = 12 * onset\n",
        "                        note_idx_start.append(onset_pr)     ## do it faster by array wise operation idx_start = 12 (onset_list+ diration)\n",
        "                        note_idx_end.append(note_range)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                    #print(\"note_idx_start test\",note_idx_start)\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        #prediction = model.predict(voices, lens, monophonic)             #for voice vise masking\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        #print(\"prediction\",prediction.shape)\n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "                        #print(\"truth label:\", label)\n",
        "                \n",
        "                    for i, j ,p in zip(note_idx_start, note_idx_end, pitch_list):\n",
        "                        \n",
        "                        pred_list = prediction[i:j,p]\n",
        "\n",
        "                        #print(\"pred_list\",pred_list)\n",
        "                        truth_list = [label for i in range(len(pred_list))]\n",
        "\n",
        "                        #print(\"truth:\",truth_list)\n",
        "                        #print(\"pred:\",pred_list)\n",
        "                        \n",
        "                        result = all(elem == pred_list[0] for elem in pred_list)\n",
        "                        if result == False:\n",
        "                            #print(\"vote pred list:\", pred_list, p,i,j )\n",
        "\n",
        "                            major, major_idx = torch.mode(pred_list,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list = [major for i in pred_list]\n",
        "                            #print(\"vote pred list:\", pred_list )\n",
        "\n",
        "                        \n",
        "                        #print(sklearn.metrics.f1_score(truth_list, pred_list,average='macro'))\n",
        "                                                 \n",
        "                        total_predictions_dict[str(label)].append(pred_list)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "    return total_predictions_dict , total_truth_dict"
      ],
      "metadata": {
        "id": "KG7ZAlNZGUw7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics"
      ],
      "metadata": {
        "id": "s2WntQHizkfF"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#accuracy_sum_list = [0 for i in range(5)]\n",
        "\n",
        "def evaluate_accuracy_for_one(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "                                      \n",
        "            \n",
        "            #if idx == 0 or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "\n",
        "                if len(part)== 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = part_3.note_array\n",
        "\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                    ground_truth_label_list = [0,1,2,3]              \n",
        "                    total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                    total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                    accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                \n",
        "                if len(part)== 3:\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "                    ground_truth_label_list = [0,1,2]\n",
        "                    total_predictions_dict = {'0': [], '1': [], '2': [] }\n",
        "                    total_truth_dict = {'0': [], '1': [], '2': [] }\n",
        "                    accordance_dict = {'0': [], '1': [], '2': []}\n",
        "\n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        \n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                            \n",
        "\n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                      counting = 0\n",
        "                      for j in range(len(total_predictions_dict[i])):\n",
        "                          if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                      count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                    print(\"accuracy 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"accuracy 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"accuracy 2, sample {}:\".format(idx),acc_2)\n",
        "\n",
        "                    if len(total_predictions_dict.keys())==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"accuracy 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    if len(part)==4:\n",
        "                      pred_3 = accordance_dict[\"3\"]\n",
        "                      truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                      f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                      f_score_dict[\"3\"].append(f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "      return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "      return statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])\n",
        "      #return total_predictions_dict, total_truth_dict"
      ],
      "metadata": {
        "id": "RWxVG3XAYTcC"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_for_one(model,train_dataloader,part_dic,F1=False)\n",
        "acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "Jd2KVdjpktIM",
        "outputId": "3d0e8ef1-8e31-41a7-eb60-ad6d0b0184cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0, sample 0: 0.9953917050691244\n",
            "accuracy 1, sample 0: 0.5247524752475248\n",
            "accuracy 2, sample 0: 0.0\n",
            "accuracy 3, sample 0: 1.0\n",
            "accuracy 0, sample 1: 0.9073756432246999\n",
            "accuracy 1, sample 1: 0.4427710843373494\n",
            "accuracy 2, sample 1: 0.0\n",
            "accuracy 0, sample 2: 0.9672131147540983\n",
            "accuracy 1, sample 2: 0.6942148760330579\n",
            "accuracy 2, sample 2: 0.0\n",
            "accuracy 3, sample 2: 0.9709090909090909\n",
            "accuracy 0, sample 3: 0.9260450160771704\n",
            "accuracy 1, sample 3: 0.5\n",
            "accuracy 2, sample 3: 0.0\n",
            "accuracy 0, sample 4: 0.9314285714285714\n",
            "accuracy 1, sample 4: 0.4852941176470588\n",
            "accuracy 2, sample 4: 0.0\n",
            "accuracy 0, sample 5: 0.9886363636363636\n",
            "accuracy 1, sample 5: 0.7590618336886994\n",
            "accuracy 2, sample 5: 0.0\n",
            "accuracy 0, sample 6: 0.9855595667870036\n",
            "accuracy 1, sample 6: 0.5336134453781513\n",
            "accuracy 2, sample 6: 0.0\n",
            "accuracy 0, sample 7: 0.9431818181818182\n",
            "accuracy 1, sample 7: 0.5493562231759657\n",
            "accuracy 2, sample 7: 0.0\n",
            "accuracy 0, sample 8: 0.9702970297029703\n",
            "accuracy 1, sample 8: 0.6443148688046647\n",
            "accuracy 2, sample 8: 0.0\n",
            "accuracy 3, sample 8: 0.9965034965034965\n",
            "accuracy 0, sample 9: 0.9295774647887324\n",
            "accuracy 1, sample 9: 0.7665198237885462\n",
            "accuracy 2, sample 9: 0.0\n",
            "accuracy 0, sample 10: 1.0\n",
            "accuracy 1, sample 10: 0.6313559322033898\n",
            "accuracy 2, sample 10: 0.0\n",
            "accuracy 3, sample 10: 1.0\n",
            "accuracy 0, sample 11: 0.9400921658986175\n",
            "accuracy 1, sample 11: 0.4563106796116505\n",
            "accuracy 2, sample 11: 0.0\n",
            "accuracy 3, sample 11: 1.0\n",
            "accuracy 0, sample 12: 0.9952380952380953\n",
            "accuracy 1, sample 12: 0.7015706806282722\n",
            "accuracy 2, sample 12: 0.0\n",
            "accuracy 3, sample 12: 1.0\n",
            "accuracy 0, sample 13: 0.9888888888888889\n",
            "accuracy 1, sample 13: 0.6069868995633187\n",
            "accuracy 2, sample 13: 0.0\n",
            "accuracy 3, sample 13: 1.0\n",
            "accuracy 0, sample 14: 0.9838274932614556\n",
            "accuracy 1, sample 14: 0.7321428571428571\n",
            "accuracy 2, sample 14: 0.0\n",
            "accuracy 0, sample 15: 0.9679144385026738\n",
            "accuracy 1, sample 15: 0.4966442953020134\n",
            "accuracy 2, sample 15: 0.0\n",
            "accuracy 0, sample 16: 0.9954337899543378\n",
            "accuracy 1, sample 16: 0.6580310880829016\n",
            "accuracy 2, sample 16: 0.0\n",
            "accuracy 3, sample 16: 0.9848484848484849\n",
            "accuracy 0, sample 17: 0.960377358490566\n",
            "accuracy 1, sample 17: 0.5335892514395394\n",
            "accuracy 2, sample 17: 0.0\n",
            "accuracy 3, sample 17: 1.0\n",
            "accuracy 0, sample 18: 0.9800664451827242\n",
            "accuracy 1, sample 18: 0.6106442577030813\n",
            "accuracy 2, sample 18: 0.0\n",
            "accuracy 0, sample 19: 0.9904306220095693\n",
            "accuracy 1, sample 19: 0.5931372549019608\n",
            "accuracy 2, sample 19: 0.0\n",
            "accuracy 3, sample 19: 0.9803921568627451\n",
            "accuracy 0, sample 20: 0.9801980198019802\n",
            "accuracy 1, sample 20: 0.5770750988142292\n",
            "accuracy 2, sample 20: 0.0\n",
            "accuracy 0, sample 21: 0.9934065934065934\n",
            "accuracy 1, sample 21: 0.8706896551724138\n",
            "accuracy 2, sample 21: 0.0\n",
            "accuracy 0, sample 22: 0.975609756097561\n",
            "accuracy 1, sample 22: 0.640495867768595\n",
            "accuracy 2, sample 22: 0.0\n",
            "accuracy 3, sample 22: 0.9904761904761905\n",
            "accuracy 0, sample 23: 0.9630996309963099\n",
            "accuracy 1, sample 23: 0.5076335877862596\n",
            "accuracy 2, sample 23: 0.0\n",
            "accuracy 0, sample 24: 1.0\n",
            "accuracy 1, sample 24: 0.5497076023391813\n",
            "accuracy 2, sample 24: 0.0\n",
            "accuracy 3, sample 24: 0.9868995633187773\n",
            "accuracy 0, sample 25: 0.9890710382513661\n",
            "accuracy 1, sample 25: 0.4418604651162791\n",
            "accuracy 2, sample 25: 0.0\n",
            "accuracy 3, sample 25: 1.0\n",
            "accuracy 0, sample 26: 0.9939393939393939\n",
            "accuracy 1, sample 26: 0.7\n",
            "accuracy 2, sample 26: 0.0\n",
            "accuracy 3, sample 26: 1.0\n",
            "accuracy 0, sample 27: 0.9808743169398907\n",
            "accuracy 1, sample 27: 0.7795527156549521\n",
            "accuracy 2, sample 27: 0.0\n",
            "accuracy 0, sample 28: 0.9506493506493506\n",
            "accuracy 1, sample 28: 0.6355140186915887\n",
            "accuracy 2, sample 28: 0.0\n",
            "accuracy 0, sample 29: 0.9814814814814815\n",
            "accuracy 1, sample 29: 0.5644699140401146\n",
            "accuracy 2, sample 29: 0.0\n",
            "accuracy 0, sample 30: 0.9691943127962085\n",
            "accuracy 1, sample 30: 0.7041942604856513\n",
            "accuracy 2, sample 30: 0.0\n",
            "accuracy 0, sample 31: 0.9534883720930233\n",
            "accuracy 1, sample 31: 0.5967741935483871\n",
            "accuracy 2, sample 31: 0.0\n",
            "accuracy 0, sample 32: 0.9975845410628019\n",
            "accuracy 1, sample 32: 0.5950617283950618\n",
            "accuracy 2, sample 32: 0.0\n",
            "accuracy 3, sample 32: 1.0\n",
            "accuracy 0, sample 33: 0.9669117647058824\n",
            "accuracy 1, sample 33: 0.4918032786885246\n",
            "accuracy 2, sample 33: 0.0\n",
            "accuracy 3, sample 33: 0.9840425531914894\n",
            "accuracy 0, sample 34: 0.9858299595141701\n",
            "accuracy 1, sample 34: 0.7509803921568627\n",
            "accuracy 2, sample 34: 0.0\n",
            "accuracy 0, sample 35: 0.9911111111111112\n",
            "accuracy 1, sample 35: 0.6441441441441441\n",
            "accuracy 2, sample 35: 0.0\n",
            "accuracy 0, sample 36: 0.9603960396039604\n",
            "accuracy 1, sample 36: 0.4110169491525424\n",
            "accuracy 2, sample 36: 0.0\n",
            "accuracy 0, sample 37: 0.9906832298136646\n",
            "accuracy 1, sample 37: 0.6646525679758308\n",
            "accuracy 2, sample 37: 0.0\n",
            "accuracy 0, sample 38: 0.9866666666666667\n",
            "accuracy 1, sample 38: 0.46308724832214765\n",
            "accuracy 2, sample 38: 0.0\n",
            "accuracy 3, sample 38: 0.9933628318584071\n",
            "accuracy 0, sample 39: 0.9823943661971831\n",
            "accuracy 1, sample 39: 0.5408450704225352\n",
            "accuracy 2, sample 39: 0.0\n",
            "accuracy 3, sample 39: 0.9938837920489296\n",
            "accuracy 0, sample 40: 0.9519230769230769\n",
            "accuracy 1, sample 40: 0.5635179153094463\n",
            "accuracy 2, sample 40: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9729631369055892, 0.60032655167475, 0.0, 0.9934065644454229)"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "A5zTIEuf_D8r",
        "outputId": "93113655-d968-487b-f453-58f5015744e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9729631369055892, 0.60032655167475, 0.0, 0.9934065644454229)"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_v0, f1_v1, f1_v2, f1_v3 = evaluate_accuracy_for_one(model,train_dataloader,part_dic,F1=True)\n",
        "print(f1_v0, f1_v1, f1_v2, f1_v3)"
      ],
      "metadata": {
        "id": "FLLmyO6o5vW5",
        "outputId": "7825cfd3-0721-4d51-de7e-1ee62282fe41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9648944700692857 0.8346029368497071 0.823704309034179 0.9888475836431226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "for gt, i in enumerate(dict_pred.keys()):\n",
        "  counting = 0\n",
        "  for j in range(len(dict_pred[i])):\n",
        "      if dict_pred[i][j][0] == gt:\n",
        "        counting +=1\n",
        "\n",
        "      \n",
        "  count_dict_2[i].append(counting)\n",
        "\n",
        "print(count_dict_2[\"0\"],count_dict_2[\"1\"],count_dict_2[\"2\"],count_dict_2[\"3\"])\n",
        "\n",
        "if len(dict_pred.keys())==4:\n",
        "    print(\"accuracy:\",count_dict_2[\"0\"][0]/len(dict_pred[\"0\"]),count_dict_2[\"1\"][0]/len(dict_pred[\"1\"]),count_dict_2[\"2\"][0]/len(dict_pred[\"2\"]),count_dict_2[\"3\"][0]/len(dict_pred[\"3\"]))\n",
        "\n",
        "if len(dict_pred.keys())==3:\n",
        "    print(\"accuracy:\",count_dict_2[\"0\"][0]/len(dict_pred[\"0\"]),count_dict_2[\"1\"][0]/len(dict_pred[\"1\"]),count_dict_2[\"2\"][0]/len(dict_pred[\"2\"]))"
      ],
      "metadata": {
        "id": "VYBpV_hMfGGY",
        "outputId": "327ab57b-1280-4aa0-da16-890262714944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[529] [147] [0] []\n",
            "accuracy: 0.9073756432246999 0.4427710843373494 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### take 0 -> compare to truth of 0,1,2,3 -> overall voice\n",
        "\n",
        "count_list = []\n",
        "\n",
        "count_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "truth_dic = {'0': 0, '1': 1, '2': 2, '3': 3 }\n",
        "\n",
        "voice_entry_list = [\"0\", \"1\", \"2\", \"3\"]\n",
        "for voice_entry_one in voice_entry_list:\n",
        "    for voice_entry_two in voice_entry_list:\n",
        "        count_list = []\n",
        "        #print(\"voices:\",voice_entry_one,voice_entry_two)\n",
        "        for i in range(len(dict_pred[voice_entry_one])):\n",
        "            if dict_pred[voice_entry_one][i][0] == truth_dic[voice_entry_two]:      #dict_truth[voice_entry_two][i][0]:\n",
        "                count_list.append(1)\n",
        "            else:\n",
        "                count_list.append(0)\n",
        "        count_dict[voice_entry_one].append(count_list)\n",
        "\n",
        "dictionary_sum={}\n",
        "for i in voice_entry_list:\n",
        "    v0_match,v1_match,v2_match,v3_match = count_dict[i]\n",
        "    sum_v0 = np.sum(v0_match)\n",
        "    sum_v1 = np.sum(v1_match)\n",
        "    sum_v2 = np.sum(v2_match)\n",
        "    sum_v3 = np.sum(v3_match)\n",
        "    dictionary_sum[\"v0\"] = sum_v0\n",
        "    dictionary_sum[\"v1\"] = sum_v1\n",
        "    dictionary_sum[\"v2\"] = sum_v2\n",
        "    dictionary_sum[\"v3\"] = sum_v3\n",
        "\n",
        "    val_list = list(dictionary_sum.values())\n",
        "    \n",
        "    print(\"voice{} matches with\".format(i))\n",
        "    print(\"dict\",dictionary_sum)\n",
        "\n",
        "    max_sum = max(sum_v0,sum_v1,sum_v2,sum_v3)\n",
        "\n",
        "\n",
        "    print(\"max_sum\", val_list.index(max_sum) )\n",
        "\n",
        "    print(\"accuracy voice{}:\".format(i), max_sum/(sum_v0+sum_v1+sum_v2+sum_v3) )\n",
        "    print(\"________________\")\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "BoQcV_i038DD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7edfcb34-7d26-4ffe-be4b-6988d699e54c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "voice0 matches with\n",
            "dict {'v0': 236, 'v1': 8, 'v2': 0, 'v3': 0}\n",
            "max_sum 0\n",
            "accuracy voice0: 0.9672131147540983\n",
            "________________\n",
            " \n",
            "voice1 matches with\n",
            "dict {'v0': 33, 'v1': 84, 'v2': 0, 'v3': 4}\n",
            "max_sum 1\n",
            "accuracy voice1: 0.6942148760330579\n",
            "________________\n",
            " \n",
            "voice2 matches with\n",
            "dict {'v0': 10, 'v1': 84, 'v2': 0, 'v3': 46}\n",
            "max_sum 1\n",
            "accuracy voice2: 0.6\n",
            "________________\n",
            " \n",
            "voice3 matches with\n",
            "dict {'v0': 3, 'v1': 5, 'v2': 0, 'v3': 267}\n",
            "max_sum 3\n",
            "accuracy voice3: 0.9709090909090909\n",
            "________________\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR MONOPHONIC F1\n",
        "\n",
        "# start with GT\n",
        "# look at first note in pred-> save note label\n",
        "# look at second note in pred-> if same note as before : SUCESS if it is not: FAIL\n",
        " # DO This for all 4 voices\n",
        " ## in GT there is always the same voice following -> would always be an array of 1\n",
        "\n",
        "## POLYPHONIC \n",
        "\n",
        "# prbl after 1 note there can be multiple diff voices .. chords\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-bR7gcej90qh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you have the ground truth on the different parts that you get when you import your score. Each part correspond to a voice. So if your note array contains all notes of all voices, you have for each note in your note array a number that is the ground truth voice (that you take from the part) and a number that is the predicted voice (that you take from the maximum vote)."
      ],
      "metadata": {
        "id": "Z5q305YzvjMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start time, duration , pitch to separate \n",
        "\n",
        "use the onset_beat and duration_beat\n",
        "\n",
        "multiply them according to the values set when producing the pianorolls \n",
        "\n",
        "-> get the position in the pianoroll\n",
        "\n",
        "time_div = 12\n",
        "\n"
      ],
      "metadata": {
        "id": "EmvxtyaVKG27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "A94mchm4LV6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy_v0\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v1\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v2\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v3\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Accuracy0','Accuracy1','Accuracy2','Accuracy3'])\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TgJDHaxAgYN",
        "outputId": "6c569ed0-deb3-4ac3-b0a9-556d5ed885b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend('Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxMs8GEfMvPE",
        "outputId": "47a681ec-7517-4a28-f5d7-6d6e5905ad79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"train_loss\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqMcJT5aFL01",
        "outputId": "b52c37ff-8697-4afc-8f2d-1a50d6cc7997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old training loop - matrix and non matrix format"
      ],
      "metadata": {
        "id": "4olpdwzyG8dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        #accuracy_v0_sum = 0\n",
        "        #accuracy_v1_sum = 0\n",
        "        #accuracy_v2_sum = 0\n",
        "        #accuracy_v3_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4: \n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                    v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                    mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_pred_flat:\", v_pred_flat.shape)\n",
        "                    \"\"\"\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4:                   \n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                    v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                    mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                    print(\"old mask\", mask_ori.shape)\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_ori_flat\", v_ori_flat.shape)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)   \n",
        "                    print(\"acc\",acc)                    \n",
        "                    accuracy_sum += acc \n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                    acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                    for i in range(len(prediction[0,:])):\n",
        "                      acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                      accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    \n",
        "                    \"\"\"\n",
        "                    pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "\n",
        "                    acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                    acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                    acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                    if nbr_voices == 4:\n",
        "                        acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                    \n",
        "                    # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    if nbr_voices == 4:\n",
        "                        accuracy_v3_sum += acc_v3 / len(lens)\n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "            train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "            train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "            train_accuracy_v3 = accuracy_v3_sum / 18   ## bc only 18 pieces with len 3\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "            history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "            history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "            #if nbr_voices == 4:\n",
        "            history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3)) \n",
        "            #else:\n",
        "            #    print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2)) \n",
        "            \"\"\"\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_v0_sum = 0\n",
        "                accuracy_v1_sum = 0\n",
        "                accuracy_v2_sum = 0\n",
        "                accuracy_v3_sum = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                        truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                        acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                        for i in range(len(prediction[0,:])):\n",
        "                          acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                          val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                        #print(\"val_accuracy_sum_list[3]\",val_accuracy_sum_list[3])\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(train_dataloader)\n",
        "                    #val_acc_list[3] = val_acc_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_acc_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "                #history[\"val_acc_new\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        # Predict the model's output on a batch\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                            \n",
        "                        # compute the accuracy \n",
        "                        acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                        acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                        acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                        if nbr_voices == 4:\n",
        "                            acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                            \n",
        "                            \n",
        "                        # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                        accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                        accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                        accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                        if nbr_voices == 4:\n",
        "                            accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                    val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                    val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                    val_accuracy_v3 = accuracy_v3_sum / 18  ##len(val_dataloader). - bc 18 pieces only with voice 3\n",
        "\n",
        "\n",
        "                    val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    val_acc_list[3] = val_accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    val_acc_list[4] = val_accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                    \n",
        "\n",
        "\n",
        "                history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "                history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "                history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "                #if nbr_voices == 4:\n",
        "                history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "                #else:\n",
        "                #    print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2))\n",
        "\n",
        "\n",
        "                history[\"val_acc_new\"].append(val_acc_list)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "\n",
        "                        \"\"\"\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                        if nbr_voices == 4: \n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                        v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                        mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)                      \n",
        "                        if nbr_voices == 4:                   \n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                        v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                        mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)\n",
        "                        accuracy_sum += acc \n",
        "                        \"\"\"\n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "nfDV8MKGHE3J"
      }
    }
  ]
}