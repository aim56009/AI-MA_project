{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject_final_tensor_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "SsyC2uB0KfaT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb374b49-b02a-49fb-d6d5-828816c173e2"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install partitura\n",
        "! pip install git+https://github.com/CPJKU/partitura.git@develop\n",
        "import partitura\n",
        "import statistics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/CPJKU/partitura.git@develop\n",
            "  Cloning https://github.com/CPJKU/partitura.git (to revision develop) to /tmp/pip-req-build-2amyx3j9\n",
            "  Running command git clone -q https://github.com/CPJKU/partitura.git /tmp/pip-req-build-2amyx3j9\n",
            "  Running command git checkout -b develop --track origin/develop\n",
            "  Switched to a new branch 'develop'\n",
            "  Branch 'develop' set up to track remote branch 'develop' from 'origin'.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.4.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (4.2.6)\n",
            "Collecting lark-parser\n",
            "  Downloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting xmlschema\n",
            "  Downloading xmlschema-1.11.2-py3-none-any.whl (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 12.6 MB/s \n",
            "\u001b[?25hCollecting mido\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting elementpath<3.0.0,>=2.5.0\n",
            "  Downloading elementpath-2.5.3-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 9.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: partitura\n",
            "  Building wheel for partitura (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for partitura: filename=partitura-0.4.0-py3-none-any.whl size=380904 sha256=39c0840727c0b59439f3696aa71d618fddda04cfd0d47b87f45a7391ebe9f3eb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-11d4ceyu/wheels/5b/5c/dd/5df71566a2874d9f172fa71c45ee9d83461b63e81dcb2597c5\n",
            "Successfully built partitura\n",
            "Installing collected packages: elementpath, xmlschema, mido, lark-parser, partitura\n",
            "Successfully installed elementpath-2.5.3 lark-parser-0.12.0 mido-1.2.10 partitura-0.4.0 xmlschema-1.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path_to_musicxml = \"AI-MA_project/chorales_converted/chor001.xml\"\n",
        "#part = partitura.load_musicxml(path_to_musicxml)\n",
        "#partitura.save_score_midi(part,\"chor001_5.mid\",5)"
      ],
      "metadata": {
        "id": "j6dq9ptQGmHB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader - Set the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "fugues = True\n",
        "\n",
        "if fugues == True:\n",
        "    PATH_TO_DATA = \"AI-MA_project/bach_pr_fugues\"\n",
        "else:\n",
        "    PATH_TO_DATA = \"AI-MA_project/pianoroll_88\"\n",
        "\n",
        "batch_size = 1 \n",
        "workers = 0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_new(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        self.name_list = [\"1f01\",\"1f02\",\"1f03\",\"1f04\",\"1f05\",\"1f06\",\"1f07\",\"1f08\",\"1f09\",\"1f10\",\"1f11\",\"1f12\",\"1f13\",\"1f14\",\"1f15\",\"1f16\",\"1f17\",\"1f18\",\"1f19\",\"1f20\",\"1f21\",\"1f22\",\"1f23\",\"1f24\",\"2f01\",\"2f02\",\"2f03\",\"2f04\",\"2f05\",\"2f06\",\"2f07\",\"2f08\",\"2f09\",\"2f10\",\"2f11\",\"2f12\",\"2f13\",\"2f14\",\"2f15\",\"2f16\",\"2f17\",\"2f18\",\"2f19\",\"2f20\",\"2f21\",\"2f22\",\"2f23\",\"2f24\"]\n",
        "        self.name_list_voice_3 =  ['1f01', '1f05', '1f12', '1f14', '1f16', '1f17', '1f18', '1f23', '1f24', '2f02', '2f05', '2f07', '2f08', '2f09', '2f16', '2f17',  '2f22', '2f23']\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "\n",
        "                        file_names_list.append(name[-8:-4])\n",
        "\n",
        "                    if \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[49:53] + \".pkl\" in sorted(glob.glob(os.path.join(PATH_TO_DATA, \"voice_3\", \"*.pkl\"))):\n",
        "                        nbr_voices_list.append(4)\n",
        "                    else:\n",
        "                        nbr_voices_list.append(3)\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "\n",
        "\n",
        "            if iLabel == 3:  \n",
        "                voice_files = []\n",
        "                file_names_3 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[2], \"*.pkl\")))   \n",
        "                \n",
        "                ###### loop over all filnames in voices_2 and if an element there is not present in voices_3: append \"missing\" to the voice_files of label=3 => important bc. self.pr_dict[voice_3] has then len 42 and otherwise it would only have len 18  .. these \"missing\" el are not considered later in the dataloader (if len=3 is a diff case of get_idx)\n",
        "                for name in file_names_2:\n",
        "                    if name[45:49] in self.name_list_voice_3:\n",
        "                      correct_name_3 = \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[45:49] + \".pkl\"\n",
        "                      with open(correct_name_3 ,'rb') as f:  \n",
        "                            loaded_obj = pickle.load(f)  \n",
        "                            voice_files.append(loaded_obj)\n",
        "                    else:\n",
        "                      voice_files.append(\"missing\")\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                \n",
        "\n",
        "                \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                          loaded_obj = pickle.load(f)     \n",
        "                          voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 4:\n",
        "            for key, value in self.pr_dict.items():\n",
        "              out_list.append(self.pr_dict[key][idx])\n",
        "                              \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.tensor(out_list[3].T)\n",
        "            v_all = torch.tensor(out_list[4].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 3:\n",
        "\n",
        "            for key, value in self.pr_dict.items():\n",
        "                if key != \"voice_3\":\n",
        "                  out_list.append(self.pr_dict[key][idx]) \n",
        "            \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.zeros(v2.shape)\n",
        "            v_all = torch.tensor(out_list[3].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "            \n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        "
      ],
      "metadata": {
        "id": "3FxK6qr1FqIl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_chor(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        file_names_list.append(name[-7:-4])      # e.g. name = AI-MA_project/pianoroll_88/voice_all/voice_all_001.pkl\n",
        "\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "              \n",
        "\n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])\n",
        "                            \n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "        file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "        voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "        \n",
        "        return (voices, length, 4, file_name)     # 4 bc nbr voices is always 4"
      ],
      "metadata": {
        "id": "3uQnok3VGngZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "if fugues == True:\n",
        "    dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "else:\n",
        "    dataset = MusicDataset_chor(PATH_TO_DATA)\n",
        "\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 1:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXsRYzQSUuQU",
        "outputId": "b0eff311-6d23-47d5-bb40-d5bcd947c0cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1f03 tensor([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "pianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\n",
        "pianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\n",
        "pianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\n",
        "pianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\n",
        "pianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\n",
        "\n",
        "time_unit = \"beat\"\n",
        "time_div = 12\n",
        "piano_range = True\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fR2HaA_BqeHw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e6e20272-33b4-4ee1-e088-e56fa54660d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\\npianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\\npianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\\npianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\\npianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\\n\\ntime_unit = \"beat\"\\ntime_div = 12\\npiano_range = True\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "fig, ax = plt.subplots(1, figsize=(20, 10))\n",
        "ax.imshow(pianoroll_all, origin=\"lower\", cmap='gray', interpolation='nearest', aspect='auto')\n",
        "ax.set_xlabel(f'Time ({time_unit}s/{time_div})')\n",
        "ax.set_ylabel('Piano key' if piano_range else 'MIDI pitch')\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "sRoyBJJVqX_u",
        "outputId": "c6fbcfef-95b6-47a3-a46c-1da18973e6ae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfig, ax = plt.subplots(1, figsize=(20, 10))\\nax.imshow(pianoroll_all, origin=\"lower\", cmap=\\'gray\\', interpolation=\\'nearest\\', aspect=\\'auto\\')\\nax.set_xlabel(f\\'Time ({time_unit}s/{time_div})\\')\\nax.set_ylabel(\\'Piano key\\' if piano_range else \\'MIDI pitch\\')\\nplt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "  if i ==10:\n",
        "    all_voices, length, nbr_voices, _ = sample_batched\n",
        "    all_voices_pr = all_voices[0,:,:,-1].numpy()\n",
        "    \n",
        "    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\n",
        "    print(note_array.shape)\n",
        "    print(note_array[:10])\n",
        "    print(note_array.dtype.names)\n",
        "\n",
        "    #print(i,nbr_voices,all_voices.shape)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "b4QCaMEi3nw7",
        "outputId": "044d51b3-a645-4b1f-aed8-6e9382b5bbdf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor i, sample_batched in enumerate(loader):\\n    all_voices, length, nbr_voices = sample_batched\\n    if nbr_voices ==3:\\n      print(i,nbr_voices,all_voices.shape)\\n    else:\\n      print(i,nbr_voices)\\n\\nfor i, sample_batched in enumerate(loader):\\n  if i ==10:\\n    all_voices, length, nbr_voices, _ = sample_batched\\n    all_voices_pr = all_voices[0,:,:,-1].numpy()\\n    \\n    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\\n    print(note_array.shape)\\n    print(note_array[:10])\\n    print(note_array.dtype.names)\\n\\n    #print(i,nbr_voices,all_voices.shape)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Music - Model\n"
      ],
      "metadata": {
        "id": "JNqxeacDwxNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UNET "
      ],
      "metadata": {
        "id": "QAIfIM69VHI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "XMdlm0_Vyyhc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_0 = []\n",
        "loss_1 = []\n",
        "loss_2 = []\n",
        "loss_3 = []\n",
        "loss_all = []\n",
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"sum\")                       # use weight parameters maybe take 1/88       \n",
        "\n",
        "    \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                      ### squeeze output here before returning                                       \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()    ### v3 is automatically tensor containing only 0 bc dataloader does this for pieces with 3 voices\n",
        "        \n",
        "        weight_v0 = v0.sum()\n",
        "        weight_v1 = v1.sum()\n",
        "        weight_v2 = v2.sum()\n",
        "        weight_v3 = v3.sum()\n",
        "\n",
        "        stack_tensors_gt = torch.stack([v0, v1, v2, v3], dim=0)\n",
        "        stack_gt = torch.argmax(stack_tensors_gt,axis=0)\n",
        "\n",
        "        weight_tensor = torch.stack([weight_v0,weight_v1,weight_v2,weight_v3])\n",
        "        criterion_weighted = nn.CrossEntropyLoss(weight=weight_tensor,reduction='mean',ignore_index=99)        \n",
        "        stack_pred = torch.stack([score_0, score_1, score_2, score_3], dim=0).softmax(dim=0)[None, :]\n",
        "        \n",
        "        #stack_gt.to(device)\n",
        "        #stack_pred.to(device)\n",
        "        mask_pred = torch.squeeze(voices[:,:,:,-1])== 0\n",
        "        stack_gt[mask_pred] = 99\n",
        "\n",
        "        loss = criterion_weighted(stack_pred, stack_gt[None,:])\n",
        "\n",
        "        print(\"loss:\",loss)\n",
        "        return loss   \n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        stack_pred = torch.stack([score_0, score_1, score_2, score_3], dim=0).softmax(dim=0)[None, :]     ######added softmax\n",
        "        stack_pred = stack_pred.view(1,4,-1)[:,:,voices[:,:,:,-1].bool().flatten()]\n",
        "\n",
        "\n",
        "        stack_tensors_gt = torch.stack([v0, v1, v2, v3], dim=0).cpu().detach().numpy()\n",
        "        stack_gt = torch.tensor(np.argmax(stack_tensors_gt,axis=0)) # [None, :]\n",
        "        gt_where_1 = stack_gt.view(1,-1)[:,voices[:,:,:,-1].bool().flatten()]\n",
        "\n",
        "        gt_where_1.to(device)\n",
        "        stack_pred.to(device)\n",
        "\n",
        "        nbr_relevant_idx = voices[:,:,:,-1].sum()\n",
        "        loss = self.loss(stack_pred, gt_where_1)/ nbr_relevant_idx\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "        \"\"\"\n",
        "        stack_pred = torch.stack([score_0, score_1, score_2, score_3], dim=0).softmax(dim=0)[None, :]     ######added softmax\n",
        "        \n",
        "        print(\"v0 == cuda\", v0.is_cuda)\n",
        "        stack_tensors_gt = torch.stack([v0, v1, v2, v3], dim=0)#.cpu().detach().numpy()\n",
        "        stack_gt = torch.tensor(torch.argmax(stack_tensors_gt,axis=0))[None, :]\n",
        "        \n",
        "\n",
        "        stack_gt.to(device)\n",
        "        stack_pred.to(device)\n",
        "\n",
        "        print(\"stack_pred == cuda\", stack_pred.is_cuda)\n",
        "        print(\"stack_gt == cuda\",stack_gt.is_cuda)\n",
        "\n",
        "        nbr_relevant_idx = voices[:,:,:,-1].sum()\n",
        "        loss = self.loss(stack_pred, stack_gt)/ nbr_relevant_idx\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        #loss_all.append(self.loss(stack_pred, gt_where_1).cpu().detach().numpy())\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "\n",
        "        else:\n",
        "            sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "            prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            \n",
        "            mask_pred = np.squeeze(sentences)== 0\n",
        "            v_pred_argm[mask_pred] = -1\n",
        "\n",
        "            return v_pred_argm        "
      ],
      "metadata": {
        "id": "CviiPTPOPW04"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"CNN\"\n",
        "lr = 0.0001  \n",
        "monophonic = True\n",
        "his = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "79cPe11WL6J0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "28e59f64-6787-48b4-dff1-2bcb8eaf7014"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"CNN\"\\nlr = 0.0001  \\nmonophonic = True\\nhis = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Define Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### versuch hier mal nur für 4 voices zu trainieren\n",
        "\n",
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "                \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            #if nbr_voices == 3:\n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "        print(\"Train Loss: {}\".format(train_loss)) \n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "bGKBso5D92hG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"RNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ge8pY70uHxF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a2969ea4-9455-4c14-96a1-06c651fa393c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"RNN\"\\nmonophonic = True\\nhis = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= [\"CNN\",\"RNN\"]\n",
        "monophonic_list = [True,False]\n",
        "\n",
        "for net in network_type:\n",
        "    for monophonic in monophonic_list: \n",
        "        print(\"network set to:\",net,\"monophnic:\",monophonic)\n",
        "        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Bs6-iNEBu8o",
        "outputId": "645075a6-869b-41ce-af0b-e18aa58ae39f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= [\"CNN\",\"RNN\"]\\nmonophonic_list = [True,False]\\n\\nfor net in network_type:\\n    for monophonic in monophonic_list: \\n        print(\"network set to:\",net,\"monophnic:\",monophonic)\\n        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        if fugues == True:\n",
        "        ### uncomment for fugues ###\n",
        "            train_dataset = MusicDataset_new(PATH_TO_DATA) \n",
        "        ### uncomment for chorals ###\n",
        "        else:\n",
        "            train_dataset = MusicDataset_chor(PATH_TO_DATA) \n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        ### uncomment for fugues ###\n",
        "        if fugues == True:\n",
        "            dataset = MusicDataset_new(PATH_TO_DATA) \n",
        "        ### uncomment for chorals ###\n",
        "        else:\n",
        "            dataset = MusicDataset_chor(PATH_TO_DATA)\n",
        "        \n",
        "        \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicNetwork\n",
        "epochs = 10\n",
        "lr = 0.001 # was 0.001    #0.00001\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Experiment"
      ],
      "metadata": {
        "id": "bdetlQP-LoRX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm",
        "outputId": "80134803-4f2b-42da-bdbd-faa6bc2137b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataloader 34 val_dataloader 7\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Training on device: cuda\n",
            "monophonic set to: True\n",
            "loss: tensor(1.3728, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2979, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3939, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3039, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.4590, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2397, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2096, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1467, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2007, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1475, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1574, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2957, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0790, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0644, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3024, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2252, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2208, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2306, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0557, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9911, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2546, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1854, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2273, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2651, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2078, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1335, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9574, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0237, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3544, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9032, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1092, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0049, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0322, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9109, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 1.1754024256678188\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "loss: tensor(0.8902, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8504, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2137, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9044, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3642, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8481, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9196, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9549, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8814, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8229, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9022, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1990, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9097, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8002, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2176, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0959, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0751, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0695, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9879, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9230, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0829, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1993, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2546, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8065, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8634, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0854, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8385, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8952, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2124, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8761, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0665, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9201, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9428, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8221, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9910482851897969\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "loss: tensor(0.8814, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8112, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1443, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8289, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3137, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8005, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8666, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8638, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8566, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8249, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8439, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1994, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9165, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7778, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2151, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1172, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0837, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0669, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9625, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9116, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0436, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1198, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2128, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8166, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8701, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0838, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8104, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8653, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2502, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8133, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0803, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8807, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9092, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7988, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9659227010081796\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "loss: tensor(0.8085, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7875, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1968, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8111, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3358, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7808, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8473, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8689, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8540, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8145, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8348, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1711, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8850, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7644, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2045, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0938, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0675, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0586, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9443, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9014, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0240, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0794, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1700, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8012, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8368, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0692, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8019, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8601, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2311, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8257, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0648, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8463, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8941, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7967, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9509386315065271\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "loss: tensor(0.8147, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7836, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1570, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8279, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3300, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7947, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8067, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8620, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8567, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8106, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8528, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1462, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8883, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7694, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2050, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1121, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0575, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0516, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0688, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8925, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0226, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0977, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1797, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8179, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8524, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0663, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8065, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8524, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2400, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7966, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0574, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8847, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9007, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7939, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9546135909417096\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.8022, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7836, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0973, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8033, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3338, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7827, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7870, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8586, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8400, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8001, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8480, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1374, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8758, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7641, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2092, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1048, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0744, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0727, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8631, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8368, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0146, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0359, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1428, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8040, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8412, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0452, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7948, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8379, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2084, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8055, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0444, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8511, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8787, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7927, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9344816540970522\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.8310, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7871, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0602, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8081, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3096, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7774, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7866, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8632, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8368, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7992, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8396, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1318, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8749, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7636, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2146, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1152, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0831, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0877, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8562, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8309, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0149, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0395, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1444, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7892, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8310, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0437, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7911, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8354, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2068, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8044, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0412, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8519, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8728, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7926, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9328160110641929\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.8377, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7890, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0537, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8109, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2999, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7789, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7901, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8703, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8332, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7972, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8388, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1287, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8781, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7642, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2151, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1152, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0832, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0872, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8519, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8266, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0164, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0417, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1462, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7850, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8291, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0451, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7919, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8363, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2101, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8008, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0418, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8480, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8723, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7931, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9325791194158441\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.8350, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7882, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0513, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8104, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2943, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7797, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7924, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8728, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8319, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7973, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8399, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1252, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8785, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7638, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2155, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1103, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0783, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0793, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8507, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8240, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0155, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0413, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1445, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7827, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8249, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0411, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7883, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8341, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2094, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7994, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0398, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8423, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8691, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7924, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9307023304350236\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.8311, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7861, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0515, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8074, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2926, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7764, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7871, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8745, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8306, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7975, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8342, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1235, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8792, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7636, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2166, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1100, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0751, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0755, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8501, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8237, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0136, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0354, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1349, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7834, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8215, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0347, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7847, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8309, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2032, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8000, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0350, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8427, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8621, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7897, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9281743782408097\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy evalutaion F-scores"
      ],
      "metadata": {
        "id": "sJbWsH72N2Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. create folder with part object of all pieces \n",
        "2. load a piece from dataloader with true labels, the mixed piece and the part object \n",
        "3. create notearray from part object\n",
        "4. take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "\n",
        "Output: pianoroll\n",
        "\n",
        "1 note in notearray could be mulitple bins\n",
        "\n",
        "take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "note start at same time with different pitch -> different notes\n",
        "\n",
        "for each note array find corresponding matrix -> \n",
        "\n",
        "\n",
        "if note is only composed by 1 bin: save indx of vocie -> save it to note array\n",
        "\n",
        "if more than 1: look what are idx that compose this note -> majority note -> save it for the note array (if its 50/50 take it random -> count how often this happens) \n",
        "\n",
        "\n",
        "with idx : in note_array find which note corresponds to what voice"
      ],
      "metadata": {
        "id": "CFClch37N6nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader "
      ],
      "metadata": {
        "id": "afYHFVNMlMnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b1f6209-089a-4ab0-fd1d-7140f728675c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f5e2d546f90>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## makes cell output nothing\n",
        "%%capture  \n",
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch10.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4TAhTQcpmx8m"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dic with key:filename, val: part_obj  for fugues"
      ],
      "metadata": {
        "id": "5RVmMv6Q9CJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if PATH_TO_DATA == \"AI-MA_project/bach_pr_fugues\":\n",
        "    path_parts = \"AI-MA_project/bach_fugues\"\n",
        "    part_dic = {}\n",
        "\n",
        "    #### create a list with all filenames in the right order ####\n",
        "    file_names_part = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.mid'): continue\n",
        "        file_names_part.append(filename[3:7])\n",
        "    #print(file_names_part)\n",
        "\n",
        "    #### create a list with all part objects in the right order ####\n",
        "    part_list = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.mid'): continue\n",
        "        fullname = os.path.join(path_parts, filename)\n",
        "        part = partitura.load_score_midi(fullname)\n",
        "        part_list.append(part)\n",
        "    #print(part_list)\n",
        "\n",
        "    #### create a dict with keys:filenames , values: part object ####\n",
        "    for i in range(len(file_names_part)):\n",
        "        part_dic[file_names_part[i]] = part_list[i]\n",
        "    \n",
        "    print(part_dic.keys(),part_dic.values())"
      ],
      "metadata": {
        "id": "_XYM_KWu2qkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5fb235-b290-4763-9772-ecc0c70ba770"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=76 velocity=64 time=30\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=78 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=119\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:419: UserWarning: pitch spelling\n",
            "  warnings.warn(\"pitch spelling\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: voice estimation\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: create_part\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add time sigs and measures\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: tie notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: find tuplets\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: done create_part\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=419\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=72 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=43 velocity=64 time=360\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=299\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=60 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=77 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=75 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=80 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=73 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=68 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=71 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=59 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=56 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=58 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=61 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=60\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=55 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=389\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=2 note=49 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=67 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=66 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=359\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['1f01', '1f02', '1f03', '1f04', '1f05', '1f06', '1f07', '1f08', '1f09', '1f10', '1f11', '1f12', '1f13', '1f14', '1f15', '1f16', '1f17', '1f18', '1f19', '1f20', '1f21', '1f22', '1f23', '1f24', '2f01', '2f02', '2f03', '2f04', '2f05', '2f06', '2f07', '2f08', '2f09', '2f10', '2f11', '2f12', '2f13', '2f14', '2f15', '2f16', '2f17', '2f18', '2f19', '2f20', '2f21', '2f22', '2f23', '2f24']) dict_values([[<partitura.score.Part object at 0x7f5e29783950>, <partitura.score.Part object at 0x7f5e296c7210>, <partitura.score.Part object at 0x7f5f70c7ed50>, <partitura.score.Part object at 0x7f5e2957c4d0>], [<partitura.score.Part object at 0x7f5e298721d0>, <partitura.score.Part object at 0x7f5e28fe40d0>, <partitura.score.Part object at 0x7f5e28ecb590>], [<partitura.score.Part object at 0x7f5e2888a990>, <partitura.score.Part object at 0x7f5e283c31d0>, <partitura.score.Part object at 0x7f5e27feb110>], [<partitura.score.Part object at 0x7f5f7ba44590>, <partitura.score.Part object at 0x7f5e31059b50>, <partitura.score.Part object at 0x7f5e22cd63d0>, <partitura.score.Part object at 0x7f5e2a911610>, <partitura.score.Part object at 0x7f5e312351d0>], [<partitura.score.Part object at 0x7f5e31562450>, <partitura.score.Part object at 0x7f5e2ad52050>, <partitura.score.Part object at 0x7f5e2a9cb950>, <partitura.score.Part object at 0x7f5e2a28e210>], [<partitura.score.Part object at 0x7f5e2fcffe90>, <partitura.score.Part object at 0x7f5e296fca50>, <partitura.score.Part object at 0x7f5e30ec44d0>], [<partitura.score.Part object at 0x7f5e30f3e0d0>, <partitura.score.Part object at 0x7f5f70c7e6d0>, <partitura.score.Part object at 0x7f5e29e10990>], [<partitura.score.Part object at 0x7f5e294de850>, <partitura.score.Part object at 0x7f5f70b96990>, <partitura.score.Part object at 0x7f5e28df8050>], [<partitura.score.Part object at 0x7f5e30f3b110>, <partitura.score.Part object at 0x7f5e3072ce50>, <partitura.score.Part object at 0x7f5e31d01110>], [<partitura.score.Part object at 0x7f5e2c196c10>, <partitura.score.Part object at 0x7f5f74a17690>], [<partitura.score.Part object at 0x7f5e2b55b690>, <partitura.score.Part object at 0x7f5e2fd4e190>, <partitura.score.Part object at 0x7f5e2c856910>], [<partitura.score.Part object at 0x7f5e2de990d0>, <partitura.score.Part object at 0x7f5e224f5050>, <partitura.score.Part object at 0x7f5e27a13110>, <partitura.score.Part object at 0x7f5e282a18d0>], [<partitura.score.Part object at 0x7f5e2debfd90>, <partitura.score.Part object at 0x7f5e2bb106d0>, <partitura.score.Part object at 0x7f5e22d2b790>], [<partitura.score.Part object at 0x7f5e2b558550>, <partitura.score.Part object at 0x7f5e31858510>, <partitura.score.Part object at 0x7f5e225b8810>, <partitura.score.Part object at 0x7f5e2f2929d0>], [<partitura.score.Part object at 0x7f5e293b2090>, <partitura.score.Part object at 0x7f5e2bf68090>, <partitura.score.Part object at 0x7f5e30724350>], [<partitura.score.Part object at 0x7f5e2db7fad0>, <partitura.score.Part object at 0x7f5e270686d0>, <partitura.score.Part object at 0x7f5e293b2f90>, <partitura.score.Part object at 0x7f5e26e2b150>], [<partitura.score.Part object at 0x7f5e27135310>, <partitura.score.Part object at 0x7f5e27500d10>, <partitura.score.Part object at 0x7f5f71143110>, <partitura.score.Part object at 0x7f5f71118a90>], [<partitura.score.Part object at 0x7f5e2713b510>, <partitura.score.Part object at 0x7f5e2fb090d0>, <partitura.score.Part object at 0x7f5f713326d0>, <partitura.score.Part object at 0x7f5e27b7f190>], [<partitura.score.Part object at 0x7f5e2c4a0d10>, <partitura.score.Part object at 0x7f5e29b956d0>, <partitura.score.Part object at 0x7f5e30d71650>], [<partitura.score.Part object at 0x7f5e2dec7b90>, <partitura.score.Part object at 0x7f5e31264090>, <partitura.score.Part object at 0x7f5e2eed4050>, <partitura.score.Part object at 0x7f5e2f856f90>], [<partitura.score.Part object at 0x7f5e31d73f10>, <partitura.score.Part object at 0x7f5e2dedd290>, <partitura.score.Part object at 0x7f5f718204d0>], [<partitura.score.Part object at 0x7f5e2db0a610>, <partitura.score.Part object at 0x7f5e2c301990>, <partitura.score.Part object at 0x7f5f710d4810>, <partitura.score.Part object at 0x7f5f70eb6550>, <partitura.score.Part object at 0x7f5f718988d0>], [<partitura.score.Part object at 0x7f5e22be0150>, <partitura.score.Part object at 0x7f5e2d548690>, <partitura.score.Part object at 0x7f5e31190050>, <partitura.score.Part object at 0x7f5e2270d250>], [<partitura.score.Part object at 0x7f5e22d46f90>, <partitura.score.Part object at 0x7f5e2ea27050>, <partitura.score.Part object at 0x7f5e2d4f9050>, <partitura.score.Part object at 0x7f5e228c4090>], [<partitura.score.Part object at 0x7f5e29084810>, <partitura.score.Part object at 0x7f5e28742d50>, <partitura.score.Part object at 0x7f5e227bee10>], [<partitura.score.Part object at 0x7f5e295356d0>, <partitura.score.Part object at 0x7f5e2f9be910>, <partitura.score.Part object at 0x7f5e2c8b4310>, <partitura.score.Part object at 0x7f5e2ed05790>], [<partitura.score.Part object at 0x7f5e2f8b3190>, <partitura.score.Part object at 0x7f5f7488d590>, <partitura.score.Part object at 0x7f5e2ab12850>], [<partitura.score.Part object at 0x7f5e2c10f110>, <partitura.score.Part object at 0x7f5e2f05e910>, <partitura.score.Part object at 0x7f5e2e520350>], [<partitura.score.Part object at 0x7f5f718f4250>, <partitura.score.Part object at 0x7f5e28e30f10>, <partitura.score.Part object at 0x7f5e2a457110>, <partitura.score.Part object at 0x7f5e2d474110>], [<partitura.score.Part object at 0x7f5e317b56d0>, <partitura.score.Part object at 0x7f5e2c92e2d0>, <partitura.score.Part object at 0x7f5e2b8a1690>], [<partitura.score.Part object at 0x7f5e317bab50>, <partitura.score.Part object at 0x7f5f713fa450>, <partitura.score.Part object at 0x7f5e2f108f50>, <partitura.score.Part object at 0x7f5e2d892b10>], [<partitura.score.Part object at 0x7f5e303b5f10>, <partitura.score.Part object at 0x7f5e31103b10>, <partitura.score.Part object at 0x7f5f71367850>, <partitura.score.Part object at 0x7f5f71caae90>], [<partitura.score.Part object at 0x7f5f718f2f90>, <partitura.score.Part object at 0x7f5e2d4a14d0>, <partitura.score.Part object at 0x7f5f74455750>, <partitura.score.Part object at 0x7f5e319f6cd0>], [<partitura.score.Part object at 0x7f5f71d17f50>, <partitura.score.Part object at 0x7f5e2d8ca650>, <partitura.score.Part object at 0x7f5e28d1c0d0>], [<partitura.score.Part object at 0x7f5e29a0f310>, <partitura.score.Part object at 0x7f5e2a6e6f50>, <partitura.score.Part object at 0x7f5f7144d210>], [<partitura.score.Part object at 0x7f5e2aca2bd0>, <partitura.score.Part object at 0x7f5e2ac8f690>, <partitura.score.Part object at 0x7f5f74dd2ad0>], [<partitura.score.Part object at 0x7f5f745d8cd0>, <partitura.score.Part object at 0x7f5f74622550>, <partitura.score.Part object at 0x7f5f744332d0>], [<partitura.score.Part object at 0x7f5f71efb890>, <partitura.score.Part object at 0x7f5f71e454d0>, <partitura.score.Part object at 0x7f5f70f8b0d0>], [<partitura.score.Part object at 0x7f5e29587d10>, <partitura.score.Part object at 0x7f5f71df3110>, <partitura.score.Part object at 0x7f5e29a0fc10>], [<partitura.score.Part object at 0x7f5f705b0f90>, <partitura.score.Part object at 0x7f5e2cd3fdd0>, <partitura.score.Part object at 0x7f5e31c06510>, <partitura.score.Part object at 0x7f5e319228d0>], [<partitura.score.Part object at 0x7f5f706bb450>, <partitura.score.Part object at 0x7f5e30e1a590>, <partitura.score.Part object at 0x7f5e30cde090>, <partitura.score.Part object at 0x7f5e30b2d790>], [<partitura.score.Part object at 0x7f5e303cca90>, <partitura.score.Part object at 0x7f5e30110050>, <partitura.score.Part object at 0x7f5e2fa53650>], [<partitura.score.Part object at 0x7f5e303ce690>, <partitura.score.Part object at 0x7f5e2efd4050>, <partitura.score.Part object at 0x7f5e2ee74bd0>], [<partitura.score.Part object at 0x7f5e2eb82290>, <partitura.score.Part object at 0x7f5e2e90c6d0>, <partitura.score.Part object at 0x7f5e2e7e3e90>], [<partitura.score.Part object at 0x7f5e2de3b650>, <partitura.score.Part object at 0x7f5e2e1f0fd0>, <partitura.score.Part object at 0x7f5e2d824690>], [<partitura.score.Part object at 0x7f5e2cbacbd0>, <partitura.score.Part object at 0x7f5e2cd41a10>, <partitura.score.Part object at 0x7f5e2c80a290>, <partitura.score.Part object at 0x7f5e2f2fe610>], [<partitura.score.Part object at 0x7f5e2c283090>, <partitura.score.Part object at 0x7f5e2c098f10>, <partitura.score.Part object at 0x7f5e2bcf4450>, <partitura.score.Part object at 0x7f5e2b24a090>], [<partitura.score.Part object at 0x7f5e2d2bf610>, <partitura.score.Part object at 0x7f5e2b31aed0>, <partitura.score.Part object at 0x7f5e2a323810>]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dic with key:filename, val: part_obj  for chorales"
      ],
      "metadata": {
        "id": "6D9oTp_lNQbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if PATH_TO_DATA == \"AI-MA_project/pianoroll_88\":\n",
        "    path_parts = \"AI-MA_project/chorales_converted\"\n",
        "    part_dic = {}\n",
        "\n",
        "    #### create a list with all filenames in the right order ####\n",
        "    file_names_part = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.xml'): continue\n",
        "        file_names_part.append(filename[4:7])\n",
        "    #print(file_names_part)\n",
        "\n",
        "    #### create a list with all part objects in the right order ####\n",
        "    part_list = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.xml'): continue\n",
        "        fullname = os.path.join(path_parts, filename)\n",
        "        part = partitura.load_musicxml(fullname)\n",
        "        part_list.append(part)\n",
        "    #print(part_list)\n",
        "\n",
        "    #### create a dict with keys:filenames , values: part object ####\n",
        "    for i in range(len(file_names_part)):\n",
        "        part_dic[file_names_part[i]] = part_list[i]\n",
        "    \n",
        "    print(\"part_dic.keys()\",part_dic.keys())\n",
        "    print(\"part_dic.values()\",part_dic.values())"
      ],
      "metadata": {
        "id": "_4q58c16NjbE"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate chorales"
      ],
      "metadata": {
        "id": "yphGmsr-NSV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate chorals"
      ],
      "metadata": {
        "id": "v4TJGKiUs086"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_one_choral(model, train_dataloader, part_dic,F1):\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match                                      \n",
        "            \n",
        "            #if idx > 40: # or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "                part_3 = part[3]\n",
        "\n",
        "                #note_array_0 = part_0.note_array\n",
        "                #note_array_1 = part_1.note_array\n",
        "                #note_array_2 = part_2.note_array\n",
        "                #note_array_3 = part_3.note_array                \n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                   \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]                   \n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        \n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "                        else:\n",
        "                            accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                        counting = 0\n",
        "                        ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                        for j in range(len(total_predictions_dict[i])):\n",
        "                            if total_predictions_dict[i][j][0] == gt:\n",
        "                                counting +=1  \n",
        "                        count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                    acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                    print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                    \n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "                    acc_score_dict[\"3\"].append(acc_3)\n",
        "                \n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    print(\"f1_v0 , sample {}:\".format(idx),f1_v0)\n",
        "                    print(\"f1_v1 , sample {}:\".format(idx),f1_v1)\n",
        "                    print(\"f1_v2 , sample {}:\".format(idx),f1_v2)\n",
        "                    \n",
        "                    if len(part)==4:\n",
        "                        pred_3 = accordance_dict[\"3\"]\n",
        "                        truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                        f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                        f_score_dict[\"3\"].append(f1_v3)\n",
        "                        print(\"f1_v3 , sample {}:\".format(idx),f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "UXr2DeiLSyLm"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_one_choral(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "ghAmXcXTTtD1"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    plt.plot(acc_score_dict[\"0\"],'-o')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend(['Accuracy0'])\n",
        "    plt.title('Accuracy vs Epochs')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "J92mdaQG0MXZ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate on F1 meassure"
      ],
      "metadata": {
        "id": "zYMy2JARxEBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    acc_0 , acc_1, acc_2, acc_3 = evaluate_one_choral(model,val_dataloader,part_dic,F1=True)\n",
        "    acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "RWxVG3XAYTcC"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate fugues"
      ],
      "metadata": {
        "id": "TzTpXHznL02j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import statistics\n",
        "\n",
        "\n",
        "def evaluate_accuracy_for_all(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "                                      \n",
        "                print(\"nbr_voices:\",nbr_voices)\n",
        "            #if idx == 0 or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "                if len(part)== 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                   \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        \n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "                            \n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                      counting = 0\n",
        "                      ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                      for j in range(len(total_predictions_dict[i])):\n",
        "\n",
        "\n",
        "                          if total_predictions_dict[i][j][0] == 3:\n",
        "                            print(\"prediction/gt:\",total_predictions_dict[i][j][0],gt)\n",
        "\n",
        "\n",
        "                          if total_predictions_dict[i][j][0] == gt:                     \n",
        "                            counting +=1  \n",
        "                      count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "\n",
        "                    if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    print(\"f1_v0 , sample {}:\".format(idx),f1_v0)\n",
        "                    print(\"f1_v1 , sample {}:\".format(idx),f1_v1)\n",
        "                    print(\"f1_v2 , sample {}:\".format(idx),f1_v2)\n",
        "                    if len(part)==4:\n",
        "                      pred_3 = accordance_dict[\"3\"]\n",
        "                      truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                      f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                      f_score_dict[\"3\"].append(f1_v3)\n",
        "                      print(\"f1_v3 , sample {}:\".format(idx),f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])\n",
        "        #return total_predictions_dict, total_truth_dict"
      ],
      "metadata": {
        "id": "0xbN5YU8nGT0"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True:\n",
        "    dict_pred , dict_gt, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "20MP5Gk5kc2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1827a6ab-cd96-4a9a-ba91-b8f441358d17"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nbr_voices: tensor([3])\n",
            "acc 0, sample 0: 0.9549763033175356\n",
            "acc 1, sample 0: 0.847682119205298\n",
            "acc 2, sample 0: 0.9904761904761905\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 1: 0.9857142857142858\n",
            "acc 1, sample 1: 0.9162303664921466\n",
            "acc 2, sample 1: 0.7426160337552743\n",
            "acc 3, sample 1: 0.0\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 2: 0.9522058823529411\n",
            "acc 1, sample 2: 0.6627634660421545\n",
            "acc 2, sample 2: 0.6892857142857143\n",
            "acc 3, sample 2: 0.0\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 3: 0.9016393442622951\n",
            "acc 1, sample 3: 0.7851239669421488\n",
            "acc 2, sample 3: 0.7714285714285715\n",
            "acc 3, sample 3: 0.0\n",
            "nbr_voices: tensor([3])\n",
            "acc 0, sample 4: 0.966996699669967\n",
            "acc 1, sample 4: 0.7786561264822134\n",
            "acc 2, sample 4: 0.9724409448818898\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 5: 0.9377358490566038\n",
            "acc 1, sample 5: 0.7581573896353166\n",
            "acc 2, sample 5: 0.6964705882352941\n",
            "acc 3, sample 5: 0.0\n",
            "nbr_voices: tensor([3])\n",
            "acc 0, sample 6: 0.946843853820598\n",
            "acc 1, sample 6: 0.8543417366946778\n",
            "acc 2, sample 6: 0.9973333333333333\n",
            "total_predictions_dict dict_keys(['0', '1', '2', '3'])\n",
            "0.9494446025991752 0.8004221673562794 0.8371501966280382 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, train_dataloader, part_dic,F1):\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader): \n",
        "        print(\"nbr_voices:\",nbr_voices)\n",
        "\n",
        "test(model,val_dataloader,part_dic,F1=False)"
      ],
      "metadata": {
        "id": "p0HZ9d5TO_Aj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0da0d35f-2e1d-4c39-868e-284aa2105f64"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nbr_voices: tensor([3])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([3])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss3 * 1.5 = (0.8989219661919758, 0.7166572856993837, 0.8185917288474246, 0.0)"
      ],
      "metadata": {
        "id": "6nwnRTADQvQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 epoch, no loss modifier:\n",
        "#ACC:(0.9165209182020722,\n",
        "# 0.7864434689151618,\n",
        "# 0.8130949796045199,\n",
        "# 0.003652274754166715)\n",
        "\n",
        "# 20 epoch, no loss modifier:\n",
        "#(0.7962210840410273, 0.8669639629052727, 0.751302181991106, 0.0)\n",
        "\n",
        "# 20 ep, loss3 *1,5\n",
        "#(0.8721136343927623,\n",
        "# 0.8319586824413445,\n",
        "# 0.7563218924966578,\n",
        "# 0.09029535181592076)"
      ],
      "metadata": {
        "id": "6mCYnJLnHfYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate fugues F1 score"
      ],
      "metadata": {
        "id": "52P6em3ANb1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True:\n",
        "    f1_v0, f1_v1, f1_v2, f1_v3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=True)\n",
        "    print(f1_v0, f1_v1, f1_v2, f1_v3)"
      ],
      "metadata": {
        "id": "FLLmyO6o5vW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### take 0 -> compare to truth of 0,1,2,3 -> overall voice\n",
        "\n",
        "count_list = []\n",
        "\n",
        "count_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "truth_dic = {'0': 0, '1': 1, '2': 2, '3': 3 }\n",
        "\n",
        "voice_entry_list = [\"0\", \"1\", \"2\", \"3\"]\n",
        "for voice_entry_one in voice_entry_list:\n",
        "    for voice_entry_two in voice_entry_list:\n",
        "        count_list = []\n",
        "        #print(\"voices:\",voice_entry_one,voice_entry_two)\n",
        "        for i in range(len(dict_pred[voice_entry_one])):\n",
        "            if dict_pred[voice_entry_one][i][0] == truth_dic[voice_entry_two]:      #dict_truth[voice_entry_two][i][0]:\n",
        "                count_list.append(1)\n",
        "            else:\n",
        "                count_list.append(0)\n",
        "        count_dict[voice_entry_one].append(count_list)\n",
        "\n",
        "dictionary_sum={}\n",
        "for i in voice_entry_list:\n",
        "    v0_match,v1_match,v2_match,v3_match = count_dict[i]\n",
        "    sum_v0 = np.sum(v0_match)\n",
        "    sum_v1 = np.sum(v1_match)\n",
        "    sum_v2 = np.sum(v2_match)\n",
        "    sum_v3 = np.sum(v3_match)\n",
        "    dictionary_sum[\"v0\"] = sum_v0\n",
        "    dictionary_sum[\"v1\"] = sum_v1\n",
        "    dictionary_sum[\"v2\"] = sum_v2\n",
        "    dictionary_sum[\"v3\"] = sum_v3\n",
        "\n",
        "    val_list = list(dictionary_sum.values())\n",
        "    \n",
        "    print(\"voice{} matches with\".format(i))\n",
        "    print(\"dict\",dictionary_sum)\n",
        "\n",
        "    max_sum = max(sum_v0,sum_v1,sum_v2,sum_v3)\n",
        "\n",
        "\n",
        "    print(\"max_sum\", val_list.index(max_sum) )\n",
        "\n",
        "    print(\"accuracy voice{}:\".format(i), max_sum/(sum_v0+sum_v1+sum_v2+sum_v3) )\n",
        "    print(\"________________\")\n",
        "    print(\" \")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BoQcV_i038DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR MONOPHONIC F1\n",
        "\n",
        "# start with GT\n",
        "# look at first note in pred-> save note label\n",
        "# look at second note in pred-> if same note as before : SUCESS if it is not: FAIL\n",
        " # DO This for all 4 voices\n",
        " ## in GT there is always the same voice following -> would always be an array of 1\n",
        "\n",
        "## POLYPHONIC \n",
        "\n",
        "# prbl after 1 note there can be multiple diff voices .. chords"
      ],
      "metadata": {
        "id": "-bR7gcej90qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you have the ground truth on the different parts that you get when you import your score. Each part correspond to a voice. So if your note array contains all notes of all voices, you have for each note in your note array a number that is the ground truth voice (that you take from the part) and a number that is the predicted voice (that you take from the maximum vote)."
      ],
      "metadata": {
        "id": "Z5q305YzvjMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start time, duration , pitch to separate \n",
        "\n",
        "use the onset_beat and duration_beat\n",
        "\n",
        "multiply them according to the values set when producing the pianorolls \n",
        "\n",
        "-> get the position in the pianoroll\n",
        "\n",
        "time_div = 12\n",
        "\n"
      ],
      "metadata": {
        "id": "EmvxtyaVKG27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "A94mchm4LV6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy_v0\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v1\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v2\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v3\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Accuracy0','Accuracy1','Accuracy2','Accuracy3'])\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TgJDHaxAgYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend('Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxMs8GEfMvPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"train_loss\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqMcJT5aFL01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chew chorals\n",
        "\n"
      ],
      "metadata": {
        "id": "fS4tzYkxr06Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_chew_pr (file_name, sentences):\n",
        "    path = \"AI-MA_project/chorales_converted/\"\n",
        "    fullname = os.path.join(path, \"chor\"+ file_name +\".xml\")\n",
        "    part = partitura.load_musicxml(fullname)\n",
        "        \n",
        "\n",
        "    ### apply chews method ### \n",
        "    chew_sep = partitura.musicanalysis.estimate_voices(part, monophonic_voices=True)\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_zero = np.where(chew_sep==1)\n",
        "    pos_one = np.where(chew_sep==2)\n",
        "    pos_two = np.where(chew_sep==3)\n",
        "    pos_three = np.where(chew_sep==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    part_zero = partitura.utils.ensure_notearray(part)[pos_zero]\n",
        "    part_one = partitura.utils.ensure_notearray(part)[pos_one]\n",
        "    part_two = partitura.utils.ensure_notearray(part)[pos_two]\n",
        "    part_three = partitura.utils.ensure_notearray(part)[pos_three]\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    pr_zero = partitura.utils.compute_pianoroll(part_zero, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(part_one, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(part_two, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    pr_three = partitura.utils.compute_pianoroll(part_three, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_three = pr_three.toarray()\n",
        "    \n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    #print(\"scores_comb.shape\",scores_comb.shape)\n",
        "    #print(\"sentences[:,None,:,:].shape\",sentences[:,None,:,:].shape)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "    \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "55-Dy39Cwg5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_chew(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    #note_counter_0 = 0\n",
        "    #note_counter_1 = 0\n",
        "    #note_counter_2 = 0\n",
        "    #note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx != 26 and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                #note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                #note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                #note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                #note_array_3 = part_3.note_array\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "                #note_counter_0 += len(note_array_0)\n",
        "                #note_counter_1 += len(note_array_1)\n",
        "                #note_counter_2 += len(note_array_2)\n",
        "                #note_counter_3 += len(note_array_3)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################            \n",
        "                    prediction = calculate_chew_pr(file_name,voices[:,:,:,-1]) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    #print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "NVvLm9pdryYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sampel 26&27 dont work "
      ],
      "metadata": {
        "id": "yD0w6nJQ82iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_chew(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "UD11ziChvL_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# McLeod chorals"
      ],
      "metadata": {
        "id": "9ejZujau-TXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mcleod_pr (file_name, sentences):\n",
        "    path = \"AI-MA_project/hmm_sep/\"\n",
        "    fullname = os.path.join(path, \"chor\"+ file_name +\".mid\")\n",
        "        \n",
        "\n",
        "    ### apply chews method ### \n",
        "    part_hmm = partitura.load_score_midi(fullname,part_voice_assign_mode=2)\n",
        "    voice_info = partitura.utils.note_array_from_part(part_hmm)[\"voice\"]\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(voice_info==1)\n",
        "    pos_1 = np.where(voice_info==2)\n",
        "    pos_2 = np.where(voice_info==3)\n",
        "    pos_3 = np.where(voice_info==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    note_array_0= partitura.utils.ensure_notearray(part_hmm)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part_hmm)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part_hmm)[pos_2]\n",
        "    note_array_3 = partitura.utils.ensure_notearray(part_hmm)[pos_3]\n",
        "\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_three = pr_three.toarray()\n",
        "    \n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    #print(\"scores_comb.shape\",scores_comb.shape)\n",
        "    #print(\"sentences[:,None,:,:].shape\",sentences[:,None,:,:].shape)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "    \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "pQP8pq9a-S6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mc_leod(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx not in [16,17,18,27,28,32,44,45,48,49,50]: # and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                #note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                #note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                #note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                #note_array_3 = part_3.note_array\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################                   \n",
        "                    prediction = calculate_mcleod_pr(file_name,voices[:,:,:,-1]) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "2igkR5SI-Z1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "does not work for samples 16,17,18,27,28,32,44,45,48,49,50"
      ],
      "metadata": {
        "id": "g1JC5yeMBnnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_mc_leod(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "oxRr270dC_mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chew fugues"
      ],
      "metadata": {
        "id": "uenr6Uavaic3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_chew_pr_fugue (file_name, sentences, nbr_voices):\n",
        "    path = \"AI-MA_project/bach_fugues/\"\n",
        "    fullname = os.path.join(path, \"wtc\"+ file_name +\".mid\")\n",
        "\n",
        "    part = partitura.load_score_midi(fullname)\n",
        "\n",
        "\n",
        "    ### apply chews method ### \n",
        "    chew_sep = partitura.musicanalysis.estimate_voices(part, monophonic_voices=True)\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(chew_sep==1)\n",
        "    pos_1 = np.where(chew_sep==2)\n",
        "    pos_2 = np.where(chew_sep==3)\n",
        "    if nbr_voices ==4:\n",
        "        pos_3 = np.where(chew_sep==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    \n",
        "    note_array_0 = partitura.utils.ensure_notearray(part)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part)[pos_2]\n",
        "    if nbr_voices==4:\n",
        "        note_array_3 = partitura.utils.ensure_notearray(part)[pos_3]\n",
        "\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    onset_beat_0 = note_array_0['onset_beat'][-1]\n",
        "    duration_beat_0 = note_array_0['duration_beat'][-1]\n",
        "    beat_0 = onset_beat_0 + duration_beat_0\n",
        "    \n",
        "    onset_beat_1 = note_array_1['onset_beat'][-1]\n",
        "    duration_beat_1 = note_array_1['duration_beat'][-1]\n",
        "    beat_1 = onset_beat_1 + duration_beat_1\n",
        "    \n",
        "    onset_beat_2 = note_array_2['onset_beat'][-1]\n",
        "    duration_beat_2 = note_array_2['duration_beat'][-1]\n",
        "    beat_2 = onset_beat_2 + duration_beat_2\n",
        "\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_0)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_1)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_2)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    if nbr_voices==4:\n",
        "        onset_beat_3 = note_array_3['onset_beat'][-1]\n",
        "        duration_beat_3 = note_array_3['duration_beat'][-1]\n",
        "        beat_3 = onset_beat_3 + duration_beat_3\n",
        "        pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_3)\n",
        "        pr_three = pr_three.toarray()\n",
        "    else:\n",
        "        pr_three = np.zeros(pr_two.shape)\n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0)) \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "vNsE3VVMa9Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_chew_fugue( train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx != 26 and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                \n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "\n",
        "                if len(part) == 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################\n",
        "                    prediction = calculate_chew_pr_fugue(file_name,voices[:,:,:,-1],nbr_voices) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "C50fX7xjasD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True: \n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_chew_fugue(val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "yWz4I9b7asVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# McLeod fugues"
      ],
      "metadata": {
        "id": "xofnsEX4DAME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mcleod_pr (file_name, sentences,nbr_voices):\n",
        "    path = \"AI-MA_project/bach_fugues/\"\n",
        "    fullname = os.path.join(path, \"wtc\"+ file_name +\".mid\")\n",
        "    ### apply chews method ### \n",
        "    part_hmm = partitura.load_score_midi(fullname,part_voice_assign_mode=2)\n",
        "    voice_info = partitura.utils.note_array_from_part(part_hmm)[\"voice\"]\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(voice_info==1)\n",
        "    pos_1 = np.where(voice_info==2)\n",
        "    pos_2 = np.where(voice_info==3)\n",
        "    if nbr_voices ==4:\n",
        "        pos_3 = np.where(voice_info==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    note_array_0= partitura.utils.ensure_notearray(part_hmm)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part_hmm)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part_hmm)[pos_2]\n",
        "    if nbr_voices==4:\n",
        "        note_array_3 = partitura.utils.ensure_notearray(part_hmm)[pos_3]\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    onset_beat_0 = note_array_0['onset_beat'][-1]\n",
        "    duration_beat_0 = note_array_0['duration_beat'][-1]\n",
        "    beat_0 = onset_beat_0 + duration_beat_0\n",
        "    \n",
        "    onset_beat_1 = note_array_1['onset_beat'][-1]\n",
        "    duration_beat_1 = note_array_1['duration_beat'][-1]\n",
        "    beat_1 = onset_beat_1 + duration_beat_1\n",
        "    \n",
        "    onset_beat_2 = note_array_2['onset_beat'][-1]\n",
        "    duration_beat_2 = note_array_2['duration_beat'][-1]\n",
        "    beat_2 = onset_beat_2 + duration_beat_2\n",
        "\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_0)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_1)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_2)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    if nbr_voices==4:\n",
        "        onset_beat_3 = note_array_3['onset_beat'][-1]\n",
        "        duration_beat_3 = note_array_3['duration_beat'][-1]\n",
        "        beat_3 = onset_beat_3 + duration_beat_3\n",
        "        pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_3)\n",
        "        pr_three = pr_three.toarray()\n",
        "    else:\n",
        "        pr_three = np.zeros(pr_two.shape)\n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0)) \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "CU4iH-25aD5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mc_leod_fugues(train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            #if idx not in [16,17,18,27,28,32,44,45,48,49,50]: # and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "\n",
        "                if len(part) == 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "           \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "          \n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################                   \n",
        "                    prediction = calculate_mcleod_pr(file_name,voices[:,:,:,-1],nbr_voices) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "DgFfq9Q4Csei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True: \n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_mc_leod_fugues(val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "5kSrFU8e-uol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old training loop with acc computation (wrong)"
      ],
      "metadata": {
        "id": "4olpdwzyG8dQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                                   ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "                \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            #if nbr_voices == 4:\n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]             \n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "                    ### before\n",
        "                    #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                    #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                    #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                    #for i in range(len(prediction[0,:])):\n",
        "                    #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                    #  accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                    #prediction = model.predict(voices, lens, monophonic)                    #for voice vise masking\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)         #for mixed voice masking        \n",
        "\n",
        "\n",
        "                    ## ground truth in shape 1280x88 -> mixed voice\n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    truth = v_ori_argm       \n",
        "\n",
        "                    # outsource accurcy to further down -> just a placeholder right now\n",
        "                    v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            #train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            #train_acc_list[3] = accuracy_sum_list[3] / 18                        ## bc only 18 pieces with len 3\n",
        "            #train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "            #history[\"train_loss\"].append(train_loss)\n",
        "            #history[\"train_acc\"].append(train_acc_list)\n",
        "            #print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "            \n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "                        voices = voices.to(device).float()\n",
        "                        ### before\n",
        "                        #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                        #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                        #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                        #for i in range(len(prediction[0,:])):\n",
        "                        #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                        #  val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    #val_acc_list[3] = val_accuracy_sum_list[3] / 18                         ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_accuracy_sum_list[4] / 2                          ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                #history[\"val_acc\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        #prediction = model.predict(voices, lens, monophonic)                #for voice vise masking\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)     # for masking with mixed voice\n",
        "\n",
        "\n",
        "\n",
        "                        ## ground truth in shape 1280x88 -> mixed voice\n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        truth = v_ori_argm       \n",
        "\n",
        "                        # outsource accurcy to further down -> just a placeholder right now\n",
        "                        v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "                    \n",
        "                history[\"val_acc\"].append(val_accuracy)\n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    # save the model\n",
        "    #torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eAdXIq3cAe8q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}