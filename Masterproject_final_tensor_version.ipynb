{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject_final_tensor_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "SsyC2uB0KfaT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1",
        "outputId": "a790d76e-e959-4611-c9d9-31948364d747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install partitura\n",
        "import partitura"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: partitura in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from partitura) (4.2.6)\n",
            "Requirement already satisfied: mido in /usr/local/lib/python3.7/dist-packages (from partitura) (1.2.10)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.21.6)\n",
            "Requirement already satisfied: xmlschema in /usr/local/lib/python3.7/dist-packages (from partitura) (1.11.1)\n",
            "Requirement already satisfied: lark-parser in /usr/local/lib/python3.7/dist-packages (from partitura) (0.12.0)\n",
            "Requirement already satisfied: elementpath<3.0.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from xmlschema->partitura) (2.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d811791d-eea6-4c5c-99ed-4899aa8ab944"
      },
      "source": [
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AI-MA_project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "batch_size = 1 \n",
        "PATH_TO_DATA = \"AI-MA_project/bach_pr_fugues\"\n",
        "#PATH_TO_DATA = \"AI-MA_project/pianoroll_88\"\n",
        "workers = 0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1-u8zYd-gyo"
      },
      "source": [
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "    \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "  \n",
        "\n",
        "    def __getitem__(self, idx):          \n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])    \n",
        "\n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "\n",
        "\n",
        "        return (v0, v1, v2, v3, v_all, length)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_new(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        self.name_list = [\"1f01\",\"1f02\",\"1f03\",\"1f04\",\"1f05\",\"1f06\",\"1f07\",\"1f08\",\"1f09\",\"1f10\",\"1f11\",\"1f12\",\"1f13\",\"1f14\",\"1f15\",\"1f16\",\"1f17\",\"1f18\",\"1f19\",\"1f20\",\"1f21\",\"1f22\",\"1f23\",\"1f24\",\"2f01\",\"2f02\",\"2f03\",\"2f04\",\"2f05\",\"2f06\",\"2f07\",\"2f08\",\"2f09\",\"2f10\",\"2f11\",\"2f12\",\"2f13\",\"2f14\",\"2f15\",\"2f16\",\"2f17\",\"2f18\",\"2f19\",\"2f20\",\"2f21\",\"2f22\",\"2f23\",\"2f24\"]\n",
        "        self.name_list_voice_3 =  ['1f01', '1f05', '1f12', '1f14', '1f16', '1f17', '1f18', '1f23', '1f24', '2f02', '2f05', '2f07', '2f08', '2f09', '2f16', '2f17',  '2f22', '2f23']\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "\n",
        "                        file_names_list.append(name[-8:-4])\n",
        "\n",
        "                    if \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[49:53] + \".pkl\" in sorted(glob.glob(os.path.join(PATH_TO_DATA, \"voice_3\", \"*.pkl\"))):\n",
        "                        nbr_voices_list.append(4)\n",
        "                    else:\n",
        "                        nbr_voices_list.append(3)\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "\n",
        "\n",
        "            if iLabel == 3:  \n",
        "                voice_files = []\n",
        "                file_names_3 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[2], \"*.pkl\")))   \n",
        "                \n",
        "                ###### loop over all filnames in voices_2 and if an element there is not present in voices_3: append \"missing\" to the voice_files of label=3 => important bc. self.pr_dict[voice_3] has then len 42 and otherwise it would only have len 18  .. these \"missing\" el are not considered later in the dataloader (if len=3 is a diff case of get_idx)\n",
        "                for name in file_names_2:\n",
        "                    if name[45:49] in self.name_list_voice_3:\n",
        "                      correct_name_3 = \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[45:49] + \".pkl\"\n",
        "                      with open(correct_name_3 ,'rb') as f:  \n",
        "                            loaded_obj = pickle.load(f)  \n",
        "                            voice_files.append(loaded_obj)\n",
        "                    else:\n",
        "                      voice_files.append(\"missing\")\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                \n",
        "\n",
        "                \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                          loaded_obj = pickle.load(f)     \n",
        "                          voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 4:\n",
        "            for key, value in self.pr_dict.items():\n",
        "              out_list.append(self.pr_dict[key][idx])\n",
        "                              \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.tensor(out_list[3].T)\n",
        "            v_all = torch.tensor(out_list[4].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 3:\n",
        "\n",
        "            for key, value in self.pr_dict.items():\n",
        "                if key != \"voice_3\":\n",
        "                  out_list.append(self.pr_dict[key][idx]) \n",
        "            \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.zeros(v2.shape)\n",
        "            v_all = torch.tensor(out_list[3].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "            \n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        "
      ],
      "metadata": {
        "id": "3FxK6qr1FqIl"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "#dataset = MusicDataset(PATH_TO_DATA)\n",
        "dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices, file_name = sample_batched\n",
        "    print(file_name)\n"
      ],
      "metadata": {
        "id": "LXsRYzQSUuQU",
        "outputId": "86a5f374-5baa-4c7d-c325-769cf7f44374",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('1f01',)\n",
            "('1f03',)\n",
            "('1f05',)\n",
            "('1f06',)\n",
            "('1f07',)\n",
            "('1f08',)\n",
            "('1f09',)\n",
            "('1f11',)\n",
            "('1f12',)\n",
            "('1f13',)\n",
            "('1f14',)\n",
            "('1f16',)\n",
            "('1f17',)\n",
            "('1f18',)\n",
            "('1f19',)\n",
            "('1f21',)\n",
            "('1f23',)\n",
            "('1f24',)\n",
            "('2f01',)\n",
            "('2f02',)\n",
            "('2f03',)\n",
            "('2f04',)\n",
            "('2f05',)\n",
            "('2f06',)\n",
            "('2f07',)\n",
            "('2f08',)\n",
            "('2f09',)\n",
            "('2f11',)\n",
            "('2f12',)\n",
            "('2f13',)\n",
            "('2f14',)\n",
            "('2f15',)\n",
            "('2f16',)\n",
            "('2f17',)\n",
            "('2f18',)\n",
            "('2f19',)\n",
            "('2f20',)\n",
            "('2f21',)\n",
            "('2f22',)\n",
            "('2f23',)\n",
            "('2f24',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "  if i ==10:\n",
        "    all_voices, length, nbr_voices, _ = sample_batched\n",
        "    all_voices_pr = all_voices[0,:,:,-1].numpy()\n",
        "    \n",
        "    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\n",
        "    print(note_array.shape)\n",
        "    print(note_array[:10])\n",
        "    print(note_array.dtype.names)\n",
        "\n",
        "    #print(i,nbr_voices,all_voices.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4QCaMEi3nw7",
        "outputId": "bdca0717-6280-41bd-c0db-7ed367a0ad91"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2868,)\n",
            "[(0, 2.75, 0.08333334, 1) (1, 2.75, 0.08333334, 1)\n",
            " (2, 2.75, 0.08333334, 1) (3, 2.75, 0.08333334, 1)\n",
            " (4, 2.75, 0.08333334, 1) (5, 2.75, 0.08333334, 1)\n",
            " (6, 2.75, 0.08333334, 1) (7, 2.75, 0.08333334, 1)\n",
            " (8, 2.75, 0.08333334, 1) (9, 2.75, 0.08333334, 1)]\n",
            "('pitch', 'onset_beat', 'duration_beat', 'velocity')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Music - Model\n"
      ],
      "metadata": {
        "id": "JNqxeacDwxNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UNET "
      ],
      "metadata": {
        "id": "QAIfIM69VHI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "XMdlm0_Vyyhc"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "          \n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                                                            \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()\n",
        "\n",
        "        if nbr_voices==4:\n",
        "            loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) + self.loss(score_3, v3)           \n",
        "        else:\n",
        "            loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) \n",
        "        \n",
        "        return loss\n",
        "        \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "            \n",
        "\n",
        "        else:\n",
        "            # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
        "            #print(\"predictmethod\",scores_comb.shape)\n",
        "            predicted = scores_comb.argmax(dim=3)\n",
        "            return np.squeeze(predicted.cpu().numpy())"
      ],
      "metadata": {
        "id": "CviiPTPOPW04"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"RNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(2, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "79cPe11WL6J0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "16b901d9-6edd-4062-8882-e39a8da5fe0e"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"RNN\"\\nmonophonic = True\\nhis = start_experiment(2, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Define Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                                   ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                    acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                    for i in range(len(prediction[0,:])):\n",
        "                      acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                      accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    \"\"\"\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                    comp_list = [0,0,0,0]\n",
        "                    idx_list = [-1,-1,-1,-1]\n",
        "                    voices_v = [0,0,0,0]\n",
        "\n",
        "                    for i in range(4):\n",
        "                      for j in range(4):\n",
        "                        intermed = sum(prediction[:,i] == truth[:,j])\n",
        "                        if intermed > comp_list[i] :\n",
        "                          comp_list[i] = intermed\n",
        "                          idx_list[i] = j\n",
        "                          voices_v[i] = intermed / len(prediction[:,i])\n",
        "                    avc = 100 * sum(voices_v) / 4\n",
        "                    \"\"\"\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                        ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                        truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                        acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                        for i in range(len(prediction[0,:])):\n",
        "                          acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                          val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    val_acc_list[3] = val_accuracy_sum_list[3] / 18                         ## bc only 18 pieces with len 3\n",
        "                    val_acc_list[4] = val_accuracy_sum_list[4] / 2                          ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "                history[\"val_acc\"].append(val_acc_list)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "    # save the model\n",
        "    #torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#network_type= \"RNN\"\n",
        "#monophonic = True\n",
        "#his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "metadata": {
        "id": "ge8pY70uHxF9"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= [\"CNN\",\"RNN\"]\n",
        "monophonic_list = [True,False]\n",
        "\n",
        "for net in network_type:\n",
        "    for monophonic in monophonic_list: \n",
        "        print(\"network set to:\",net,\"monophnic:\",monophonic)\n",
        "        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Bs6-iNEBu8o",
        "outputId": "357ec1d7-3696-40ab-e6a4-1125e002c05f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= [\"CNN\",\"RNN\"]\\nmonophonic_list = [True,False]\\n\\nfor net in network_type:\\n    for monophonic in monophonic_list: \\n        print(\"network set to:\",net,\"monophnic:\",monophonic)\\n        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        path_train, path_validation = sklearn.model_selection.train_test_split(PATH_TO_DATA, test_size=0.15, random_state=10,)\n",
        "\n",
        "        print(\"Train and validation lenghts: \", len(path_train), len(path_validation))\n",
        "        train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "        validation_dataset = MusicDataset_new(path_validation) #MusicDataset(path_validation)\n",
        "        \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicNetwork\n",
        "epochs = 1\n",
        "lr = 0.001  \n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train + valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Experiment"
      ],
      "metadata": {
        "id": "bdetlQP-LoRX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm",
        "outputId": "260cad55-1001-41e8-c2d3-b892dac1b3f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and validation lenghts:  23 5\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Training on device: cuda\n",
            "monophonic set to: True\n",
            "Train Loss: 8.171084095792073, Train Accuracy_0 : 0.6097271391610395, Train Accuracy_1 : 0.5057788278599922,Train Accuracy_2 : 0.45231095841076574, Train Accuracy_3 : 0.27450934522422543, Train Accuracy_4 : 0.0\n",
            " Validation Accuracy_0 : 0.8001004338235765, Validation Accuracy_1 : 0.6689595430938469, Validation Accuracy_2 : 0.5696251888349994, Validation Accuracy_3 : 0.5475513578220581, Validation Accuracy_4 : 0.0\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy evalutaion F-scores"
      ],
      "metadata": {
        "id": "sJbWsH72N2Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. create folder with part object of all pieces \n",
        "2. load a piece from dataloader with true labels, the mixed piece and the part object \n",
        "3. create notearray from part object\n",
        "4. take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "\n",
        "Output: pianoroll\n",
        "\n",
        "1 note in notearray could be mulitple bins\n",
        "\n",
        "take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "note start at same time with different pitch -> different notes\n",
        "\n",
        "for each note array find corresponding matrix -> \n",
        "\n",
        "\n",
        "if note is only composed by 1 bin: save indx of vocie -> save it to note array\n",
        "\n",
        "if more than 1: look what are idx that compose this note -> majority note -> save it for the note array (if its 50/50 take it random -> count how often this happens) \n",
        "\n",
        "\n",
        "with idx : in note_array find which note corresponds to what voice"
      ],
      "metadata": {
        "id": "CFClch37N6nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "metadata": {
        "id": "afYHFVNMlMnJ"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch1.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4TAhTQcpmx8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "geht gerade nur für monophonic True"
      ],
      "metadata": {
        "id": "v4TJGKiUs086"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_sum_list = [0 for i in range(5)]\n",
        "\n",
        "def evaluate_accuracy(model, train_dataloader, part_objects=True):\n",
        "    # angenommen die files kommen geordnet aus dem val_dataloader -> wichtig: voice muss mit part_object matchen also zb. Voice_fugue48 zusammen mit Part_fugue48\n",
        "    \n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):  \n",
        "            model.eval()\n",
        "            voices = voices.to(device).float()\n",
        "            monophonic=True\n",
        "            with torch.no_grad():\n",
        "\n",
        "                prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "\n",
        "                \n",
        "                acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                \n",
        "                \n",
        "                for i in range(len(prediction[0,:])):\n",
        "                  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                  accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "                \n",
        "                #print(accuracy_sum_list)\n",
        "            print(file_name)\n",
        "\n",
        "    \n",
        "    train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "    \n",
        "    train_acc_list[3] = accuracy_sum_list[3] / 18                        ## bc only 18 pieces with len 3\n",
        "    train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "    return train_acc_list"
      ],
      "metadata": {
        "id": "EH3o-VHpN4op"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_accuracy(model,train_dataloader)"
      ],
      "metadata": {
        "id": "oG0MpEImjGzV",
        "outputId": "8c48325c-cc68-4b74-db35-3ef2fc91d339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('1f01',)\n",
            "('1f03',)\n",
            "('1f05',)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-ee0bbf4af021>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-133-6400a6d16aa7>\u001b[0m in \u001b[0;36mevaluate_accuracy\u001b[0;34m(model, train_dataloader, part_objects)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonophonic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-ff81cee08546>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentences, sentences_len, monophonic)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Compute the outputs from the linear units.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mscores_comb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmonophonic\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-ff81cee08546>\u001b[0m in \u001b[0;36mcompute_outputs\u001b[0;34m(self, sentences, sentences_len)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m           \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-117-69b725fc1feb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mconcatenated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdouble_conv_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"wtc1f01.mid\"\n",
        "\n",
        "\n",
        "import_path = os.path.join(\"AI-MA_project/bach_fugues\", name)\n",
        "#part = partitura.io.importmidi.load_midi(import_path)\n",
        "part = partitura.load_score_midi(import_path)"
      ],
      "metadata": {
        "id": "PbH6BGSbERTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "part_zero = part[0]\n",
        "part_one = part[1]\n",
        "part_two = part[2]\n",
        "part_three = part[3]"
      ],
      "metadata": {
        "id": "uy_khyqJLyki"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "note_array = part_zero.note_array\n",
        "note_array\n",
        "\n",
        "print(note_array.shape)\n",
        "print(note_array[:10])\n",
        "print(note_array.dtype.names)"
      ],
      "metadata": {
        "id": "eH_yK_CMLkys",
        "outputId": "09953912-3192-4fa8-b204-008b59a84508",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(217,)\n",
            "[( 6.   , 0.5  ,  6.   , 0.5  ,  720, 60, 67, 1, 'n0')\n",
            " ( 6.5  , 0.5  ,  6.5  , 0.5  ,  780, 60, 69, 1, 'n1')\n",
            " ( 7.   , 0.5  ,  7.   , 0.5  ,  840, 60, 71, 1, 'n2')\n",
            " ( 7.5  , 0.75 ,  7.5  , 0.75 ,  900, 90, 72, 1, 'n3')\n",
            " ( 8.25 , 0.125,  8.25 , 0.125,  990, 15, 74, 1, 'n4')\n",
            " ( 8.375, 0.125,  8.375, 0.125, 1005, 15, 72, 1, 'n5')\n",
            " ( 8.5  , 0.5  ,  8.5  , 0.5  , 1020, 60, 71, 1, 'n6')\n",
            " ( 9.   , 0.5  ,  9.   , 0.5  , 1080, 60, 76, 1, 'n7')\n",
            " ( 9.5  , 0.5  ,  9.5  , 0.5  , 1140, 60, 69, 1, 'n8')\n",
            " (10.   , 0.75 , 10.   , 0.75 , 1200, 90, 74, 1, 'n9')]\n",
            "('onset_beat', 'duration_beat', 'onset_quarter', 'duration_quarter', 'onset_div', 'duration_div', 'pitch', 'voice', 'id')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "A94mchm4LV6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy_v0\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v1\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v2\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v3\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Accuracy0','Accuracy1','Accuracy2','Accuracy3'])\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TgJDHaxAgYN",
        "outputId": "e21e28ac-6702-4b34-d3d9-59b4917a6a47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend('Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxMs8GEfMvPE",
        "outputId": "d094798a-610b-4f70-b6eb-01d806ab0427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"train_loss\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqMcJT5aFL01",
        "outputId": "f3e5030d-50d7-4f82-8547-2a460fdf10bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old training loop - matrix and non matrix format"
      ],
      "metadata": {
        "id": "4olpdwzyG8dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        #accuracy_v0_sum = 0\n",
        "        #accuracy_v1_sum = 0\n",
        "        #accuracy_v2_sum = 0\n",
        "        #accuracy_v3_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4: \n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                    v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                    mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_pred_flat:\", v_pred_flat.shape)\n",
        "                    \"\"\"\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4:                   \n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                    v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                    mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                    print(\"old mask\", mask_ori.shape)\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_ori_flat\", v_ori_flat.shape)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)   \n",
        "                    print(\"acc\",acc)                    \n",
        "                    accuracy_sum += acc \n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                    acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                    for i in range(len(prediction[0,:])):\n",
        "                      acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                      accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    \n",
        "                    \"\"\"\n",
        "                    pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "\n",
        "                    acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                    acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                    acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                    if nbr_voices == 4:\n",
        "                        acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                    \n",
        "                    # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    if nbr_voices == 4:\n",
        "                        accuracy_v3_sum += acc_v3 / len(lens)\n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "            train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "            train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "            train_accuracy_v3 = accuracy_v3_sum / 18   ## bc only 18 pieces with len 3\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "            history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "            history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "            #if nbr_voices == 4:\n",
        "            history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3)) \n",
        "            #else:\n",
        "            #    print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2)) \n",
        "            \"\"\"\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_v0_sum = 0\n",
        "                accuracy_v1_sum = 0\n",
        "                accuracy_v2_sum = 0\n",
        "                accuracy_v3_sum = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                        truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                        acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                        for i in range(len(prediction[0,:])):\n",
        "                          acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                          val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                        #print(\"val_accuracy_sum_list[3]\",val_accuracy_sum_list[3])\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(train_dataloader)\n",
        "                    #val_acc_list[3] = val_acc_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_acc_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "                #history[\"val_acc_new\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        # Predict the model's output on a batch\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                            \n",
        "                        # compute the accuracy \n",
        "                        acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                        acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                        acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                        if nbr_voices == 4:\n",
        "                            acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                            \n",
        "                            \n",
        "                        # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                        accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                        accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                        accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                        if nbr_voices == 4:\n",
        "                            accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                    val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                    val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                    val_accuracy_v3 = accuracy_v3_sum / 18  ##len(val_dataloader). - bc 18 pieces only with voice 3\n",
        "\n",
        "\n",
        "                    val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    val_acc_list[3] = val_accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    val_acc_list[4] = val_accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                    \n",
        "\n",
        "\n",
        "                history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "                history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "                history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "                #if nbr_voices == 4:\n",
        "                history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "                #else:\n",
        "                #    print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2))\n",
        "\n",
        "\n",
        "                history[\"val_acc_new\"].append(val_acc_list)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "\n",
        "                        \"\"\"\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                        if nbr_voices == 4: \n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                        v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                        mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)                      \n",
        "                        if nbr_voices == 4:                   \n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                        v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                        mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)\n",
        "                        accuracy_sum += acc \n",
        "                        \"\"\"\n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "nfDV8MKGHE3J"
      }
    }
  ]
}