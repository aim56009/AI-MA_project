{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject_final_tensor_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "SsyC2uB0KfaT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d64643-a5cd-4416-c4f1-bb334af112ce"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install partitura\n",
        "! pip install git+https://github.com/CPJKU/partitura.git@develop\n",
        "import partitura\n",
        "import statistics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/CPJKU/partitura.git@develop\n",
            "  Cloning https://github.com/CPJKU/partitura.git (to revision develop) to /tmp/pip-req-build-oj1dxah2\n",
            "  Running command git clone -q https://github.com/CPJKU/partitura.git /tmp/pip-req-build-oj1dxah2\n",
            "  Running command git checkout -b develop --track origin/develop\n",
            "  Switched to a new branch 'develop'\n",
            "  Branch 'develop' set up to track remote branch 'develop' from 'origin'.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.4.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (4.2.6)\n",
            "Collecting lark-parser\n",
            "  Downloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting xmlschema\n",
            "  Downloading xmlschema-1.11.2-py3-none-any.whl (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 47.7 MB/s \n",
            "\u001b[?25hCollecting mido\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting elementpath<3.0.0,>=2.5.0\n",
            "  Downloading elementpath-2.5.3-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 53.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: partitura\n",
            "  Building wheel for partitura (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for partitura: filename=partitura-0.4.0-py3-none-any.whl size=380904 sha256=8f16f463ea3887a0eb098c5037670d1f60e0a2c919341ab7ea77d4a13974fda7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qlf2wj33/wheels/5b/5c/dd/5df71566a2874d9f172fa71c45ee9d83461b63e81dcb2597c5\n",
            "Successfully built partitura\n",
            "Installing collected packages: elementpath, xmlschema, mido, lark-parser, partitura\n",
            "Successfully installed elementpath-2.5.3 lark-parser-0.12.0 mido-1.2.10 partitura-0.4.0 xmlschema-1.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path_to_musicxml = \"AI-MA_project/chorales_converted/chor001.xml\"\n",
        "#part = partitura.load_musicxml(path_to_musicxml)\n",
        "#partitura.save_score_midi(part,\"chor001_5.mid\",5)"
      ],
      "metadata": {
        "id": "j6dq9ptQGmHB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader - Set the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "fugues = False\n",
        "\n",
        "if fugues == True:\n",
        "    PATH_TO_DATA = \"AI-MA_project/bach_pr_fugues\"\n",
        "else:\n",
        "    PATH_TO_DATA = \"AI-MA_project/pianoroll_88\"\n",
        "\n",
        "batch_size = 1 \n",
        "workers = 0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_new(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        self.name_list = [\"1f01\",\"1f02\",\"1f03\",\"1f04\",\"1f05\",\"1f06\",\"1f07\",\"1f08\",\"1f09\",\"1f10\",\"1f11\",\"1f12\",\"1f13\",\"1f14\",\"1f15\",\"1f16\",\"1f17\",\"1f18\",\"1f19\",\"1f20\",\"1f21\",\"1f22\",\"1f23\",\"1f24\",\"2f01\",\"2f02\",\"2f03\",\"2f04\",\"2f05\",\"2f06\",\"2f07\",\"2f08\",\"2f09\",\"2f10\",\"2f11\",\"2f12\",\"2f13\",\"2f14\",\"2f15\",\"2f16\",\"2f17\",\"2f18\",\"2f19\",\"2f20\",\"2f21\",\"2f22\",\"2f23\",\"2f24\"]\n",
        "        self.name_list_voice_3 =  ['1f01', '1f05', '1f12', '1f14', '1f16', '1f17', '1f18', '1f23', '1f24', '2f02', '2f05', '2f07', '2f08', '2f09', '2f16', '2f17',  '2f22', '2f23']\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "\n",
        "                        file_names_list.append(name[-8:-4])\n",
        "\n",
        "                    if \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[49:53] + \".pkl\" in sorted(glob.glob(os.path.join(PATH_TO_DATA, \"voice_3\", \"*.pkl\"))):\n",
        "                        nbr_voices_list.append(4)\n",
        "                    else:\n",
        "                        nbr_voices_list.append(3)\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "\n",
        "\n",
        "            if iLabel == 3:  \n",
        "                voice_files = []\n",
        "                file_names_3 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[2], \"*.pkl\")))   \n",
        "                \n",
        "                ###### loop over all filnames in voices_2 and if an element there is not present in voices_3: append \"missing\" to the voice_files of label=3 => important bc. self.pr_dict[voice_3] has then len 42 and otherwise it would only have len 18  .. these \"missing\" el are not considered later in the dataloader (if len=3 is a diff case of get_idx)\n",
        "                for name in file_names_2:\n",
        "                    if name[45:49] in self.name_list_voice_3:\n",
        "                      correct_name_3 = \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[45:49] + \".pkl\"\n",
        "                      with open(correct_name_3 ,'rb') as f:  \n",
        "                            loaded_obj = pickle.load(f)  \n",
        "                            voice_files.append(loaded_obj)\n",
        "                    else:\n",
        "                      voice_files.append(\"missing\")\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                \n",
        "\n",
        "                \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                          loaded_obj = pickle.load(f)     \n",
        "                          voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 4:\n",
        "            for key, value in self.pr_dict.items():\n",
        "              out_list.append(self.pr_dict[key][idx])\n",
        "                              \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.tensor(out_list[3].T)\n",
        "            v_all = torch.tensor(out_list[4].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 3:\n",
        "\n",
        "            for key, value in self.pr_dict.items():\n",
        "                if key != \"voice_3\":\n",
        "                  out_list.append(self.pr_dict[key][idx]) \n",
        "            \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.zeros(v2.shape)\n",
        "            v_all = torch.tensor(out_list[3].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "            \n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        "
      ],
      "metadata": {
        "id": "3FxK6qr1FqIl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_chor(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        file_names_list.append(name[-7:-4])      # e.g. name = AI-MA_project/pianoroll_88/voice_all/voice_all_001.pkl\n",
        "\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "              \n",
        "\n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])\n",
        "                            \n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "        file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "        voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "        \n",
        "        return (voices, length, 4, file_name)     # 4 bc nbr voices is always 4"
      ],
      "metadata": {
        "id": "3uQnok3VGngZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "if fugues == True:\n",
        "    dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "else:\n",
        "    dataset = MusicDataset_chor(PATH_TO_DATA)\n",
        "\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 1:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXsRYzQSUuQU",
        "outputId": "758232c1-305c-435c-fbe6-aedb08b5c664"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "002 tensor([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\n",
        "pianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\n",
        "pianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\n",
        "pianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\n",
        "pianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\n",
        "\n",
        "time_unit = \"beat\"\n",
        "time_div = 12\n",
        "piano_range = True"
      ],
      "metadata": {
        "id": "fR2HaA_BqeHw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, figsize=(20, 10))\n",
        "ax.imshow(pianoroll_all, origin=\"lower\", cmap='gray', interpolation='nearest', aspect='auto')\n",
        "ax.set_xlabel(f'Time ({time_unit}s/{time_div})')\n",
        "ax.set_ylabel('Piano key' if piano_range else 'MIDI pitch')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "sRoyBJJVqX_u",
        "outputId": "794382fa-5613-44a9-b3ef-a71b5e1a52fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "  if i ==10:\n",
        "    all_voices, length, nbr_voices, _ = sample_batched\n",
        "    all_voices_pr = all_voices[0,:,:,-1].numpy()\n",
        "    \n",
        "    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\n",
        "    print(note_array.shape)\n",
        "    print(note_array[:10])\n",
        "    print(note_array.dtype.names)\n",
        "\n",
        "    #print(i,nbr_voices,all_voices.shape)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "b4QCaMEi3nw7",
        "outputId": "3f147e00-aa16-41c5-b3c3-65073371ac4f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor i, sample_batched in enumerate(loader):\\n    all_voices, length, nbr_voices = sample_batched\\n    if nbr_voices ==3:\\n      print(i,nbr_voices,all_voices.shape)\\n    else:\\n      print(i,nbr_voices)\\n\\nfor i, sample_batched in enumerate(loader):\\n  if i ==10:\\n    all_voices, length, nbr_voices, _ = sample_batched\\n    all_voices_pr = all_voices[0,:,:,-1].numpy()\\n    \\n    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\\n    print(note_array.shape)\\n    print(note_array[:10])\\n    print(note_array.dtype.names)\\n\\n    #print(i,nbr_voices,all_voices.shape)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Music - Model\n"
      ],
      "metadata": {
        "id": "JNqxeacDwxNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UNET "
      ],
      "metadata": {
        "id": "QAIfIM69VHI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "XMdlm0_Vyyhc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_0 = []\n",
        "loss_1 = []\n",
        "loss_2 = []\n",
        "loss_3 = []\n",
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")                       # use weight parameters maybe take 1/88       \n",
        "\n",
        "    \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                      ### squeeze output here before returning                                       \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()    ### v3 is automatically tensor containing only 0 bc dataloader does this for pieces with 3 voices\n",
        "        \n",
        "\n",
        "        \"\"\"\n",
        "        if nbr_voices==4:\n",
        "            v3 = voices[:,:,:,3].squeeze()\n",
        "        else:\n",
        "            v3 = torch.zeros(v0.shape,  device=\"cuda\")\n",
        "        \"\"\"\n",
        "\n",
        "        loss = self.loss(score_0, v0) +  self.loss(score_1, v1) +  self.loss(score_2, v2) +  self.loss(score_3, v3) #*1.5\n",
        "            \n",
        "        loss_0.append(self.loss(score_0, v0).cpu().detach().numpy())\n",
        "        loss_1.append(self.loss(score_1, v1).cpu().detach().numpy())\n",
        "        loss_2.append(self.loss(score_2, v2).cpu().detach().numpy())\n",
        "        loss_3.append(self.loss(score_3, v3).cpu().detach().numpy())\n",
        "        print(\"self.loss(score_0, v0)\",self.loss(score_0, v0).cpu().detach().numpy())\n",
        "        print(\"self.loss(score_1, v1)\",self.loss(score_1, v1).cpu().detach().numpy())\n",
        "        print(\"self.loss(score_2, v2)\",self.loss(score_2, v2).cpu().detach().numpy())\n",
        "        print(\"self.loss(score_3, v3)\",self.loss(score_3, v3).cpu().detach().numpy())\n",
        "\n",
        "        print(\"loss\",loss) \n",
        "\n",
        "\n",
        "        return loss   #change also to matrix version\n",
        "        \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "            \n",
        "\n",
        "        else:\n",
        "            sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "            prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            \n",
        "            mask_pred = np.squeeze(sentences)== 0\n",
        "            v_pred_argm[mask_pred] = -1\n",
        "\n",
        "            return v_pred_argm \n",
        "                       "
      ],
      "metadata": {
        "id": "CviiPTPOPW04"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"CNN\"\n",
        "lr = 0.0001  \n",
        "monophonic = True\n",
        "his = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "79cPe11WL6J0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b2402d08-e463-4cf0-850b-b0ffc9e46853"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"CNN\"\\nlr = 0.0001  \\nmonophonic = True\\nhis = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Define Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                                   ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "                \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]             \n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "                    ### before\n",
        "                    #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                    #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                    #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                    #for i in range(len(prediction[0,:])):\n",
        "                    #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                    #  accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                    #prediction = model.predict(voices, lens, monophonic)                    #for voice vise masking\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)         #for mixed voice masking        \n",
        "\n",
        "\n",
        "                    ## ground truth in shape 1280x88 -> mixed voice\n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    truth = v_ori_argm       \n",
        "\n",
        "                    # outsource accurcy to further down -> just a placeholder right now\n",
        "                    v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            #train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            #train_acc_list[3] = accuracy_sum_list[3] / 18                        ## bc only 18 pieces with len 3\n",
        "            #train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "            #history[\"train_loss\"].append(train_loss)\n",
        "            #history[\"train_acc\"].append(train_acc_list)\n",
        "            #print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "            \n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "                        voices = voices.to(device).float()\n",
        "                        ### before\n",
        "                        #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                        #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                        #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                        #for i in range(len(prediction[0,:])):\n",
        "                        #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                        #  val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    #val_acc_list[3] = val_accuracy_sum_list[3] / 18                         ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_accuracy_sum_list[4] / 2                          ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                #history[\"val_acc\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        #prediction = model.predict(voices, lens, monophonic)                #for voice vise masking\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)     # for masking with mixed voice\n",
        "\n",
        "\n",
        "\n",
        "                        ## ground truth in shape 1280x88 -> mixed voice\n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        truth = v_ori_argm       \n",
        "\n",
        "                        # outsource accurcy to further down -> just a placeholder right now\n",
        "                        v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "                    \n",
        "                history[\"val_acc\"].append(val_accuracy)\n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    # save the model\n",
        "    #torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"RNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ge8pY70uHxF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "81f351f7-b659-4144-f230-a026dc4e1ba9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"RNN\"\\nmonophonic = True\\nhis = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= [\"CNN\",\"RNN\"]\n",
        "monophonic_list = [True,False]\n",
        "\n",
        "for net in network_type:\n",
        "    for monophonic in monophonic_list: \n",
        "        print(\"network set to:\",net,\"monophnic:\",monophonic)\n",
        "        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Bs6-iNEBu8o",
        "outputId": "f36870a8-41cd-4650-88a0-cf92bb238de6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= [\"CNN\",\"RNN\"]\\nmonophonic_list = [True,False]\\n\\nfor net in network_type:\\n    for monophonic in monophonic_list: \\n        print(\"network set to:\",net,\"monophnic:\",monophonic)\\n        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        if fugues == True:\n",
        "        ### uncomment for fugues ###\n",
        "            train_dataset = MusicDataset_new(PATH_TO_DATA) \n",
        "        ### uncomment for chorals ###\n",
        "        else:\n",
        "            train_dataset = MusicDataset_chor(PATH_TO_DATA) \n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        ### uncomment for fugues ###\n",
        "        if fugues == True:\n",
        "            dataset = MusicDataset_new(PATH_TO_DATA) \n",
        "        ### uncomment for chorals ###\n",
        "        else:\n",
        "            dataset = MusicDataset_chor(PATH_TO_DATA)\n",
        "        \n",
        "        \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicNetwork\n",
        "epochs = 10\n",
        "lr = 0.00001 # was 0.001\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Experiment"
      ],
      "metadata": {
        "id": "bdetlQP-LoRX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm",
        "outputId": "57cf93e0-d322-4a46-bbed-576abd467267",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "self.loss(score_2, v2) 0.037935864\n",
            "self.loss(score_3, v3) 0.011198581\n",
            "loss tensor(0.1385, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022141099\n",
            "self.loss(score_1, v1) 0.023783907\n",
            "self.loss(score_2, v2) 0.025872914\n",
            "self.loss(score_3, v3) 0.008755596\n",
            "loss tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0179757\n",
            "self.loss(score_1, v1) 0.015100288\n",
            "self.loss(score_2, v2) 0.019445304\n",
            "self.loss(score_3, v3) 0.009659775\n",
            "loss tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.059078455\n",
            "self.loss(score_1, v1) 0.033121318\n",
            "self.loss(score_2, v2) 0.03895897\n",
            "self.loss(score_3, v3) 0.009121877\n",
            "loss tensor(0.1403, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010980639\n",
            "self.loss(score_1, v1) 0.055183537\n",
            "self.loss(score_2, v2) 0.05129804\n",
            "self.loss(score_3, v3) 0.010535905\n",
            "loss tensor(0.1280, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016947452\n",
            "self.loss(score_1, v1) 0.01894936\n",
            "self.loss(score_2, v2) 0.009173485\n",
            "self.loss(score_3, v3) 0.009271566\n",
            "loss tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018119147\n",
            "self.loss(score_1, v1) 0.022029687\n",
            "self.loss(score_2, v2) 0.021262923\n",
            "self.loss(score_3, v3) 0.010892725\n",
            "loss tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.046847146\n",
            "self.loss(score_1, v1) 0.020128207\n",
            "self.loss(score_2, v2) 0.031645846\n",
            "self.loss(score_3, v3) 0.014339337\n",
            "loss tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02720073\n",
            "self.loss(score_1, v1) 0.026303578\n",
            "self.loss(score_2, v2) 0.0237317\n",
            "self.loss(score_3, v3) 0.01115605\n",
            "loss tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02850565\n",
            "self.loss(score_1, v1) 0.018754795\n",
            "self.loss(score_2, v2) 0.007213755\n",
            "self.loss(score_3, v3) 0.007125318\n",
            "loss tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010821793\n",
            "self.loss(score_1, v1) 0.01917996\n",
            "self.loss(score_2, v2) 0.02071212\n",
            "self.loss(score_3, v3) 0.009440097\n",
            "loss tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008967579\n",
            "self.loss(score_1, v1) 0.009832786\n",
            "self.loss(score_2, v2) 0.012033271\n",
            "self.loss(score_3, v3) 0.010413331\n",
            "loss tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0062382016\n",
            "self.loss(score_1, v1) 0.008379432\n",
            "self.loss(score_2, v2) 0.011975433\n",
            "self.loss(score_3, v3) 0.008994677\n",
            "loss tensor(0.0356, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.039087124\n",
            "self.loss(score_1, v1) 0.033118535\n",
            "self.loss(score_2, v2) 0.06262801\n",
            "self.loss(score_3, v3) 0.011677282\n",
            "loss tensor(0.1465, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016306996\n",
            "self.loss(score_1, v1) 0.04894634\n",
            "self.loss(score_2, v2) 0.051620737\n",
            "self.loss(score_3, v3) 0.015178682\n",
            "loss tensor(0.1321, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011051565\n",
            "self.loss(score_1, v1) 0.009854387\n",
            "self.loss(score_2, v2) 0.008097788\n",
            "self.loss(score_3, v3) 0.008270711\n",
            "loss tensor(0.0373, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011262802\n",
            "self.loss(score_1, v1) 0.026611933\n",
            "self.loss(score_2, v2) 0.027722277\n",
            "self.loss(score_3, v3) 0.009257574\n",
            "loss tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014018001\n",
            "self.loss(score_1, v1) 0.0235946\n",
            "self.loss(score_2, v2) 0.023546396\n",
            "self.loss(score_3, v3) 0.06507011\n",
            "loss tensor(0.1262, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00721795\n",
            "self.loss(score_1, v1) 0.015253926\n",
            "self.loss(score_2, v2) 0.02654075\n",
            "self.loss(score_3, v3) 0.011214777\n",
            "loss tensor(0.0602, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.06625589\n",
            "self.loss(score_1, v1) 0.023127005\n",
            "self.loss(score_2, v2) 0.016079668\n",
            "self.loss(score_3, v3) 0.010912669\n",
            "loss tensor(0.1164, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017287897\n",
            "self.loss(score_1, v1) 0.012991537\n",
            "self.loss(score_2, v2) 0.012889774\n",
            "self.loss(score_3, v3) 0.010478664\n",
            "loss tensor(0.0536, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.04431917\n",
            "self.loss(score_1, v1) 0.053447116\n",
            "self.loss(score_2, v2) 0.030258054\n",
            "self.loss(score_3, v3) 0.008473606\n",
            "loss tensor(0.1365, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0113482075\n",
            "self.loss(score_1, v1) 0.012002013\n",
            "self.loss(score_2, v2) 0.005877782\n",
            "self.loss(score_3, v3) 0.0052334866\n",
            "loss tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009661458\n",
            "self.loss(score_1, v1) 0.01293655\n",
            "self.loss(score_2, v2) 0.018102776\n",
            "self.loss(score_3, v3) 0.0091329655\n",
            "loss tensor(0.0498, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008252603\n",
            "self.loss(score_1, v1) 0.013093741\n",
            "self.loss(score_2, v2) 0.011845421\n",
            "self.loss(score_3, v3) 0.007407212\n",
            "loss tensor(0.0406, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014783469\n",
            "self.loss(score_1, v1) 0.023454843\n",
            "self.loss(score_2, v2) 0.03304847\n",
            "self.loss(score_3, v3) 0.00873585\n",
            "loss tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008819683\n",
            "self.loss(score_1, v1) 0.010835928\n",
            "self.loss(score_2, v2) 0.02078407\n",
            "self.loss(score_3, v3) 0.009783008\n",
            "loss tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.04727249\n",
            "self.loss(score_1, v1) 0.027743412\n",
            "self.loss(score_2, v2) 0.037808213\n",
            "self.loss(score_3, v3) 0.0078787785\n",
            "loss tensor(0.1207, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015211138\n",
            "self.loss(score_1, v1) 0.017622482\n",
            "self.loss(score_2, v2) 0.011365469\n",
            "self.loss(score_3, v3) 0.009706115\n",
            "loss tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012169938\n",
            "self.loss(score_1, v1) 0.051217698\n",
            "self.loss(score_2, v2) 0.045603227\n",
            "self.loss(score_3, v3) 0.104795635\n",
            "loss tensor(0.2138, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009509832\n",
            "self.loss(score_1, v1) 0.0166274\n",
            "self.loss(score_2, v2) 0.023065185\n",
            "self.loss(score_3, v3) 0.011981046\n",
            "loss tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008262559\n",
            "self.loss(score_1, v1) 0.009682771\n",
            "self.loss(score_2, v2) 0.011124852\n",
            "self.loss(score_3, v3) 0.008440771\n",
            "loss tensor(0.0375, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.030808637\n",
            "self.loss(score_1, v1) 0.02570765\n",
            "self.loss(score_2, v2) 0.011000035\n",
            "self.loss(score_3, v3) 0.008430458\n",
            "loss tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009724814\n",
            "self.loss(score_1, v1) 0.011566121\n",
            "self.loss(score_2, v2) 0.021557288\n",
            "self.loss(score_3, v3) 0.007879849\n",
            "loss tensor(0.0507, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008624607\n",
            "self.loss(score_1, v1) 0.017133875\n",
            "self.loss(score_2, v2) 0.026195481\n",
            "self.loss(score_3, v3) 0.013247304\n",
            "loss tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012194543\n",
            "self.loss(score_1, v1) 0.02114254\n",
            "self.loss(score_2, v2) 0.02774628\n",
            "self.loss(score_3, v3) 0.007315126\n",
            "loss tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007369781\n",
            "self.loss(score_1, v1) 0.013829617\n",
            "self.loss(score_2, v2) 0.01557985\n",
            "self.loss(score_3, v3) 0.007766434\n",
            "loss tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015161872\n",
            "self.loss(score_1, v1) 0.033280827\n",
            "self.loss(score_2, v2) 0.0444427\n",
            "self.loss(score_3, v3) 0.012318174\n",
            "loss tensor(0.1052, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.027753081\n",
            "self.loss(score_1, v1) 0.022910066\n",
            "self.loss(score_2, v2) 0.01996587\n",
            "self.loss(score_3, v3) 0.013214173\n",
            "loss tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010223652\n",
            "self.loss(score_1, v1) 0.04606347\n",
            "self.loss(score_2, v2) 0.05714147\n",
            "self.loss(score_3, v3) 0.043181587\n",
            "loss tensor(0.1566, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017703518\n",
            "self.loss(score_1, v1) 0.011429767\n",
            "self.loss(score_2, v2) 0.017572764\n",
            "self.loss(score_3, v3) 0.010810978\n",
            "loss tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.028872719\n",
            "self.loss(score_1, v1) 0.027912289\n",
            "self.loss(score_2, v2) 0.011739531\n",
            "self.loss(score_3, v3) 0.0070695896\n",
            "loss tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021474818\n",
            "self.loss(score_1, v1) 0.054713205\n",
            "self.loss(score_2, v2) 0.04132605\n",
            "self.loss(score_3, v3) 0.010061462\n",
            "loss tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.07902465\n",
            "self.loss(score_1, v1) 0.044109832\n",
            "self.loss(score_2, v2) 0.08457161\n",
            "self.loss(score_3, v3) 0.008941817\n",
            "loss tensor(0.2166, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.032078855\n",
            "self.loss(score_1, v1) 0.016590409\n",
            "self.loss(score_2, v2) 0.012161131\n",
            "self.loss(score_3, v3) 0.0069148107\n",
            "loss tensor(0.0677, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.03317338\n",
            "self.loss(score_1, v1) 0.021627922\n",
            "self.loss(score_2, v2) 0.028508918\n",
            "self.loss(score_3, v3) 0.010191747\n",
            "loss tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.004992734\n",
            "self.loss(score_1, v1) 0.016559217\n",
            "self.loss(score_2, v2) 0.03611736\n",
            "self.loss(score_3, v3) 0.0060367454\n",
            "loss tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.025849495\n",
            "self.loss(score_1, v1) 0.03482037\n",
            "self.loss(score_2, v2) 0.031151677\n",
            "self.loss(score_3, v3) 0.0092044445\n",
            "loss tensor(0.1010, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0111655705\n",
            "self.loss(score_1, v1) 0.014015365\n",
            "self.loss(score_2, v2) 0.013006642\n",
            "self.loss(score_3, v3) 0.013472893\n",
            "loss tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008876028\n",
            "self.loss(score_1, v1) 0.012940584\n",
            "self.loss(score_2, v2) 0.014241916\n",
            "self.loss(score_3, v3) 0.0068420433\n",
            "loss tensor(0.0429, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020440921\n",
            "self.loss(score_1, v1) 0.024305291\n",
            "self.loss(score_2, v2) 0.024965039\n",
            "self.loss(score_3, v3) 0.0087086465\n",
            "loss tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.025608758\n",
            "self.loss(score_1, v1) 0.041896444\n",
            "self.loss(score_2, v2) 0.04626288\n",
            "self.loss(score_3, v3) 0.008654602\n",
            "loss tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008943523\n",
            "self.loss(score_1, v1) 0.008713894\n",
            "self.loss(score_2, v2) 0.017769149\n",
            "self.loss(score_3, v3) 0.011959493\n",
            "loss tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021008931\n",
            "self.loss(score_1, v1) 0.018636636\n",
            "self.loss(score_2, v2) 0.01410458\n",
            "self.loss(score_3, v3) 0.00894555\n",
            "loss tensor(0.0627, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008965073\n",
            "self.loss(score_1, v1) 0.010586751\n",
            "self.loss(score_2, v2) 0.018618654\n",
            "self.loss(score_3, v3) 0.010911067\n",
            "loss tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015062945\n",
            "self.loss(score_1, v1) 0.055509184\n",
            "self.loss(score_2, v2) 0.06390995\n",
            "self.loss(score_3, v3) 0.010967614\n",
            "loss tensor(0.1454, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.1002270743120343, Train Accuracy : 0.9992453464856429\n",
            " Validation Accuracy : 6.602039042286186\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) 0.007085977\n",
            "self.loss(score_1, v1) 0.028191915\n",
            "self.loss(score_2, v2) 0.089385815\n",
            "self.loss(score_3, v3) 0.031846747\n",
            "loss tensor(0.1565, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007634371\n",
            "self.loss(score_1, v1) 0.010597063\n",
            "self.loss(score_2, v2) 0.074806474\n",
            "self.loss(score_3, v3) 0.040358074\n",
            "loss tensor(0.1334, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.06245135\n",
            "self.loss(score_1, v1) 0.066711165\n",
            "self.loss(score_2, v2) 0.056150723\n",
            "self.loss(score_3, v3) 0.0076137767\n",
            "loss tensor(0.1929, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.05356253\n",
            "self.loss(score_1, v1) 0.032845255\n",
            "self.loss(score_2, v2) 0.025791\n",
            "self.loss(score_3, v3) 0.010204055\n",
            "loss tensor(0.1224, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017971687\n",
            "self.loss(score_1, v1) 0.064965054\n",
            "self.loss(score_2, v2) 0.04412971\n",
            "self.loss(score_3, v3) 0.0076203714\n",
            "loss tensor(0.1347, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0083807865\n",
            "self.loss(score_1, v1) 0.009650092\n",
            "self.loss(score_2, v2) 0.008441878\n",
            "self.loss(score_3, v3) 0.0066918023\n",
            "loss tensor(0.0332, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008924344\n",
            "self.loss(score_1, v1) 0.012858806\n",
            "self.loss(score_2, v2) 0.012729341\n",
            "self.loss(score_3, v3) 0.008168001\n",
            "loss tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013505553\n",
            "self.loss(score_1, v1) 0.01504029\n",
            "self.loss(score_2, v2) 0.010536224\n",
            "self.loss(score_3, v3) 0.006037085\n",
            "loss tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009858811\n",
            "self.loss(score_1, v1) 0.013846469\n",
            "self.loss(score_2, v2) 0.018476032\n",
            "self.loss(score_3, v3) 0.008919314\n",
            "loss tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008951201\n",
            "self.loss(score_1, v1) 0.010522691\n",
            "self.loss(score_2, v2) 0.020802045\n",
            "self.loss(score_3, v3) 0.013178089\n",
            "loss tensor(0.0535, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022589859\n",
            "self.loss(score_1, v1) 0.048769247\n",
            "self.loss(score_2, v2) 0.042916685\n",
            "self.loss(score_3, v3) 0.008818929\n",
            "loss tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011424695\n",
            "self.loss(score_1, v1) 0.012984906\n",
            "self.loss(score_2, v2) 0.013900398\n",
            "self.loss(score_3, v3) 0.007836668\n",
            "loss tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017903246\n",
            "self.loss(score_1, v1) 0.04746288\n",
            "self.loss(score_2, v2) 0.055500764\n",
            "self.loss(score_3, v3) 0.008265922\n",
            "loss tensor(0.1291, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.027131634\n",
            "self.loss(score_1, v1) 0.027382512\n",
            "self.loss(score_2, v2) 0.053253174\n",
            "self.loss(score_3, v3) 0.009471191\n",
            "loss tensor(0.1172, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009353934\n",
            "self.loss(score_1, v1) 0.014212403\n",
            "self.loss(score_2, v2) 0.01990985\n",
            "self.loss(score_3, v3) 0.0074680736\n",
            "loss tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.053455364\n",
            "self.loss(score_1, v1) 0.0285259\n",
            "self.loss(score_2, v2) 0.015714165\n",
            "self.loss(score_3, v3) 0.0101510715\n",
            "loss tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014711079\n",
            "self.loss(score_1, v1) 0.017622557\n",
            "self.loss(score_2, v2) 0.013464306\n",
            "self.loss(score_3, v3) 0.009559576\n",
            "loss tensor(0.0554, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013268986\n",
            "self.loss(score_1, v1) 0.01687404\n",
            "self.loss(score_2, v2) 0.017512646\n",
            "self.loss(score_3, v3) 0.008294415\n",
            "loss tensor(0.0560, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.056456637\n",
            "self.loss(score_1, v1) 0.06470235\n",
            "self.loss(score_2, v2) 0.058132716\n",
            "self.loss(score_3, v3) 0.01081307\n",
            "loss tensor(0.1901, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019457562\n",
            "self.loss(score_1, v1) 0.035520554\n",
            "self.loss(score_2, v2) 0.055622928\n",
            "self.loss(score_3, v3) 0.013811227\n",
            "loss tensor(0.1244, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014759526\n",
            "self.loss(score_1, v1) 0.020838786\n",
            "self.loss(score_2, v2) 0.023062138\n",
            "self.loss(score_3, v3) 0.008602851\n",
            "loss tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.029214716\n",
            "self.loss(score_1, v1) 0.024516536\n",
            "self.loss(score_2, v2) 0.012804224\n",
            "self.loss(score_3, v3) 0.008989581\n",
            "loss tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006928223\n",
            "self.loss(score_1, v1) 0.015504971\n",
            "self.loss(score_2, v2) 0.027429916\n",
            "self.loss(score_3, v3) 0.011558177\n",
            "loss tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00731509\n",
            "self.loss(score_1, v1) 0.0075031808\n",
            "self.loss(score_2, v2) 0.009228949\n",
            "self.loss(score_3, v3) 0.0066920766\n",
            "loss tensor(0.0307, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008721011\n",
            "self.loss(score_1, v1) 0.009797976\n",
            "self.loss(score_2, v2) 0.009453405\n",
            "self.loss(score_3, v3) 0.0069809146\n",
            "loss tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010128079\n",
            "self.loss(score_1, v1) 0.01469149\n",
            "self.loss(score_2, v2) 0.019335615\n",
            "self.loss(score_3, v3) 0.009681422\n",
            "loss tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02244872\n",
            "self.loss(score_1, v1) 0.026925007\n",
            "self.loss(score_2, v2) 0.02322146\n",
            "self.loss(score_3, v3) 0.009168864\n",
            "loss tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008339064\n",
            "self.loss(score_1, v1) 0.011540914\n",
            "self.loss(score_2, v2) 0.013338155\n",
            "self.loss(score_3, v3) 0.00986797\n",
            "loss tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017437866\n",
            "self.loss(score_1, v1) 0.03471747\n",
            "self.loss(score_2, v2) 0.03617347\n",
            "self.loss(score_3, v3) 0.012549514\n",
            "loss tensor(0.1009, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009607841\n",
            "self.loss(score_1, v1) 0.01337078\n",
            "self.loss(score_2, v2) 0.028029807\n",
            "self.loss(score_3, v3) 0.010305749\n",
            "loss tensor(0.0613, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.029475922\n",
            "self.loss(score_1, v1) 0.05267367\n",
            "self.loss(score_2, v2) 0.057909023\n",
            "self.loss(score_3, v3) 0.008856053\n",
            "loss tensor(0.1489, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011595598\n",
            "self.loss(score_1, v1) 0.011358343\n",
            "self.loss(score_2, v2) 0.011741725\n",
            "self.loss(score_3, v3) 0.007652212\n",
            "loss tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.12559311\n",
            "self.loss(score_1, v1) 0.13912557\n",
            "self.loss(score_2, v2) 0.15113766\n",
            "self.loss(score_3, v3) 0.41919032\n",
            "loss tensor(0.8350, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013294877\n",
            "self.loss(score_1, v1) 0.0151498765\n",
            "self.loss(score_2, v2) 0.012060075\n",
            "self.loss(score_3, v3) 0.00881235\n",
            "loss tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008950764\n",
            "self.loss(score_1, v1) 0.014356763\n",
            "self.loss(score_2, v2) 0.025475476\n",
            "self.loss(score_3, v3) 0.010567288\n",
            "loss tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01059225\n",
            "self.loss(score_1, v1) 0.01551553\n",
            "self.loss(score_2, v2) 0.015592602\n",
            "self.loss(score_3, v3) 0.009361144\n",
            "loss tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008782226\n",
            "self.loss(score_1, v1) 0.011832869\n",
            "self.loss(score_2, v2) 0.013273878\n",
            "self.loss(score_3, v3) 0.011746231\n",
            "loss tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010015882\n",
            "self.loss(score_1, v1) 0.019221155\n",
            "self.loss(score_2, v2) 0.0441895\n",
            "self.loss(score_3, v3) 0.01564047\n",
            "loss tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.038289137\n",
            "self.loss(score_1, v1) 0.02237288\n",
            "self.loss(score_2, v2) 0.016793596\n",
            "self.loss(score_3, v3) 0.0099536665\n",
            "loss tensor(0.0874, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017095078\n",
            "self.loss(score_1, v1) 0.032316852\n",
            "self.loss(score_2, v2) 0.04318176\n",
            "self.loss(score_3, v3) 0.06979852\n",
            "loss tensor(0.1624, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.039761007\n",
            "self.loss(score_1, v1) 0.016932312\n",
            "self.loss(score_2, v2) 0.008443939\n",
            "self.loss(score_3, v3) 0.005642382\n",
            "loss tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.09582291\n",
            "self.loss(score_1, v1) 0.071289904\n",
            "self.loss(score_2, v2) 0.06460867\n",
            "self.loss(score_3, v3) 0.0073644123\n",
            "loss tensor(0.2391, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011560734\n",
            "self.loss(score_1, v1) 0.011547284\n",
            "self.loss(score_2, v2) 0.014386138\n",
            "self.loss(score_3, v3) 0.013337923\n",
            "loss tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007475714\n",
            "self.loss(score_1, v1) 0.011047519\n",
            "self.loss(score_2, v2) 0.015436394\n",
            "self.loss(score_3, v3) 0.008435557\n",
            "loss tensor(0.0424, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012082305\n",
            "self.loss(score_1, v1) 0.016773414\n",
            "self.loss(score_2, v2) 0.014176212\n",
            "self.loss(score_3, v3) 0.009893063\n",
            "loss tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009708314\n",
            "self.loss(score_1, v1) 0.01746244\n",
            "self.loss(score_2, v2) 0.016299255\n",
            "self.loss(score_3, v3) 0.007165302\n",
            "loss tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007994663\n",
            "self.loss(score_1, v1) 0.0158518\n",
            "self.loss(score_2, v2) 0.014377247\n",
            "self.loss(score_3, v3) 0.012009131\n",
            "loss tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015918234\n",
            "self.loss(score_1, v1) 0.015024315\n",
            "self.loss(score_2, v2) 0.05621407\n",
            "self.loss(score_3, v3) 0.0077300896\n",
            "loss tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007815949\n",
            "self.loss(score_1, v1) 0.007866189\n",
            "self.loss(score_2, v2) 0.012863469\n",
            "self.loss(score_3, v3) 0.00819687\n",
            "loss tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0118243685\n",
            "self.loss(score_1, v1) 0.018987736\n",
            "self.loss(score_2, v2) 0.02235498\n",
            "self.loss(score_3, v3) 0.015000562\n",
            "loss tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.030483013\n",
            "self.loss(score_1, v1) 0.04588398\n",
            "self.loss(score_2, v2) 0.054914296\n",
            "self.loss(score_3, v3) 0.0191092\n",
            "loss tensor(0.1504, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0081816735\n",
            "self.loss(score_1, v1) 0.011563892\n",
            "self.loss(score_2, v2) 0.015144519\n",
            "self.loss(score_3, v3) 0.009117906\n",
            "loss tensor(0.0440, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.028714677\n",
            "self.loss(score_1, v1) 0.055686176\n",
            "self.loss(score_2, v2) 0.10178622\n",
            "self.loss(score_3, v3) 0.079575084\n",
            "loss tensor(0.2658, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015305776\n",
            "self.loss(score_1, v1) 0.01462567\n",
            "self.loss(score_2, v2) 0.01270898\n",
            "self.loss(score_3, v3) 0.010110752\n",
            "loss tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.10093778\n",
            "self.loss(score_1, v1) 0.060580727\n",
            "self.loss(score_2, v2) 0.061117504\n",
            "self.loss(score_3, v3) 0.014480727\n",
            "loss tensor(0.2371, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014321187\n",
            "self.loss(score_1, v1) 0.015850756\n",
            "self.loss(score_2, v2) 0.02025471\n",
            "self.loss(score_3, v3) 0.011084071\n",
            "loss tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.025665801\n",
            "self.loss(score_1, v1) 0.033046603\n",
            "self.loss(score_2, v2) 0.022458212\n",
            "self.loss(score_3, v3) 0.011027919\n",
            "loss tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014932803\n",
            "self.loss(score_1, v1) 0.026038166\n",
            "self.loss(score_2, v2) 0.033874653\n",
            "self.loss(score_3, v3) 0.011153561\n",
            "loss tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018027727\n",
            "self.loss(score_1, v1) 0.020220067\n",
            "self.loss(score_2, v2) 0.013882618\n",
            "self.loss(score_3, v3) 0.0060571567\n",
            "loss tensor(0.0582, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010666578\n",
            "self.loss(score_1, v1) 0.008932442\n",
            "self.loss(score_2, v2) 0.010130794\n",
            "self.loss(score_3, v3) 0.009801555\n",
            "loss tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01060918\n",
            "self.loss(score_1, v1) 0.04758731\n",
            "self.loss(score_2, v2) 0.0447236\n",
            "self.loss(score_3, v3) 0.004327788\n",
            "loss tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.023159778\n",
            "self.loss(score_1, v1) 0.040659647\n",
            "self.loss(score_2, v2) 0.029903866\n",
            "self.loss(score_3, v3) 0.010522493\n",
            "loss tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011474066\n",
            "self.loss(score_1, v1) 0.0096096825\n",
            "self.loss(score_2, v2) 0.011949844\n",
            "self.loss(score_3, v3) 0.009463227\n",
            "loss tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.14799096\n",
            "self.loss(score_1, v1) 0.059541445\n",
            "self.loss(score_2, v2) 0.032603275\n",
            "self.loss(score_3, v3) 0.033819918\n",
            "loss tensor(0.2740, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.030151235\n",
            "self.loss(score_1, v1) 0.045267362\n",
            "self.loss(score_2, v2) 0.04623423\n",
            "self.loss(score_3, v3) 0.011297206\n",
            "loss tensor(0.1330, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.043415785\n",
            "self.loss(score_1, v1) 0.030486342\n",
            "self.loss(score_2, v2) 0.044656754\n",
            "self.loss(score_3, v3) 0.011443938\n",
            "loss tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015810259\n",
            "self.loss(score_1, v1) 0.024629852\n",
            "self.loss(score_2, v2) 0.04371254\n",
            "self.loss(score_3, v3) 0.011265062\n",
            "loss tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01143735\n",
            "self.loss(score_1, v1) 0.010304053\n",
            "self.loss(score_2, v2) 0.017483134\n",
            "self.loss(score_3, v3) 0.009865329\n",
            "loss tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01630056\n",
            "self.loss(score_1, v1) 0.014837448\n",
            "self.loss(score_2, v2) 0.011450815\n",
            "self.loss(score_3, v3) 0.0077278484\n",
            "loss tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012677848\n",
            "self.loss(score_1, v1) 0.022275701\n",
            "self.loss(score_2, v2) 0.028922936\n",
            "self.loss(score_3, v3) 0.010530122\n",
            "loss tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009901146\n",
            "self.loss(score_1, v1) 0.015555837\n",
            "self.loss(score_2, v2) 0.028008716\n",
            "self.loss(score_3, v3) 0.008812413\n",
            "loss tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.079017125\n",
            "self.loss(score_1, v1) 0.09603828\n",
            "self.loss(score_2, v2) 0.075263724\n",
            "self.loss(score_3, v3) 0.00933622\n",
            "loss tensor(0.2597, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.025855746\n",
            "self.loss(score_1, v1) 0.027572013\n",
            "self.loss(score_2, v2) 0.028581595\n",
            "self.loss(score_3, v3) 0.012701582\n",
            "loss tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02413664\n",
            "self.loss(score_1, v1) 0.048269633\n",
            "self.loss(score_2, v2) 0.08932636\n",
            "self.loss(score_3, v3) 0.04037463\n",
            "loss tensor(0.2021, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009468935\n",
            "self.loss(score_1, v1) 0.011330202\n",
            "self.loss(score_2, v2) 0.019703545\n",
            "self.loss(score_3, v3) 0.00734908\n",
            "loss tensor(0.0479, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0070040277\n",
            "self.loss(score_1, v1) 0.0067932135\n",
            "self.loss(score_2, v2) 0.008531215\n",
            "self.loss(score_3, v3) 0.0070285997\n",
            "loss tensor(0.0294, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016655317\n",
            "self.loss(score_1, v1) 0.017391514\n",
            "self.loss(score_2, v2) 0.01332709\n",
            "self.loss(score_3, v3) 0.0091396365\n",
            "loss tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013361712\n",
            "self.loss(score_1, v1) 0.021053096\n",
            "self.loss(score_2, v2) 0.026588224\n",
            "self.loss(score_3, v3) 0.010159958\n",
            "loss tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012384373\n",
            "self.loss(score_1, v1) 0.026453894\n",
            "self.loss(score_2, v2) 0.02958529\n",
            "self.loss(score_3, v3) 0.008228877\n",
            "loss tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008879965\n",
            "self.loss(score_1, v1) 0.0105845975\n",
            "self.loss(score_2, v2) 0.00841601\n",
            "self.loss(score_3, v3) 0.0056942604\n",
            "loss tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.037298717\n",
            "self.loss(score_1, v1) 0.036564577\n",
            "self.loss(score_2, v2) 0.038826823\n",
            "self.loss(score_3, v3) 0.0071616424\n",
            "loss tensor(0.1199, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00887848\n",
            "self.loss(score_1, v1) 0.017956087\n",
            "self.loss(score_2, v2) 0.023740213\n",
            "self.loss(score_3, v3) 0.007983415\n",
            "loss tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0074581993\n",
            "self.loss(score_1, v1) 0.014047352\n",
            "self.loss(score_2, v2) 0.008905455\n",
            "self.loss(score_3, v3) 0.008096458\n",
            "loss tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013395019\n",
            "self.loss(score_1, v1) 0.017730825\n",
            "self.loss(score_2, v2) 0.012644712\n",
            "self.loss(score_3, v3) 0.00761868\n",
            "loss tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010533769\n",
            "self.loss(score_1, v1) 0.03878634\n",
            "self.loss(score_2, v2) 0.02247052\n",
            "self.loss(score_3, v3) 0.009564685\n",
            "loss tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014779604\n",
            "self.loss(score_1, v1) 0.021355355\n",
            "self.loss(score_2, v2) 0.02527286\n",
            "self.loss(score_3, v3) 0.010678788\n",
            "loss tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014105676\n",
            "self.loss(score_1, v1) 0.049261436\n",
            "self.loss(score_2, v2) 0.05735353\n",
            "self.loss(score_3, v3) 0.08672959\n",
            "loss tensor(0.2075, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018524364\n",
            "self.loss(score_1, v1) 0.017768173\n",
            "self.loss(score_2, v2) 0.032821022\n",
            "self.loss(score_3, v3) 0.0071524447\n",
            "loss tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007057292\n",
            "self.loss(score_1, v1) 0.009617999\n",
            "self.loss(score_2, v2) 0.01690989\n",
            "self.loss(score_3, v3) 0.010239372\n",
            "loss tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0079525225\n",
            "self.loss(score_1, v1) 0.019176306\n",
            "self.loss(score_2, v2) 0.02644596\n",
            "self.loss(score_3, v3) 0.008060794\n",
            "loss tensor(0.0616, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.04674388\n",
            "self.loss(score_1, v1) 0.026276063\n",
            "self.loss(score_2, v2) 0.035434447\n",
            "self.loss(score_3, v3) 0.0071298257\n",
            "loss tensor(0.1156, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.023360766\n",
            "self.loss(score_1, v1) 0.014702352\n",
            "self.loss(score_2, v2) 0.012830026\n",
            "self.loss(score_3, v3) 0.012832038\n",
            "loss tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022977905\n",
            "self.loss(score_1, v1) 0.027062405\n",
            "self.loss(score_2, v2) 0.026459388\n",
            "self.loss(score_3, v3) 0.012588287\n",
            "loss tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016919533\n",
            "self.loss(score_1, v1) 0.025261406\n",
            "self.loss(score_2, v2) 0.03728223\n",
            "self.loss(score_3, v3) 0.01482066\n",
            "loss tensor(0.0943, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013276437\n",
            "self.loss(score_1, v1) 0.013653845\n",
            "self.loss(score_2, v2) 0.016318223\n",
            "self.loss(score_3, v3) 0.007531666\n",
            "loss tensor(0.0508, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0661538\n",
            "self.loss(score_1, v1) 0.04356236\n",
            "self.loss(score_2, v2) 0.026442818\n",
            "self.loss(score_3, v3) 0.0078436285\n",
            "loss tensor(0.1440, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019719187\n",
            "self.loss(score_1, v1) 0.04046229\n",
            "self.loss(score_2, v2) 0.036073137\n",
            "self.loss(score_3, v3) 0.011113437\n",
            "loss tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008892207\n",
            "self.loss(score_1, v1) 0.010879969\n",
            "self.loss(score_2, v2) 0.009776544\n",
            "self.loss(score_3, v3) 0.008118887\n",
            "loss tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0077556814\n",
            "self.loss(score_1, v1) 0.01273148\n",
            "self.loss(score_2, v2) 0.021110404\n",
            "self.loss(score_3, v3) 0.008891048\n",
            "loss tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.038132496\n",
            "self.loss(score_1, v1) 0.058788966\n",
            "self.loss(score_2, v2) 0.064964324\n",
            "self.loss(score_3, v3) 0.021797497\n",
            "loss tensor(0.1837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009654981\n",
            "self.loss(score_1, v1) 0.050992053\n",
            "self.loss(score_2, v2) 0.083859526\n",
            "self.loss(score_3, v3) 0.06292395\n",
            "loss tensor(0.2074, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.064111896\n",
            "self.loss(score_1, v1) 0.08624344\n",
            "self.loss(score_2, v2) 0.04594368\n",
            "self.loss(score_3, v3) 0.023828717\n",
            "loss tensor(0.2201, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.034581933\n",
            "self.loss(score_1, v1) 0.024403606\n",
            "self.loss(score_2, v2) 0.026368858\n",
            "self.loss(score_3, v3) 0.00751326\n",
            "loss tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0174965\n",
            "self.loss(score_1, v1) 0.017552825\n",
            "self.loss(score_2, v2) 0.023048254\n",
            "self.loss(score_3, v3) 0.011112011\n",
            "loss tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006714846\n",
            "self.loss(score_1, v1) 0.00766802\n",
            "self.loss(score_2, v2) 0.00899257\n",
            "self.loss(score_3, v3) 0.006768722\n",
            "loss tensor(0.0301, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010512075\n",
            "self.loss(score_1, v1) 0.027021399\n",
            "self.loss(score_2, v2) 0.060976125\n",
            "self.loss(score_3, v3) 0.009743534\n",
            "loss tensor(0.1083, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.028830117\n",
            "self.loss(score_1, v1) 0.08626096\n",
            "self.loss(score_2, v2) 0.039558157\n",
            "self.loss(score_3, v3) 0.012297568\n",
            "loss tensor(0.1669, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010716338\n",
            "self.loss(score_1, v1) 0.011915853\n",
            "self.loss(score_2, v2) 0.010622788\n",
            "self.loss(score_3, v3) 0.009032243\n",
            "loss tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010250966\n",
            "self.loss(score_1, v1) 0.018760707\n",
            "self.loss(score_2, v2) 0.020529976\n",
            "self.loss(score_3, v3) 0.0101881325\n",
            "loss tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0075927204\n",
            "self.loss(score_1, v1) 0.054512065\n",
            "self.loss(score_2, v2) 0.195278\n",
            "self.loss(score_3, v3) 0.21433017\n",
            "loss tensor(0.4717, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009342934\n",
            "self.loss(score_1, v1) 0.016506186\n",
            "self.loss(score_2, v2) 0.01484268\n",
            "self.loss(score_3, v3) 0.020018755\n",
            "loss tensor(0.0607, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014003664\n",
            "self.loss(score_1, v1) 0.015120087\n",
            "self.loss(score_2, v2) 0.027834889\n",
            "self.loss(score_3, v3) 0.07485914\n",
            "loss tensor(0.1318, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014992047\n",
            "self.loss(score_1, v1) 0.074373566\n",
            "self.loss(score_2, v2) 0.05224272\n",
            "self.loss(score_3, v3) 0.011763417\n",
            "loss tensor(0.1534, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013428915\n",
            "self.loss(score_1, v1) 0.024010133\n",
            "self.loss(score_2, v2) 0.021447148\n",
            "self.loss(score_3, v3) 0.009189234\n",
            "loss tensor(0.0681, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017919874\n",
            "self.loss(score_1, v1) 0.020059383\n",
            "self.loss(score_2, v2) 0.023615696\n",
            "self.loss(score_3, v3) 0.011192983\n",
            "loss tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006973939\n",
            "self.loss(score_1, v1) 0.006713981\n",
            "self.loss(score_2, v2) 0.013351138\n",
            "self.loss(score_3, v3) 0.009316442\n",
            "loss tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015852714\n",
            "self.loss(score_1, v1) 0.016166463\n",
            "self.loss(score_2, v2) 0.0123550175\n",
            "self.loss(score_3, v3) 0.009642965\n",
            "loss tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012390749\n",
            "self.loss(score_1, v1) 0.014960679\n",
            "self.loss(score_2, v2) 0.0078002634\n",
            "self.loss(score_3, v3) 0.007655369\n",
            "loss tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008089789\n",
            "self.loss(score_1, v1) 0.0084574\n",
            "self.loss(score_2, v2) 0.008278172\n",
            "self.loss(score_3, v3) 0.0046648374\n",
            "loss tensor(0.0295, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008781251\n",
            "self.loss(score_1, v1) 0.010368247\n",
            "self.loss(score_2, v2) 0.0066691707\n",
            "self.loss(score_3, v3) 0.0041719577\n",
            "loss tensor(0.0300, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.14854257\n",
            "self.loss(score_1, v1) 0.059351444\n",
            "self.loss(score_2, v2) 0.06434595\n",
            "self.loss(score_3, v3) 0.03764193\n",
            "loss tensor(0.3099, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00894517\n",
            "self.loss(score_1, v1) 0.008598802\n",
            "self.loss(score_2, v2) 0.009664682\n",
            "self.loss(score_3, v3) 0.0053335666\n",
            "loss tensor(0.0325, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013676687\n",
            "self.loss(score_1, v1) 0.034126792\n",
            "self.loss(score_2, v2) 0.020524794\n",
            "self.loss(score_3, v3) 0.008046729\n",
            "loss tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0059981784\n",
            "self.loss(score_1, v1) 0.011777902\n",
            "self.loss(score_2, v2) 0.010248124\n",
            "self.loss(score_3, v3) 0.0050988304\n",
            "loss tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0071495418\n",
            "self.loss(score_1, v1) 0.01484451\n",
            "self.loss(score_2, v2) 0.016732266\n",
            "self.loss(score_3, v3) 0.009303508\n",
            "loss tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018930746\n",
            "self.loss(score_1, v1) 0.020611206\n",
            "self.loss(score_2, v2) 0.026479477\n",
            "self.loss(score_3, v3) 0.015331631\n",
            "loss tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015219139\n",
            "self.loss(score_1, v1) 0.02494651\n",
            "self.loss(score_2, v2) 0.04683412\n",
            "self.loss(score_3, v3) 0.009493764\n",
            "loss tensor(0.0965, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012100191\n",
            "self.loss(score_1, v1) 0.016536094\n",
            "self.loss(score_2, v2) 0.020332439\n",
            "self.loss(score_3, v3) 0.013612871\n",
            "loss tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0140949525\n",
            "self.loss(score_1, v1) 0.013392895\n",
            "self.loss(score_2, v2) 0.009588105\n",
            "self.loss(score_3, v3) 0.008503247\n",
            "loss tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012387308\n",
            "self.loss(score_1, v1) 0.013307981\n",
            "self.loss(score_2, v2) 0.011569982\n",
            "self.loss(score_3, v3) 0.010337495\n",
            "loss tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009851945\n",
            "self.loss(score_1, v1) 0.019317938\n",
            "self.loss(score_2, v2) 0.014490085\n",
            "self.loss(score_3, v3) 0.08865126\n",
            "loss tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010773106\n",
            "self.loss(score_1, v1) 0.012852222\n",
            "self.loss(score_2, v2) 0.016125467\n",
            "self.loss(score_3, v3) 0.007191699\n",
            "loss tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008934535\n",
            "self.loss(score_1, v1) 0.022301115\n",
            "self.loss(score_2, v2) 0.019535819\n",
            "self.loss(score_3, v3) 0.01201267\n",
            "loss tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009820045\n",
            "self.loss(score_1, v1) 0.03234072\n",
            "self.loss(score_2, v2) 0.06738235\n",
            "self.loss(score_3, v3) 0.037584025\n",
            "loss tensor(0.1471, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0099899685\n",
            "self.loss(score_1, v1) 0.083944105\n",
            "self.loss(score_2, v2) 0.12417024\n",
            "self.loss(score_3, v3) 0.07588143\n",
            "loss tensor(0.2940, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017772553\n",
            "self.loss(score_1, v1) 0.02714726\n",
            "self.loss(score_2, v2) 0.020430535\n",
            "self.loss(score_3, v3) 0.026495248\n",
            "loss tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018395206\n",
            "self.loss(score_1, v1) 0.0270877\n",
            "self.loss(score_2, v2) 0.03260691\n",
            "self.loss(score_3, v3) 0.013061385\n",
            "loss tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008277598\n",
            "self.loss(score_1, v1) 0.009696789\n",
            "self.loss(score_2, v2) 0.010379645\n",
            "self.loss(score_3, v3) 0.006820112\n",
            "loss tensor(0.0352, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0101383235\n",
            "self.loss(score_1, v1) 0.023000222\n",
            "self.loss(score_2, v2) 0.023907822\n",
            "self.loss(score_3, v3) 0.018620428\n",
            "loss tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009187164\n",
            "self.loss(score_1, v1) 0.016342608\n",
            "self.loss(score_2, v2) 0.017169906\n",
            "self.loss(score_3, v3) 0.007468092\n",
            "loss tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020848492\n",
            "self.loss(score_1, v1) 0.019070178\n",
            "self.loss(score_2, v2) 0.018212618\n",
            "self.loss(score_3, v3) 0.012763376\n",
            "loss tensor(0.0709, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0094128065\n",
            "self.loss(score_1, v1) 0.021408275\n",
            "self.loss(score_2, v2) 0.023012212\n",
            "self.loss(score_3, v3) 0.009655403\n",
            "loss tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017258167\n",
            "self.loss(score_1, v1) 0.014764978\n",
            "self.loss(score_2, v2) 0.014404373\n",
            "self.loss(score_3, v3) 0.008650588\n",
            "loss tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009625272\n",
            "self.loss(score_1, v1) 0.018417416\n",
            "self.loss(score_2, v2) 0.022771068\n",
            "self.loss(score_3, v3) 0.0109907\n",
            "loss tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014065726\n",
            "self.loss(score_1, v1) 0.038128432\n",
            "self.loss(score_2, v2) 0.042398766\n",
            "self.loss(score_3, v3) 0.017698001\n",
            "loss tensor(0.1123, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008999954\n",
            "self.loss(score_1, v1) 0.020537715\n",
            "self.loss(score_2, v2) 0.018225618\n",
            "self.loss(score_3, v3) 0.009938978\n",
            "loss tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009007823\n",
            "self.loss(score_1, v1) 0.009374253\n",
            "self.loss(score_2, v2) 0.010656962\n",
            "self.loss(score_3, v3) 0.007299928\n",
            "loss tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020161832\n",
            "self.loss(score_1, v1) 0.03025235\n",
            "self.loss(score_2, v2) 0.031938363\n",
            "self.loss(score_3, v3) 0.012345699\n",
            "loss tensor(0.0947, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011673727\n",
            "self.loss(score_1, v1) 0.010084954\n",
            "self.loss(score_2, v2) 0.010166222\n",
            "self.loss(score_3, v3) 0.012679663\n",
            "loss tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016136656\n",
            "self.loss(score_1, v1) 0.028028006\n",
            "self.loss(score_2, v2) 0.02268634\n",
            "self.loss(score_3, v3) 0.010236117\n",
            "loss tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0730999\n",
            "self.loss(score_1, v1) 0.014262407\n",
            "self.loss(score_2, v2) 0.044637658\n",
            "self.loss(score_3, v3) 0.02696176\n",
            "loss tensor(0.1590, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016338823\n",
            "self.loss(score_1, v1) 0.022708522\n",
            "self.loss(score_2, v2) 0.017184516\n",
            "self.loss(score_3, v3) 0.00923679\n",
            "loss tensor(0.0655, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.025271792\n",
            "self.loss(score_1, v1) 0.045435622\n",
            "self.loss(score_2, v2) 0.015179924\n",
            "self.loss(score_3, v3) 0.008895392\n",
            "loss tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017189479\n",
            "self.loss(score_1, v1) 0.025523348\n",
            "self.loss(score_2, v2) 0.021937203\n",
            "self.loss(score_3, v3) 0.012767362\n",
            "loss tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.023962794\n",
            "self.loss(score_1, v1) 0.023863953\n",
            "self.loss(score_2, v2) 0.0111416625\n",
            "self.loss(score_3, v3) 0.007783916\n",
            "loss tensor(0.0668, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017680952\n",
            "self.loss(score_1, v1) 0.022143403\n",
            "self.loss(score_2, v2) 0.011982905\n",
            "self.loss(score_3, v3) 0.007598514\n",
            "loss tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008912272\n",
            "self.loss(score_1, v1) 0.016225148\n",
            "self.loss(score_2, v2) 0.022033164\n",
            "self.loss(score_3, v3) 0.013187577\n",
            "loss tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022837734\n",
            "self.loss(score_1, v1) 0.04198862\n",
            "self.loss(score_2, v2) 0.05000887\n",
            "self.loss(score_3, v3) 0.009200538\n",
            "loss tensor(0.1240, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0060975486\n",
            "self.loss(score_1, v1) 0.012870446\n",
            "self.loss(score_2, v2) 0.017007094\n",
            "self.loss(score_3, v3) 0.0069209775\n",
            "loss tensor(0.0429, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01790129\n",
            "self.loss(score_1, v1) 0.027586265\n",
            "self.loss(score_2, v2) 0.045593716\n",
            "self.loss(score_3, v3) 0.013028072\n",
            "loss tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0071883583\n",
            "self.loss(score_1, v1) 0.008728351\n",
            "self.loss(score_2, v2) 0.012966143\n",
            "self.loss(score_3, v3) 0.008383021\n",
            "loss tensor(0.0373, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00788709\n",
            "self.loss(score_1, v1) 0.015967818\n",
            "self.loss(score_2, v2) 0.02521952\n",
            "self.loss(score_3, v3) 0.010938889\n",
            "loss tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.06971494\n",
            "self.loss(score_1, v1) 0.07612205\n",
            "self.loss(score_2, v2) 0.053586636\n",
            "self.loss(score_3, v3) 0.049110867\n",
            "loss tensor(0.2485, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0068607945\n",
            "self.loss(score_1, v1) 0.0140117975\n",
            "self.loss(score_2, v2) 0.022539655\n",
            "self.loss(score_3, v3) 0.00862705\n",
            "loss tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006293817\n",
            "self.loss(score_1, v1) 0.012530289\n",
            "self.loss(score_2, v2) 0.015185718\n",
            "self.loss(score_3, v3) 0.007603683\n",
            "loss tensor(0.0416, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022071918\n",
            "self.loss(score_1, v1) 0.034816902\n",
            "self.loss(score_2, v2) 0.021061772\n",
            "self.loss(score_3, v3) 0.011837246\n",
            "loss tensor(0.0898, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022106288\n",
            "self.loss(score_1, v1) 0.09154364\n",
            "self.loss(score_2, v2) 0.08073229\n",
            "self.loss(score_3, v3) 0.02341448\n",
            "loss tensor(0.2178, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01637846\n",
            "self.loss(score_1, v1) 0.014466538\n",
            "self.loss(score_2, v2) 0.015321333\n",
            "self.loss(score_3, v3) 0.010659148\n",
            "loss tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01952201\n",
            "self.loss(score_1, v1) 0.033424232\n",
            "self.loss(score_2, v2) 0.04829934\n",
            "self.loss(score_3, v3) 0.008673037\n",
            "loss tensor(0.1099, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016552461\n",
            "self.loss(score_1, v1) 0.03387169\n",
            "self.loss(score_2, v2) 0.06774229\n",
            "self.loss(score_3, v3) 0.052687626\n",
            "loss tensor(0.1709, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0075253975\n",
            "self.loss(score_1, v1) 0.07142912\n",
            "self.loss(score_2, v2) 0.096159704\n",
            "self.loss(score_3, v3) 0.059606254\n",
            "loss tensor(0.2347, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010487216\n",
            "self.loss(score_1, v1) 0.0135510955\n",
            "self.loss(score_2, v2) 0.019110972\n",
            "self.loss(score_3, v3) 0.007996881\n",
            "loss tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00406927\n",
            "self.loss(score_1, v1) 0.010509058\n",
            "self.loss(score_2, v2) 0.016945336\n",
            "self.loss(score_3, v3) 0.00439648\n",
            "loss tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009962669\n",
            "self.loss(score_1, v1) 0.0115668615\n",
            "self.loss(score_2, v2) 0.011397039\n",
            "self.loss(score_3, v3) 0.0101409415\n",
            "loss tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0070665274\n",
            "self.loss(score_1, v1) 0.020766899\n",
            "self.loss(score_2, v2) 0.020096714\n",
            "self.loss(score_3, v3) 0.0116722435\n",
            "loss tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007801757\n",
            "self.loss(score_1, v1) 0.007137866\n",
            "self.loss(score_2, v2) 0.00734514\n",
            "self.loss(score_3, v3) 0.009061674\n",
            "loss tensor(0.0313, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01038218\n",
            "self.loss(score_1, v1) 0.008237906\n",
            "self.loss(score_2, v2) 0.005140118\n",
            "self.loss(score_3, v3) 0.0053319423\n",
            "loss tensor(0.0291, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012740704\n",
            "self.loss(score_1, v1) 0.012548879\n",
            "self.loss(score_2, v2) 0.011513341\n",
            "self.loss(score_3, v3) 0.008774455\n",
            "loss tensor(0.0456, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.026724756\n",
            "self.loss(score_1, v1) 0.015952328\n",
            "self.loss(score_2, v2) 0.015811127\n",
            "self.loss(score_3, v3) 0.010796759\n",
            "loss tensor(0.0693, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0055587203\n",
            "self.loss(score_1, v1) 0.0068998085\n",
            "self.loss(score_2, v2) 0.0062542227\n",
            "self.loss(score_3, v3) 0.0048693432\n",
            "loss tensor(0.0236, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013213516\n",
            "self.loss(score_1, v1) 0.021899838\n",
            "self.loss(score_2, v2) 0.05017638\n",
            "self.loss(score_3, v3) 0.013967394\n",
            "loss tensor(0.0993, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.032378312\n",
            "self.loss(score_1, v1) 0.06519574\n",
            "self.loss(score_2, v2) 0.010851026\n",
            "self.loss(score_3, v3) 0.00650937\n",
            "loss tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0072059203\n",
            "self.loss(score_1, v1) 0.011932475\n",
            "self.loss(score_2, v2) 0.009850097\n",
            "self.loss(score_3, v3) 0.00826137\n",
            "loss tensor(0.0372, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008669575\n",
            "self.loss(score_1, v1) 0.013351988\n",
            "self.loss(score_2, v2) 0.018198745\n",
            "self.loss(score_3, v3) 0.011599708\n",
            "loss tensor(0.0518, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015335421\n",
            "self.loss(score_1, v1) 0.038134124\n",
            "self.loss(score_2, v2) 0.052087042\n",
            "self.loss(score_3, v3) 0.012926082\n",
            "loss tensor(0.1185, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0103177605\n",
            "self.loss(score_1, v1) 0.029439071\n",
            "self.loss(score_2, v2) 0.018154474\n",
            "self.loss(score_3, v3) 0.007405985\n",
            "loss tensor(0.0653, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013595906\n",
            "self.loss(score_1, v1) 0.03200514\n",
            "self.loss(score_2, v2) 0.01793961\n",
            "self.loss(score_3, v3) 0.007908291\n",
            "loss tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016197432\n",
            "self.loss(score_1, v1) 0.017554823\n",
            "self.loss(score_2, v2) 0.021883925\n",
            "self.loss(score_3, v3) 0.011268343\n",
            "loss tensor(0.0669, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0060049356\n",
            "self.loss(score_1, v1) 0.010871433\n",
            "self.loss(score_2, v2) 0.039719924\n",
            "self.loss(score_3, v3) 0.00517644\n",
            "loss tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02059258\n",
            "self.loss(score_1, v1) 0.023339521\n",
            "self.loss(score_2, v2) 0.008670856\n",
            "self.loss(score_3, v3) 0.0077845273\n",
            "loss tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008854803\n",
            "self.loss(score_1, v1) 0.011504709\n",
            "self.loss(score_2, v2) 0.01733871\n",
            "self.loss(score_3, v3) 0.010077215\n",
            "loss tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006583199\n",
            "self.loss(score_1, v1) 0.008733772\n",
            "self.loss(score_2, v2) 0.008738879\n",
            "self.loss(score_3, v3) 0.008235688\n",
            "loss tensor(0.0323, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.085845016\n",
            "self.loss(score_1, v1) 0.04709713\n",
            "self.loss(score_2, v2) 0.022820776\n",
            "self.loss(score_3, v3) 0.009168109\n",
            "loss tensor(0.1649, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009458235\n",
            "self.loss(score_1, v1) 0.007857937\n",
            "self.loss(score_2, v2) 0.008031328\n",
            "self.loss(score_3, v3) 0.0073865512\n",
            "loss tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009238827\n",
            "self.loss(score_1, v1) 0.018310115\n",
            "self.loss(score_2, v2) 0.018451294\n",
            "self.loss(score_3, v3) 0.013568601\n",
            "loss tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019348592\n",
            "self.loss(score_1, v1) 0.057706326\n",
            "self.loss(score_2, v2) 0.096012995\n",
            "self.loss(score_3, v3) 0.21887992\n",
            "loss tensor(0.3919, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010495273\n",
            "self.loss(score_1, v1) 0.010681693\n",
            "self.loss(score_2, v2) 0.010467617\n",
            "self.loss(score_3, v3) 0.0075478205\n",
            "loss tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.07591853\n",
            "self.loss(score_1, v1) 0.04861526\n",
            "self.loss(score_2, v2) 0.044517875\n",
            "self.loss(score_3, v3) 0.009589734\n",
            "loss tensor(0.1786, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02317465\n",
            "self.loss(score_1, v1) 0.03126976\n",
            "self.loss(score_2, v2) 0.05114218\n",
            "self.loss(score_3, v3) 0.007456841\n",
            "loss tensor(0.1130, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010983417\n",
            "self.loss(score_1, v1) 0.01817837\n",
            "self.loss(score_2, v2) 0.021671547\n",
            "self.loss(score_3, v3) 0.014922891\n",
            "loss tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016000586\n",
            "self.loss(score_1, v1) 0.013008869\n",
            "self.loss(score_2, v2) 0.0059605567\n",
            "self.loss(score_3, v3) 0.00571834\n",
            "loss tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019417401\n",
            "self.loss(score_1, v1) 0.04507021\n",
            "self.loss(score_2, v2) 0.0350347\n",
            "self.loss(score_3, v3) 0.008299356\n",
            "loss tensor(0.1078, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.056908987\n",
            "self.loss(score_1, v1) 0.04616496\n",
            "self.loss(score_2, v2) 0.017313184\n",
            "self.loss(score_3, v3) 0.00904986\n",
            "loss tensor(0.1294, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0075277346\n",
            "self.loss(score_1, v1) 0.020484934\n",
            "self.loss(score_2, v2) 0.021903096\n",
            "self.loss(score_3, v3) 0.010109192\n",
            "loss tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.05506323\n",
            "self.loss(score_1, v1) 0.029658092\n",
            "self.loss(score_2, v2) 0.03716762\n",
            "self.loss(score_3, v3) 0.008099436\n",
            "loss tensor(0.1300, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018418353\n",
            "self.loss(score_1, v1) 0.045083974\n",
            "self.loss(score_2, v2) 0.011098775\n",
            "self.loss(score_3, v3) 0.0074912044\n",
            "loss tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0153277395\n",
            "self.loss(score_1, v1) 0.018124474\n",
            "self.loss(score_2, v2) 0.00949351\n",
            "self.loss(score_3, v3) 0.008756663\n",
            "loss tensor(0.0517, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013567471\n",
            "self.loss(score_1, v1) 0.018851142\n",
            "self.loss(score_2, v2) 0.05588924\n",
            "self.loss(score_3, v3) 0.010425293\n",
            "loss tensor(0.0987, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.06272858\n",
            "self.loss(score_1, v1) 0.09856566\n",
            "self.loss(score_2, v2) 0.09293679\n",
            "self.loss(score_3, v3) 0.07031533\n",
            "loss tensor(0.3245, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006972114\n",
            "self.loss(score_1, v1) 0.011200397\n",
            "self.loss(score_2, v2) 0.024086267\n",
            "self.loss(score_3, v3) 0.009051748\n",
            "loss tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0712209\n",
            "self.loss(score_1, v1) 0.05400306\n",
            "self.loss(score_2, v2) 0.03239042\n",
            "self.loss(score_3, v3) 0.0099827675\n",
            "loss tensor(0.1676, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011910435\n",
            "self.loss(score_1, v1) 0.009650251\n",
            "self.loss(score_2, v2) 0.009065192\n",
            "self.loss(score_3, v3) 0.011974574\n",
            "loss tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009284867\n",
            "self.loss(score_1, v1) 0.025412267\n",
            "self.loss(score_2, v2) 0.013646524\n",
            "self.loss(score_3, v3) 0.006502757\n",
            "loss tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00867151\n",
            "self.loss(score_1, v1) 0.015313638\n",
            "self.loss(score_2, v2) 0.014620701\n",
            "self.loss(score_3, v3) 0.009955313\n",
            "loss tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014540188\n",
            "self.loss(score_1, v1) 0.016041761\n",
            "self.loss(score_2, v2) 0.024886023\n",
            "self.loss(score_3, v3) 0.032931417\n",
            "loss tensor(0.0884, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007566345\n",
            "self.loss(score_1, v1) 0.010050755\n",
            "self.loss(score_2, v2) 0.01589576\n",
            "self.loss(score_3, v3) 0.006561205\n",
            "loss tensor(0.0401, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009035899\n",
            "self.loss(score_1, v1) 0.009160886\n",
            "self.loss(score_2, v2) 0.010006239\n",
            "self.loss(score_3, v3) 0.007131354\n",
            "loss tensor(0.0353, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020109722\n",
            "self.loss(score_1, v1) 0.0706467\n",
            "self.loss(score_2, v2) 0.07431433\n",
            "self.loss(score_3, v3) 0.025801381\n",
            "loss tensor(0.1909, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012984458\n",
            "self.loss(score_1, v1) 0.013963963\n",
            "self.loss(score_2, v2) 0.015174408\n",
            "self.loss(score_3, v3) 0.00831599\n",
            "loss tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008767575\n",
            "self.loss(score_1, v1) 0.011191666\n",
            "self.loss(score_2, v2) 0.008075584\n",
            "self.loss(score_3, v3) 0.0082268175\n",
            "loss tensor(0.0363, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017939745\n",
            "self.loss(score_1, v1) 0.015095323\n",
            "self.loss(score_2, v2) 0.022553338\n",
            "self.loss(score_3, v3) 0.009964853\n",
            "loss tensor(0.0656, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011755718\n",
            "self.loss(score_1, v1) 0.021229826\n",
            "self.loss(score_2, v2) 0.0110108415\n",
            "self.loss(score_3, v3) 0.010032618\n",
            "loss tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010850653\n",
            "self.loss(score_1, v1) 0.017402343\n",
            "self.loss(score_2, v2) 0.04059855\n",
            "self.loss(score_3, v3) 0.0087084435\n",
            "loss tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021142831\n",
            "self.loss(score_1, v1) 0.020521441\n",
            "self.loss(score_2, v2) 0.018700812\n",
            "self.loss(score_3, v3) 0.0072380784\n",
            "loss tensor(0.0676, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019141665\n",
            "self.loss(score_1, v1) 0.026380995\n",
            "self.loss(score_2, v2) 0.04520932\n",
            "self.loss(score_3, v3) 0.01312183\n",
            "loss tensor(0.1039, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013627508\n",
            "self.loss(score_1, v1) 0.041286007\n",
            "self.loss(score_2, v2) 0.0421739\n",
            "self.loss(score_3, v3) 0.017521529\n",
            "loss tensor(0.1146, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02105023\n",
            "self.loss(score_1, v1) 0.03765479\n",
            "self.loss(score_2, v2) 0.06244263\n",
            "self.loss(score_3, v3) 0.058494918\n",
            "loss tensor(0.1796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.043898765\n",
            "self.loss(score_1, v1) 0.04371566\n",
            "self.loss(score_2, v2) 0.08820785\n",
            "self.loss(score_3, v3) 0.012344333\n",
            "loss tensor(0.1882, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015746348\n",
            "self.loss(score_1, v1) 0.034440886\n",
            "self.loss(score_2, v2) 0.024386523\n",
            "self.loss(score_3, v3) 0.013510524\n",
            "loss tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014915649\n",
            "self.loss(score_1, v1) 0.021548744\n",
            "self.loss(score_2, v2) 0.012735058\n",
            "self.loss(score_3, v3) 0.008182118\n",
            "loss tensor(0.0574, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.026982\n",
            "self.loss(score_1, v1) 0.030055502\n",
            "self.loss(score_2, v2) 0.009699825\n",
            "self.loss(score_3, v3) 0.007808568\n",
            "loss tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0069103986\n",
            "self.loss(score_1, v1) 0.006038026\n",
            "self.loss(score_2, v2) 0.008857067\n",
            "self.loss(score_3, v3) 0.0055020005\n",
            "loss tensor(0.0273, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009258653\n",
            "self.loss(score_1, v1) 0.017006192\n",
            "self.loss(score_2, v2) 0.03067244\n",
            "self.loss(score_3, v3) 0.010488073\n",
            "loss tensor(0.0674, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.058249645\n",
            "self.loss(score_1, v1) 0.019790454\n",
            "self.loss(score_2, v2) 0.01738275\n",
            "self.loss(score_3, v3) 0.009664288\n",
            "loss tensor(0.1051, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010100992\n",
            "self.loss(score_1, v1) 0.032784443\n",
            "self.loss(score_2, v2) 0.0282217\n",
            "self.loss(score_3, v3) 0.011759229\n",
            "loss tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019316785\n",
            "self.loss(score_1, v1) 0.034512255\n",
            "self.loss(score_2, v2) 0.022269206\n",
            "self.loss(score_3, v3) 0.008899743\n",
            "loss tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.004813704\n",
            "self.loss(score_1, v1) 0.01320683\n",
            "self.loss(score_2, v2) 0.019934883\n",
            "self.loss(score_3, v3) 0.009462097\n",
            "loss tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022631893\n",
            "self.loss(score_1, v1) 0.06030199\n",
            "self.loss(score_2, v2) 0.031447705\n",
            "self.loss(score_3, v3) 0.008481778\n",
            "loss tensor(0.1229, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015173124\n",
            "self.loss(score_1, v1) 0.013895484\n",
            "self.loss(score_2, v2) 0.015926823\n",
            "self.loss(score_3, v3) 0.009847728\n",
            "loss tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011968166\n",
            "self.loss(score_1, v1) 0.028393513\n",
            "self.loss(score_2, v2) 0.03067587\n",
            "self.loss(score_3, v3) 0.010537483\n",
            "loss tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010452461\n",
            "self.loss(score_1, v1) 0.010384182\n",
            "self.loss(score_2, v2) 0.01728497\n",
            "self.loss(score_3, v3) 0.013769067\n",
            "loss tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007920937\n",
            "self.loss(score_1, v1) 0.034289397\n",
            "self.loss(score_2, v2) 0.013864403\n",
            "self.loss(score_3, v3) 0.006821078\n",
            "loss tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021933466\n",
            "self.loss(score_1, v1) 0.084425196\n",
            "self.loss(score_2, v2) 0.05306381\n",
            "self.loss(score_3, v3) 0.012394066\n",
            "loss tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0095448755\n",
            "self.loss(score_1, v1) 0.012860535\n",
            "self.loss(score_2, v2) 0.020979045\n",
            "self.loss(score_3, v3) 0.007650204\n",
            "loss tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0067103826\n",
            "self.loss(score_1, v1) 0.009836581\n",
            "self.loss(score_2, v2) 0.010673596\n",
            "self.loss(score_3, v3) 0.005566528\n",
            "loss tensor(0.0328, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01866813\n",
            "self.loss(score_1, v1) 0.01733779\n",
            "self.loss(score_2, v2) 0.015181873\n",
            "self.loss(score_3, v3) 0.009150337\n",
            "loss tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.05086503\n",
            "self.loss(score_1, v1) 0.026518952\n",
            "self.loss(score_2, v2) 0.015478103\n",
            "self.loss(score_3, v3) 0.010175644\n",
            "loss tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.04635532\n",
            "self.loss(score_1, v1) 0.027822575\n",
            "self.loss(score_2, v2) 0.027397608\n",
            "self.loss(score_3, v3) 0.012173153\n",
            "loss tensor(0.1137, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013045459\n",
            "self.loss(score_1, v1) 0.011336663\n",
            "self.loss(score_2, v2) 0.015847951\n",
            "self.loss(score_3, v3) 0.013658091\n",
            "loss tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0148535\n",
            "self.loss(score_1, v1) 0.02429302\n",
            "self.loss(score_2, v2) 0.04138091\n",
            "self.loss(score_3, v3) 0.009044341\n",
            "loss tensor(0.0896, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008167452\n",
            "self.loss(score_1, v1) 0.0074380813\n",
            "self.loss(score_2, v2) 0.008508291\n",
            "self.loss(score_3, v3) 0.0050550755\n",
            "loss tensor(0.0292, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.025121316\n",
            "self.loss(score_1, v1) 0.068061806\n",
            "self.loss(score_2, v2) 0.09082498\n",
            "self.loss(score_3, v3) 0.021705143\n",
            "loss tensor(0.2057, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022558996\n",
            "self.loss(score_1, v1) 0.015219727\n",
            "self.loss(score_2, v2) 0.01634769\n",
            "self.loss(score_3, v3) 0.0079775965\n",
            "loss tensor(0.0621, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01304636\n",
            "self.loss(score_1, v1) 0.01283979\n",
            "self.loss(score_2, v2) 0.009645182\n",
            "self.loss(score_3, v3) 0.0124415755\n",
            "loss tensor(0.0480, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.029743511\n",
            "self.loss(score_1, v1) 0.04403341\n",
            "self.loss(score_2, v2) 0.053399704\n",
            "self.loss(score_3, v3) 0.018516831\n",
            "loss tensor(0.1457, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008979611\n",
            "self.loss(score_1, v1) 0.017163564\n",
            "self.loss(score_2, v2) 0.03308231\n",
            "self.loss(score_3, v3) 0.01269858\n",
            "loss tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.03811637\n",
            "self.loss(score_1, v1) 0.02749792\n",
            "self.loss(score_2, v2) 0.045127954\n",
            "self.loss(score_3, v3) 0.010779876\n",
            "loss tensor(0.1215, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020200305\n",
            "self.loss(score_1, v1) 0.015666055\n",
            "self.loss(score_2, v2) 0.018172272\n",
            "self.loss(score_3, v3) 0.008802065\n",
            "loss tensor(0.0628, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.045423333\n",
            "self.loss(score_1, v1) 0.038029935\n",
            "self.loss(score_2, v2) 0.035207026\n",
            "self.loss(score_3, v3) 0.010411792\n",
            "loss tensor(0.1291, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020825485\n",
            "self.loss(score_1, v1) 0.022124952\n",
            "self.loss(score_2, v2) 0.024186706\n",
            "self.loss(score_3, v3) 0.008226592\n",
            "loss tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01667431\n",
            "self.loss(score_1, v1) 0.013953272\n",
            "self.loss(score_2, v2) 0.017738268\n",
            "self.loss(score_3, v3) 0.009232676\n",
            "loss tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.052854255\n",
            "self.loss(score_1, v1) 0.02994999\n",
            "self.loss(score_2, v2) 0.034315493\n",
            "self.loss(score_3, v3) 0.008520625\n",
            "loss tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010331584\n",
            "self.loss(score_1, v1) 0.050732315\n",
            "self.loss(score_2, v2) 0.047847286\n",
            "self.loss(score_3, v3) 0.010010915\n",
            "loss tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016080853\n",
            "self.loss(score_1, v1) 0.018166749\n",
            "self.loss(score_2, v2) 0.008443115\n",
            "self.loss(score_3, v3) 0.008677622\n",
            "loss tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016739674\n",
            "self.loss(score_1, v1) 0.02014935\n",
            "self.loss(score_2, v2) 0.019814827\n",
            "self.loss(score_3, v3) 0.010310871\n",
            "loss tensor(0.0670, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.04438357\n",
            "self.loss(score_1, v1) 0.018799333\n",
            "self.loss(score_2, v2) 0.02814887\n",
            "self.loss(score_3, v3) 0.013500021\n",
            "loss tensor(0.1048, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.024365816\n",
            "self.loss(score_1, v1) 0.023062471\n",
            "self.loss(score_2, v2) 0.021031553\n",
            "self.loss(score_3, v3) 0.010556906\n",
            "loss tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.025063725\n",
            "self.loss(score_1, v1) 0.016096571\n",
            "self.loss(score_2, v2) 0.0066949087\n",
            "self.loss(score_3, v3) 0.0066494215\n",
            "loss tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010121809\n",
            "self.loss(score_1, v1) 0.017963903\n",
            "self.loss(score_2, v2) 0.019491246\n",
            "self.loss(score_3, v3) 0.008858517\n",
            "loss tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008399064\n",
            "self.loss(score_1, v1) 0.009310957\n",
            "self.loss(score_2, v2) 0.0111185415\n",
            "self.loss(score_3, v3) 0.009840208\n",
            "loss tensor(0.0387, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.005839964\n",
            "self.loss(score_1, v1) 0.0077522\n",
            "self.loss(score_2, v2) 0.011173068\n",
            "self.loss(score_3, v3) 0.0087078065\n",
            "loss tensor(0.0335, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.036074746\n",
            "self.loss(score_1, v1) 0.028590895\n",
            "self.loss(score_2, v2) 0.055865247\n",
            "self.loss(score_3, v3) 0.0108587\n",
            "loss tensor(0.1314, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014734063\n",
            "self.loss(score_1, v1) 0.044285294\n",
            "self.loss(score_2, v2) 0.04483026\n",
            "self.loss(score_3, v3) 0.014502518\n",
            "loss tensor(0.1184, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010326051\n",
            "self.loss(score_1, v1) 0.00934951\n",
            "self.loss(score_2, v2) 0.0074939844\n",
            "self.loss(score_3, v3) 0.0076680705\n",
            "loss tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010583379\n",
            "self.loss(score_1, v1) 0.024542063\n",
            "self.loss(score_2, v2) 0.025917238\n",
            "self.loss(score_3, v3) 0.008880619\n",
            "loss tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012673742\n",
            "self.loss(score_1, v1) 0.022848219\n",
            "self.loss(score_2, v2) 0.020987246\n",
            "self.loss(score_3, v3) 0.061073743\n",
            "loss tensor(0.1176, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0067299563\n",
            "self.loss(score_1, v1) 0.0135736\n",
            "self.loss(score_2, v2) 0.023608577\n",
            "self.loss(score_3, v3) 0.0106215635\n",
            "loss tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.05920388\n",
            "self.loss(score_1, v1) 0.021495888\n",
            "self.loss(score_2, v2) 0.015096204\n",
            "self.loss(score_3, v3) 0.010142146\n",
            "loss tensor(0.1059, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015788283\n",
            "self.loss(score_1, v1) 0.012017312\n",
            "self.loss(score_2, v2) 0.011876888\n",
            "self.loss(score_3, v3) 0.009842015\n",
            "loss tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.04022199\n",
            "self.loss(score_1, v1) 0.041051578\n",
            "self.loss(score_2, v2) 0.024777574\n",
            "self.loss(score_3, v3) 0.007891297\n",
            "loss tensor(0.1139, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010642076\n",
            "self.loss(score_1, v1) 0.01136567\n",
            "self.loss(score_2, v2) 0.0053691682\n",
            "self.loss(score_3, v3) 0.004858699\n",
            "loss tensor(0.0322, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00893242\n",
            "self.loss(score_1, v1) 0.012390997\n",
            "self.loss(score_2, v2) 0.016990423\n",
            "self.loss(score_3, v3) 0.008697022\n",
            "loss tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007667471\n",
            "self.loss(score_1, v1) 0.012191752\n",
            "self.loss(score_2, v2) 0.011082062\n",
            "self.loss(score_3, v3) 0.007029393\n",
            "loss tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014251167\n",
            "self.loss(score_1, v1) 0.020537622\n",
            "self.loss(score_2, v2) 0.029147683\n",
            "self.loss(score_3, v3) 0.0081318235\n",
            "loss tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0081662\n",
            "self.loss(score_1, v1) 0.009863157\n",
            "self.loss(score_2, v2) 0.019433057\n",
            "self.loss(score_3, v3) 0.009202106\n",
            "loss tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.04235563\n",
            "self.loss(score_1, v1) 0.02569698\n",
            "self.loss(score_2, v2) 0.02935205\n",
            "self.loss(score_3, v3) 0.0073311855\n",
            "loss tensor(0.1047, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014447976\n",
            "self.loss(score_1, v1) 0.016500356\n",
            "self.loss(score_2, v2) 0.010627456\n",
            "self.loss(score_3, v3) 0.009014285\n",
            "loss tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011165576\n",
            "self.loss(score_1, v1) 0.045977347\n",
            "self.loss(score_2, v2) 0.041052118\n",
            "self.loss(score_3, v3) 0.1037502\n",
            "loss tensor(0.2019, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008856833\n",
            "self.loss(score_1, v1) 0.015772365\n",
            "self.loss(score_2, v2) 0.021017462\n",
            "self.loss(score_3, v3) 0.011271283\n",
            "loss tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0076218965\n",
            "self.loss(score_1, v1) 0.0089327535\n",
            "self.loss(score_2, v2) 0.010000115\n",
            "self.loss(score_3, v3) 0.007906229\n",
            "loss tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02947617\n",
            "self.loss(score_1, v1) 0.0245229\n",
            "self.loss(score_2, v2) 0.0101279365\n",
            "self.loss(score_3, v3) 0.0079240855\n",
            "loss tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009125506\n",
            "self.loss(score_1, v1) 0.01117707\n",
            "self.loss(score_2, v2) 0.018934546\n",
            "self.loss(score_3, v3) 0.0073646186\n",
            "loss tensor(0.0466, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008004444\n",
            "self.loss(score_1, v1) 0.015573\n",
            "self.loss(score_2, v2) 0.023505496\n",
            "self.loss(score_3, v3) 0.0123871155\n",
            "loss tensor(0.0595, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011433243\n",
            "self.loss(score_1, v1) 0.019548208\n",
            "self.loss(score_2, v2) 0.024790531\n",
            "self.loss(score_3, v3) 0.0068480247\n",
            "loss tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0069091534\n",
            "self.loss(score_1, v1) 0.012464841\n",
            "self.loss(score_2, v2) 0.014308329\n",
            "self.loss(score_3, v3) 0.007375952\n",
            "loss tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014012184\n",
            "self.loss(score_1, v1) 0.030507907\n",
            "self.loss(score_2, v2) 0.040109143\n",
            "self.loss(score_3, v3) 0.01155254\n",
            "loss tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.024948826\n",
            "self.loss(score_1, v1) 0.020690313\n",
            "self.loss(score_2, v2) 0.018181745\n",
            "self.loss(score_3, v3) 0.012568506\n",
            "loss tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00957055\n",
            "self.loss(score_1, v1) 0.036228955\n",
            "self.loss(score_2, v2) 0.053879406\n",
            "self.loss(score_3, v3) 0.041221883\n",
            "loss tensor(0.1409, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016782831\n",
            "self.loss(score_1, v1) 0.010776923\n",
            "self.loss(score_2, v2) 0.01609104\n",
            "self.loss(score_3, v3) 0.010087416\n",
            "loss tensor(0.0537, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0278699\n",
            "self.loss(score_1, v1) 0.026661037\n",
            "self.loss(score_2, v2) 0.010883368\n",
            "self.loss(score_3, v3) 0.006577962\n",
            "loss tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020425541\n",
            "self.loss(score_1, v1) 0.05108728\n",
            "self.loss(score_2, v2) 0.037690125\n",
            "self.loss(score_3, v3) 0.009615799\n",
            "loss tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.07272603\n",
            "self.loss(score_1, v1) 0.038725793\n",
            "self.loss(score_2, v2) 0.071295686\n",
            "self.loss(score_3, v3) 0.008583575\n",
            "loss tensor(0.1913, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.028943041\n",
            "self.loss(score_1, v1) 0.016075231\n",
            "self.loss(score_2, v2) 0.009933362\n",
            "self.loss(score_3, v3) 0.006494686\n",
            "loss tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.03088128\n",
            "self.loss(score_1, v1) 0.019960586\n",
            "self.loss(score_2, v2) 0.02579882\n",
            "self.loss(score_3, v3) 0.009677794\n",
            "loss tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0045920163\n",
            "self.loss(score_1, v1) 0.014720272\n",
            "self.loss(score_2, v2) 0.02986746\n",
            "self.loss(score_3, v3) 0.0057516545\n",
            "loss tensor(0.0549, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.024148095\n",
            "self.loss(score_1, v1) 0.026304968\n",
            "self.loss(score_2, v2) 0.022760028\n",
            "self.loss(score_3, v3) 0.008830324\n",
            "loss tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010481343\n",
            "self.loss(score_1, v1) 0.01333267\n",
            "self.loss(score_2, v2) 0.01204556\n",
            "self.loss(score_3, v3) 0.012728261\n",
            "loss tensor(0.0486, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00841881\n",
            "self.loss(score_1, v1) 0.011630016\n",
            "self.loss(score_2, v2) 0.0133543145\n",
            "self.loss(score_3, v3) 0.006446942\n",
            "loss tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019634843\n",
            "self.loss(score_1, v1) 0.023235377\n",
            "self.loss(score_2, v2) 0.022202954\n",
            "self.loss(score_3, v3) 0.0081800185\n",
            "loss tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.024827758\n",
            "self.loss(score_1, v1) 0.041093256\n",
            "self.loss(score_2, v2) 0.042471886\n",
            "self.loss(score_3, v3) 0.008199594\n",
            "loss tensor(0.1166, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0083471285\n",
            "self.loss(score_1, v1) 0.008254702\n",
            "self.loss(score_2, v2) 0.01621561\n",
            "self.loss(score_3, v3) 0.01128439\n",
            "loss tensor(0.0441, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019429985\n",
            "self.loss(score_1, v1) 0.017795661\n",
            "self.loss(score_2, v2) 0.012901968\n",
            "self.loss(score_3, v3) 0.008438402\n",
            "loss tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008383882\n",
            "self.loss(score_1, v1) 0.009982878\n",
            "self.loss(score_2, v2) 0.017369624\n",
            "self.loss(score_3, v3) 0.010246928\n",
            "loss tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014149789\n",
            "self.loss(score_1, v1) 0.050134063\n",
            "self.loss(score_2, v2) 0.05953775\n",
            "self.loss(score_3, v3) 0.010524419\n",
            "loss tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.09017706076335755, Train Accuracy : 0.9992805228670049\n",
            " Validation Accuracy : 6.602244421454096\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) 0.006513127\n",
            "self.loss(score_1, v1) 0.024440018\n",
            "self.loss(score_2, v2) 0.08063685\n",
            "self.loss(score_3, v3) 0.030884009\n",
            "loss tensor(0.1425, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006988348\n",
            "self.loss(score_1, v1) 0.009669224\n",
            "self.loss(score_2, v2) 0.07002871\n",
            "self.loss(score_3, v3) 0.037900284\n",
            "loss tensor(0.1246, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.05584995\n",
            "self.loss(score_1, v1) 0.05633933\n",
            "self.loss(score_2, v2) 0.05245511\n",
            "self.loss(score_3, v3) 0.007136889\n",
            "loss tensor(0.1718, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.049684938\n",
            "self.loss(score_1, v1) 0.030100306\n",
            "self.loss(score_2, v2) 0.02329082\n",
            "self.loss(score_3, v3) 0.009651514\n",
            "loss tensor(0.1127, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016293375\n",
            "self.loss(score_1, v1) 0.05902739\n",
            "self.loss(score_2, v2) 0.037637986\n",
            "self.loss(score_3, v3) 0.007237058\n",
            "loss tensor(0.1202, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008031558\n",
            "self.loss(score_1, v1) 0.008971337\n",
            "self.loss(score_2, v2) 0.007808202\n",
            "self.loss(score_3, v3) 0.006262589\n",
            "loss tensor(0.0311, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008269716\n",
            "self.loss(score_1, v1) 0.011881764\n",
            "self.loss(score_2, v2) 0.01181885\n",
            "self.loss(score_3, v3) 0.007613341\n",
            "loss tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012459568\n",
            "self.loss(score_1, v1) 0.014131076\n",
            "self.loss(score_2, v2) 0.009812971\n",
            "self.loss(score_3, v3) 0.0055975425\n",
            "loss tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009117694\n",
            "self.loss(score_1, v1) 0.012927552\n",
            "self.loss(score_2, v2) 0.016930617\n",
            "self.loss(score_3, v3) 0.008402769\n",
            "loss tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008309487\n",
            "self.loss(score_1, v1) 0.009741631\n",
            "self.loss(score_2, v2) 0.019032307\n",
            "self.loss(score_3, v3) 0.012411921\n",
            "loss tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02131959\n",
            "self.loss(score_1, v1) 0.040298194\n",
            "self.loss(score_2, v2) 0.035143957\n",
            "self.loss(score_3, v3) 0.0081751505\n",
            "loss tensor(0.1049, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010733657\n",
            "self.loss(score_1, v1) 0.012135796\n",
            "self.loss(score_2, v2) 0.012713927\n",
            "self.loss(score_3, v3) 0.007424743\n",
            "loss tensor(0.0430, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016847467\n",
            "self.loss(score_1, v1) 0.043301597\n",
            "self.loss(score_2, v2) 0.05159959\n",
            "self.loss(score_3, v3) 0.007892372\n",
            "loss tensor(0.1196, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.024333233\n",
            "self.loss(score_1, v1) 0.025440069\n",
            "self.loss(score_2, v2) 0.04653436\n",
            "self.loss(score_3, v3) 0.009104255\n",
            "loss tensor(0.1054, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008722702\n",
            "self.loss(score_1, v1) 0.013340856\n",
            "self.loss(score_2, v2) 0.018423954\n",
            "self.loss(score_3, v3) 0.0070545934\n",
            "loss tensor(0.0475, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.04707951\n",
            "self.loss(score_1, v1) 0.024730295\n",
            "self.loss(score_2, v2) 0.013841167\n",
            "self.loss(score_3, v3) 0.009500621\n",
            "loss tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013840062\n",
            "self.loss(score_1, v1) 0.01630946\n",
            "self.loss(score_2, v2) 0.012322903\n",
            "self.loss(score_3, v3) 0.0090263635\n",
            "loss tensor(0.0515, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012728019\n",
            "self.loss(score_1, v1) 0.015890991\n",
            "self.loss(score_2, v2) 0.01583625\n",
            "self.loss(score_3, v3) 0.0078111277\n",
            "loss tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.05016948\n",
            "self.loss(score_1, v1) 0.054359943\n",
            "self.loss(score_2, v2) 0.051805884\n",
            "self.loss(score_3, v3) 0.010193231\n",
            "loss tensor(0.1665, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018047595\n",
            "self.loss(score_1, v1) 0.026592093\n",
            "self.loss(score_2, v2) 0.04157504\n",
            "self.loss(score_3, v3) 0.012363453\n",
            "loss tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013633486\n",
            "self.loss(score_1, v1) 0.019040307\n",
            "self.loss(score_2, v2) 0.020771665\n",
            "self.loss(score_3, v3) 0.008020878\n",
            "loss tensor(0.0615, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.026856909\n",
            "self.loss(score_1, v1) 0.021841332\n",
            "self.loss(score_2, v2) 0.011805504\n",
            "self.loss(score_3, v3) 0.008396799\n",
            "loss tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006444721\n",
            "self.loss(score_1, v1) 0.014410843\n",
            "self.loss(score_2, v2) 0.025681887\n",
            "self.loss(score_3, v3) 0.010966513\n",
            "loss tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0067784623\n",
            "self.loss(score_1, v1) 0.0069383048\n",
            "self.loss(score_2, v2) 0.008284776\n",
            "self.loss(score_3, v3) 0.006312796\n",
            "loss tensor(0.0283, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008036535\n",
            "self.loss(score_1, v1) 0.009127639\n",
            "self.loss(score_2, v2) 0.00856977\n",
            "self.loss(score_3, v3) 0.0065519805\n",
            "loss tensor(0.0323, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009436226\n",
            "self.loss(score_1, v1) 0.013456791\n",
            "self.loss(score_2, v2) 0.017865775\n",
            "self.loss(score_3, v3) 0.009092835\n",
            "loss tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021285187\n",
            "self.loss(score_1, v1) 0.02536606\n",
            "self.loss(score_2, v2) 0.021272128\n",
            "self.loss(score_3, v3) 0.008719028\n",
            "loss tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0078073693\n",
            "self.loss(score_1, v1) 0.010764588\n",
            "self.loss(score_2, v2) 0.012466782\n",
            "self.loss(score_3, v3) 0.009234638\n",
            "loss tensor(0.0403, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016098911\n",
            "self.loss(score_1, v1) 0.031020435\n",
            "self.loss(score_2, v2) 0.03183813\n",
            "self.loss(score_3, v3) 0.01183442\n",
            "loss tensor(0.0908, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008867176\n",
            "self.loss(score_1, v1) 0.012670157\n",
            "self.loss(score_2, v2) 0.025708865\n",
            "self.loss(score_3, v3) 0.009752939\n",
            "loss tensor(0.0570, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.028691102\n",
            "self.loss(score_1, v1) 0.04765297\n",
            "self.loss(score_2, v2) 0.052640934\n",
            "self.loss(score_3, v3) 0.0083874455\n",
            "loss tensor(0.1374, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0107082175\n",
            "self.loss(score_1, v1) 0.010612452\n",
            "self.loss(score_2, v2) 0.011084951\n",
            "self.loss(score_3, v3) 0.007207411\n",
            "loss tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.11377374\n",
            "self.loss(score_1, v1) 0.11072375\n",
            "self.loss(score_2, v2) 0.12268199\n",
            "self.loss(score_3, v3) 0.3920316\n",
            "loss tensor(0.7392, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012278465\n",
            "self.loss(score_1, v1) 0.01414669\n",
            "self.loss(score_2, v2) 0.011075819\n",
            "self.loss(score_3, v3) 0.008275173\n",
            "loss tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008411602\n",
            "self.loss(score_1, v1) 0.013462954\n",
            "self.loss(score_2, v2) 0.022871425\n",
            "self.loss(score_3, v3) 0.009986842\n",
            "loss tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009869075\n",
            "self.loss(score_1, v1) 0.013879513\n",
            "self.loss(score_2, v2) 0.014347511\n",
            "self.loss(score_3, v3) 0.00881486\n",
            "loss tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008325115\n",
            "self.loss(score_1, v1) 0.0108121075\n",
            "self.loss(score_2, v2) 0.012358121\n",
            "self.loss(score_3, v3) 0.011121824\n",
            "loss tensor(0.0426, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0093872165\n",
            "self.loss(score_1, v1) 0.017265668\n",
            "self.loss(score_2, v2) 0.038965464\n",
            "self.loss(score_3, v3) 0.01552774\n",
            "loss tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.034434665\n",
            "self.loss(score_1, v1) 0.019505221\n",
            "self.loss(score_2, v2) 0.015807368\n",
            "self.loss(score_3, v3) 0.009265472\n",
            "loss tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015958773\n",
            "self.loss(score_1, v1) 0.028816534\n",
            "self.loss(score_2, v2) 0.03813539\n",
            "self.loss(score_3, v3) 0.0663925\n",
            "loss tensor(0.1493, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.036044028\n",
            "self.loss(score_1, v1) 0.014545728\n",
            "self.loss(score_2, v2) 0.0078588845\n",
            "self.loss(score_3, v3) 0.0052812067\n",
            "loss tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.08500466\n",
            "self.loss(score_1, v1) 0.06094863\n",
            "self.loss(score_2, v2) 0.057861328\n",
            "self.loss(score_3, v3) 0.0070482707\n",
            "loss tensor(0.2109, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011044769\n",
            "self.loss(score_1, v1) 0.010773001\n",
            "self.loss(score_2, v2) 0.013509348\n",
            "self.loss(score_3, v3) 0.012777041\n",
            "loss tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006878053\n",
            "self.loss(score_1, v1) 0.010454826\n",
            "self.loss(score_2, v2) 0.014075183\n",
            "self.loss(score_3, v3) 0.007981126\n",
            "loss tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01131743\n",
            "self.loss(score_1, v1) 0.015934462\n",
            "self.loss(score_2, v2) 0.013268495\n",
            "self.loss(score_3, v3) 0.009365326\n",
            "loss tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008908812\n",
            "self.loss(score_1, v1) 0.016261242\n",
            "self.loss(score_2, v2) 0.015037816\n",
            "self.loss(score_3, v3) 0.006762127\n",
            "loss tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0074120304\n",
            "self.loss(score_1, v1) 0.01483297\n",
            "self.loss(score_2, v2) 0.013802536\n",
            "self.loss(score_3, v3) 0.011365127\n",
            "loss tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015120444\n",
            "self.loss(score_1, v1) 0.013288595\n",
            "self.loss(score_2, v2) 0.051195156\n",
            "self.loss(score_3, v3) 0.0072426503\n",
            "loss tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0073134485\n",
            "self.loss(score_1, v1) 0.0072277463\n",
            "self.loss(score_2, v2) 0.011609875\n",
            "self.loss(score_3, v3) 0.00780063\n",
            "loss tensor(0.0340, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010939534\n",
            "self.loss(score_1, v1) 0.017801296\n",
            "self.loss(score_2, v2) 0.02054447\n",
            "self.loss(score_3, v3) 0.014371203\n",
            "loss tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.027193248\n",
            "self.loss(score_1, v1) 0.03923975\n",
            "self.loss(score_2, v2) 0.045696642\n",
            "self.loss(score_3, v3) 0.017721795\n",
            "loss tensor(0.1299, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007480918\n",
            "self.loss(score_1, v1) 0.01063021\n",
            "self.loss(score_2, v2) 0.014005859\n",
            "self.loss(score_3, v3) 0.008709114\n",
            "loss tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.025716195\n",
            "self.loss(score_1, v1) 0.049600147\n",
            "self.loss(score_2, v2) 0.092551395\n",
            "self.loss(score_3, v3) 0.07519435\n",
            "loss tensor(0.2431, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014340164\n",
            "self.loss(score_1, v1) 0.013818998\n",
            "self.loss(score_2, v2) 0.011709982\n",
            "self.loss(score_3, v3) 0.009443404\n",
            "loss tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.09051391\n",
            "self.loss(score_1, v1) 0.053249456\n",
            "self.loss(score_2, v2) 0.05533888\n",
            "self.loss(score_3, v3) 0.01377321\n",
            "loss tensor(0.2129, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013044942\n",
            "self.loss(score_1, v1) 0.014417552\n",
            "self.loss(score_2, v2) 0.018841628\n",
            "self.loss(score_3, v3) 0.010578019\n",
            "loss tensor(0.0569, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.023321627\n",
            "self.loss(score_1, v1) 0.030257361\n",
            "self.loss(score_2, v2) 0.020694321\n",
            "self.loss(score_3, v3) 0.010360484\n",
            "loss tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014109183\n",
            "self.loss(score_1, v1) 0.023345139\n",
            "self.loss(score_2, v2) 0.029757155\n",
            "self.loss(score_3, v3) 0.010525433\n",
            "loss tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016842473\n",
            "self.loss(score_1, v1) 0.01847426\n",
            "self.loss(score_2, v2) 0.012162301\n",
            "self.loss(score_3, v3) 0.005648724\n",
            "loss tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009917347\n",
            "self.loss(score_1, v1) 0.008321847\n",
            "self.loss(score_2, v2) 0.009569294\n",
            "self.loss(score_3, v3) 0.009448516\n",
            "loss tensor(0.0373, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009776768\n",
            "self.loss(score_1, v1) 0.041105967\n",
            "self.loss(score_2, v2) 0.0399255\n",
            "self.loss(score_3, v3) 0.0040869033\n",
            "loss tensor(0.0949, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021324107\n",
            "self.loss(score_1, v1) 0.036113627\n",
            "self.loss(score_2, v2) 0.027120577\n",
            "self.loss(score_3, v3) 0.009973166\n",
            "loss tensor(0.0945, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010759856\n",
            "self.loss(score_1, v1) 0.008905567\n",
            "self.loss(score_2, v2) 0.011071359\n",
            "self.loss(score_3, v3) 0.009045449\n",
            "loss tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.13313605\n",
            "self.loss(score_1, v1) 0.048107367\n",
            "self.loss(score_2, v2) 0.030105997\n",
            "self.loss(score_3, v3) 0.032069832\n",
            "loss tensor(0.2434, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.028094219\n",
            "self.loss(score_1, v1) 0.037163604\n",
            "self.loss(score_2, v2) 0.036666382\n",
            "self.loss(score_3, v3) 0.010868265\n",
            "loss tensor(0.1128, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.040819697\n",
            "self.loss(score_1, v1) 0.027924271\n",
            "self.loss(score_2, v2) 0.041032236\n",
            "self.loss(score_3, v3) 0.010841896\n",
            "loss tensor(0.1206, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015026326\n",
            "self.loss(score_1, v1) 0.02298738\n",
            "self.loss(score_2, v2) 0.039945137\n",
            "self.loss(score_3, v3) 0.0107490495\n",
            "loss tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010317308\n",
            "self.loss(score_1, v1) 0.009601191\n",
            "self.loss(score_2, v2) 0.015872959\n",
            "self.loss(score_3, v3) 0.00932493\n",
            "loss tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015221548\n",
            "self.loss(score_1, v1) 0.013746077\n",
            "self.loss(score_2, v2) 0.010510408\n",
            "self.loss(score_3, v3) 0.007300053\n",
            "loss tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011818252\n",
            "self.loss(score_1, v1) 0.021245616\n",
            "self.loss(score_2, v2) 0.025254678\n",
            "self.loss(score_3, v3) 0.0101431\n",
            "loss tensor(0.0685, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009449989\n",
            "self.loss(score_1, v1) 0.014487128\n",
            "self.loss(score_2, v2) 0.025445383\n",
            "self.loss(score_3, v3) 0.008308045\n",
            "loss tensor(0.0577, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.07125122\n",
            "self.loss(score_1, v1) 0.076989986\n",
            "self.loss(score_2, v2) 0.058265757\n",
            "self.loss(score_3, v3) 0.008785565\n",
            "loss tensor(0.2153, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022752706\n",
            "self.loss(score_1, v1) 0.02579449\n",
            "self.loss(score_2, v2) 0.025693534\n",
            "self.loss(score_3, v3) 0.011969694\n",
            "loss tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022074336\n",
            "self.loss(score_1, v1) 0.041726284\n",
            "self.loss(score_2, v2) 0.07766202\n",
            "self.loss(score_3, v3) 0.038248394\n",
            "loss tensor(0.1797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00885567\n",
            "self.loss(score_1, v1) 0.010532578\n",
            "self.loss(score_2, v2) 0.018247424\n",
            "self.loss(score_3, v3) 0.006952743\n",
            "loss tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0065359576\n",
            "self.loss(score_1, v1) 0.006274689\n",
            "self.loss(score_2, v2) 0.0078051477\n",
            "self.loss(score_3, v3) 0.0066764685\n",
            "loss tensor(0.0273, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015480173\n",
            "self.loss(score_1, v1) 0.015797857\n",
            "self.loss(score_2, v2) 0.011587521\n",
            "self.loss(score_3, v3) 0.0085708555\n",
            "loss tensor(0.0514, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012484858\n",
            "self.loss(score_1, v1) 0.018593544\n",
            "self.loss(score_2, v2) 0.024018534\n",
            "self.loss(score_3, v3) 0.009620541\n",
            "loss tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011278223\n",
            "self.loss(score_1, v1) 0.024105059\n",
            "self.loss(score_2, v2) 0.025731336\n",
            "self.loss(score_3, v3) 0.007643039\n",
            "loss tensor(0.0688, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008121784\n",
            "self.loss(score_1, v1) 0.009677601\n",
            "self.loss(score_2, v2) 0.0077745942\n",
            "self.loss(score_3, v3) 0.0053087226\n",
            "loss tensor(0.0309, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.03557616\n",
            "self.loss(score_1, v1) 0.032596618\n",
            "self.loss(score_2, v2) 0.034134727\n",
            "self.loss(score_3, v3) 0.006733158\n",
            "loss tensor(0.1090, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008173412\n",
            "self.loss(score_1, v1) 0.016368968\n",
            "self.loss(score_2, v2) 0.020707853\n",
            "self.loss(score_3, v3) 0.0074204723\n",
            "loss tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0069162846\n",
            "self.loss(score_1, v1) 0.0127977375\n",
            "self.loss(score_2, v2) 0.008257563\n",
            "self.loss(score_3, v3) 0.007502431\n",
            "loss tensor(0.0355, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012491814\n",
            "self.loss(score_1, v1) 0.016233642\n",
            "self.loss(score_2, v2) 0.01177929\n",
            "self.loss(score_3, v3) 0.0071104625\n",
            "loss tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009811465\n",
            "self.loss(score_1, v1) 0.033925053\n",
            "self.loss(score_2, v2) 0.018758114\n",
            "self.loss(score_3, v3) 0.009048089\n",
            "loss tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013692551\n",
            "self.loss(score_1, v1) 0.018471373\n",
            "self.loss(score_2, v2) 0.022790253\n",
            "self.loss(score_3, v3) 0.010021426\n",
            "loss tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013140534\n",
            "self.loss(score_1, v1) 0.04757579\n",
            "self.loss(score_2, v2) 0.048881408\n",
            "self.loss(score_3, v3) 0.08203433\n",
            "loss tensor(0.1916, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016656172\n",
            "self.loss(score_1, v1) 0.015894916\n",
            "self.loss(score_2, v2) 0.029503567\n",
            "self.loss(score_3, v3) 0.0067990487\n",
            "loss tensor(0.0689, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006468997\n",
            "self.loss(score_1, v1) 0.0089062005\n",
            "self.loss(score_2, v2) 0.015614891\n",
            "self.loss(score_3, v3) 0.009679675\n",
            "loss tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007380873\n",
            "self.loss(score_1, v1) 0.017732443\n",
            "self.loss(score_2, v2) 0.023531443\n",
            "self.loss(score_3, v3) 0.0075628376\n",
            "loss tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.043354865\n",
            "self.loss(score_1, v1) 0.024254607\n",
            "self.loss(score_2, v2) 0.031321652\n",
            "self.loss(score_3, v3) 0.0066923886\n",
            "loss tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021603877\n",
            "self.loss(score_1, v1) 0.013737147\n",
            "self.loss(score_2, v2) 0.011793913\n",
            "self.loss(score_3, v3) 0.012233538\n",
            "loss tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019971218\n",
            "self.loss(score_1, v1) 0.025267517\n",
            "self.loss(score_2, v2) 0.02381549\n",
            "self.loss(score_3, v3) 0.011843463\n",
            "loss tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016012201\n",
            "self.loss(score_1, v1) 0.023099273\n",
            "self.loss(score_2, v2) 0.031616762\n",
            "self.loss(score_3, v3) 0.0141509995\n",
            "loss tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012412456\n",
            "self.loss(score_1, v1) 0.012774146\n",
            "self.loss(score_2, v2) 0.014603547\n",
            "self.loss(score_3, v3) 0.0071011875\n",
            "loss tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.059875768\n",
            "self.loss(score_1, v1) 0.036889747\n",
            "self.loss(score_2, v2) 0.020924095\n",
            "self.loss(score_3, v3) 0.007286905\n",
            "loss tensor(0.1250, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018127428\n",
            "self.loss(score_1, v1) 0.03589733\n",
            "self.loss(score_2, v2) 0.03189803\n",
            "self.loss(score_3, v3) 0.010518261\n",
            "loss tensor(0.0964, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008185548\n",
            "self.loss(score_1, v1) 0.0101394\n",
            "self.loss(score_2, v2) 0.009057261\n",
            "self.loss(score_3, v3) 0.007625413\n",
            "loss tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0071578636\n",
            "self.loss(score_1, v1) 0.01190336\n",
            "self.loss(score_2, v2) 0.01937703\n",
            "self.loss(score_3, v3) 0.008438133\n",
            "loss tensor(0.0469, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0364481\n",
            "self.loss(score_1, v1) 0.05273696\n",
            "self.loss(score_2, v2) 0.058088746\n",
            "self.loss(score_3, v3) 0.020973004\n",
            "loss tensor(0.1682, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008849508\n",
            "self.loss(score_1, v1) 0.043518405\n",
            "self.loss(score_2, v2) 0.06670627\n",
            "self.loss(score_3, v3) 0.057721596\n",
            "loss tensor(0.1768, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0579214\n",
            "self.loss(score_1, v1) 0.07567347\n",
            "self.loss(score_2, v2) 0.043023985\n",
            "self.loss(score_3, v3) 0.022761991\n",
            "loss tensor(0.1994, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.03253161\n",
            "self.loss(score_1, v1) 0.02241152\n",
            "self.loss(score_2, v2) 0.024292687\n",
            "self.loss(score_3, v3) 0.007018765\n",
            "loss tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016513983\n",
            "self.loss(score_1, v1) 0.01610726\n",
            "self.loss(score_2, v2) 0.020831084\n",
            "self.loss(score_3, v3) 0.010503283\n",
            "loss tensor(0.0640, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006283202\n",
            "self.loss(score_1, v1) 0.0071259197\n",
            "self.loss(score_2, v2) 0.008480826\n",
            "self.loss(score_3, v3) 0.006315872\n",
            "loss tensor(0.0282, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009774096\n",
            "self.loss(score_1, v1) 0.024634426\n",
            "self.loss(score_2, v2) 0.05359944\n",
            "self.loss(score_3, v3) 0.009145114\n",
            "loss tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02440141\n",
            "self.loss(score_1, v1) 0.07123166\n",
            "self.loss(score_2, v2) 0.03228403\n",
            "self.loss(score_3, v3) 0.011726458\n",
            "loss tensor(0.1396, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00997565\n",
            "self.loss(score_1, v1) 0.010917608\n",
            "self.loss(score_2, v2) 0.009812993\n",
            "self.loss(score_3, v3) 0.008510947\n",
            "loss tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0095387455\n",
            "self.loss(score_1, v1) 0.016556911\n",
            "self.loss(score_2, v2) 0.01937742\n",
            "self.loss(score_3, v3) 0.009588053\n",
            "loss tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006986991\n",
            "self.loss(score_1, v1) 0.049599648\n",
            "self.loss(score_2, v2) 0.14753236\n",
            "self.loss(score_3, v3) 0.18913075\n",
            "loss tensor(0.3932, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008551962\n",
            "self.loss(score_1, v1) 0.015583791\n",
            "self.loss(score_2, v2) 0.013138361\n",
            "self.loss(score_3, v3) 0.018511694\n",
            "loss tensor(0.0558, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012781876\n",
            "self.loss(score_1, v1) 0.014134158\n",
            "self.loss(score_2, v2) 0.025411958\n",
            "self.loss(score_3, v3) 0.07072381\n",
            "loss tensor(0.1231, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014187002\n",
            "self.loss(score_1, v1) 0.062264085\n",
            "self.loss(score_2, v2) 0.04625441\n",
            "self.loss(score_3, v3) 0.011576114\n",
            "loss tensor(0.1343, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012419939\n",
            "self.loss(score_1, v1) 0.022588596\n",
            "self.loss(score_2, v2) 0.020157848\n",
            "self.loss(score_3, v3) 0.008673607\n",
            "loss tensor(0.0638, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01662762\n",
            "self.loss(score_1, v1) 0.018134382\n",
            "self.loss(score_2, v2) 0.021466877\n",
            "self.loss(score_3, v3) 0.01042545\n",
            "loss tensor(0.0667, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0065420493\n",
            "self.loss(score_1, v1) 0.006180528\n",
            "self.loss(score_2, v2) 0.011917245\n",
            "self.loss(score_3, v3) 0.008862438\n",
            "loss tensor(0.0335, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0147382235\n",
            "self.loss(score_1, v1) 0.014773375\n",
            "self.loss(score_2, v2) 0.010907122\n",
            "self.loss(score_3, v3) 0.00900688\n",
            "loss tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011672985\n",
            "self.loss(score_1, v1) 0.013478853\n",
            "self.loss(score_2, v2) 0.007227213\n",
            "self.loss(score_3, v3) 0.00711313\n",
            "loss tensor(0.0395, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0072815875\n",
            "self.loss(score_1, v1) 0.0074940403\n",
            "self.loss(score_2, v2) 0.007366341\n",
            "self.loss(score_3, v3) 0.0043899356\n",
            "loss tensor(0.0265, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007900234\n",
            "self.loss(score_1, v1) 0.009238565\n",
            "self.loss(score_2, v2) 0.0059795855\n",
            "self.loss(score_3, v3) 0.0038596957\n",
            "loss tensor(0.0270, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.1358126\n",
            "self.loss(score_1, v1) 0.05157037\n",
            "self.loss(score_2, v2) 0.055913396\n",
            "self.loss(score_3, v3) 0.035165202\n",
            "loss tensor(0.2785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008052634\n",
            "self.loss(score_1, v1) 0.007612369\n",
            "self.loss(score_2, v2) 0.008576401\n",
            "self.loss(score_3, v3) 0.004976703\n",
            "loss tensor(0.0292, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011955579\n",
            "self.loss(score_1, v1) 0.02939878\n",
            "self.loss(score_2, v2) 0.01841751\n",
            "self.loss(score_3, v3) 0.0074965092\n",
            "loss tensor(0.0673, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.005549687\n",
            "self.loss(score_1, v1) 0.0114912195\n",
            "self.loss(score_2, v2) 0.009185106\n",
            "self.loss(score_3, v3) 0.004757162\n",
            "loss tensor(0.0310, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0067049833\n",
            "self.loss(score_1, v1) 0.013572257\n",
            "self.loss(score_2, v2) 0.015240771\n",
            "self.loss(score_3, v3) 0.008740798\n",
            "loss tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017213952\n",
            "self.loss(score_1, v1) 0.018143889\n",
            "self.loss(score_2, v2) 0.024187261\n",
            "self.loss(score_3, v3) 0.014572827\n",
            "loss tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013945079\n",
            "self.loss(score_1, v1) 0.022204973\n",
            "self.loss(score_2, v2) 0.03852135\n",
            "self.loss(score_3, v3) 0.009129474\n",
            "loss tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011113247\n",
            "self.loss(score_1, v1) 0.01527181\n",
            "self.loss(score_2, v2) 0.018546699\n",
            "self.loss(score_3, v3) 0.012853325\n",
            "loss tensor(0.0578, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013200878\n",
            "self.loss(score_1, v1) 0.011682228\n",
            "self.loss(score_2, v2) 0.00836758\n",
            "self.loss(score_3, v3) 0.007963346\n",
            "loss tensor(0.0412, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011290795\n",
            "self.loss(score_1, v1) 0.012169572\n",
            "self.loss(score_2, v2) 0.01052053\n",
            "self.loss(score_3, v3) 0.009616346\n",
            "loss tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009096106\n",
            "self.loss(score_1, v1) 0.01731685\n",
            "self.loss(score_2, v2) 0.012680452\n",
            "self.loss(score_3, v3) 0.085436165\n",
            "loss tensor(0.1245, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010079066\n",
            "self.loss(score_1, v1) 0.01164868\n",
            "self.loss(score_2, v2) 0.0145272715\n",
            "self.loss(score_3, v3) 0.0068149036\n",
            "loss tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008361346\n",
            "self.loss(score_1, v1) 0.019784532\n",
            "self.loss(score_2, v2) 0.01802392\n",
            "self.loss(score_3, v3) 0.011409781\n",
            "loss tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009325403\n",
            "self.loss(score_1, v1) 0.027043214\n",
            "self.loss(score_2, v2) 0.060802937\n",
            "self.loss(score_3, v3) 0.036597736\n",
            "loss tensor(0.1338, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009358738\n",
            "self.loss(score_1, v1) 0.07263286\n",
            "self.loss(score_2, v2) 0.10981316\n",
            "self.loss(score_3, v3) 0.07320819\n",
            "loss tensor(0.2650, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01595375\n",
            "self.loss(score_1, v1) 0.02359084\n",
            "self.loss(score_2, v2) 0.017918615\n",
            "self.loss(score_3, v3) 0.025595594\n",
            "loss tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0169252\n",
            "self.loss(score_1, v1) 0.024440682\n",
            "self.loss(score_2, v2) 0.028139664\n",
            "self.loss(score_3, v3) 0.012320209\n",
            "loss tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007660377\n",
            "self.loss(score_1, v1) 0.008993811\n",
            "self.loss(score_2, v2) 0.009400444\n",
            "self.loss(score_3, v3) 0.006393404\n",
            "loss tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009433415\n",
            "self.loss(score_1, v1) 0.021072\n",
            "self.loss(score_2, v2) 0.021321768\n",
            "self.loss(score_3, v3) 0.017785857\n",
            "loss tensor(0.0696, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008363899\n",
            "self.loss(score_1, v1) 0.015201603\n",
            "self.loss(score_2, v2) 0.015701383\n",
            "self.loss(score_3, v3) 0.007021642\n",
            "loss tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018972734\n",
            "self.loss(score_1, v1) 0.017430658\n",
            "self.loss(score_2, v2) 0.016272133\n",
            "self.loss(score_3, v3) 0.0120455725\n",
            "loss tensor(0.0647, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008791009\n",
            "self.loss(score_1, v1) 0.01981732\n",
            "self.loss(score_2, v2) 0.020287506\n",
            "self.loss(score_3, v3) 0.009138264\n",
            "loss tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016511273\n",
            "self.loss(score_1, v1) 0.013781526\n",
            "self.loss(score_2, v2) 0.0136502115\n",
            "self.loss(score_3, v3) 0.008066366\n",
            "loss tensor(0.0520, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008946082\n",
            "self.loss(score_1, v1) 0.016292939\n",
            "self.loss(score_2, v2) 0.021548161\n",
            "self.loss(score_3, v3) 0.010365283\n",
            "loss tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012880327\n",
            "self.loss(score_1, v1) 0.03327549\n",
            "self.loss(score_2, v2) 0.035902597\n",
            "self.loss(score_3, v3) 0.017025586\n",
            "loss tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008452287\n",
            "self.loss(score_1, v1) 0.018360617\n",
            "self.loss(score_2, v2) 0.016870122\n",
            "self.loss(score_3, v3) 0.009466593\n",
            "loss tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008336428\n",
            "self.loss(score_1, v1) 0.008736255\n",
            "self.loss(score_2, v2) 0.009808218\n",
            "self.loss(score_3, v3) 0.006810043\n",
            "loss tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018540159\n",
            "self.loss(score_1, v1) 0.02666158\n",
            "self.loss(score_2, v2) 0.028731283\n",
            "self.loss(score_3, v3) 0.011731073\n",
            "loss tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010296983\n",
            "self.loss(score_1, v1) 0.00877752\n",
            "self.loss(score_2, v2) 0.009490512\n",
            "self.loss(score_3, v3) 0.01190754\n",
            "loss tensor(0.0405, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014593437\n",
            "self.loss(score_1, v1) 0.02516399\n",
            "self.loss(score_2, v2) 0.020983309\n",
            "self.loss(score_3, v3) 0.009634571\n",
            "loss tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.06463508\n",
            "self.loss(score_1, v1) 0.013056511\n",
            "self.loss(score_2, v2) 0.041515738\n",
            "self.loss(score_3, v3) 0.026325345\n",
            "loss tensor(0.1455, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01521637\n",
            "self.loss(score_1, v1) 0.019838043\n",
            "self.loss(score_2, v2) 0.015819434\n",
            "self.loss(score_3, v3) 0.008692398\n",
            "loss tensor(0.0596, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022941943\n",
            "self.loss(score_1, v1) 0.04190596\n",
            "self.loss(score_2, v2) 0.013856\n",
            "self.loss(score_3, v3) 0.0083487285\n",
            "loss tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015464551\n",
            "self.loss(score_1, v1) 0.021946263\n",
            "self.loss(score_2, v2) 0.019378036\n",
            "self.loss(score_3, v3) 0.0118990345\n",
            "loss tensor(0.0687, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022696583\n",
            "self.loss(score_1, v1) 0.022443136\n",
            "self.loss(score_2, v2) 0.01003047\n",
            "self.loss(score_3, v3) 0.007316885\n",
            "loss tensor(0.0625, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016144933\n",
            "self.loss(score_1, v1) 0.02020916\n",
            "self.loss(score_2, v2) 0.011031658\n",
            "self.loss(score_3, v3) 0.0070345155\n",
            "loss tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008175173\n",
            "self.loss(score_1, v1) 0.015131389\n",
            "self.loss(score_2, v2) 0.020818634\n",
            "self.loss(score_3, v3) 0.012569604\n",
            "loss tensor(0.0567, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020061847\n",
            "self.loss(score_1, v1) 0.03602321\n",
            "self.loss(score_2, v2) 0.04462451\n",
            "self.loss(score_3, v3) 0.008673775\n",
            "loss tensor(0.1094, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00556399\n",
            "self.loss(score_1, v1) 0.011549612\n",
            "self.loss(score_2, v2) 0.015073614\n",
            "self.loss(score_3, v3) 0.006443312\n",
            "loss tensor(0.0386, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016494494\n",
            "self.loss(score_1, v1) 0.023930766\n",
            "self.loss(score_2, v2) 0.039652612\n",
            "self.loss(score_3, v3) 0.0121320095\n",
            "loss tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0067647523\n",
            "self.loss(score_1, v1) 0.008025388\n",
            "self.loss(score_2, v2) 0.012059451\n",
            "self.loss(score_3, v3) 0.007878612\n",
            "loss tensor(0.0347, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0073154205\n",
            "self.loss(score_1, v1) 0.015006059\n",
            "self.loss(score_2, v2) 0.023297742\n",
            "self.loss(score_3, v3) 0.010324996\n",
            "loss tensor(0.0559, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.06490336\n",
            "self.loss(score_1, v1) 0.07426856\n",
            "self.loss(score_2, v2) 0.051321592\n",
            "self.loss(score_3, v3) 0.047038596\n",
            "loss tensor(0.2375, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006417513\n",
            "self.loss(score_1, v1) 0.0128149865\n",
            "self.loss(score_2, v2) 0.020197723\n",
            "self.loss(score_3, v3) 0.0081552\n",
            "loss tensor(0.0476, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0059018023\n",
            "self.loss(score_1, v1) 0.011286888\n",
            "self.loss(score_2, v2) 0.0135200005\n",
            "self.loss(score_3, v3) 0.007092037\n",
            "loss tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020590577\n",
            "self.loss(score_1, v1) 0.028733576\n",
            "self.loss(score_2, v2) 0.018807177\n",
            "self.loss(score_3, v3) 0.0112102525\n",
            "loss tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020143697\n",
            "self.loss(score_1, v1) 0.079053536\n",
            "self.loss(score_2, v2) 0.06854878\n",
            "self.loss(score_3, v3) 0.023080686\n",
            "loss tensor(0.1908, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015300242\n",
            "self.loss(score_1, v1) 0.013609525\n",
            "self.loss(score_2, v2) 0.0141097\n",
            "self.loss(score_3, v3) 0.009905534\n",
            "loss tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017927501\n",
            "self.loss(score_1, v1) 0.032138474\n",
            "self.loss(score_2, v2) 0.042127896\n",
            "self.loss(score_3, v3) 0.008156353\n",
            "loss tensor(0.1004, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015751617\n",
            "self.loss(score_1, v1) 0.031125609\n",
            "self.loss(score_2, v2) 0.05943725\n",
            "self.loss(score_3, v3) 0.050304495\n",
            "loss tensor(0.1566, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0069283843\n",
            "self.loss(score_1, v1) 0.06252696\n",
            "self.loss(score_2, v2) 0.08339\n",
            "self.loss(score_3, v3) 0.055750936\n",
            "loss tensor(0.2086, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009744727\n",
            "self.loss(score_1, v1) 0.012707066\n",
            "self.loss(score_2, v2) 0.01732174\n",
            "self.loss(score_3, v3) 0.007507278\n",
            "loss tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0037673358\n",
            "self.loss(score_1, v1) 0.009533572\n",
            "self.loss(score_2, v2) 0.015542351\n",
            "self.loss(score_3, v3) 0.0041485243\n",
            "loss tensor(0.0330, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0092352815\n",
            "self.loss(score_1, v1) 0.010613986\n",
            "self.loss(score_2, v2) 0.010554427\n",
            "self.loss(score_3, v3) 0.009529645\n",
            "loss tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006533037\n",
            "self.loss(score_1, v1) 0.01801174\n",
            "self.loss(score_2, v2) 0.01958298\n",
            "self.loss(score_3, v3) 0.0108655095\n",
            "loss tensor(0.0550, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0072580343\n",
            "self.loss(score_1, v1) 0.006604747\n",
            "self.loss(score_2, v2) 0.006779792\n",
            "self.loss(score_3, v3) 0.008403974\n",
            "loss tensor(0.0290, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009823407\n",
            "self.loss(score_1, v1) 0.007788763\n",
            "self.loss(score_2, v2) 0.0046610874\n",
            "self.loss(score_3, v3) 0.00495174\n",
            "loss tensor(0.0272, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011674715\n",
            "self.loss(score_1, v1) 0.011551167\n",
            "self.loss(score_2, v2) 0.010523635\n",
            "self.loss(score_3, v3) 0.008166265\n",
            "loss tensor(0.0419, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.023953676\n",
            "self.loss(score_1, v1) 0.014560951\n",
            "self.loss(score_2, v2) 0.014501784\n",
            "self.loss(score_3, v3) 0.010114834\n",
            "loss tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.005113638\n",
            "self.loss(score_1, v1) 0.0063921902\n",
            "self.loss(score_2, v2) 0.0057349643\n",
            "self.loss(score_3, v3) 0.0045009125\n",
            "loss tensor(0.0217, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012436693\n",
            "self.loss(score_1, v1) 0.02031262\n",
            "self.loss(score_2, v2) 0.04514932\n",
            "self.loss(score_3, v3) 0.013253173\n",
            "loss tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.026322933\n",
            "self.loss(score_1, v1) 0.055220313\n",
            "self.loss(score_2, v2) 0.010262727\n",
            "self.loss(score_3, v3) 0.0059934645\n",
            "loss tensor(0.0978, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006587947\n",
            "self.loss(score_1, v1) 0.01109959\n",
            "self.loss(score_2, v2) 0.009078168\n",
            "self.loss(score_3, v3) 0.0076943943\n",
            "loss tensor(0.0345, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008022199\n",
            "self.loss(score_1, v1) 0.012512867\n",
            "self.loss(score_2, v2) 0.016895203\n",
            "self.loss(score_3, v3) 0.010787094\n",
            "loss tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014009003\n",
            "self.loss(score_1, v1) 0.034046\n",
            "self.loss(score_2, v2) 0.046291616\n",
            "self.loss(score_3, v3) 0.012157116\n",
            "loss tensor(0.1065, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009300941\n",
            "self.loss(score_1, v1) 0.025442678\n",
            "self.loss(score_2, v2) 0.015113303\n",
            "self.loss(score_3, v3) 0.0069414354\n",
            "loss tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011584065\n",
            "self.loss(score_1, v1) 0.028079709\n",
            "self.loss(score_2, v2) 0.016540844\n",
            "self.loss(score_3, v3) 0.007394471\n",
            "loss tensor(0.0636, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013952059\n",
            "self.loss(score_1, v1) 0.015753532\n",
            "self.loss(score_2, v2) 0.0201064\n",
            "self.loss(score_3, v3) 0.0104618855\n",
            "loss tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.005580757\n",
            "self.loss(score_1, v1) 0.009451261\n",
            "self.loss(score_2, v2) 0.03522827\n",
            "self.loss(score_3, v3) 0.004845842\n",
            "loss tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019718446\n",
            "self.loss(score_1, v1) 0.022194538\n",
            "self.loss(score_2, v2) 0.008053534\n",
            "self.loss(score_3, v3) 0.0072099213\n",
            "loss tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008205894\n",
            "self.loss(score_1, v1) 0.010661783\n",
            "self.loss(score_2, v2) 0.015942004\n",
            "self.loss(score_3, v3) 0.009554881\n",
            "loss tensor(0.0444, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006019441\n",
            "self.loss(score_1, v1) 0.00793026\n",
            "self.loss(score_2, v2) 0.007941278\n",
            "self.loss(score_3, v3) 0.007735108\n",
            "loss tensor(0.0296, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.08120974\n",
            "self.loss(score_1, v1) 0.04457218\n",
            "self.loss(score_2, v2) 0.019297307\n",
            "self.loss(score_3, v3) 0.008397876\n",
            "loss tensor(0.1535, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008834987\n",
            "self.loss(score_1, v1) 0.0072782454\n",
            "self.loss(score_2, v2) 0.0073608276\n",
            "self.loss(score_3, v3) 0.0068747015\n",
            "loss tensor(0.0303, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008549154\n",
            "self.loss(score_1, v1) 0.016732588\n",
            "self.loss(score_2, v2) 0.01709997\n",
            "self.loss(score_3, v3) 0.01281366\n",
            "loss tensor(0.0552, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017779551\n",
            "self.loss(score_1, v1) 0.0549601\n",
            "self.loss(score_2, v2) 0.07961394\n",
            "self.loss(score_3, v3) 0.20035274\n",
            "loss tensor(0.3527, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009871408\n",
            "self.loss(score_1, v1) 0.009661651\n",
            "self.loss(score_2, v2) 0.009410058\n",
            "self.loss(score_3, v3) 0.0069949413\n",
            "loss tensor(0.0359, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0651467\n",
            "self.loss(score_1, v1) 0.03650275\n",
            "self.loss(score_2, v2) 0.03249263\n",
            "self.loss(score_3, v3) 0.008880117\n",
            "loss tensor(0.1430, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021858027\n",
            "self.loss(score_1, v1) 0.026775181\n",
            "self.loss(score_2, v2) 0.044264786\n",
            "self.loss(score_3, v3) 0.006970679\n",
            "loss tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010145856\n",
            "self.loss(score_1, v1) 0.016996868\n",
            "self.loss(score_2, v2) 0.020110913\n",
            "self.loss(score_3, v3) 0.014109007\n",
            "loss tensor(0.0614, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015272285\n",
            "self.loss(score_1, v1) 0.0116523495\n",
            "self.loss(score_2, v2) 0.005441939\n",
            "self.loss(score_3, v3) 0.0052713268\n",
            "loss tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017920401\n",
            "self.loss(score_1, v1) 0.040286563\n",
            "self.loss(score_2, v2) 0.032095753\n",
            "self.loss(score_3, v3) 0.0077983337\n",
            "loss tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.052079912\n",
            "self.loss(score_1, v1) 0.0407766\n",
            "self.loss(score_2, v2) 0.01569087\n",
            "self.loss(score_3, v3) 0.008494508\n",
            "loss tensor(0.1170, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006982332\n",
            "self.loss(score_1, v1) 0.018585892\n",
            "self.loss(score_2, v2) 0.019898236\n",
            "self.loss(score_3, v3) 0.009614392\n",
            "loss tensor(0.0551, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.050768916\n",
            "self.loss(score_1, v1) 0.028660499\n",
            "self.loss(score_2, v2) 0.032165818\n",
            "self.loss(score_3, v3) 0.0076505737\n",
            "loss tensor(0.1192, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017781397\n",
            "self.loss(score_1, v1) 0.04311423\n",
            "self.loss(score_2, v2) 0.009873782\n",
            "self.loss(score_3, v3) 0.007038293\n",
            "loss tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014425024\n",
            "self.loss(score_1, v1) 0.016059779\n",
            "self.loss(score_2, v2) 0.008619318\n",
            "self.loss(score_3, v3) 0.008212244\n",
            "loss tensor(0.0473, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012552796\n",
            "self.loss(score_1, v1) 0.017402716\n",
            "self.loss(score_2, v2) 0.0522076\n",
            "self.loss(score_3, v3) 0.009864946\n",
            "loss tensor(0.0920, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.05705228\n",
            "self.loss(score_1, v1) 0.0873578\n",
            "self.loss(score_2, v2) 0.08232796\n",
            "self.loss(score_3, v3) 0.06720677\n",
            "loss tensor(0.2939, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0065419297\n",
            "self.loss(score_1, v1) 0.010411899\n",
            "self.loss(score_2, v2) 0.021651108\n",
            "self.loss(score_3, v3) 0.00845494\n",
            "loss tensor(0.0471, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.06538231\n",
            "self.loss(score_1, v1) 0.047473427\n",
            "self.loss(score_2, v2) 0.029765004\n",
            "self.loss(score_3, v3) 0.009347555\n",
            "loss tensor(0.1520, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011092592\n",
            "self.loss(score_1, v1) 0.008941538\n",
            "self.loss(score_2, v2) 0.008411381\n",
            "self.loss(score_3, v3) 0.011269721\n",
            "loss tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00885903\n",
            "self.loss(score_1, v1) 0.02467671\n",
            "self.loss(score_2, v2) 0.012569623\n",
            "self.loss(score_3, v3) 0.006071097\n",
            "loss tensor(0.0522, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008118682\n",
            "self.loss(score_1, v1) 0.014314272\n",
            "self.loss(score_2, v2) 0.013132648\n",
            "self.loss(score_3, v3) 0.00927359\n",
            "loss tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013124382\n",
            "self.loss(score_1, v1) 0.015385872\n",
            "self.loss(score_2, v2) 0.02268187\n",
            "self.loss(score_3, v3) 0.031173415\n",
            "loss tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006981504\n",
            "self.loss(score_1, v1) 0.009688024\n",
            "self.loss(score_2, v2) 0.013838861\n",
            "self.loss(score_3, v3) 0.0061222613\n",
            "loss tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008386946\n",
            "self.loss(score_1, v1) 0.008528537\n",
            "self.loss(score_2, v2) 0.009164015\n",
            "self.loss(score_3, v3) 0.006617767\n",
            "loss tensor(0.0327, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018873625\n",
            "self.loss(score_1, v1) 0.0634395\n",
            "self.loss(score_2, v2) 0.063926175\n",
            "self.loss(score_3, v3) 0.025117189\n",
            "loss tensor(0.1714, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011972151\n",
            "self.loss(score_1, v1) 0.01220909\n",
            "self.loss(score_2, v2) 0.013334997\n",
            "self.loss(score_3, v3) 0.007705922\n",
            "loss tensor(0.0452, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008075755\n",
            "self.loss(score_1, v1) 0.010203073\n",
            "self.loss(score_2, v2) 0.0073713353\n",
            "self.loss(score_3, v3) 0.0077473465\n",
            "loss tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0166687\n",
            "self.loss(score_1, v1) 0.014193146\n",
            "self.loss(score_2, v2) 0.019731361\n",
            "self.loss(score_3, v3) 0.009516938\n",
            "loss tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010887646\n",
            "self.loss(score_1, v1) 0.018968176\n",
            "self.loss(score_2, v2) 0.01054351\n",
            "self.loss(score_3, v3) 0.009456002\n",
            "loss tensor(0.0499, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0098522315\n",
            "self.loss(score_1, v1) 0.015561911\n",
            "self.loss(score_2, v2) 0.03632434\n",
            "self.loss(score_3, v3) 0.008019206\n",
            "loss tensor(0.0698, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020102872\n",
            "self.loss(score_1, v1) 0.019341111\n",
            "self.loss(score_2, v2) 0.01718193\n",
            "self.loss(score_3, v3) 0.006725566\n",
            "loss tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017372662\n",
            "self.loss(score_1, v1) 0.022751488\n",
            "self.loss(score_2, v2) 0.040226173\n",
            "self.loss(score_3, v3) 0.012382174\n",
            "loss tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012196867\n",
            "self.loss(score_1, v1) 0.03666624\n",
            "self.loss(score_2, v2) 0.037318084\n",
            "self.loss(score_3, v3) 0.016601266\n",
            "loss tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019082822\n",
            "self.loss(score_1, v1) 0.03138731\n",
            "self.loss(score_2, v2) 0.055393595\n",
            "self.loss(score_3, v3) 0.055879142\n",
            "loss tensor(0.1617, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.03792005\n",
            "self.loss(score_1, v1) 0.038217336\n",
            "self.loss(score_2, v2) 0.07643445\n",
            "self.loss(score_3, v3) 0.01198065\n",
            "loss tensor(0.1646, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014773026\n",
            "self.loss(score_1, v1) 0.03056389\n",
            "self.loss(score_2, v2) 0.020965252\n",
            "self.loss(score_3, v3) 0.012838537\n",
            "loss tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013919298\n",
            "self.loss(score_1, v1) 0.019856267\n",
            "self.loss(score_2, v2) 0.011933176\n",
            "self.loss(score_3, v3) 0.007637835\n",
            "loss tensor(0.0533, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.024657482\n",
            "self.loss(score_1, v1) 0.02746141\n",
            "self.loss(score_2, v2) 0.009039802\n",
            "self.loss(score_3, v3) 0.007275231\n",
            "loss tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0063590836\n",
            "self.loss(score_1, v1) 0.005593277\n",
            "self.loss(score_2, v2) 0.00808483\n",
            "self.loss(score_3, v3) 0.005139186\n",
            "loss tensor(0.0252, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008382436\n",
            "self.loss(score_1, v1) 0.016129104\n",
            "self.loss(score_2, v2) 0.027340787\n",
            "self.loss(score_3, v3) 0.010020639\n",
            "loss tensor(0.0619, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.052854866\n",
            "self.loss(score_1, v1) 0.017565075\n",
            "self.loss(score_2, v2) 0.015787672\n",
            "self.loss(score_3, v3) 0.009009001\n",
            "loss tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009277345\n",
            "self.loss(score_1, v1) 0.028967306\n",
            "self.loss(score_2, v2) 0.025296018\n",
            "self.loss(score_3, v3) 0.010909228\n",
            "loss tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018129013\n",
            "self.loss(score_1, v1) 0.031758193\n",
            "self.loss(score_2, v2) 0.019820457\n",
            "self.loss(score_3, v3) 0.008345956\n",
            "loss tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0043932176\n",
            "self.loss(score_1, v1) 0.011837892\n",
            "self.loss(score_2, v2) 0.016734377\n",
            "self.loss(score_3, v3) 0.008995778\n",
            "loss tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020546794\n",
            "self.loss(score_1, v1) 0.055841457\n",
            "self.loss(score_2, v2) 0.028498499\n",
            "self.loss(score_3, v3) 0.007966032\n",
            "loss tensor(0.1129, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013942603\n",
            "self.loss(score_1, v1) 0.012646559\n",
            "self.loss(score_2, v2) 0.014475157\n",
            "self.loss(score_3, v3) 0.009257868\n",
            "loss tensor(0.0503, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01082046\n",
            "self.loss(score_1, v1) 0.024129026\n",
            "self.loss(score_2, v2) 0.027185034\n",
            "self.loss(score_3, v3) 0.009794989\n",
            "loss tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00966108\n",
            "self.loss(score_1, v1) 0.009578995\n",
            "self.loss(score_2, v2) 0.015664326\n",
            "self.loss(score_3, v3) 0.012911336\n",
            "loss tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0073423134\n",
            "self.loss(score_1, v1) 0.032139532\n",
            "self.loss(score_2, v2) 0.01218922\n",
            "self.loss(score_3, v3) 0.006335776\n",
            "loss tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020298151\n",
            "self.loss(score_1, v1) 0.07855089\n",
            "self.loss(score_2, v2) 0.04686278\n",
            "self.loss(score_3, v3) 0.011654714\n",
            "loss tensor(0.1574, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008857732\n",
            "self.loss(score_1, v1) 0.011921584\n",
            "self.loss(score_2, v2) 0.01802064\n",
            "self.loss(score_3, v3) 0.007088387\n",
            "loss tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0060437503\n",
            "self.loss(score_1, v1) 0.008891003\n",
            "self.loss(score_2, v2) 0.009656322\n",
            "self.loss(score_3, v3) 0.005202204\n",
            "loss tensor(0.0298, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01744758\n",
            "self.loss(score_1, v1) 0.016487213\n",
            "self.loss(score_2, v2) 0.013886918\n",
            "self.loss(score_3, v3) 0.008585454\n",
            "loss tensor(0.0564, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.044354714\n",
            "self.loss(score_1, v1) 0.022844449\n",
            "self.loss(score_2, v2) 0.013429291\n",
            "self.loss(score_3, v3) 0.009455605\n",
            "loss tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.042854644\n",
            "self.loss(score_1, v1) 0.025593514\n",
            "self.loss(score_2, v2) 0.024362888\n",
            "self.loss(score_3, v3) 0.011406544\n",
            "loss tensor(0.1042, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012145486\n",
            "self.loss(score_1, v1) 0.010578668\n",
            "self.loss(score_2, v2) 0.014485541\n",
            "self.loss(score_3, v3) 0.0128420405\n",
            "loss tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013413983\n",
            "self.loss(score_1, v1) 0.023131832\n",
            "self.loss(score_2, v2) 0.03590672\n",
            "self.loss(score_3, v3) 0.008496393\n",
            "loss tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007242461\n",
            "self.loss(score_1, v1) 0.006666053\n",
            "self.loss(score_2, v2) 0.0077836104\n",
            "self.loss(score_3, v3) 0.0047070715\n",
            "loss tensor(0.0264, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02272587\n",
            "self.loss(score_1, v1) 0.060255323\n",
            "self.loss(score_2, v2) 0.0830563\n",
            "self.loss(score_3, v3) 0.020897409\n",
            "loss tensor(0.1869, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020283531\n",
            "self.loss(score_1, v1) 0.013739525\n",
            "self.loss(score_2, v2) 0.014746231\n",
            "self.loss(score_3, v3) 0.0074443794\n",
            "loss tensor(0.0562, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0123606725\n",
            "self.loss(score_1, v1) 0.012000098\n",
            "self.loss(score_2, v2) 0.008963303\n",
            "self.loss(score_3, v3) 0.011663763\n",
            "loss tensor(0.0450, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.026550552\n",
            "self.loss(score_1, v1) 0.037439205\n",
            "self.loss(score_2, v2) 0.044174712\n",
            "self.loss(score_3, v3) 0.017004486\n",
            "loss tensor(0.1252, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008388894\n",
            "self.loss(score_1, v1) 0.015837047\n",
            "self.loss(score_2, v2) 0.029272093\n",
            "self.loss(score_3, v3) 0.012216829\n",
            "loss tensor(0.0657, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.03595808\n",
            "self.loss(score_1, v1) 0.024867073\n",
            "self.loss(score_2, v2) 0.040468395\n",
            "self.loss(score_3, v3) 0.010104412\n",
            "loss tensor(0.1114, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018776147\n",
            "self.loss(score_1, v1) 0.0142307365\n",
            "self.loss(score_2, v2) 0.016740916\n",
            "self.loss(score_3, v3) 0.008258356\n",
            "loss tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.042483103\n",
            "self.loss(score_1, v1) 0.034451548\n",
            "self.loss(score_2, v2) 0.03233234\n",
            "self.loss(score_3, v3) 0.009677222\n",
            "loss tensor(0.1189, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019438006\n",
            "self.loss(score_1, v1) 0.02025818\n",
            "self.loss(score_2, v2) 0.022546444\n",
            "self.loss(score_3, v3) 0.0077139377\n",
            "loss tensor(0.0700, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015385258\n",
            "self.loss(score_1, v1) 0.012902397\n",
            "self.loss(score_2, v2) 0.01619838\n",
            "self.loss(score_3, v3) 0.008740773\n",
            "loss tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.04650383\n",
            "self.loss(score_1, v1) 0.0269358\n",
            "self.loss(score_2, v2) 0.029997822\n",
            "self.loss(score_3, v3) 0.007911311\n",
            "loss tensor(0.1113, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009699731\n",
            "self.loss(score_1, v1) 0.04643447\n",
            "self.loss(score_2, v2) 0.044476252\n",
            "self.loss(score_3, v3) 0.009456071\n",
            "loss tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015128054\n",
            "self.loss(score_1, v1) 0.017268963\n",
            "self.loss(score_2, v2) 0.007747001\n",
            "self.loss(score_3, v3) 0.008046828\n",
            "loss tensor(0.0482, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015448509\n",
            "self.loss(score_1, v1) 0.018652085\n",
            "self.loss(score_2, v2) 0.018395498\n",
            "self.loss(score_3, v3) 0.009714911\n",
            "loss tensor(0.0622, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.041740783\n",
            "self.loss(score_1, v1) 0.017397484\n",
            "self.loss(score_2, v2) 0.025280196\n",
            "self.loss(score_3, v3) 0.012641454\n",
            "loss tensor(0.0971, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021760667\n",
            "self.loss(score_1, v1) 0.02080568\n",
            "self.loss(score_2, v2) 0.019054387\n",
            "self.loss(score_3, v3) 0.009916794\n",
            "loss tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022019979\n",
            "self.loss(score_1, v1) 0.014152464\n",
            "self.loss(score_2, v2) 0.0061894786\n",
            "self.loss(score_3, v3) 0.006179747\n",
            "loss tensor(0.0485, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009401078\n",
            "self.loss(score_1, v1) 0.016874466\n",
            "self.loss(score_2, v2) 0.018282117\n",
            "self.loss(score_3, v3) 0.008271035\n",
            "loss tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007827776\n",
            "self.loss(score_1, v1) 0.008733702\n",
            "self.loss(score_2, v2) 0.010282312\n",
            "self.loss(score_3, v3) 0.009177842\n",
            "loss tensor(0.0360, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0054326337\n",
            "self.loss(score_1, v1) 0.0071377116\n",
            "self.loss(score_2, v2) 0.010420916\n",
            "self.loss(score_3, v3) 0.0083265845\n",
            "loss tensor(0.0313, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.033287592\n",
            "self.loss(score_1, v1) 0.025386471\n",
            "self.loss(score_2, v2) 0.050023105\n",
            "self.loss(score_3, v3) 0.010086522\n",
            "loss tensor(0.1188, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013238942\n",
            "self.loss(score_1, v1) 0.03946935\n",
            "self.loss(score_2, v2) 0.039070882\n",
            "self.loss(score_3, v3) 0.013754982\n",
            "loss tensor(0.1055, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009623132\n",
            "self.loss(score_1, v1) 0.008772512\n",
            "self.loss(score_2, v2) 0.006912902\n",
            "self.loss(score_3, v3) 0.0070901704\n",
            "loss tensor(0.0324, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0099279955\n",
            "self.loss(score_1, v1) 0.022507034\n",
            "self.loss(score_2, v2) 0.024289869\n",
            "self.loss(score_3, v3) 0.00845032\n",
            "loss tensor(0.0652, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011452088\n",
            "self.loss(score_1, v1) 0.02181273\n",
            "self.loss(score_2, v2) 0.019021953\n",
            "self.loss(score_3, v3) 0.05739785\n",
            "loss tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006241195\n",
            "self.loss(score_1, v1) 0.012111181\n",
            "self.loss(score_2, v2) 0.021054208\n",
            "self.loss(score_3, v3) 0.00996012\n",
            "loss tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.051959846\n",
            "self.loss(score_1, v1) 0.019534232\n",
            "self.loss(score_2, v2) 0.014243478\n",
            "self.loss(score_3, v3) 0.009416589\n",
            "loss tensor(0.0952, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014347413\n",
            "self.loss(score_1, v1) 0.011061499\n",
            "self.loss(score_2, v2) 0.0108891\n",
            "self.loss(score_3, v3) 0.009179221\n",
            "loss tensor(0.0455, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.036144465\n",
            "self.loss(score_1, v1) 0.033357665\n",
            "self.loss(score_2, v2) 0.021209782\n",
            "self.loss(score_3, v3) 0.007339125\n",
            "loss tensor(0.0981, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009907131\n",
            "self.loss(score_1, v1) 0.010670728\n",
            "self.loss(score_2, v2) 0.004896649\n",
            "self.loss(score_3, v3) 0.0044884477\n",
            "loss tensor(0.0300, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008251863\n",
            "self.loss(score_1, v1) 0.01168942\n",
            "self.loss(score_2, v2) 0.015715394\n",
            "self.loss(score_3, v3) 0.008246746\n",
            "loss tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007086923\n",
            "self.loss(score_1, v1) 0.011297854\n",
            "self.loss(score_2, v2) 0.010286479\n",
            "self.loss(score_3, v3) 0.0065944972\n",
            "loss tensor(0.0353, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0134319775\n",
            "self.loss(score_1, v1) 0.018105045\n",
            "self.loss(score_2, v2) 0.025919149\n",
            "self.loss(score_3, v3) 0.0075582764\n",
            "loss tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0075620976\n",
            "self.loss(score_1, v1) 0.0089730155\n",
            "self.loss(score_2, v2) 0.018009901\n",
            "self.loss(score_3, v3) 0.008583924\n",
            "loss tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.03736715\n",
            "self.loss(score_1, v1) 0.023856454\n",
            "self.loss(score_2, v2) 0.022402372\n",
            "self.loss(score_3, v3) 0.006794914\n",
            "loss tensor(0.0904, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013608297\n",
            "self.loss(score_1, v1) 0.015330161\n",
            "self.loss(score_2, v2) 0.009911062\n",
            "self.loss(score_3, v3) 0.008337056\n",
            "loss tensor(0.0472, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010252316\n",
            "self.loss(score_1, v1) 0.040825944\n",
            "self.loss(score_2, v2) 0.036286034\n",
            "self.loss(score_3, v3) 0.10197284\n",
            "loss tensor(0.1893, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008214483\n",
            "self.loss(score_1, v1) 0.014782271\n",
            "self.loss(score_2, v2) 0.019302513\n",
            "self.loss(score_3, v3) 0.010531757\n",
            "loss tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007013379\n",
            "self.loss(score_1, v1) 0.008206468\n",
            "self.loss(score_2, v2) 0.009042483\n",
            "self.loss(score_3, v3) 0.0073524103\n",
            "loss tensor(0.0316, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.028122663\n",
            "self.loss(score_1, v1) 0.023341909\n",
            "self.loss(score_2, v2) 0.009316439\n",
            "self.loss(score_3, v3) 0.007407309\n",
            "loss tensor(0.0682, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008505898\n",
            "self.loss(score_1, v1) 0.01077636\n",
            "self.loss(score_2, v2) 0.016637811\n",
            "self.loss(score_3, v3) 0.0068507697\n",
            "loss tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007409528\n",
            "self.loss(score_1, v1) 0.014117912\n",
            "self.loss(score_2, v2) 0.021239437\n",
            "self.loss(score_3, v3) 0.011499387\n",
            "loss tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010615454\n",
            "self.loss(score_1, v1) 0.01800257\n",
            "self.loss(score_2, v2) 0.022213073\n",
            "self.loss(score_3, v3) 0.006384117\n",
            "loss tensor(0.0572, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006438892\n",
            "self.loss(score_1, v1) 0.011402151\n",
            "self.loss(score_2, v2) 0.013139433\n",
            "self.loss(score_3, v3) 0.006933664\n",
            "loss tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0128908185\n",
            "self.loss(score_1, v1) 0.027791066\n",
            "self.loss(score_2, v2) 0.03612855\n",
            "self.loss(score_3, v3) 0.010790427\n",
            "loss tensor(0.0876, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022258835\n",
            "self.loss(score_1, v1) 0.018843884\n",
            "self.loss(score_2, v2) 0.016539523\n",
            "self.loss(score_3, v3) 0.011907145\n",
            "loss tensor(0.0695, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008954474\n",
            "self.loss(score_1, v1) 0.029136904\n",
            "self.loss(score_2, v2) 0.050263762\n",
            "self.loss(score_3, v3) 0.03909959\n",
            "loss tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015799962\n",
            "self.loss(score_1, v1) 0.010134068\n",
            "self.loss(score_2, v2) 0.014903509\n",
            "self.loss(score_3, v3) 0.009331368\n",
            "loss tensor(0.0502, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.026750408\n",
            "self.loss(score_1, v1) 0.025388218\n",
            "self.loss(score_2, v2) 0.0100848\n",
            "self.loss(score_3, v3) 0.0060935984\n",
            "loss tensor(0.0683, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019309815\n",
            "self.loss(score_1, v1) 0.04726148\n",
            "self.loss(score_2, v2) 0.033944707\n",
            "self.loss(score_3, v3) 0.00913726\n",
            "loss tensor(0.1097, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.06635645\n",
            "self.loss(score_1, v1) 0.03329382\n",
            "self.loss(score_2, v2) 0.058867812\n",
            "self.loss(score_3, v3) 0.00817097\n",
            "loss tensor(0.1667, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.025817743\n",
            "self.loss(score_1, v1) 0.0152501175\n",
            "self.loss(score_2, v2) 0.00851805\n",
            "self.loss(score_3, v3) 0.0060537746\n",
            "loss tensor(0.0556, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.028488563\n",
            "self.loss(score_1, v1) 0.018418275\n",
            "self.loss(score_2, v2) 0.023283122\n",
            "self.loss(score_3, v3) 0.009095248\n",
            "loss tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0042075184\n",
            "self.loss(score_1, v1) 0.013069614\n",
            "self.loss(score_2, v2) 0.024997523\n",
            "self.loss(score_3, v3) 0.0054384163\n",
            "loss tensor(0.0477, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022293866\n",
            "self.loss(score_1, v1) 0.021104582\n",
            "self.loss(score_2, v2) 0.018088056\n",
            "self.loss(score_3, v3) 0.008414745\n",
            "loss tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009794992\n",
            "self.loss(score_1, v1) 0.012550466\n",
            "self.loss(score_2, v2) 0.011165114\n",
            "self.loss(score_3, v3) 0.011928125\n",
            "loss tensor(0.0454, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007901168\n",
            "self.loss(score_1, v1) 0.010578871\n",
            "self.loss(score_2, v2) 0.012368794\n",
            "self.loss(score_3, v3) 0.006019572\n",
            "loss tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018740041\n",
            "self.loss(score_1, v1) 0.02208848\n",
            "self.loss(score_2, v2) 0.019994628\n",
            "self.loss(score_3, v3) 0.0076256646\n",
            "loss tensor(0.0684, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.023684878\n",
            "self.loss(score_1, v1) 0.040016994\n",
            "self.loss(score_2, v2) 0.03867131\n",
            "self.loss(score_3, v3) 0.0077129025\n",
            "loss tensor(0.1101, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0077595566\n",
            "self.loss(score_1, v1) 0.0077782352\n",
            "self.loss(score_2, v2) 0.014761165\n",
            "self.loss(score_3, v3) 0.010580542\n",
            "loss tensor(0.0409, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01792061\n",
            "self.loss(score_1, v1) 0.016739452\n",
            "self.loss(score_2, v2) 0.011900654\n",
            "self.loss(score_3, v3) 0.007896922\n",
            "loss tensor(0.0545, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007809553\n",
            "self.loss(score_1, v1) 0.009252331\n",
            "self.loss(score_2, v2) 0.016116505\n",
            "self.loss(score_3, v3) 0.009532837\n",
            "loss tensor(0.0427, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0131947445\n",
            "self.loss(score_1, v1) 0.045948472\n",
            "self.loss(score_2, v2) 0.05571851\n",
            "self.loss(score_3, v3) 0.010017874\n",
            "loss tensor(0.1249, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.08161700118309373, Train Accuracy : 0.9993075800943831\n",
            " Validation Accuracy : 6.602408143979099\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) 0.005970526\n",
            "self.loss(score_1, v1) 0.021147331\n",
            "self.loss(score_2, v2) 0.0721764\n",
            "self.loss(score_3, v3) 0.029879855\n",
            "loss tensor(0.1292, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0063776085\n",
            "self.loss(score_1, v1) 0.008838687\n",
            "self.loss(score_2, v2) 0.06551105\n",
            "self.loss(score_3, v3) 0.03522684\n",
            "loss tensor(0.1160, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.04997673\n",
            "self.loss(score_1, v1) 0.04839622\n",
            "self.loss(score_2, v2) 0.04857022\n",
            "self.loss(score_3, v3) 0.006654857\n",
            "loss tensor(0.1536, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.045293782\n",
            "self.loss(score_1, v1) 0.02743612\n",
            "self.loss(score_2, v2) 0.021024374\n",
            "self.loss(score_3, v3) 0.009060235\n",
            "loss tensor(0.1028, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014411229\n",
            "self.loss(score_1, v1) 0.05333673\n",
            "self.loss(score_2, v2) 0.032487918\n",
            "self.loss(score_3, v3) 0.0067899437\n",
            "loss tensor(0.1070, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00761111\n",
            "self.loss(score_1, v1) 0.008339297\n",
            "self.loss(score_2, v2) 0.0072266525\n",
            "self.loss(score_3, v3) 0.0058132457\n",
            "loss tensor(0.0290, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0076495097\n",
            "self.loss(score_1, v1) 0.010815665\n",
            "self.loss(score_2, v2) 0.0110271\n",
            "self.loss(score_3, v3) 0.0070524383\n",
            "loss tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01143671\n",
            "self.loss(score_1, v1) 0.01311957\n",
            "self.loss(score_2, v2) 0.009107391\n",
            "self.loss(score_3, v3) 0.005180594\n",
            "loss tensor(0.0388, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008385128\n",
            "self.loss(score_1, v1) 0.012123081\n",
            "self.loss(score_2, v2) 0.015577497\n",
            "self.loss(score_3, v3) 0.007856849\n",
            "loss tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0076980055\n",
            "self.loss(score_1, v1) 0.009077456\n",
            "self.loss(score_2, v2) 0.017490664\n",
            "self.loss(score_3, v3) 0.011543692\n",
            "loss tensor(0.0458, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020078758\n",
            "self.loss(score_1, v1) 0.03397258\n",
            "self.loss(score_2, v2) 0.028938966\n",
            "self.loss(score_3, v3) 0.0075563025\n",
            "loss tensor(0.0905, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010031452\n",
            "self.loss(score_1, v1) 0.011290055\n",
            "self.loss(score_2, v2) 0.011605632\n",
            "self.loss(score_3, v3) 0.006985987\n",
            "loss tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015708745\n",
            "self.loss(score_1, v1) 0.03934836\n",
            "self.loss(score_2, v2) 0.047651436\n",
            "self.loss(score_3, v3) 0.0074979826\n",
            "loss tensor(0.1102, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021760946\n",
            "self.loss(score_1, v1) 0.023466585\n",
            "self.loss(score_2, v2) 0.04092908\n",
            "self.loss(score_3, v3) 0.008693234\n",
            "loss tensor(0.0948, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008084055\n",
            "self.loss(score_1, v1) 0.012575198\n",
            "self.loss(score_2, v2) 0.017052785\n",
            "self.loss(score_3, v3) 0.006592701\n",
            "loss tensor(0.0443, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.040650606\n",
            "self.loss(score_1, v1) 0.021714343\n",
            "self.loss(score_2, v2) 0.01234985\n",
            "self.loss(score_3, v3) 0.008852503\n",
            "loss tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013007394\n",
            "self.loss(score_1, v1) 0.015088977\n",
            "self.loss(score_2, v2) 0.011218164\n",
            "self.loss(score_3, v3) 0.008462046\n",
            "loss tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01214712\n",
            "self.loss(score_1, v1) 0.0147923995\n",
            "self.loss(score_2, v2) 0.014429521\n",
            "self.loss(score_3, v3) 0.007304524\n",
            "loss tensor(0.0487, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.044740677\n",
            "self.loss(score_1, v1) 0.0459924\n",
            "self.loss(score_2, v2) 0.045663323\n",
            "self.loss(score_3, v3) 0.009537177\n",
            "loss tensor(0.1459, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01659971\n",
            "self.loss(score_1, v1) 0.021059811\n",
            "self.loss(score_2, v2) 0.03213425\n",
            "self.loss(score_3, v3) 0.011106966\n",
            "loss tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012631606\n",
            "self.loss(score_1, v1) 0.017329648\n",
            "self.loss(score_2, v2) 0.018713333\n",
            "self.loss(score_3, v3) 0.0074705873\n",
            "loss tensor(0.0561, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.024538832\n",
            "self.loss(score_1, v1) 0.019391341\n",
            "self.loss(score_2, v2) 0.010865704\n",
            "self.loss(score_3, v3) 0.007812635\n",
            "loss tensor(0.0626, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.005967079\n",
            "self.loss(score_1, v1) 0.013470833\n",
            "self.loss(score_2, v2) 0.02416611\n",
            "self.loss(score_3, v3) 0.010324431\n",
            "loss tensor(0.0539, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0062266467\n",
            "self.loss(score_1, v1) 0.0064427573\n",
            "self.loss(score_2, v2) 0.007497476\n",
            "self.loss(score_3, v3) 0.0059188977\n",
            "loss tensor(0.0261, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0073964996\n",
            "self.loss(score_1, v1) 0.008446794\n",
            "self.loss(score_2, v2) 0.0077880216\n",
            "self.loss(score_3, v3) 0.0061095427\n",
            "loss tensor(0.0297, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008773565\n",
            "self.loss(score_1, v1) 0.012342366\n",
            "self.loss(score_2, v2) 0.016506933\n",
            "self.loss(score_3, v3) 0.008488143\n",
            "loss tensor(0.0461, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01975678\n",
            "self.loss(score_1, v1) 0.023353327\n",
            "self.loss(score_2, v2) 0.019666748\n",
            "self.loss(score_3, v3) 0.008212035\n",
            "loss tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007269897\n",
            "self.loss(score_1, v1) 0.010029491\n",
            "self.loss(score_2, v2) 0.011649262\n",
            "self.loss(score_3, v3) 0.008585084\n",
            "loss tensor(0.0375, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014703818\n",
            "self.loss(score_1, v1) 0.027401589\n",
            "self.loss(score_2, v2) 0.027798045\n",
            "self.loss(score_3, v3) 0.011118424\n",
            "loss tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008208912\n",
            "self.loss(score_1, v1) 0.012032181\n",
            "self.loss(score_2, v2) 0.023503436\n",
            "self.loss(score_3, v3) 0.009158631\n",
            "loss tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.027792698\n",
            "self.loss(score_1, v1) 0.043150794\n",
            "self.loss(score_2, v2) 0.047484394\n",
            "self.loss(score_3, v3) 0.007878399\n",
            "loss tensor(0.1263, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009887691\n",
            "self.loss(score_1, v1) 0.009846882\n",
            "self.loss(score_2, v2) 0.010424784\n",
            "self.loss(score_3, v3) 0.0067479876\n",
            "loss tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.10176759\n",
            "self.loss(score_1, v1) 0.09069852\n",
            "self.loss(score_2, v2) 0.098812826\n",
            "self.loss(score_3, v3) 0.36355966\n",
            "loss tensor(0.6548, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011260378\n",
            "self.loss(score_1, v1) 0.013074993\n",
            "self.loss(score_2, v2) 0.010158699\n",
            "self.loss(score_3, v3) 0.0077250265\n",
            "loss tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007854628\n",
            "self.loss(score_1, v1) 0.012606339\n",
            "self.loss(score_2, v2) 0.020615371\n",
            "self.loss(score_3, v3) 0.00937574\n",
            "loss tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009156504\n",
            "self.loss(score_1, v1) 0.012429644\n",
            "self.loss(score_2, v2) 0.013245074\n",
            "self.loss(score_3, v3) 0.008230496\n",
            "loss tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0077997847\n",
            "self.loss(score_1, v1) 0.009939286\n",
            "self.loss(score_2, v2) 0.011487238\n",
            "self.loss(score_3, v3) 0.010461941\n",
            "loss tensor(0.0397, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008716643\n",
            "self.loss(score_1, v1) 0.015693232\n",
            "self.loss(score_2, v2) 0.034309644\n",
            "self.loss(score_3, v3) 0.014971984\n",
            "loss tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.030571805\n",
            "self.loss(score_1, v1) 0.017215502\n",
            "self.loss(score_2, v2) 0.014862948\n",
            "self.loss(score_3, v3) 0.008606629\n",
            "loss tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01485091\n",
            "self.loss(score_1, v1) 0.026540259\n",
            "self.loss(score_2, v2) 0.032894116\n",
            "self.loss(score_3, v3) 0.062474158\n",
            "loss tensor(0.1368, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.03234352\n",
            "self.loss(score_1, v1) 0.012759193\n",
            "self.loss(score_2, v2) 0.0072989115\n",
            "self.loss(score_3, v3) 0.0049214903\n",
            "loss tensor(0.0573, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.07490393\n",
            "self.loss(score_1, v1) 0.05217372\n",
            "self.loss(score_2, v2) 0.050748035\n",
            "self.loss(score_3, v3) 0.00669585\n",
            "loss tensor(0.1845, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010471504\n",
            "self.loss(score_1, v1) 0.010036818\n",
            "self.loss(score_2, v2) 0.012596328\n",
            "self.loss(score_3, v3) 0.012034999\n",
            "loss tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006309402\n",
            "self.loss(score_1, v1) 0.009751416\n",
            "self.loss(score_2, v2) 0.012875025\n",
            "self.loss(score_3, v3) 0.0074760783\n",
            "loss tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010610764\n",
            "self.loss(score_1, v1) 0.01496452\n",
            "self.loss(score_2, v2) 0.012409998\n",
            "self.loss(score_3, v3) 0.008807214\n",
            "loss tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008089674\n",
            "self.loss(score_1, v1) 0.015104747\n",
            "self.loss(score_2, v2) 0.013937061\n",
            "self.loss(score_3, v3) 0.0063356794\n",
            "loss tensor(0.0435, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0068411175\n",
            "self.loss(score_1, v1) 0.01392726\n",
            "self.loss(score_2, v2) 0.013123242\n",
            "self.loss(score_3, v3) 0.010680347\n",
            "loss tensor(0.0446, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01427215\n",
            "self.loss(score_1, v1) 0.011971141\n",
            "self.loss(score_2, v2) 0.04607217\n",
            "self.loss(score_3, v3) 0.0067448937\n",
            "loss tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006817181\n",
            "self.loss(score_1, v1) 0.0066067786\n",
            "self.loss(score_2, v2) 0.010402054\n",
            "self.loss(score_3, v3) 0.007350786\n",
            "loss tensor(0.0312, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010099331\n",
            "self.loss(score_1, v1) 0.016685259\n",
            "self.loss(score_2, v2) 0.018834792\n",
            "self.loss(score_3, v3) 0.013566555\n",
            "loss tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.024247222\n",
            "self.loss(score_1, v1) 0.033659227\n",
            "self.loss(score_2, v2) 0.038130146\n",
            "self.loss(score_3, v3) 0.01631488\n",
            "loss tensor(0.1124, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0068329084\n",
            "self.loss(score_1, v1) 0.009785441\n",
            "self.loss(score_2, v2) 0.0130274445\n",
            "self.loss(score_3, v3) 0.008258927\n",
            "loss tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02292402\n",
            "self.loss(score_1, v1) 0.04266266\n",
            "self.loss(score_2, v2) 0.08403328\n",
            "self.loss(score_3, v3) 0.07144135\n",
            "loss tensor(0.2211, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01341503\n",
            "self.loss(score_1, v1) 0.012935954\n",
            "self.loss(score_2, v2) 0.010774547\n",
            "self.loss(score_3, v3) 0.008787267\n",
            "loss tensor(0.0459, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0811836\n",
            "self.loss(score_1, v1) 0.04708825\n",
            "self.loss(score_2, v2) 0.050045762\n",
            "self.loss(score_3, v3) 0.012986549\n",
            "loss tensor(0.1913, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011905678\n",
            "self.loss(score_1, v1) 0.013212155\n",
            "self.loss(score_2, v2) 0.017526103\n",
            "self.loss(score_3, v3) 0.010055977\n",
            "loss tensor(0.0527, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021034192\n",
            "self.loss(score_1, v1) 0.027458888\n",
            "self.loss(score_2, v2) 0.018973816\n",
            "self.loss(score_3, v3) 0.009660588\n",
            "loss tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0131652355\n",
            "self.loss(score_1, v1) 0.021167556\n",
            "self.loss(score_2, v2) 0.026238611\n",
            "self.loss(score_3, v3) 0.009882212\n",
            "loss tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015618702\n",
            "self.loss(score_1, v1) 0.016820492\n",
            "self.loss(score_2, v2) 0.010673927\n",
            "self.loss(score_3, v3) 0.005257814\n",
            "loss tensor(0.0484, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009182809\n",
            "self.loss(score_1, v1) 0.0077326046\n",
            "self.loss(score_2, v2) 0.00890675\n",
            "self.loss(score_3, v3) 0.008831596\n",
            "loss tensor(0.0347, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009052306\n",
            "self.loss(score_1, v1) 0.03574453\n",
            "self.loss(score_2, v2) 0.03540255\n",
            "self.loss(score_3, v3) 0.0038489043\n",
            "loss tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019427354\n",
            "self.loss(score_1, v1) 0.031917766\n",
            "self.loss(score_2, v2) 0.024528941\n",
            "self.loss(score_3, v3) 0.009436132\n",
            "loss tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009974407\n",
            "self.loss(score_1, v1) 0.008221394\n",
            "self.loss(score_2, v2) 0.010206076\n",
            "self.loss(score_3, v3) 0.008552828\n",
            "loss tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.11691373\n",
            "self.loss(score_1, v1) 0.037054315\n",
            "self.loss(score_2, v2) 0.028010687\n",
            "self.loss(score_3, v3) 0.030362362\n",
            "loss tensor(0.2123, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.025834369\n",
            "self.loss(score_1, v1) 0.031068213\n",
            "self.loss(score_2, v2) 0.029635157\n",
            "self.loss(score_3, v3) 0.010366951\n",
            "loss tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.038165487\n",
            "self.loss(score_1, v1) 0.025623692\n",
            "self.loss(score_2, v2) 0.03760642\n",
            "self.loss(score_3, v3) 0.010219789\n",
            "loss tensor(0.1116, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014038941\n",
            "self.loss(score_1, v1) 0.021481149\n",
            "self.loss(score_2, v2) 0.0360064\n",
            "self.loss(score_3, v3) 0.010212231\n",
            "loss tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009272143\n",
            "self.loss(score_1, v1) 0.0089077335\n",
            "self.loss(score_2, v2) 0.014411946\n",
            "self.loss(score_3, v3) 0.008786492\n",
            "loss tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014124557\n",
            "self.loss(score_1, v1) 0.012693326\n",
            "self.loss(score_2, v2) 0.009650352\n",
            "self.loss(score_3, v3) 0.006850805\n",
            "loss tensor(0.0433, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010951373\n",
            "self.loss(score_1, v1) 0.020261895\n",
            "self.loss(score_2, v2) 0.0220414\n",
            "self.loss(score_3, v3) 0.009651749\n",
            "loss tensor(0.0629, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008942411\n",
            "self.loss(score_1, v1) 0.013368395\n",
            "self.loss(score_2, v2) 0.022855451\n",
            "self.loss(score_3, v3) 0.007853231\n",
            "loss tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.06405171\n",
            "self.loss(score_1, v1) 0.06310464\n",
            "self.loss(score_2, v2) 0.046580356\n",
            "self.loss(score_3, v3) 0.008222157\n",
            "loss tensor(0.1820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019569037\n",
            "self.loss(score_1, v1) 0.023942454\n",
            "self.loss(score_2, v2) 0.022859894\n",
            "self.loss(score_3, v3) 0.011225016\n",
            "loss tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01982418\n",
            "self.loss(score_1, v1) 0.036419094\n",
            "self.loss(score_2, v2) 0.067893244\n",
            "self.loss(score_3, v3) 0.035974283\n",
            "loss tensor(0.1601, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00821175\n",
            "self.loss(score_1, v1) 0.009772608\n",
            "self.loss(score_2, v2) 0.016774945\n",
            "self.loss(score_3, v3) 0.0065477863\n",
            "loss tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0060423347\n",
            "self.loss(score_1, v1) 0.0057653077\n",
            "self.loss(score_2, v2) 0.007114957\n",
            "self.loss(score_3, v3) 0.006293576\n",
            "loss tensor(0.0252, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014258816\n",
            "self.loss(score_1, v1) 0.014298987\n",
            "self.loss(score_2, v2) 0.010191814\n",
            "self.loss(score_3, v3) 0.008022375\n",
            "loss tensor(0.0468, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011581192\n",
            "self.loss(score_1, v1) 0.016242834\n",
            "self.loss(score_2, v2) 0.021110848\n",
            "self.loss(score_3, v3) 0.009015176\n",
            "loss tensor(0.0580, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010202142\n",
            "self.loss(score_1, v1) 0.021887057\n",
            "self.loss(score_2, v2) 0.0224904\n",
            "self.loss(score_3, v3) 0.0070757293\n",
            "loss tensor(0.0617, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0074201417\n",
            "self.loss(score_1, v1) 0.008839949\n",
            "self.loss(score_2, v2) 0.0071738376\n",
            "self.loss(score_3, v3) 0.004923255\n",
            "loss tensor(0.0284, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.033618018\n",
            "self.loss(score_1, v1) 0.028832411\n",
            "self.loss(score_2, v2) 0.030354228\n",
            "self.loss(score_3, v3) 0.006293089\n",
            "loss tensor(0.0991, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0074968347\n",
            "self.loss(score_1, v1) 0.015101538\n",
            "self.loss(score_2, v2) 0.01796324\n",
            "self.loss(score_3, v3) 0.0068519805\n",
            "loss tensor(0.0474, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006370919\n",
            "self.loss(score_1, v1) 0.011664494\n",
            "self.loss(score_2, v2) 0.007656702\n",
            "self.loss(score_3, v3) 0.0069321417\n",
            "loss tensor(0.0326, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011589135\n",
            "self.loss(score_1, v1) 0.014762306\n",
            "self.loss(score_2, v2) 0.010974578\n",
            "self.loss(score_3, v3) 0.006614228\n",
            "loss tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009014919\n",
            "self.loss(score_1, v1) 0.028938599\n",
            "self.loss(score_2, v2) 0.015946336\n",
            "self.loss(score_3, v3) 0.008491601\n",
            "loss tensor(0.0624, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012600308\n",
            "self.loss(score_1, v1) 0.016348362\n",
            "self.loss(score_2, v2) 0.020726694\n",
            "self.loss(score_3, v3) 0.009346726\n",
            "loss tensor(0.0590, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012170289\n",
            "self.loss(score_1, v1) 0.045197792\n",
            "self.loss(score_2, v2) 0.04241611\n",
            "self.loss(score_3, v3) 0.07714828\n",
            "loss tensor(0.1769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014797542\n",
            "self.loss(score_1, v1) 0.014219761\n",
            "self.loss(score_2, v2) 0.026528174\n",
            "self.loss(score_3, v3) 0.006414358\n",
            "loss tensor(0.0620, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.005912983\n",
            "self.loss(score_1, v1) 0.008224284\n",
            "self.loss(score_2, v2) 0.014468769\n",
            "self.loss(score_3, v3) 0.009124034\n",
            "loss tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0068114884\n",
            "self.loss(score_1, v1) 0.016250141\n",
            "self.loss(score_2, v2) 0.0208786\n",
            "self.loss(score_3, v3) 0.00706192\n",
            "loss tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0399208\n",
            "self.loss(score_1, v1) 0.022326004\n",
            "self.loss(score_2, v2) 0.027570805\n",
            "self.loss(score_3, v3) 0.00625671\n",
            "loss tensor(0.0961, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019546205\n",
            "self.loss(score_1, v1) 0.012764313\n",
            "self.loss(score_2, v2) 0.010840934\n",
            "self.loss(score_3, v3) 0.011575228\n",
            "loss tensor(0.0547, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017084045\n",
            "self.loss(score_1, v1) 0.023391524\n",
            "self.loss(score_2, v2) 0.021319805\n",
            "self.loss(score_3, v3) 0.011102892\n",
            "loss tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014954582\n",
            "self.loss(score_1, v1) 0.02090437\n",
            "self.loss(score_2, v2) 0.027084868\n",
            "self.loss(score_3, v3) 0.013401076\n",
            "loss tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011568987\n",
            "self.loss(score_1, v1) 0.011885268\n",
            "self.loss(score_2, v2) 0.012977064\n",
            "self.loss(score_3, v3) 0.006651717\n",
            "loss tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.053909916\n",
            "self.loss(score_1, v1) 0.03200809\n",
            "self.loss(score_2, v2) 0.01716021\n",
            "self.loss(score_3, v3) 0.006767905\n",
            "loss tensor(0.1098, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01662244\n",
            "self.loss(score_1, v1) 0.031972293\n",
            "self.loss(score_2, v2) 0.028217593\n",
            "self.loss(score_3, v3) 0.009908366\n",
            "loss tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007517013\n",
            "self.loss(score_1, v1) 0.009269491\n",
            "self.loss(score_2, v2) 0.008292167\n",
            "self.loss(score_3, v3) 0.007135328\n",
            "loss tensor(0.0322, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006604531\n",
            "self.loss(score_1, v1) 0.011092029\n",
            "self.loss(score_2, v2) 0.017766394\n",
            "self.loss(score_3, v3) 0.007974651\n",
            "loss tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.034642816\n",
            "self.loss(score_1, v1) 0.04696637\n",
            "self.loss(score_2, v2) 0.051562965\n",
            "self.loss(score_3, v3) 0.020015262\n",
            "loss tensor(0.1532, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008093504\n",
            "self.loss(score_1, v1) 0.037241437\n",
            "self.loss(score_2, v2) 0.052083533\n",
            "self.loss(score_3, v3) 0.052139185\n",
            "loss tensor(0.1496, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.05276057\n",
            "self.loss(score_1, v1) 0.0670066\n",
            "self.loss(score_2, v2) 0.040200148\n",
            "self.loss(score_3, v3) 0.021683648\n",
            "loss tensor(0.1817, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.030313326\n",
            "self.loss(score_1, v1) 0.020482337\n",
            "self.loss(score_2, v2) 0.022177743\n",
            "self.loss(score_3, v3) 0.0065258266\n",
            "loss tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015407162\n",
            "self.loss(score_1, v1) 0.014717157\n",
            "self.loss(score_2, v2) 0.018872222\n",
            "self.loss(score_3, v3) 0.009857267\n",
            "loss tensor(0.0589, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0058534876\n",
            "self.loss(score_1, v1) 0.006539034\n",
            "self.loss(score_2, v2) 0.007910046\n",
            "self.loss(score_3, v3) 0.0058557144\n",
            "loss tensor(0.0262, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009036518\n",
            "self.loss(score_1, v1) 0.022294551\n",
            "self.loss(score_2, v2) 0.04680217\n",
            "self.loss(score_3, v3) 0.008535506\n",
            "loss tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020681323\n",
            "self.loss(score_1, v1) 0.05955542\n",
            "self.loss(score_2, v2) 0.02710837\n",
            "self.loss(score_3, v3) 0.011138359\n",
            "loss tensor(0.1185, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00926869\n",
            "self.loss(score_1, v1) 0.009972731\n",
            "self.loss(score_2, v2) 0.009001566\n",
            "self.loss(score_3, v3) 0.0079763355\n",
            "loss tensor(0.0362, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008836595\n",
            "self.loss(score_1, v1) 0.014800794\n",
            "self.loss(score_2, v2) 0.018302359\n",
            "self.loss(score_3, v3) 0.008979869\n",
            "loss tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006413921\n",
            "self.loss(score_1, v1) 0.04495622\n",
            "self.loss(score_2, v2) 0.11705585\n",
            "self.loss(score_3, v3) 0.16949135\n",
            "loss tensor(0.3379, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007829184\n",
            "self.loss(score_1, v1) 0.014677733\n",
            "self.loss(score_2, v2) 0.011765297\n",
            "self.loss(score_3, v3) 0.017035544\n",
            "loss tensor(0.0513, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011730729\n",
            "self.loss(score_1, v1) 0.013169466\n",
            "self.loss(score_2, v2) 0.02342544\n",
            "self.loss(score_3, v3) 0.066643625\n",
            "loss tensor(0.1150, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013345813\n",
            "self.loss(score_1, v1) 0.052061874\n",
            "self.loss(score_2, v2) 0.041235603\n",
            "self.loss(score_3, v3) 0.011309115\n",
            "loss tensor(0.1180, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011441964\n",
            "self.loss(score_1, v1) 0.021168433\n",
            "self.loss(score_2, v2) 0.018687733\n",
            "self.loss(score_3, v3) 0.008138598\n",
            "loss tensor(0.0594, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015292431\n",
            "self.loss(score_1, v1) 0.016418895\n",
            "self.loss(score_2, v2) 0.019474652\n",
            "self.loss(score_3, v3) 0.009708835\n",
            "loss tensor(0.0609, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006094967\n",
            "self.loss(score_1, v1) 0.005656782\n",
            "self.loss(score_2, v2) 0.010678242\n",
            "self.loss(score_3, v3) 0.008365694\n",
            "loss tensor(0.0308, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013615\n",
            "self.loss(score_1, v1) 0.013425689\n",
            "self.loss(score_2, v2) 0.0096999435\n",
            "self.loss(score_3, v3) 0.008402661\n",
            "loss tensor(0.0451, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010908627\n",
            "self.loss(score_1, v1) 0.01222069\n",
            "self.loss(score_2, v2) 0.0066609397\n",
            "self.loss(score_3, v3) 0.00658972\n",
            "loss tensor(0.0364, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00661909\n",
            "self.loss(score_1, v1) 0.0067301444\n",
            "self.loss(score_2, v2) 0.00663973\n",
            "self.loss(score_3, v3) 0.0041075326\n",
            "loss tensor(0.0241, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007060537\n",
            "self.loss(score_1, v1) 0.008242522\n",
            "self.loss(score_2, v2) 0.005433129\n",
            "self.loss(score_3, v3) 0.0035693748\n",
            "loss tensor(0.0243, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.123109445\n",
            "self.loss(score_1, v1) 0.044323195\n",
            "self.loss(score_2, v2) 0.0479298\n",
            "self.loss(score_3, v3) 0.03271039\n",
            "loss tensor(0.2481, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0071544955\n",
            "self.loss(score_1, v1) 0.006768211\n",
            "self.loss(score_2, v2) 0.0077246698\n",
            "self.loss(score_3, v3) 0.004624846\n",
            "loss tensor(0.0263, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010462843\n",
            "self.loss(score_1, v1) 0.025385944\n",
            "self.loss(score_2, v2) 0.016488776\n",
            "self.loss(score_3, v3) 0.0069762277\n",
            "loss tensor(0.0593, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0050915945\n",
            "self.loss(score_1, v1) 0.01089524\n",
            "self.loss(score_2, v2) 0.0083939005\n",
            "self.loss(score_3, v3) 0.004419958\n",
            "loss tensor(0.0288, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006260575\n",
            "self.loss(score_1, v1) 0.012418649\n",
            "self.loss(score_2, v2) 0.0138575565\n",
            "self.loss(score_3, v3) 0.008173183\n",
            "loss tensor(0.0407, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015637236\n",
            "self.loss(score_1, v1) 0.01620161\n",
            "self.loss(score_2, v2) 0.022124\n",
            "self.loss(score_3, v3) 0.013799963\n",
            "loss tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012728526\n",
            "self.loss(score_1, v1) 0.01995619\n",
            "self.loss(score_2, v2) 0.03150607\n",
            "self.loss(score_3, v3) 0.008744569\n",
            "loss tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010172102\n",
            "self.loss(score_1, v1) 0.014033536\n",
            "self.loss(score_2, v2) 0.01687535\n",
            "self.loss(score_3, v3) 0.012081592\n",
            "loss tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012321726\n",
            "self.loss(score_1, v1) 0.010396496\n",
            "self.loss(score_2, v2) 0.007430401\n",
            "self.loss(score_3, v3) 0.00742162\n",
            "loss tensor(0.0376, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010262594\n",
            "self.loss(score_1, v1) 0.011095878\n",
            "self.loss(score_2, v2) 0.009585795\n",
            "self.loss(score_3, v3) 0.008934504\n",
            "loss tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008355541\n",
            "self.loss(score_1, v1) 0.01579948\n",
            "self.loss(score_2, v2) 0.011013676\n",
            "self.loss(score_3, v3) 0.08151819\n",
            "loss tensor(0.1167, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009348391\n",
            "self.loss(score_1, v1) 0.010436099\n",
            "self.loss(score_2, v2) 0.013081081\n",
            "self.loss(score_3, v3) 0.0064361035\n",
            "loss tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007773747\n",
            "self.loss(score_1, v1) 0.017492957\n",
            "self.loss(score_2, v2) 0.01672264\n",
            "self.loss(score_3, v3) 0.010775284\n",
            "loss tensor(0.0528, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008766173\n",
            "self.loss(score_1, v1) 0.024618318\n",
            "self.loss(score_2, v2) 0.05563718\n",
            "self.loss(score_3, v3) 0.035033833\n",
            "loss tensor(0.1241, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008650713\n",
            "self.loss(score_1, v1) 0.0646642\n",
            "self.loss(score_2, v2) 0.09416817\n",
            "self.loss(score_3, v3) 0.06879153\n",
            "loss tensor(0.2363, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014244994\n",
            "self.loss(score_1, v1) 0.020566683\n",
            "self.loss(score_2, v2) 0.015968932\n",
            "self.loss(score_3, v3) 0.02468007\n",
            "loss tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015379624\n",
            "self.loss(score_1, v1) 0.021807585\n",
            "self.loss(score_2, v2) 0.024110217\n",
            "self.loss(score_3, v3) 0.011552686\n",
            "loss tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0070485384\n",
            "self.loss(score_1, v1) 0.008305138\n",
            "self.loss(score_2, v2) 0.008521922\n",
            "self.loss(score_3, v3) 0.0059559\n",
            "loss tensor(0.0298, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008736967\n",
            "self.loss(score_1, v1) 0.019187722\n",
            "self.loss(score_2, v2) 0.019115236\n",
            "self.loss(score_3, v3) 0.0168632\n",
            "loss tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007546403\n",
            "self.loss(score_1, v1) 0.014027395\n",
            "self.loss(score_2, v2) 0.0143918125\n",
            "self.loss(score_3, v3) 0.0065564765\n",
            "loss tensor(0.0425, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017277801\n",
            "self.loss(score_1, v1) 0.015869604\n",
            "self.loss(score_2, v2) 0.014623593\n",
            "self.loss(score_3, v3) 0.011332019\n",
            "loss tensor(0.0591, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008157367\n",
            "self.loss(score_1, v1) 0.018317316\n",
            "self.loss(score_2, v2) 0.017971434\n",
            "self.loss(score_3, v3) 0.008593473\n",
            "loss tensor(0.0530, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015722781\n",
            "self.loss(score_1, v1) 0.012989351\n",
            "self.loss(score_2, v2) 0.01270693\n",
            "self.loss(score_3, v3) 0.0075018173\n",
            "loss tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008292747\n",
            "self.loss(score_1, v1) 0.01451095\n",
            "self.loss(score_2, v2) 0.020348158\n",
            "self.loss(score_3, v3) 0.009727553\n",
            "loss tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011790458\n",
            "self.loss(score_1, v1) 0.029201174\n",
            "self.loss(score_2, v2) 0.030706335\n",
            "self.loss(score_3, v3) 0.016272187\n",
            "loss tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007894645\n",
            "self.loss(score_1, v1) 0.016408121\n",
            "self.loss(score_2, v2) 0.015580131\n",
            "self.loss(score_3, v3) 0.008971461\n",
            "loss tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0076721306\n",
            "self.loss(score_1, v1) 0.00807277\n",
            "self.loss(score_2, v2) 0.008988647\n",
            "self.loss(score_3, v3) 0.0063303695\n",
            "loss tensor(0.0311, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016962612\n",
            "self.loss(score_1, v1) 0.02339152\n",
            "self.loss(score_2, v2) 0.025692968\n",
            "self.loss(score_3, v3) 0.01111087\n",
            "loss tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009112487\n",
            "self.loss(score_1, v1) 0.007648951\n",
            "self.loss(score_2, v2) 0.00878089\n",
            "self.loss(score_3, v3) 0.011115825\n",
            "loss tensor(0.0367, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013095758\n",
            "self.loss(score_1, v1) 0.022182574\n",
            "self.loss(score_2, v2) 0.019356567\n",
            "self.loss(score_3, v3) 0.009027454\n",
            "loss tensor(0.0637, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.056141328\n",
            "self.loss(score_1, v1) 0.011865052\n",
            "self.loss(score_2, v2) 0.038649492\n",
            "self.loss(score_3, v3) 0.025596997\n",
            "loss tensor(0.1323, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014165591\n",
            "self.loss(score_1, v1) 0.01751942\n",
            "self.loss(score_2, v2) 0.014553569\n",
            "self.loss(score_3, v3) 0.008159901\n",
            "loss tensor(0.0544, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020887151\n",
            "self.loss(score_1, v1) 0.038161088\n",
            "self.loss(score_2, v2) 0.012534251\n",
            "self.loss(score_3, v3) 0.0078009684\n",
            "loss tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013785648\n",
            "self.loss(score_1, v1) 0.018767375\n",
            "self.loss(score_2, v2) 0.017278967\n",
            "self.loss(score_3, v3) 0.011015696\n",
            "loss tensor(0.0608, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021399448\n",
            "self.loss(score_1, v1) 0.02109158\n",
            "self.loss(score_2, v2) 0.009037929\n",
            "self.loss(score_3, v3) 0.006838919\n",
            "loss tensor(0.0584, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014670779\n",
            "self.loss(score_1, v1) 0.01834289\n",
            "self.loss(score_2, v2) 0.010063236\n",
            "self.loss(score_3, v3) 0.0065020234\n",
            "loss tensor(0.0496, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0074724895\n",
            "self.loss(score_1, v1) 0.013874482\n",
            "self.loss(score_2, v2) 0.019674838\n",
            "self.loss(score_3, v3) 0.011843137\n",
            "loss tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017348057\n",
            "self.loss(score_1, v1) 0.031401128\n",
            "self.loss(score_2, v2) 0.039386522\n",
            "self.loss(score_3, v3) 0.008103969\n",
            "loss tensor(0.0962, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00510154\n",
            "self.loss(score_1, v1) 0.010283234\n",
            "self.loss(score_2, v2) 0.013306861\n",
            "self.loss(score_3, v3) 0.0059374003\n",
            "loss tensor(0.0346, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015136836\n",
            "self.loss(score_1, v1) 0.020863049\n",
            "self.loss(score_2, v2) 0.03419317\n",
            "self.loss(score_3, v3) 0.0112508\n",
            "loss tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006324672\n",
            "self.loss(score_1, v1) 0.00735266\n",
            "self.loss(score_2, v2) 0.011195224\n",
            "self.loss(score_3, v3) 0.00733889\n",
            "loss tensor(0.0322, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0067720953\n",
            "self.loss(score_1, v1) 0.013977084\n",
            "self.loss(score_2, v2) 0.021482252\n",
            "self.loss(score_3, v3) 0.009643284\n",
            "loss tensor(0.0519, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.060165294\n",
            "self.loss(score_1, v1) 0.07122687\n",
            "self.loss(score_2, v2) 0.048671257\n",
            "self.loss(score_3, v3) 0.045064703\n",
            "loss tensor(0.2251, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.005994518\n",
            "self.loss(score_1, v1) 0.011806741\n",
            "self.loss(score_2, v2) 0.018117579\n",
            "self.loss(score_3, v3) 0.0076324\n",
            "loss tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0055140755\n",
            "self.loss(score_1, v1) 0.010318925\n",
            "self.loss(score_2, v2) 0.012001163\n",
            "self.loss(score_3, v3) 0.0065560043\n",
            "loss tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019062486\n",
            "self.loss(score_1, v1) 0.024276318\n",
            "self.loss(score_2, v2) 0.016960282\n",
            "self.loss(score_3, v3) 0.01053796\n",
            "loss tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01825056\n",
            "self.loss(score_1, v1) 0.06787042\n",
            "self.loss(score_2, v2) 0.058655184\n",
            "self.loss(score_3, v3) 0.02223996\n",
            "loss tensor(0.1670, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01423839\n",
            "self.loss(score_1, v1) 0.012779597\n",
            "self.loss(score_2, v2) 0.01294208\n",
            "self.loss(score_3, v3) 0.009176872\n",
            "loss tensor(0.0491, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016368108\n",
            "self.loss(score_1, v1) 0.031033138\n",
            "self.loss(score_2, v2) 0.03780947\n",
            "self.loss(score_3, v3) 0.007622073\n",
            "loss tensor(0.0928, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014827689\n",
            "self.loss(score_1, v1) 0.028774973\n",
            "self.loss(score_2, v2) 0.052985433\n",
            "self.loss(score_3, v3) 0.0481825\n",
            "loss tensor(0.1448, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0063446336\n",
            "self.loss(score_1, v1) 0.05535974\n",
            "self.loss(score_2, v2) 0.07085154\n",
            "self.loss(score_3, v3) 0.05170586\n",
            "loss tensor(0.1843, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009006534\n",
            "self.loss(score_1, v1) 0.011889806\n",
            "self.loss(score_2, v2) 0.015660865\n",
            "self.loss(score_3, v3) 0.007008829\n",
            "loss tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0034811986\n",
            "self.loss(score_1, v1) 0.008581311\n",
            "self.loss(score_2, v2) 0.014285862\n",
            "self.loss(score_3, v3) 0.0038761855\n",
            "loss tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008534836\n",
            "self.loss(score_1, v1) 0.009686051\n",
            "self.loss(score_2, v2) 0.009697227\n",
            "self.loss(score_3, v3) 0.008902556\n",
            "loss tensor(0.0368, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0060185767\n",
            "self.loss(score_1, v1) 0.015997825\n",
            "self.loss(score_2, v2) 0.01881503\n",
            "self.loss(score_3, v3) 0.010032423\n",
            "loss tensor(0.0509, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00671722\n",
            "self.loss(score_1, v1) 0.006090316\n",
            "self.loss(score_2, v2) 0.0062472834\n",
            "self.loss(score_3, v3) 0.0077587813\n",
            "loss tensor(0.0268, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009229541\n",
            "self.loss(score_1, v1) 0.0073315506\n",
            "self.loss(score_2, v2) 0.004212864\n",
            "self.loss(score_3, v3) 0.004564549\n",
            "loss tensor(0.0253, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010696307\n",
            "self.loss(score_1, v1) 0.010607934\n",
            "self.loss(score_2, v2) 0.009571461\n",
            "self.loss(score_3, v3) 0.007560285\n",
            "loss tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021304319\n",
            "self.loss(score_1, v1) 0.013443336\n",
            "self.loss(score_2, v2) 0.013295742\n",
            "self.loss(score_3, v3) 0.009436296\n",
            "loss tensor(0.0575, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.004678718\n",
            "self.loss(score_1, v1) 0.0059062047\n",
            "self.loss(score_2, v2) 0.0052352375\n",
            "self.loss(score_3, v3) 0.004135019\n",
            "loss tensor(0.0200, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011539135\n",
            "self.loss(score_1, v1) 0.018634725\n",
            "self.loss(score_2, v2) 0.040338803\n",
            "self.loss(score_3, v3) 0.012390159\n",
            "loss tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.021769118\n",
            "self.loss(score_1, v1) 0.047170013\n",
            "self.loss(score_2, v2) 0.009581071\n",
            "self.loss(score_3, v3) 0.0055074836\n",
            "loss tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006012694\n",
            "self.loss(score_1, v1) 0.010245506\n",
            "self.loss(score_2, v2) 0.0083818445\n",
            "self.loss(score_3, v3) 0.0070973937\n",
            "loss tensor(0.0317, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0073866034\n",
            "self.loss(score_1, v1) 0.011743723\n",
            "self.loss(score_2, v2) 0.015547361\n",
            "self.loss(score_3, v3) 0.010001002\n",
            "loss tensor(0.0447, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012771443\n",
            "self.loss(score_1, v1) 0.030403288\n",
            "self.loss(score_2, v2) 0.041076597\n",
            "self.loss(score_3, v3) 0.0113886\n",
            "loss tensor(0.0956, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008383847\n",
            "self.loss(score_1, v1) 0.021801746\n",
            "self.loss(score_2, v2) 0.012690489\n",
            "self.loss(score_3, v3) 0.0064675948\n",
            "loss tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009942781\n",
            "self.loss(score_1, v1) 0.02460079\n",
            "self.loss(score_2, v2) 0.015111875\n",
            "self.loss(score_3, v3) 0.0068726595\n",
            "loss tensor(0.0565, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01213063\n",
            "self.loss(score_1, v1) 0.014218307\n",
            "self.loss(score_2, v2) 0.01822831\n",
            "self.loss(score_3, v3) 0.009679048\n",
            "loss tensor(0.0543, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0051537296\n",
            "self.loss(score_1, v1) 0.008364107\n",
            "self.loss(score_2, v2) 0.03074446\n",
            "self.loss(score_3, v3) 0.004500724\n",
            "loss tensor(0.0488, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01880321\n",
            "self.loss(score_1, v1) 0.021099454\n",
            "self.loss(score_2, v2) 0.007460848\n",
            "self.loss(score_3, v3) 0.0066595203\n",
            "loss tensor(0.0540, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0075561665\n",
            "self.loss(score_1, v1) 0.009867053\n",
            "self.loss(score_2, v2) 0.014670343\n",
            "self.loss(score_3, v3) 0.008981081\n",
            "loss tensor(0.0411, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0054800864\n",
            "self.loss(score_1, v1) 0.0072022383\n",
            "self.loss(score_2, v2) 0.0071713664\n",
            "self.loss(score_3, v3) 0.007203489\n",
            "loss tensor(0.0271, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.07571676\n",
            "self.loss(score_1, v1) 0.041761782\n",
            "self.loss(score_2, v2) 0.01672112\n",
            "self.loss(score_3, v3) 0.007706198\n",
            "loss tensor(0.1419, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008206824\n",
            "self.loss(score_1, v1) 0.0067114686\n",
            "self.loss(score_2, v2) 0.006723844\n",
            "self.loss(score_3, v3) 0.0063392753\n",
            "loss tensor(0.0280, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007866014\n",
            "self.loss(score_1, v1) 0.015250665\n",
            "self.loss(score_2, v2) 0.015919628\n",
            "self.loss(score_3, v3) 0.012019826\n",
            "loss tensor(0.0511, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016076919\n",
            "self.loss(score_1, v1) 0.051579606\n",
            "self.loss(score_2, v2) 0.066356316\n",
            "self.loss(score_3, v3) 0.18471347\n",
            "loss tensor(0.3187, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009233795\n",
            "self.loss(score_1, v1) 0.008818709\n",
            "self.loss(score_2, v2) 0.008541206\n",
            "self.loss(score_3, v3) 0.0064305933\n",
            "loss tensor(0.0330, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.054501183\n",
            "self.loss(score_1, v1) 0.031420995\n",
            "self.loss(score_2, v2) 0.026796473\n",
            "self.loss(score_3, v3) 0.008165841\n",
            "loss tensor(0.1209, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020502865\n",
            "self.loss(score_1, v1) 0.023249298\n",
            "self.loss(score_2, v2) 0.038293283\n",
            "self.loss(score_3, v3) 0.0064669084\n",
            "loss tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00933133\n",
            "self.loss(score_1, v1) 0.01579474\n",
            "self.loss(score_2, v2) 0.018464303\n",
            "self.loss(score_3, v3) 0.013228868\n",
            "loss tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014416398\n",
            "self.loss(score_1, v1) 0.0105494065\n",
            "self.loss(score_2, v2) 0.0049787303\n",
            "self.loss(score_3, v3) 0.0048370315\n",
            "loss tensor(0.0348, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016227832\n",
            "self.loss(score_1, v1) 0.035585914\n",
            "self.loss(score_2, v2) 0.029187974\n",
            "self.loss(score_3, v3) 0.007264613\n",
            "loss tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.04727305\n",
            "self.loss(score_1, v1) 0.036139254\n",
            "self.loss(score_2, v2) 0.014284172\n",
            "self.loss(score_3, v3) 0.007922758\n",
            "loss tensor(0.1056, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006448312\n",
            "self.loss(score_1, v1) 0.016853524\n",
            "self.loss(score_2, v2) 0.018202167\n",
            "self.loss(score_3, v3) 0.009051067\n",
            "loss tensor(0.0506, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.046212867\n",
            "self.loss(score_1, v1) 0.027280914\n",
            "self.loss(score_2, v2) 0.028227648\n",
            "self.loss(score_3, v3) 0.0071739927\n",
            "loss tensor(0.1089, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016777784\n",
            "self.loss(score_1, v1) 0.04089563\n",
            "self.loss(score_2, v2) 0.008820723\n",
            "self.loss(score_3, v3) 0.006569017\n",
            "loss tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013403404\n",
            "self.loss(score_1, v1) 0.01430063\n",
            "self.loss(score_2, v2) 0.00779371\n",
            "self.loss(score_3, v3) 0.007650562\n",
            "loss tensor(0.0431, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01147587\n",
            "self.loss(score_1, v1) 0.0162197\n",
            "self.loss(score_2, v2) 0.048733864\n",
            "self.loss(score_3, v3) 0.009284331\n",
            "loss tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.051386937\n",
            "self.loss(score_1, v1) 0.07689068\n",
            "self.loss(score_2, v2) 0.07207193\n",
            "self.loss(score_3, v3) 0.06353312\n",
            "loss tensor(0.2639, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0061287773\n",
            "self.loss(score_1, v1) 0.009709914\n",
            "self.loss(score_2, v2) 0.019082624\n",
            "self.loss(score_3, v3) 0.007849609\n",
            "loss tensor(0.0428, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.059727028\n",
            "self.loss(score_1, v1) 0.041610505\n",
            "self.loss(score_2, v2) 0.026999546\n",
            "self.loss(score_3, v3) 0.008721657\n",
            "loss tensor(0.1371, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01026958\n",
            "self.loss(score_1, v1) 0.00826078\n",
            "self.loss(score_2, v2) 0.007793043\n",
            "self.loss(score_3, v3) 0.010548679\n",
            "loss tensor(0.0369, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008420433\n",
            "self.loss(score_1, v1) 0.023884412\n",
            "self.loss(score_2, v2) 0.0115033565\n",
            "self.loss(score_3, v3) 0.0056308685\n",
            "loss tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007561312\n",
            "self.loss(score_1, v1) 0.013359981\n",
            "self.loss(score_2, v2) 0.011988855\n",
            "self.loss(score_3, v3) 0.0086082155\n",
            "loss tensor(0.0415, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01178535\n",
            "self.loss(score_1, v1) 0.0146408435\n",
            "self.loss(score_2, v2) 0.020642888\n",
            "self.loss(score_3, v3) 0.02939624\n",
            "loss tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0064034853\n",
            "self.loss(score_1, v1) 0.009278758\n",
            "self.loss(score_2, v2) 0.012319115\n",
            "self.loss(score_3, v3) 0.005679323\n",
            "loss tensor(0.0337, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0077514425\n",
            "self.loss(score_1, v1) 0.007907472\n",
            "self.loss(score_2, v2) 0.008341062\n",
            "self.loss(score_3, v3) 0.006112597\n",
            "loss tensor(0.0301, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017376322\n",
            "self.loss(score_1, v1) 0.056662414\n",
            "self.loss(score_2, v2) 0.055447534\n",
            "self.loss(score_3, v3) 0.024138587\n",
            "loss tensor(0.1536, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010957298\n",
            "self.loss(score_1, v1) 0.010755566\n",
            "self.loss(score_2, v2) 0.011618511\n",
            "self.loss(score_3, v3) 0.0071118474\n",
            "loss tensor(0.0404, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0073962356\n",
            "self.loss(score_1, v1) 0.009280805\n",
            "self.loss(score_2, v2) 0.0067373817\n",
            "self.loss(score_3, v3) 0.0072555114\n",
            "loss tensor(0.0307, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.015369508\n",
            "self.loss(score_1, v1) 0.013185026\n",
            "self.loss(score_2, v2) 0.017188136\n",
            "self.loss(score_3, v3) 0.009038648\n",
            "loss tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010048607\n",
            "self.loss(score_1, v1) 0.017230459\n",
            "self.loss(score_2, v2) 0.010059239\n",
            "self.loss(score_3, v3) 0.008844548\n",
            "loss tensor(0.0462, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008979712\n",
            "self.loss(score_1, v1) 0.013975627\n",
            "self.loss(score_2, v2) 0.031994514\n",
            "self.loss(score_3, v3) 0.0073632644\n",
            "loss tensor(0.0623, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01907459\n",
            "self.loss(score_1, v1) 0.018181963\n",
            "self.loss(score_2, v2) 0.015702616\n",
            "self.loss(score_3, v3) 0.0062166513\n",
            "loss tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0155973025\n",
            "self.loss(score_1, v1) 0.019646602\n",
            "self.loss(score_2, v2) 0.035794545\n",
            "self.loss(score_3, v3) 0.011627078\n",
            "loss tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.010925353\n",
            "self.loss(score_1, v1) 0.032992978\n",
            "self.loss(score_2, v2) 0.0329954\n",
            "self.loss(score_3, v3) 0.015565098\n",
            "loss tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017244188\n",
            "self.loss(score_1, v1) 0.027520662\n",
            "self.loss(score_2, v2) 0.04826012\n",
            "self.loss(score_3, v3) 0.05238574\n",
            "loss tensor(0.1454, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.03251896\n",
            "self.loss(score_1, v1) 0.033251792\n",
            "self.loss(score_2, v2) 0.06535495\n",
            "self.loss(score_3, v3) 0.011440063\n",
            "loss tensor(0.1426, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.013753035\n",
            "self.loss(score_1, v1) 0.027750334\n",
            "self.loss(score_2, v2) 0.018439123\n",
            "self.loss(score_3, v3) 0.012033598\n",
            "loss tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012990276\n",
            "self.loss(score_1, v1) 0.018299758\n",
            "self.loss(score_2, v2) 0.011134727\n",
            "self.loss(score_3, v3) 0.007080222\n",
            "loss tensor(0.0495, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02201876\n",
            "self.loss(score_1, v1) 0.024646675\n",
            "self.loss(score_2, v2) 0.008410227\n",
            "self.loss(score_3, v3) 0.0067410753\n",
            "loss tensor(0.0618, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0057917833\n",
            "self.loss(score_1, v1) 0.0051868856\n",
            "self.loss(score_2, v2) 0.007377579\n",
            "self.loss(score_3, v3) 0.004776956\n",
            "loss tensor(0.0231, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007610937\n",
            "self.loss(score_1, v1) 0.015163441\n",
            "self.loss(score_2, v2) 0.024596745\n",
            "self.loss(score_3, v3) 0.009447221\n",
            "loss tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.047613233\n",
            "self.loss(score_1, v1) 0.015764099\n",
            "self.loss(score_2, v2) 0.014298955\n",
            "self.loss(score_3, v3) 0.008344083\n",
            "loss tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008483129\n",
            "self.loss(score_1, v1) 0.025403647\n",
            "self.loss(score_2, v2) 0.022557706\n",
            "self.loss(score_3, v3) 0.010095716\n",
            "loss tensor(0.0665, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016814493\n",
            "self.loss(score_1, v1) 0.029119425\n",
            "self.loss(score_2, v2) 0.017617153\n",
            "self.loss(score_3, v3) 0.0077893864\n",
            "loss tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.004013885\n",
            "self.loss(score_1, v1) 0.0104881525\n",
            "self.loss(score_2, v2) 0.014276782\n",
            "self.loss(score_3, v3) 0.008499274\n",
            "loss tensor(0.0373, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018439734\n",
            "self.loss(score_1, v1) 0.05151954\n",
            "self.loss(score_2, v2) 0.025622694\n",
            "self.loss(score_3, v3) 0.0074198246\n",
            "loss tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012773945\n",
            "self.loss(score_1, v1) 0.011552182\n",
            "self.loss(score_2, v2) 0.013012192\n",
            "self.loss(score_3, v3) 0.008632635\n",
            "loss tensor(0.0460, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00974395\n",
            "self.loss(score_1, v1) 0.020520128\n",
            "self.loss(score_2, v2) 0.024176307\n",
            "self.loss(score_3, v3) 0.009056201\n",
            "loss tensor(0.0635, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008897909\n",
            "self.loss(score_1, v1) 0.008810094\n",
            "self.loss(score_2, v2) 0.01419987\n",
            "self.loss(score_3, v3) 0.012004012\n",
            "loss tensor(0.0439, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006774144\n",
            "self.loss(score_1, v1) 0.029895771\n",
            "self.loss(score_2, v2) 0.010716989\n",
            "self.loss(score_3, v3) 0.00585393\n",
            "loss tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018568123\n",
            "self.loss(score_1, v1) 0.07262949\n",
            "self.loss(score_2, v2) 0.041499194\n",
            "self.loss(score_3, v3) 0.010905353\n",
            "loss tensor(0.1436, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008210913\n",
            "self.loss(score_1, v1) 0.011264645\n",
            "self.loss(score_2, v2) 0.015320803\n",
            "self.loss(score_3, v3) 0.0065257996\n",
            "loss tensor(0.0413, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0054945634\n",
            "self.loss(score_1, v1) 0.008116333\n",
            "self.loss(score_2, v2) 0.00874647\n",
            "self.loss(score_3, v3) 0.004828377\n",
            "loss tensor(0.0272, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016151758\n",
            "self.loss(score_1, v1) 0.015469539\n",
            "self.loss(score_2, v2) 0.01271035\n",
            "self.loss(score_3, v3) 0.008003749\n",
            "loss tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.037921034\n",
            "self.loss(score_1, v1) 0.020001806\n",
            "self.loss(score_2, v2) 0.011822565\n",
            "self.loss(score_3, v3) 0.008751999\n",
            "loss tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.039553866\n",
            "self.loss(score_1, v1) 0.023414277\n",
            "self.loss(score_2, v2) 0.021487383\n",
            "self.loss(score_3, v3) 0.010642958\n",
            "loss tensor(0.0951, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011281601\n",
            "self.loss(score_1, v1) 0.009834016\n",
            "self.loss(score_2, v2) 0.013184849\n",
            "self.loss(score_3, v3) 0.011962711\n",
            "loss tensor(0.0463, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012019819\n",
            "self.loss(score_1, v1) 0.022012606\n",
            "self.loss(score_2, v2) 0.03176102\n",
            "self.loss(score_3, v3) 0.0079293735\n",
            "loss tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006400683\n",
            "self.loss(score_1, v1) 0.0059984876\n",
            "self.loss(score_2, v2) 0.0071176374\n",
            "self.loss(score_3, v3) 0.0043456964\n",
            "loss tensor(0.0239, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020407954\n",
            "self.loss(score_1, v1) 0.05272505\n",
            "self.loss(score_2, v2) 0.07550591\n",
            "self.loss(score_3, v3) 0.019883811\n",
            "loss tensor(0.1685, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018014956\n",
            "self.loss(score_1, v1) 0.012379815\n",
            "self.loss(score_2, v2) 0.013247588\n",
            "self.loss(score_3, v3) 0.0069045643\n",
            "loss tensor(0.0505, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011582172\n",
            "self.loss(score_1, v1) 0.011229443\n",
            "self.loss(score_2, v2) 0.008325774\n",
            "self.loss(score_3, v3) 0.010854994\n",
            "loss tensor(0.0420, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.023650538\n",
            "self.loss(score_1, v1) 0.031944565\n",
            "self.loss(score_2, v2) 0.03641116\n",
            "self.loss(score_3, v3) 0.015439296\n",
            "loss tensor(0.1074, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007800555\n",
            "self.loss(score_1, v1) 0.014618468\n",
            "self.loss(score_2, v2) 0.025940742\n",
            "self.loss(score_3, v3) 0.011656195\n",
            "loss tensor(0.0600, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.033791665\n",
            "self.loss(score_1, v1) 0.022570662\n",
            "self.loss(score_2, v2) 0.035838835\n",
            "self.loss(score_3, v3) 0.009419665\n",
            "loss tensor(0.1016, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017294215\n",
            "self.loss(score_1, v1) 0.01292346\n",
            "self.loss(score_2, v2) 0.015299463\n",
            "self.loss(score_3, v3) 0.0076971864\n",
            "loss tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.03930922\n",
            "self.loss(score_1, v1) 0.031068247\n",
            "self.loss(score_2, v2) 0.029465174\n",
            "self.loss(score_3, v3) 0.008964466\n",
            "loss tensor(0.1088, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017989805\n",
            "self.loss(score_1, v1) 0.01843277\n",
            "self.loss(score_2, v2) 0.020936474\n",
            "self.loss(score_3, v3) 0.007191484\n",
            "loss tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0141008515\n",
            "self.loss(score_1, v1) 0.011870526\n",
            "self.loss(score_2, v2) 0.014686976\n",
            "self.loss(score_3, v3) 0.008224102\n",
            "loss tensor(0.0489, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.040142823\n",
            "self.loss(score_1, v1) 0.02393393\n",
            "self.loss(score_2, v2) 0.02619874\n",
            "self.loss(score_3, v3) 0.007285706\n",
            "loss tensor(0.0976, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009075733\n",
            "self.loss(score_1, v1) 0.042234845\n",
            "self.loss(score_2, v2) 0.041140042\n",
            "self.loss(score_3, v3) 0.008878761\n",
            "loss tensor(0.1013, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014130141\n",
            "self.loss(score_1, v1) 0.016225697\n",
            "self.loss(score_2, v2) 0.007101554\n",
            "self.loss(score_3, v3) 0.007410012\n",
            "loss tensor(0.0449, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014202387\n",
            "self.loss(score_1, v1) 0.017168779\n",
            "self.loss(score_2, v2) 0.017130736\n",
            "self.loss(score_3, v3) 0.009099443\n",
            "loss tensor(0.0576, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.038849663\n",
            "self.loss(score_1, v1) 0.015912078\n",
            "self.loss(score_2, v2) 0.022786653\n",
            "self.loss(score_3, v3) 0.011798929\n",
            "loss tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019336153\n",
            "self.loss(score_1, v1) 0.018967135\n",
            "self.loss(score_2, v2) 0.017322645\n",
            "self.loss(score_3, v3) 0.009251829\n",
            "loss tensor(0.0649, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019313572\n",
            "self.loss(score_1, v1) 0.012695876\n",
            "self.loss(score_2, v2) 0.005696266\n",
            "self.loss(score_3, v3) 0.0057037836\n",
            "loss tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008667223\n",
            "self.loss(score_1, v1) 0.015799867\n",
            "self.loss(score_2, v2) 0.017117584\n",
            "self.loss(score_3, v3) 0.0076805414\n",
            "loss tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0072454293\n",
            "self.loss(score_1, v1) 0.008163918\n",
            "self.loss(score_2, v2) 0.009478141\n",
            "self.loss(score_3, v3) 0.0085091125\n",
            "loss tensor(0.0334, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.005023504\n",
            "self.loss(score_1, v1) 0.0065421155\n",
            "self.loss(score_2, v2) 0.009712667\n",
            "self.loss(score_3, v3) 0.007883137\n",
            "loss tensor(0.0292, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.030671349\n",
            "self.loss(score_1, v1) 0.022573048\n",
            "self.loss(score_2, v2) 0.044581145\n",
            "self.loss(score_3, v3) 0.009340874\n",
            "loss tensor(0.1072, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011837371\n",
            "self.loss(score_1, v1) 0.034560367\n",
            "self.loss(score_2, v2) 0.034313116\n",
            "self.loss(score_3, v3) 0.012907135\n",
            "loss tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008893538\n",
            "self.loss(score_1, v1) 0.008214787\n",
            "self.loss(score_2, v2) 0.006361867\n",
            "self.loss(score_3, v3) 0.0065193963\n",
            "loss tensor(0.0300, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009282265\n",
            "self.loss(score_1, v1) 0.020266304\n",
            "self.loss(score_2, v2) 0.022767244\n",
            "self.loss(score_3, v3) 0.007963562\n",
            "loss tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.01032908\n",
            "self.loss(score_1, v1) 0.02075358\n",
            "self.loss(score_2, v2) 0.017335856\n",
            "self.loss(score_3, v3) 0.053879578\n",
            "loss tensor(0.1023, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.005762672\n",
            "self.loss(score_1, v1) 0.010786396\n",
            "self.loss(score_2, v2) 0.018716622\n",
            "self.loss(score_3, v3) 0.009283714\n",
            "loss tensor(0.0445, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0447343\n",
            "self.loss(score_1, v1) 0.017680904\n",
            "self.loss(score_2, v2) 0.013437928\n",
            "self.loss(score_3, v3) 0.00870793\n",
            "loss tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012971055\n",
            "self.loss(score_1, v1) 0.010148302\n",
            "self.loss(score_2, v2) 0.009951547\n",
            "self.loss(score_3, v3) 0.008505964\n",
            "loss tensor(0.0416, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.032179337\n",
            "self.loss(score_1, v1) 0.02783837\n",
            "self.loss(score_2, v2) 0.018598521\n",
            "self.loss(score_3, v3) 0.006802006\n",
            "loss tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009152151\n",
            "self.loss(score_1, v1) 0.009921402\n",
            "self.loss(score_2, v2) 0.004446182\n",
            "self.loss(score_3, v3) 0.0041235867\n",
            "loss tensor(0.0276, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0075808857\n",
            "self.loss(score_1, v1) 0.010933624\n",
            "self.loss(score_2, v2) 0.014523352\n",
            "self.loss(score_3, v3) 0.0077518714\n",
            "loss tensor(0.0408, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006521507\n",
            "self.loss(score_1, v1) 0.010457784\n",
            "self.loss(score_2, v2) 0.009532849\n",
            "self.loss(score_3, v3) 0.0061308183\n",
            "loss tensor(0.0326, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0124719\n",
            "self.loss(score_1, v1) 0.016098278\n",
            "self.loss(score_2, v2) 0.023088286\n",
            "self.loss(score_3, v3) 0.0069907643\n",
            "loss tensor(0.0586, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006967363\n",
            "self.loss(score_1, v1) 0.008195657\n",
            "self.loss(score_2, v2) 0.016643947\n",
            "self.loss(score_3, v3) 0.007962717\n",
            "loss tensor(0.0398, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.032564726\n",
            "self.loss(score_1, v1) 0.022313142\n",
            "self.loss(score_2, v2) 0.017670644\n",
            "self.loss(score_3, v3) 0.0062780227\n",
            "loss tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.012769345\n",
            "self.loss(score_1, v1) 0.014098078\n",
            "self.loss(score_2, v2) 0.009208596\n",
            "self.loss(score_3, v3) 0.0076732743\n",
            "loss tensor(0.0437, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.00936984\n",
            "self.loss(score_1, v1) 0.035168078\n",
            "self.loss(score_2, v2) 0.03149038\n",
            "self.loss(score_3, v3) 0.09972899\n",
            "loss tensor(0.1758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007577674\n",
            "self.loss(score_1, v1) 0.013812706\n",
            "self.loss(score_2, v2) 0.017786\n",
            "self.loss(score_3, v3) 0.00981009\n",
            "loss tensor(0.0490, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.006440482\n",
            "self.loss(score_1, v1) 0.007521743\n",
            "self.loss(score_2, v2) 0.008197084\n",
            "self.loss(score_3, v3) 0.006803771\n",
            "loss tensor(0.0290, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.026595145\n",
            "self.loss(score_1, v1) 0.022030396\n",
            "self.loss(score_2, v2) 0.008550732\n",
            "self.loss(score_3, v3) 0.0069029\n",
            "loss tensor(0.0641, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007884299\n",
            "self.loss(score_1, v1) 0.010380339\n",
            "self.loss(score_2, v2) 0.0145606855\n",
            "self.loss(score_3, v3) 0.00634587\n",
            "loss tensor(0.0392, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0068324865\n",
            "self.loss(score_1, v1) 0.012788049\n",
            "self.loss(score_2, v2) 0.01924013\n",
            "self.loss(score_3, v3) 0.01057031\n",
            "loss tensor(0.0494, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009752423\n",
            "self.loss(score_1, v1) 0.016584339\n",
            "self.loss(score_2, v2) 0.020006003\n",
            "self.loss(score_3, v3) 0.0059296223\n",
            "loss tensor(0.0523, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.005962015\n",
            "self.loss(score_1, v1) 0.010486202\n",
            "self.loss(score_2, v2) 0.012033305\n",
            "self.loss(score_3, v3) 0.006479556\n",
            "loss tensor(0.0350, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.011798768\n",
            "self.loss(score_1, v1) 0.025198195\n",
            "self.loss(score_2, v2) 0.0322447\n",
            "self.loss(score_3, v3) 0.010058074\n",
            "loss tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.019718673\n",
            "self.loss(score_1, v1) 0.01711815\n",
            "self.loss(score_2, v2) 0.015070694\n",
            "self.loss(score_3, v3) 0.011205623\n",
            "loss tensor(0.0631, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.008377341\n",
            "self.loss(score_1, v1) 0.024351979\n",
            "self.loss(score_2, v2) 0.046555754\n",
            "self.loss(score_3, v3) 0.03664377\n",
            "loss tensor(0.1159, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.014777986\n",
            "self.loss(score_1, v1) 0.009542248\n",
            "self.loss(score_2, v2) 0.01372901\n",
            "self.loss(score_3, v3) 0.008608205\n",
            "loss tensor(0.0467, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.025512943\n",
            "self.loss(score_1, v1) 0.02408596\n",
            "self.loss(score_2, v2) 0.009358133\n",
            "self.loss(score_3, v3) 0.005618115\n",
            "loss tensor(0.0646, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.018100614\n",
            "self.loss(score_1, v1) 0.04323727\n",
            "self.loss(score_2, v2) 0.0303363\n",
            "self.loss(score_3, v3) 0.008635785\n",
            "loss tensor(0.1003, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0597497\n",
            "self.loss(score_1, v1) 0.028530054\n",
            "self.loss(score_2, v2) 0.048564255\n",
            "self.loss(score_3, v3) 0.007722663\n",
            "loss tensor(0.1446, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022720387\n",
            "self.loss(score_1, v1) 0.014166786\n",
            "self.loss(score_2, v2) 0.0075665214\n",
            "self.loss(score_3, v3) 0.005614138\n",
            "loss tensor(0.0501, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.02604451\n",
            "self.loss(score_1, v1) 0.01685925\n",
            "self.loss(score_2, v2) 0.021026824\n",
            "self.loss(score_3, v3) 0.008498906\n",
            "loss tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0038379102\n",
            "self.loss(score_1, v1) 0.011657684\n",
            "self.loss(score_2, v2) 0.021126539\n",
            "self.loss(score_3, v3) 0.005110293\n",
            "loss tensor(0.0417, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.020392822\n",
            "self.loss(score_1, v1) 0.01761525\n",
            "self.loss(score_2, v2) 0.015225853\n",
            "self.loss(score_3, v3) 0.007946742\n",
            "loss tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.009089036\n",
            "self.loss(score_1, v1) 0.01162256\n",
            "self.loss(score_2, v2) 0.010334266\n",
            "self.loss(score_3, v3) 0.011134256\n",
            "loss tensor(0.0422, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007338384\n",
            "self.loss(score_1, v1) 0.009751955\n",
            "self.loss(score_2, v2) 0.011375752\n",
            "self.loss(score_3, v3) 0.005586905\n",
            "loss tensor(0.0341, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.017808342\n",
            "self.loss(score_1, v1) 0.02097321\n",
            "self.loss(score_2, v2) 0.018040698\n",
            "self.loss(score_3, v3) 0.0070764013\n",
            "loss tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.022226974\n",
            "self.loss(score_1, v1) 0.038759116\n",
            "self.loss(score_2, v2) 0.03508714\n",
            "self.loss(score_3, v3) 0.007207182\n",
            "loss tensor(0.1033, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0071722283\n",
            "self.loss(score_1, v1) 0.0073131486\n",
            "self.loss(score_2, v2) 0.013409059\n",
            "self.loss(score_3, v3) 0.009863708\n",
            "loss tensor(0.0378, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.016378462\n",
            "self.loss(score_1, v1) 0.015669052\n",
            "self.loss(score_2, v2) 0.0109831635\n",
            "self.loss(score_3, v3) 0.0073533384\n",
            "loss tensor(0.0504, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.007257995\n",
            "self.loss(score_1, v1) 0.0084546795\n",
            "self.loss(score_2, v2) 0.014840674\n",
            "self.loss(score_3, v3) 0.008809886\n",
            "loss tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 0.0121546015\n",
            "self.loss(score_1, v1) 0.04224387\n",
            "self.loss(score_2, v2) 0.05229021\n",
            "self.loss(score_3, v3) 0.009448403\n",
            "loss tensor(0.1161, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.07382771868116347, Train Accuracy : 0.9993278520320691\n",
            " Validation Accuracy : 6.602529713260936\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_0,'-o')\n",
        "plt.plot(loss_1,'-o')\n",
        "plt.plot(loss_2,'-o')\n",
        "plt.plot(loss_3,'-o')\n",
        "plt.xlabel('sample')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['loss 0','loss 1','loss 2','loss 3'])\n",
        "plt.title('loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UPfmfthBjSZw",
        "outputId": "c7bbe166-5bfb-414d-9bbd-82208a3977d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy evalutaion F-scores"
      ],
      "metadata": {
        "id": "sJbWsH72N2Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. create folder with part object of all pieces \n",
        "2. load a piece from dataloader with true labels, the mixed piece and the part object \n",
        "3. create notearray from part object\n",
        "4. take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "\n",
        "Output: pianoroll\n",
        "\n",
        "1 note in notearray could be mulitple bins\n",
        "\n",
        "take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "note start at same time with different pitch -> different notes\n",
        "\n",
        "for each note array find corresponding matrix -> \n",
        "\n",
        "\n",
        "if note is only composed by 1 bin: save indx of vocie -> save it to note array\n",
        "\n",
        "if more than 1: look what are idx that compose this note -> majority note -> save it for the note array (if its 50/50 take it random -> count how often this happens) \n",
        "\n",
        "\n",
        "with idx : in note_array find which note corresponds to what voice"
      ],
      "metadata": {
        "id": "CFClch37N6nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader "
      ],
      "metadata": {
        "id": "afYHFVNMlMnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51b01ff0-b3d1-48bb-88d5-ed27338cf9bf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fb762c46b90>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## makes cell output nothing\n",
        "%%capture  \n",
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch10.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4TAhTQcpmx8m"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dic with key:filename, val: part_obj  for fugues"
      ],
      "metadata": {
        "id": "5RVmMv6Q9CJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if PATH_TO_DATA == \"AI-MA_project/bach_pr_fugues\":\n",
        "    path_parts = \"AI-MA_project/bach_fugues\"\n",
        "    part_dic = {}\n",
        "\n",
        "    #### create a list with all filenames in the right order ####\n",
        "    file_names_part = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.mid'): continue\n",
        "        file_names_part.append(filename[3:7])\n",
        "    #print(file_names_part)\n",
        "\n",
        "    #### create a list with all part objects in the right order ####\n",
        "    part_list = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.mid'): continue\n",
        "        fullname = os.path.join(path_parts, filename)\n",
        "        part = partitura.load_score_midi(fullname)\n",
        "        part_list.append(part)\n",
        "    #print(part_list)\n",
        "\n",
        "    #### create a dict with keys:filenames , values: part object ####\n",
        "    for i in range(len(file_names_part)):\n",
        "        part_dic[file_names_part[i]] = part_list[i]\n",
        "    \n",
        "    print(part_dic.keys(),part_dic.values())"
      ],
      "metadata": {
        "id": "_XYM_KWu2qkX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dic with key:filename, val: part_obj  for chorales"
      ],
      "metadata": {
        "id": "6D9oTp_lNQbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if PATH_TO_DATA == \"AI-MA_project/pianoroll_88\":\n",
        "    path_parts = \"AI-MA_project/chorales_converted\"\n",
        "    part_dic = {}\n",
        "\n",
        "    #### create a list with all filenames in the right order ####\n",
        "    file_names_part = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.xml'): continue\n",
        "        file_names_part.append(filename[4:7])\n",
        "    #print(file_names_part)\n",
        "\n",
        "    #### create a list with all part objects in the right order ####\n",
        "    part_list = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.xml'): continue\n",
        "        fullname = os.path.join(path_parts, filename)\n",
        "        part = partitura.load_musicxml(fullname)\n",
        "        part_list.append(part)\n",
        "    #print(part_list)\n",
        "\n",
        "    #### create a dict with keys:filenames , values: part object ####\n",
        "    for i in range(len(file_names_part)):\n",
        "        part_dic[file_names_part[i]] = part_list[i]\n",
        "    \n",
        "    print(\"part_dic.keys()\",part_dic.keys())\n",
        "    print(\"part_dic.values()\",part_dic.values())"
      ],
      "metadata": {
        "id": "_4q58c16NjbE",
        "outputId": "17cfa3c0-770d-4ca3-9ce0-f16e72a8b10f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmusicxml.py:302: UserWarning: Found repeat without start\n",
            "Starting point 0 is assumend\n",
            "  \"Starting point {} is assumend\".format(start_times[start_time_id]))\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmusicxml.py:302: UserWarning: Found repeat without start\n",
            "Starting point 64 is assumend\n",
            "  \"Starting point {} is assumend\".format(start_times[start_time_id]))\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmusicxml.py:302: UserWarning: Found repeat without start\n",
            "Starting point 128 is assumend\n",
            "  \"Starting point {} is assumend\".format(start_times[start_time_id]))\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmusicxml.py:302: UserWarning: Found repeat without start\n",
            "Starting point 272 is assumend\n",
            "  \"Starting point {} is assumend\".format(start_times[start_time_id]))\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmusicxml.py:302: UserWarning: Found repeat without start\n",
            "Starting point 160 is assumend\n",
            "  \"Starting point {} is assumend\".format(start_times[start_time_id]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part_dic.keys() dict_keys(['001', '002', '003', '004', '005', '006', '007', '008', '009', '010', '011', '012', '013', '014', '015', '016', '017', '018', '019', '020', '021', '022', '023', '024', '025', '026', '027', '028', '029', '030', '031', '032', '033', '034', '035', '036', '037', '038', '039', '040', '041', '042', '043', '044', '045', '046', '047', '048', '049', '050', '051', '052', '053', '054', '055', '056', '057', '058', '059', '060', '061', '062', '063', '064', '065', '066', '067', '068', '069', '070', '071', '072', '073', '074', '075', '076', '077', '078', '079', '080', '081', '082', '083', '084', '085', '086', '087', '088', '089', '090', '091', '092', '093', '094', '095', '096', '097', '098', '099', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371'])\n",
            "part_dic.values() dict_values([[<partitura.score.Part object at 0x7fb751ddd810>, <partitura.score.Part object at 0x7fb751ddd9d0>, <partitura.score.Part object at 0x7fb751ddd8d0>, <partitura.score.Part object at 0x7fb751ddde90>], [<partitura.score.Part object at 0x7fb751cf68d0>, <partitura.score.Part object at 0x7fb751e10a10>, <partitura.score.Part object at 0x7fb751c52150>, <partitura.score.Part object at 0x7fb751c521d0>], [<partitura.score.Part object at 0x7fb751b5ff90>, <partitura.score.Part object at 0x7fb751b36d10>, <partitura.score.Part object at 0x7fb751b36d90>, <partitura.score.Part object at 0x7fb751b36e10>], [<partitura.score.Part object at 0x7fb751a2bb90>, <partitura.score.Part object at 0x7fb7519fd210>, <partitura.score.Part object at 0x7fb7519fd290>, <partitura.score.Part object at 0x7fb7519fd310>], [<partitura.score.Part object at 0x7fb7518dda50>, <partitura.score.Part object at 0x7fb7518a3c90>, <partitura.score.Part object at 0x7fb7518a3d10>, <partitura.score.Part object at 0x7fb7518a3d90>], [<partitura.score.Part object at 0x7fb751eb7590>, <partitura.score.Part object at 0x7fb762bdd410>, <partitura.score.Part object at 0x7fb83497d2d0>, <partitura.score.Part object at 0x7fb751ddfb90>], [<partitura.score.Part object at 0x7fb762be9e10>, <partitura.score.Part object at 0x7fb762c54fd0>, <partitura.score.Part object at 0x7fb762c54290>, <partitura.score.Part object at 0x7fb762c54350>], [<partitura.score.Part object at 0x7fb7632413d0>, <partitura.score.Part object at 0x7fb7514eca50>, <partitura.score.Part object at 0x7fb7514ecad0>, <partitura.score.Part object at 0x7fb7514ecb50>], [<partitura.score.Part object at 0x7fb7513176d0>, <partitura.score.Part object at 0x7fb751290cd0>, <partitura.score.Part object at 0x7fb751290d50>, <partitura.score.Part object at 0x7fb751290dd0>], [<partitura.score.Part object at 0x7fb7514ecb10>, <partitura.score.Part object at 0x7fb75112b210>, <partitura.score.Part object at 0x7fb75112b4d0>, <partitura.score.Part object at 0x7fb75112b550>], [<partitura.score.Part object at 0x7fb75100a950>, <partitura.score.Part object at 0x7fb750fd0bd0>, <partitura.score.Part object at 0x7fb750fd0c50>, <partitura.score.Part object at 0x7fb750fd0cd0>], [<partitura.score.Part object at 0x7fb750dc75d0>, <partitura.score.Part object at 0x7fb750d58a50>, <partitura.score.Part object at 0x7fb750d58ad0>, <partitura.score.Part object at 0x7fb750d58b50>], [<partitura.score.Part object at 0x7fb750cb6710>, <partitura.score.Part object at 0x7fb750c66f50>, <partitura.score.Part object at 0x7fb750c66fd0>, <partitura.score.Part object at 0x7fb750c0b090>], [<partitura.score.Part object at 0x7fb750ac1b90>, <partitura.score.Part object at 0x7fb750ab4d10>, <partitura.score.Part object at 0x7fb750ab4d90>, <partitura.score.Part object at 0x7fb750ab4e10>], [<partitura.score.Part object at 0x7fb7630ebfd0>, <partitura.score.Part object at 0x7fb75098f190>, <partitura.score.Part object at 0x7fb75093a110>, <partitura.score.Part object at 0x7fb75093a390>], [<partitura.score.Part object at 0x7fb750746650>, <partitura.score.Part object at 0x7fb750746690>, <partitura.score.Part object at 0x7fb7507466d0>, <partitura.score.Part object at 0x7fb750746790>], [<partitura.score.Part object at 0x7fb750618e90>, <partitura.score.Part object at 0x7fb750591e50>, <partitura.score.Part object at 0x7fb750591e90>, <partitura.score.Part object at 0x7fb750591f10>], [<partitura.score.Part object at 0x7fb7504852d0>, <partitura.score.Part object at 0x7fb75044a110>, <partitura.score.Part object at 0x7fb75044a190>, <partitura.score.Part object at 0x7fb75044a210>], [<partitura.score.Part object at 0x7fb7503ad4d0>, <partitura.score.Part object at 0x7fb7503789d0>, <partitura.score.Part object at 0x7fb750378a50>, <partitura.score.Part object at 0x7fb750378ad0>], [<partitura.score.Part object at 0x7fb750261290>, <partitura.score.Part object at 0x7fb750219c50>, <partitura.score.Part object at 0x7fb750219cd0>, <partitura.score.Part object at 0x7fb750219d50>], [<partitura.score.Part object at 0x7fb750136190>, <partitura.score.Part object at 0x7fb750086dd0>, <partitura.score.Part object at 0x7fb750086e50>, <partitura.score.Part object at 0x7fb750086ed0>], [<partitura.score.Part object at 0x7fb64fff6990>, <partitura.score.Part object at 0x7fb64ffbe990>, <partitura.score.Part object at 0x7fb64ffbea10>, <partitura.score.Part object at 0x7fb64ffbea90>], [<partitura.score.Part object at 0x7fb64fe61bd0>, <partitura.score.Part object at 0x7fb64fdd0190>, <partitura.score.Part object at 0x7fb64fdd0210>, <partitura.score.Part object at 0x7fb64fdd0290>], [<partitura.score.Part object at 0x7fb64fcce890>, <partitura.score.Part object at 0x7fb64fc98c50>, <partitura.score.Part object at 0x7fb64fc98cd0>, <partitura.score.Part object at 0x7fb64fc98d50>], [<partitura.score.Part object at 0x7fb64fbb1250>, <partitura.score.Part object at 0x7fb64fb79750>, <partitura.score.Part object at 0x7fb64fb797d0>, <partitura.score.Part object at 0x7fb64fb79850>], [<partitura.score.Part object at 0x7fb64fa775d0>, <partitura.score.Part object at 0x7fb64f9c0a50>, <partitura.score.Part object at 0x7fb64f9c0ad0>, <partitura.score.Part object at 0x7fb64f9c0b50>], [<partitura.score.Part object at 0x7fb64f92c1d0>, <partitura.score.Part object at 0x7fb64f8f1210>, <partitura.score.Part object at 0x7fb64f8f1290>, <partitura.score.Part object at 0x7fb64f8f1310>], [<partitura.score.Part object at 0x7fb64f7d3b50>, <partitura.score.Part object at 0x7fb64f795f90>, <partitura.score.Part object at 0x7fb64f742050>, <partitura.score.Part object at 0x7fb64f7420d0>], [<partitura.score.Part object at 0x7fb64f6d6790>, <partitura.score.Part object at 0x7fb64f689950>, <partitura.score.Part object at 0x7fb64f689e10>, <partitura.score.Part object at 0x7fb64f689e90>], [<partitura.score.Part object at 0x7fb64f590110>, <partitura.score.Part object at 0x7fb64f55b7d0>, <partitura.score.Part object at 0x7fb64f55b850>, <partitura.score.Part object at 0x7fb64f55b8d0>], [<partitura.score.Part object at 0x7fb64f45fb90>, <partitura.score.Part object at 0x7fb64f436090>, <partitura.score.Part object at 0x7fb64f436110>, <partitura.score.Part object at 0x7fb64f436190>], [<partitura.score.Part object at 0x7fb64f32d710>, <partitura.score.Part object at 0x7fb64f2ff090>, <partitura.score.Part object at 0x7fb64f2ff110>, <partitura.score.Part object at 0x7fb64f2ff190>], [<partitura.score.Part object at 0x7fb64f1feb50>, <partitura.score.Part object at 0x7fb64f144e50>, <partitura.score.Part object at 0x7fb64f144ed0>, <partitura.score.Part object at 0x7fb64f144f50>], [<partitura.score.Part object at 0x7fb64f0b2810>, <partitura.score.Part object at 0x7fb64f07b310>, <partitura.score.Part object at 0x7fb64f07b390>, <partitura.score.Part object at 0x7fb64f07b410>], [<partitura.score.Part object at 0x7fb64eed0dd0>, <partitura.score.Part object at 0x7fb64eeb5a10>, <partitura.score.Part object at 0x7fb64eeb5a90>, <partitura.score.Part object at 0x7fb64eeb5b10>], [<partitura.score.Part object at 0x7fb64ed975d0>, <partitura.score.Part object at 0x7fb64ed4ea50>, <partitura.score.Part object at 0x7fb64ed5a150>, <partitura.score.Part object at 0x7fb64ed5a1d0>], [<partitura.score.Part object at 0x7fb64ec675d0>, <partitura.score.Part object at 0x7fb64ebc2250>, <partitura.score.Part object at 0x7fb64ebc22d0>, <partitura.score.Part object at 0x7fb64ebc2350>], [<partitura.score.Part object at 0x7fb64eadfb10>, <partitura.score.Part object at 0x7fb64eabf5d0>, <partitura.score.Part object at 0x7fb64eabf650>, <partitura.score.Part object at 0x7fb64eabf6d0>], [<partitura.score.Part object at 0x7fb64e9a7d50>, <partitura.score.Part object at 0x7fb64e967550>, <partitura.score.Part object at 0x7fb64e9675d0>, <partitura.score.Part object at 0x7fb64e967650>], [<partitura.score.Part object at 0x7fb64e87bad0>, <partitura.score.Part object at 0x7fb64e7c2d90>, <partitura.score.Part object at 0x7fb64e7c2e10>, <partitura.score.Part object at 0x7fb64e7c2e90>], [<partitura.score.Part object at 0x7fb64e775e90>, <partitura.score.Part object at 0x7fb64e730690>, <partitura.score.Part object at 0x7fb64e730710>, <partitura.score.Part object at 0x7fb64e730790>], [<partitura.score.Part object at 0x7fb64e5d9d90>, <partitura.score.Part object at 0x7fb64e5b5a50>, <partitura.score.Part object at 0x7fb64e5b5ad0>, <partitura.score.Part object at 0x7fb64e5b5b50>], [<partitura.score.Part object at 0x7fb64e4ec590>, <partitura.score.Part object at 0x7fb64e49bc10>, <partitura.score.Part object at 0x7fb64e49bc90>, <partitura.score.Part object at 0x7fb64e49bd10>], [<partitura.score.Part object at 0x7fb64e3adfd0>, <partitura.score.Part object at 0x7fb64e31a910>, <partitura.score.Part object at 0x7fb64e31a990>, <partitura.score.Part object at 0x7fb64e31aa10>], [<partitura.score.Part object at 0x7fb64e2739d0>, <partitura.score.Part object at 0x7fb64e233150>, <partitura.score.Part object at 0x7fb64e2331d0>, <partitura.score.Part object at 0x7fb64e233250>], [<partitura.score.Part object at 0x7fb64e0fb110>, <partitura.score.Part object at 0x7fb64e05a590>, <partitura.score.Part object at 0x7fb64e05a610>, <partitura.score.Part object at 0x7fb64e05a690>], [<partitura.score.Part object at 0x7fb64e233210>, <partitura.score.Part object at 0x7fb64df08110>, <partitura.score.Part object at 0x7fb64df08150>, <partitura.score.Part object at 0x7fb64df081d0>], [<partitura.score.Part object at 0x7fb64de790d0>, <partitura.score.Part object at 0x7fb64ddc4ad0>, <partitura.score.Part object at 0x7fb64ddc4b50>, <partitura.score.Part object at 0x7fb64ddc4bd0>], [<partitura.score.Part object at 0x7fb64dd31550>, <partitura.score.Part object at 0x7fb64dcf4390>, <partitura.score.Part object at 0x7fb64dcf4410>, <partitura.score.Part object at 0x7fb64dcf4490>], [<partitura.score.Part object at 0x7fb64db869d0>, <partitura.score.Part object at 0x7fb64db53fd0>, <partitura.score.Part object at 0x7fb64db02090>, <partitura.score.Part object at 0x7fb64db02110>], [<partitura.score.Part object at 0x7fb64da51dd0>, <partitura.score.Part object at 0x7fb64da26a10>, <partitura.score.Part object at 0x7fb64da26a90>, <partitura.score.Part object at 0x7fb64da26b10>], [<partitura.score.Part object at 0x7fb64d91ae90>, <partitura.score.Part object at 0x7fb64d8e0fd0>, <partitura.score.Part object at 0x7fb64d88b090>, <partitura.score.Part object at 0x7fb64d88b110>], [<partitura.score.Part object at 0x7fb64d7a6dd0>, <partitura.score.Part object at 0x7fb64d714750>, <partitura.score.Part object at 0x7fb64d7147d0>, <partitura.score.Part object at 0x7fb64d714850>], [<partitura.score.Part object at 0x7fb64d667750>, <partitura.score.Part object at 0x7fb64d627490>, <partitura.score.Part object at 0x7fb64d627510>, <partitura.score.Part object at 0x7fb64d627590>], [<partitura.score.Part object at 0x7fb64d568ad0>, <partitura.score.Part object at 0x7fb64d529cd0>, <partitura.score.Part object at 0x7fb64d529d50>, <partitura.score.Part object at 0x7fb64d529dd0>], [<partitura.score.Part object at 0x7fb64d415b10>, <partitura.score.Part object at 0x7fb64d3dc990>, <partitura.score.Part object at 0x7fb64d3dca10>, <partitura.score.Part object at 0x7fb64d3dca90>], [<partitura.score.Part object at 0x7fb64d2a0110>, <partitura.score.Part object at 0x7fb64d2014d0>, <partitura.score.Part object at 0x7fb64d201510>, <partitura.score.Part object at 0x7fb64d201590>], [<partitura.score.Part object at 0x7fb64d1bd4d0>, <partitura.score.Part object at 0x7fb64d16d950>, <partitura.score.Part object at 0x7fb64d16d9d0>, <partitura.score.Part object at 0x7fb64d16da50>], [<partitura.score.Part object at 0x7fb64cffdc90>, <partitura.score.Part object at 0x7fb64cf0de10>, <partitura.score.Part object at 0x7fb64cf0de90>, <partitura.score.Part object at 0x7fb64cf0df10>], [<partitura.score.Part object at 0x7fb64ce10790>, <partitura.score.Part object at 0x7fb64cdd98d0>, <partitura.score.Part object at 0x7fb64cdd9950>, <partitura.score.Part object at 0x7fb64cdd99d0>], [<partitura.score.Part object at 0x7fb64cd32bd0>, <partitura.score.Part object at 0x7fb64cc86190>, <partitura.score.Part object at 0x7fb64cc86210>, <partitura.score.Part object at 0x7fb64cc86290>], [<partitura.score.Part object at 0x7fb64cb41f10>, <partitura.score.Part object at 0x7fb64cb320d0>, <partitura.score.Part object at 0x7fb64cb32150>, <partitura.score.Part object at 0x7fb64cb321d0>], [<partitura.score.Part object at 0x7fb64ca72e50>, <partitura.score.Part object at 0x7fb64ca358d0>, <partitura.score.Part object at 0x7fb64ca35950>, <partitura.score.Part object at 0x7fb64ca359d0>], [<partitura.score.Part object at 0x7fb64c918c90>, <partitura.score.Part object at 0x7fb64c8ed6d0>, <partitura.score.Part object at 0x7fb64c8ed750>, <partitura.score.Part object at 0x7fb64c8ed7d0>], [<partitura.score.Part object at 0x7fb64c78c750>, <partitura.score.Part object at 0x7fb64c757d10>, <partitura.score.Part object at 0x7fb64c757d90>, <partitura.score.Part object at 0x7fb64c757e10>], [<partitura.score.Part object at 0x7fb64c67c090>, <partitura.score.Part object at 0x7fb64c612fd0>, <partitura.score.Part object at 0x7fb64c63e090>, <partitura.score.Part object at 0x7fb64c63e110>], [<partitura.score.Part object at 0x7fb64c4dc610>, <partitura.score.Part object at 0x7fb64c4bdf10>, <partitura.score.Part object at 0x7fb64c4bdf90>, <partitura.score.Part object at 0x7fb64c47a050>], [<partitura.score.Part object at 0x7fb64c35a490>, <partitura.score.Part object at 0x7fb64c328c50>, <partitura.score.Part object at 0x7fb64c328cd0>, <partitura.score.Part object at 0x7fb64c328d50>], [<partitura.score.Part object at 0x7fb64c211590>, <partitura.score.Part object at 0x7fb64c1cefd0>, <partitura.score.Part object at 0x7fb64c1f6090>, <partitura.score.Part object at 0x7fb64c1f6110>], [<partitura.score.Part object at 0x7fb64c03b490>, <partitura.score.Part object at 0x7fb64bf5a6d0>, <partitura.score.Part object at 0x7fb64bf5a750>, <partitura.score.Part object at 0x7fb64bf5a7d0>], [<partitura.score.Part object at 0x7fb64bd95950>, <partitura.score.Part object at 0x7fb64bd19cd0>, <partitura.score.Part object at 0x7fb64bd19d50>, <partitura.score.Part object at 0x7fb64bd19dd0>], [<partitura.score.Part object at 0x7fb64bbe2510>, <partitura.score.Part object at 0x7fb64bb43ed0>, <partitura.score.Part object at 0x7fb64bb43f50>, <partitura.score.Part object at 0x7fb64bb43fd0>], [<partitura.score.Part object at 0x7fb64baa70d0>, <partitura.score.Part object at 0x7fb64ba5bc10>, <partitura.score.Part object at 0x7fb64ba5bc90>, <partitura.score.Part object at 0x7fb64ba5bd10>], [<partitura.score.Part object at 0x7fb64b956b90>, <partitura.score.Part object at 0x7fb64b925f10>, <partitura.score.Part object at 0x7fb64b925f90>, <partitura.score.Part object at 0x7fb64b8d2050>], [<partitura.score.Part object at 0x7fb751790850>, <partitura.score.Part object at 0x7fb64b7e5310>, <partitura.score.Part object at 0x7fb64b7e5390>, <partitura.score.Part object at 0x7fb64b7e5410>], [<partitura.score.Part object at 0x7fb64b6d0190>, <partitura.score.Part object at 0x7fb64b689f90>, <partitura.score.Part object at 0x7fb64b6b2050>, <partitura.score.Part object at 0x7fb64b6b20d0>], [<partitura.score.Part object at 0x7fb64b5a7910>, <partitura.score.Part object at 0x7fb64b508250>, <partitura.score.Part object at 0x7fb64b5082d0>, <partitura.score.Part object at 0x7fb64b508350>], [<partitura.score.Part object at 0x7fb64b41f890>, <partitura.score.Part object at 0x7fb64b3fa290>, <partitura.score.Part object at 0x7fb64b3fa310>, <partitura.score.Part object at 0x7fb64b3fa390>], [<partitura.score.Part object at 0x7fb64b2ddf10>, <partitura.score.Part object at 0x7fb64b2ae090>, <partitura.score.Part object at 0x7fb64b2ae110>, <partitura.score.Part object at 0x7fb64b2ae190>], [<partitura.score.Part object at 0x7fb64b1b9310>, <partitura.score.Part object at 0x7fb64b112210>, <partitura.score.Part object at 0x7fb64b112290>, <partitura.score.Part object at 0x7fb64b112310>], [<partitura.score.Part object at 0x7fb64b0066d0>, <partitura.score.Part object at 0x7fb64afd08d0>, <partitura.score.Part object at 0x7fb64afd0950>, <partitura.score.Part object at 0x7fb64afd09d0>], [<partitura.score.Part object at 0x7fb64aef1690>, <partitura.score.Part object at 0x7fb64ae4aa50>, <partitura.score.Part object at 0x7fb64ae4aa90>, <partitura.score.Part object at 0x7fb64ae4ab10>], [<partitura.score.Part object at 0x7fb64ad79d50>, <partitura.score.Part object at 0x7fb64ace5610>, <partitura.score.Part object at 0x7fb64ace5690>, <partitura.score.Part object at 0x7fb64ace5710>], [<partitura.score.Part object at 0x7fb64ab92450>, <partitura.score.Part object at 0x7fb64ab6ebd0>, <partitura.score.Part object at 0x7fb64ab6ec50>, <partitura.score.Part object at 0x7fb64ab6ecd0>], [<partitura.score.Part object at 0x7fb64a9c3650>, <partitura.score.Part object at 0x7fb64a9a4910>, <partitura.score.Part object at 0x7fb64a9a4990>, <partitura.score.Part object at 0x7fb64a9a4a10>], [<partitura.score.Part object at 0x7fb64a852a50>, <partitura.score.Part object at 0x7fb64a82fdd0>, <partitura.score.Part object at 0x7fb64a82fe50>, <partitura.score.Part object at 0x7fb64a82fed0>], [<partitura.score.Part object at 0x7fb64a6e18d0>, <partitura.score.Part object at 0x7fb64a6bd690>, <partitura.score.Part object at 0x7fb64a6bd710>, <partitura.score.Part object at 0x7fb64a6bd790>], [<partitura.score.Part object at 0x7fb64a517f50>, <partitura.score.Part object at 0x7fb64a5231d0>, <partitura.score.Part object at 0x7fb64a5230d0>, <partitura.score.Part object at 0x7fb64a523910>], [<partitura.score.Part object at 0x7fb64a42c1d0>, <partitura.score.Part object at 0x7fb64a3f3590>, <partitura.score.Part object at 0x7fb64a3f3610>, <partitura.score.Part object at 0x7fb64a3f3690>], [<partitura.score.Part object at 0x7fb64a2e2b50>, <partitura.score.Part object at 0x7fb64a2b9290>, <partitura.score.Part object at 0x7fb64a2b92d0>, <partitura.score.Part object at 0x7fb64a2b9350>], [<partitura.score.Part object at 0x7fb64a1f5c90>, <partitura.score.Part object at 0x7fb64a1b5e50>, <partitura.score.Part object at 0x7fb64a1b5ed0>, <partitura.score.Part object at 0x7fb64a1b5f50>], [<partitura.score.Part object at 0x7fb649faf510>, <partitura.score.Part object at 0x7fb649ed3f50>, <partitura.score.Part object at 0x7fb649ed3fd0>, <partitura.score.Part object at 0x7fb649ead090>], [<partitura.score.Part object at 0x7fb649d81dd0>, <partitura.score.Part object at 0x7fb649d57090>, <partitura.score.Part object at 0x7fb649d57110>, <partitura.score.Part object at 0x7fb649d57190>], [<partitura.score.Part object at 0x7fb649c89e10>, <partitura.score.Part object at 0x7fb649c4a890>, <partitura.score.Part object at 0x7fb649c4a910>, <partitura.score.Part object at 0x7fb649c4a990>], [<partitura.score.Part object at 0x7fb649b9c910>, <partitura.score.Part object at 0x7fb649b5fb10>, <partitura.score.Part object at 0x7fb649b5fb90>, <partitura.score.Part object at 0x7fb649b5fc10>], [<partitura.score.Part object at 0x7fb649a5b790>, <partitura.score.Part object at 0x7fb649a2ae10>, <partitura.score.Part object at 0x7fb649a2ae90>, <partitura.score.Part object at 0x7fb649a2af10>], [<partitura.score.Part object at 0x7fb6498c1890>, <partitura.score.Part object at 0x7fb64988cb50>, <partitura.score.Part object at 0x7fb64988cbd0>, <partitura.score.Part object at 0x7fb64988cc50>], [<partitura.score.Part object at 0x7fb6497ab810>, <partitura.score.Part object at 0x7fb649707b10>, <partitura.score.Part object at 0x7fb649707b90>, <partitura.score.Part object at 0x7fb649707c10>], [<partitura.score.Part object at 0x7fb649600ed0>, <partitura.score.Part object at 0x7fb6495d4110>, <partitura.score.Part object at 0x7fb6495d4190>, <partitura.score.Part object at 0x7fb6495d4210>], [<partitura.score.Part object at 0x7fb6494c04d0>, <partitura.score.Part object at 0x7fb64948a7d0>, <partitura.score.Part object at 0x7fb64948a850>, <partitura.score.Part object at 0x7fb64948a8d0>], [<partitura.score.Part object at 0x7fb64939dcd0>, <partitura.score.Part object at 0x7fb64937cc50>, <partitura.score.Part object at 0x7fb64937ccd0>, <partitura.score.Part object at 0x7fb64937cd50>], [<partitura.score.Part object at 0x7fb649264e90>, <partitura.score.Part object at 0x7fb649224ad0>, <partitura.score.Part object at 0x7fb649224b50>, <partitura.score.Part object at 0x7fb649224bd0>], [<partitura.score.Part object at 0x7fb64910dd90>, <partitura.score.Part object at 0x7fb6490dd9d0>, <partitura.score.Part object at 0x7fb6490dda50>, <partitura.score.Part object at 0x7fb6490ddad0>], [<partitura.score.Part object at 0x7fb648fdd110>, <partitura.score.Part object at 0x7fb648fa7710>, <partitura.score.Part object at 0x7fb648fa7790>, <partitura.score.Part object at 0x7fb648fa7810>], [<partitura.score.Part object at 0x7fb648ef1e90>, <partitura.score.Part object at 0x7fb648eb6a50>, <partitura.score.Part object at 0x7fb648eb6ad0>, <partitura.score.Part object at 0x7fb648eb6b50>], [<partitura.score.Part object at 0x7fb648d950d0>, <partitura.score.Part object at 0x7fb648d5b210>, <partitura.score.Part object at 0x7fb648d5b290>, <partitura.score.Part object at 0x7fb648d5b310>], [<partitura.score.Part object at 0x7fb648c58f50>, <partitura.score.Part object at 0x7fb648c3b310>, <partitura.score.Part object at 0x7fb648c3b390>, <partitura.score.Part object at 0x7fb648c3b410>], [<partitura.score.Part object at 0x7fb648a628d0>, <partitura.score.Part object at 0x7fb6489efad0>, <partitura.score.Part object at 0x7fb6489efb50>, <partitura.score.Part object at 0x7fb6489efbd0>], [<partitura.score.Part object at 0x7fb6488e5750>, <partitura.score.Part object at 0x7fb6488a9b50>, <partitura.score.Part object at 0x7fb6488a9bd0>, <partitura.score.Part object at 0x7fb6488a9c50>], [<partitura.score.Part object at 0x7fb64876a750>, <partitura.score.Part object at 0x7fb6486d86d0>, <partitura.score.Part object at 0x7fb6486d8750>, <partitura.score.Part object at 0x7fb6486d87d0>], [<partitura.score.Part object at 0x7fb6485e3dd0>, <partitura.score.Part object at 0x7fb6485beb10>, <partitura.score.Part object at 0x7fb6485beb90>, <partitura.score.Part object at 0x7fb6485bec10>], [<partitura.score.Part object at 0x7fb648446bd0>, <partitura.score.Part object at 0x7fb648419250>, <partitura.score.Part object at 0x7fb6484192d0>, <partitura.score.Part object at 0x7fb648419350>], [<partitura.score.Part object at 0x7fb648354ad0>, <partitura.score.Part object at 0x7fb648312450>, <partitura.score.Part object at 0x7fb6483124d0>, <partitura.score.Part object at 0x7fb648312550>], [<partitura.score.Part object at 0x7fb648224dd0>, <partitura.score.Part object at 0x7fb6481864d0>, <partitura.score.Part object at 0x7fb648186510>, <partitura.score.Part object at 0x7fb648186590>], [<partitura.score.Part object at 0x7fb64808f590>, <partitura.score.Part object at 0x7fb648060790>, <partitura.score.Part object at 0x7fb648060810>, <partitura.score.Part object at 0x7fb648060890>], [<partitura.score.Part object at 0x7fb647f76ad0>, <partitura.score.Part object at 0x7fb647ed0510>, <partitura.score.Part object at 0x7fb647ed0590>, <partitura.score.Part object at 0x7fb647ed0610>], [<partitura.score.Part object at 0x7fb647cd1450>, <partitura.score.Part object at 0x7fb647c69b10>, <partitura.score.Part object at 0x7fb647c69b90>, <partitura.score.Part object at 0x7fb647c69c10>], [<partitura.score.Part object at 0x7fb647b14350>, <partitura.score.Part object at 0x7fb647ade850>, <partitura.score.Part object at 0x7fb647ade8d0>, <partitura.score.Part object at 0x7fb647ade950>], [<partitura.score.Part object at 0x7fb6479ccd50>, <partitura.score.Part object at 0x7fb64799f750>, <partitura.score.Part object at 0x7fb64799f7d0>, <partitura.score.Part object at 0x7fb64799f850>], [<partitura.score.Part object at 0x7fb64782ff50>, <partitura.score.Part object at 0x7fb6477b3b50>, <partitura.score.Part object at 0x7fb6477b3bd0>, <partitura.score.Part object at 0x7fb6477b3c50>], [<partitura.score.Part object at 0x7fb647674ad0>, <partitura.score.Part object at 0x7fb6475d4710>, <partitura.score.Part object at 0x7fb6475d4790>, <partitura.score.Part object at 0x7fb6475d4810>], [<partitura.score.Part object at 0x7fb6474b4390>, <partitura.score.Part object at 0x7fb647427350>, <partitura.score.Part object at 0x7fb6474273d0>, <partitura.score.Part object at 0x7fb647427450>], [<partitura.score.Part object at 0x7fb6472eced0>, <partitura.score.Part object at 0x7fb64725eb10>, <partitura.score.Part object at 0x7fb64725eb90>, <partitura.score.Part object at 0x7fb64725ec10>], [<partitura.score.Part object at 0x7fb647113190>, <partitura.score.Part object at 0x7fb6470f00d0>, <partitura.score.Part object at 0x7fb6470f0150>, <partitura.score.Part object at 0x7fb6470f01d0>], [<partitura.score.Part object at 0x7fb646f9c790>, <partitura.score.Part object at 0x7fb646f71b50>, <partitura.score.Part object at 0x7fb646f71bd0>, <partitura.score.Part object at 0x7fb646f71c50>], [<partitura.score.Part object at 0x7fb646e61dd0>, <partitura.score.Part object at 0x7fb646e34350>, <partitura.score.Part object at 0x7fb646e343d0>, <partitura.score.Part object at 0x7fb646e34450>], [<partitura.score.Part object at 0x7fb646ccb750>, <partitura.score.Part object at 0x7fb646ca96d0>, <partitura.score.Part object at 0x7fb646ca9750>, <partitura.score.Part object at 0x7fb646ca97d0>], [<partitura.score.Part object at 0x7fb646b90e10>, <partitura.score.Part object at 0x7fb646b51b50>, <partitura.score.Part object at 0x7fb646b51bd0>, <partitura.score.Part object at 0x7fb646b51c50>], [<partitura.score.Part object at 0x7fb646a5ddd0>, <partitura.score.Part object at 0x7fb646a397d0>, <partitura.score.Part object at 0x7fb646a39850>, <partitura.score.Part object at 0x7fb646a398d0>], [<partitura.score.Part object at 0x7fb646930890>, <partitura.score.Part object at 0x7fb6468f4b90>, <partitura.score.Part object at 0x7fb6468f4c10>, <partitura.score.Part object at 0x7fb6468f4c90>], [<partitura.score.Part object at 0x7fb64686da90>, <partitura.score.Part object at 0x7fb646811290>, <partitura.score.Part object at 0x7fb646811310>, <partitura.score.Part object at 0x7fb646811390>], [<partitura.score.Part object at 0x7fb64674b850>, <partitura.score.Part object at 0x7fb646701e90>, <partitura.score.Part object at 0x7fb646701f10>, <partitura.score.Part object at 0x7fb646701f90>], [<partitura.score.Part object at 0x7fb646395910>, <partitura.score.Part object at 0x7fb6462ae150>, <partitura.score.Part object at 0x7fb6462ae1d0>, <partitura.score.Part object at 0x7fb6462ae250>], [<partitura.score.Part object at 0x7fb645e81d50>, <partitura.score.Part object at 0x7fb645e81f10>, <partitura.score.Part object at 0x7fb645e81f90>, <partitura.score.Part object at 0x7fb645e57050>], [<partitura.score.Part object at 0x7fb645d4b590>, <partitura.score.Part object at 0x7fb645d1db90>, <partitura.score.Part object at 0x7fb645d1dc10>, <partitura.score.Part object at 0x7fb645d1dc90>], [<partitura.score.Part object at 0x7fb645bf9990>, <partitura.score.Part object at 0x7fb645b711d0>, <partitura.score.Part object at 0x7fb645b71250>, <partitura.score.Part object at 0x7fb645b712d0>], [<partitura.score.Part object at 0x7fb645abaf50>, <partitura.score.Part object at 0x7fb645a766d0>, <partitura.score.Part object at 0x7fb645a76750>, <partitura.score.Part object at 0x7fb645a767d0>], [<partitura.score.Part object at 0x7fb6458fcbd0>, <partitura.score.Part object at 0x7fb64587f910>, <partitura.score.Part object at 0x7fb64587f990>, <partitura.score.Part object at 0x7fb64587fa10>], [<partitura.score.Part object at 0x7fb64577d990>, <partitura.score.Part object at 0x7fb6456c7750>, <partitura.score.Part object at 0x7fb6456c77d0>, <partitura.score.Part object at 0x7fb6456c7850>], [<partitura.score.Part object at 0x7fb64563b3d0>, <partitura.score.Part object at 0x7fb645585b50>, <partitura.score.Part object at 0x7fb645585bd0>, <partitura.score.Part object at 0x7fb645585c50>], [<partitura.score.Part object at 0x7fb645488a90>, <partitura.score.Part object at 0x7fb64544ea50>, <partitura.score.Part object at 0x7fb64544ead0>, <partitura.score.Part object at 0x7fb64544eb50>], [<partitura.score.Part object at 0x7fb6453a2f50>, <partitura.score.Part object at 0x7fb645369bd0>, <partitura.score.Part object at 0x7fb645369c50>, <partitura.score.Part object at 0x7fb645369cd0>], [<partitura.score.Part object at 0x7fb645204b10>, <partitura.score.Part object at 0x7fb6451e1b90>, <partitura.score.Part object at 0x7fb6451e1c10>, <partitura.score.Part object at 0x7fb6451e1c90>], [<partitura.score.Part object at 0x7fb645066590>, <partitura.score.Part object at 0x7fb644fd8490>, <partitura.score.Part object at 0x7fb644fd84d0>, <partitura.score.Part object at 0x7fb644fd8550>], [<partitura.score.Part object at 0x7fb645fb7750>, <partitura.score.Part object at 0x7fb644fb43d0>, <partitura.score.Part object at 0x7fb644fb44d0>, <partitura.score.Part object at 0x7fb644ef0bd0>], [<partitura.score.Part object at 0x7fb644d45790>, <partitura.score.Part object at 0x7fb644d09ad0>, <partitura.score.Part object at 0x7fb644d09b50>, <partitura.score.Part object at 0x7fb644d09bd0>], [<partitura.score.Part object at 0x7fb644c64890>, <partitura.score.Part object at 0x7fb644c27650>, <partitura.score.Part object at 0x7fb644c276d0>, <partitura.score.Part object at 0x7fb644c27750>], [<partitura.score.Part object at 0x7fb644b33310>, <partitura.score.Part object at 0x7fb644a81b90>, <partitura.score.Part object at 0x7fb644a81c10>, <partitura.score.Part object at 0x7fb644a81c90>], [<partitura.score.Part object at 0x7fb6449cefd0>, <partitura.score.Part object at 0x7fb64498fdd0>, <partitura.score.Part object at 0x7fb64498fe50>, <partitura.score.Part object at 0x7fb64498fed0>], [<partitura.score.Part object at 0x7fb64493b350>, <partitura.score.Part object at 0x7fb6448eac90>, <partitura.score.Part object at 0x7fb6448ead10>, <partitura.score.Part object at 0x7fb6448ead90>], [<partitura.score.Part object at 0x7fb64483d1d0>, <partitura.score.Part object at 0x7fb6447f34d0>, <partitura.score.Part object at 0x7fb6447f3550>, <partitura.score.Part object at 0x7fb6447f35d0>], [<partitura.score.Part object at 0x7fb6446863d0>, <partitura.score.Part object at 0x7fb644655ad0>, <partitura.score.Part object at 0x7fb644655b50>, <partitura.score.Part object at 0x7fb644655bd0>], [<partitura.score.Part object at 0x7fb644544a10>, <partitura.score.Part object at 0x7fb64450df90>, <partitura.score.Part object at 0x7fb64453c050>, <partitura.score.Part object at 0x7fb64453c0d0>], [<partitura.score.Part object at 0x7fb64444db50>, <partitura.score.Part object at 0x7fb6444110d0>, <partitura.score.Part object at 0x7fb644411150>, <partitura.score.Part object at 0x7fb6444111d0>], [<partitura.score.Part object at 0x7fb64432b890>, <partitura.score.Part object at 0x7fb644289090>, <partitura.score.Part object at 0x7fb644289110>, <partitura.score.Part object at 0x7fb644289190>], [<partitura.score.Part object at 0x7fb6441d2650>, <partitura.score.Part object at 0x7fb6441910d0>, <partitura.score.Part object at 0x7fb644191150>, <partitura.score.Part object at 0x7fb6441911d0>], [<partitura.score.Part object at 0x7fb6440d4990>, <partitura.score.Part object at 0x7fb644099110>, <partitura.score.Part object at 0x7fb644099190>, <partitura.score.Part object at 0x7fb644099210>], [<partitura.score.Part object at 0x7fb643f69a90>, <partitura.score.Part object at 0x7fb643ed8590>, <partitura.score.Part object at 0x7fb643ed8610>, <partitura.score.Part object at 0x7fb643ed8690>], [<partitura.score.Part object at 0x7fb643e2a850>, <partitura.score.Part object at 0x7fb643de3f90>, <partitura.score.Part object at 0x7fb643d88050>, <partitura.score.Part object at 0x7fb643d880d0>], [<partitura.score.Part object at 0x7fb643d3f150>, <partitura.score.Part object at 0x7fb643cf8c10>, <partitura.score.Part object at 0x7fb643cf8c90>, <partitura.score.Part object at 0x7fb643cf8d10>], [<partitura.score.Part object at 0x7fb643bdd8d0>, <partitura.score.Part object at 0x7fb643ba30d0>, <partitura.score.Part object at 0x7fb643ba3150>, <partitura.score.Part object at 0x7fb643ba31d0>], [<partitura.score.Part object at 0x7fb643a6d050>, <partitura.score.Part object at 0x7fb643a0ec50>, <partitura.score.Part object at 0x7fb643a0ecd0>, <partitura.score.Part object at 0x7fb643a0ed50>], [<partitura.score.Part object at 0x7fb64391cd90>, <partitura.score.Part object at 0x7fb6438e6750>, <partitura.score.Part object at 0x7fb6438e67d0>, <partitura.score.Part object at 0x7fb6438e6850>], [<partitura.score.Part object at 0x7fb6437d4e90>, <partitura.score.Part object at 0x7fb643794610>, <partitura.score.Part object at 0x7fb643794690>, <partitura.score.Part object at 0x7fb643794710>], [<partitura.score.Part object at 0x7fb64375a910>, <partitura.score.Part object at 0x7fb6436703d0>, <partitura.score.Part object at 0x7fb643670450>, <partitura.score.Part object at 0x7fb6436704d0>], [<partitura.score.Part object at 0x7fb64351b3d0>, <partitura.score.Part object at 0x7fb6434ecb50>, <partitura.score.Part object at 0x7fb6434ecbd0>, <partitura.score.Part object at 0x7fb6434ecc50>], [<partitura.score.Part object at 0x7fb643393c90>, <partitura.score.Part object at 0x7fb643370c10>, <partitura.score.Part object at 0x7fb643370c90>, <partitura.score.Part object at 0x7fb643370d10>], [<partitura.score.Part object at 0x7fb6432baf50>, <partitura.score.Part object at 0x7fb64327f5d0>, <partitura.score.Part object at 0x7fb64327f650>, <partitura.score.Part object at 0x7fb64327f6d0>], [<partitura.score.Part object at 0x7fb6430c7c90>, <partitura.score.Part object at 0x7fb6430a7910>, <partitura.score.Part object at 0x7fb6430a7990>, <partitura.score.Part object at 0x7fb6430a7a10>], [<partitura.score.Part object at 0x7fb642fe4fd0>, <partitura.score.Part object at 0x7fb642fa11d0>, <partitura.score.Part object at 0x7fb642fa1690>, <partitura.score.Part object at 0x7fb642fa1710>], [<partitura.score.Part object at 0x7fb642efe210>, <partitura.score.Part object at 0x7fb642e43550>, <partitura.score.Part object at 0x7fb642e435d0>, <partitura.score.Part object at 0x7fb642e43650>], [<partitura.score.Part object at 0x7fb642d55550>, <partitura.score.Part object at 0x7fb642d21f90>, <partitura.score.Part object at 0x7fb642cd2050>, <partitura.score.Part object at 0x7fb642cd20d0>], [<partitura.score.Part object at 0x7fb642c079d0>, <partitura.score.Part object at 0x7fb642bcea50>, <partitura.score.Part object at 0x7fb642bcead0>, <partitura.score.Part object at 0x7fb642bceb50>], [<partitura.score.Part object at 0x7fb642b22e50>, <partitura.score.Part object at 0x7fb642aeaad0>, <partitura.score.Part object at 0x7fb642aeab50>, <partitura.score.Part object at 0x7fb642aeabd0>], [<partitura.score.Part object at 0x7fb642a2bcd0>, <partitura.score.Part object at 0x7fb6429ee110>, <partitura.score.Part object at 0x7fb6429ee190>, <partitura.score.Part object at 0x7fb6429ee210>], [<partitura.score.Part object at 0x7fb6428d3d10>, <partitura.score.Part object at 0x7fb64289db90>, <partitura.score.Part object at 0x7fb64289dc10>, <partitura.score.Part object at 0x7fb64289dc90>], [<partitura.score.Part object at 0x7fb6427dfdd0>, <partitura.score.Part object at 0x7fb64279ead0>, <partitura.score.Part object at 0x7fb64279eb50>, <partitura.score.Part object at 0x7fb64279ebd0>], [<partitura.score.Part object at 0x7fb6426dfad0>, <partitura.score.Part object at 0x7fb64269c810>, <partitura.score.Part object at 0x7fb64269c890>, <partitura.score.Part object at 0x7fb64269c910>], [<partitura.score.Part object at 0x7fb642533090>, <partitura.score.Part object at 0x7fb6424e0b90>, <partitura.score.Part object at 0x7fb6424e0c10>, <partitura.score.Part object at 0x7fb6424e0c90>], [<partitura.score.Part object at 0x7fb642431450>, <partitura.score.Part object at 0x7fb6423e5790>, <partitura.score.Part object at 0x7fb6423e5810>, <partitura.score.Part object at 0x7fb6423e5890>], [<partitura.score.Part object at 0x7fb6422c6dd0>, <partitura.score.Part object at 0x7fb64229d790>, <partitura.score.Part object at 0x7fb64229d810>, <partitura.score.Part object at 0x7fb64229d890>], [<partitura.score.Part object at 0x7fb6421a5b50>, <partitura.score.Part object at 0x7fb64217c5d0>, <partitura.score.Part object at 0x7fb64217c650>, <partitura.score.Part object at 0x7fb64217c6d0>], [<partitura.score.Part object at 0x7fb64200b910>, <partitura.score.Part object at 0x7fb641fdc950>, <partitura.score.Part object at 0x7fb641fdc9d0>, <partitura.score.Part object at 0x7fb641fdca50>], [<partitura.score.Part object at 0x7fb641eda990>, <partitura.score.Part object at 0x7fb641eb2490>, <partitura.score.Part object at 0x7fb641eb2510>, <partitura.score.Part object at 0x7fb641eb2590>], [<partitura.score.Part object at 0x7fb641dfcad0>, <partitura.score.Part object at 0x7fb641dbb250>, <partitura.score.Part object at 0x7fb641dbb2d0>, <partitura.score.Part object at 0x7fb641dbb350>], [<partitura.score.Part object at 0x7fb641cb27d0>, <partitura.score.Part object at 0x7fb641c00750>, <partitura.score.Part object at 0x7fb641c007d0>, <partitura.score.Part object at 0x7fb641c00850>], [<partitura.score.Part object at 0x7fb641bbe610>, <partitura.score.Part object at 0x7fb641b75b50>, <partitura.score.Part object at 0x7fb641b75bd0>, <partitura.score.Part object at 0x7fb641b75c50>], [<partitura.score.Part object at 0x7fb641a43890>, <partitura.score.Part object at 0x7fb641a040d0>, <partitura.score.Part object at 0x7fb641a04150>, <partitura.score.Part object at 0x7fb641a041d0>], [<partitura.score.Part object at 0x7fb641942890>, <partitura.score.Part object at 0x7fb64197add0>, <partitura.score.Part object at 0x7fb64197ae50>, <partitura.score.Part object at 0x7fb64197aed0>], [<partitura.score.Part object at 0x7fb641800150>, <partitura.score.Part object at 0x7fb64183abd0>, <partitura.score.Part object at 0x7fb64183ac50>, <partitura.score.Part object at 0x7fb64183acd0>], [<partitura.score.Part object at 0x7fb64171b390>, <partitura.score.Part object at 0x7fb6416e3390>, <partitura.score.Part object at 0x7fb6416e3410>, <partitura.score.Part object at 0x7fb6416e3490>], [<partitura.score.Part object at 0x7fb6416269d0>, <partitura.score.Part object at 0x7fb6415eb2d0>, <partitura.score.Part object at 0x7fb6415eb350>, <partitura.score.Part object at 0x7fb6415eb3d0>], [<partitura.score.Part object at 0x7fb641521f50>, <partitura.score.Part object at 0x7fb6414e2ad0>, <partitura.score.Part object at 0x7fb6414e2b50>, <partitura.score.Part object at 0x7fb6414e2bd0>], [<partitura.score.Part object at 0x7fb641439e10>, <partitura.score.Part object at 0x7fb641389290>, <partitura.score.Part object at 0x7fb641389310>, <partitura.score.Part object at 0x7fb641389390>], [<partitura.score.Part object at 0x7fb6412a1950>, <partitura.score.Part object at 0x7fb64127d610>, <partitura.score.Part object at 0x7fb64127d690>, <partitura.score.Part object at 0x7fb64127d710>], [<partitura.score.Part object at 0x7fb641123490>, <partitura.score.Part object at 0x7fb6410f6d90>, <partitura.score.Part object at 0x7fb6410f6e10>, <partitura.score.Part object at 0x7fb6410f6e90>], [<partitura.score.Part object at 0x7fb640e00d90>, <partitura.score.Part object at 0x7fb640d57350>, <partitura.score.Part object at 0x7fb640d573d0>, <partitura.score.Part object at 0x7fb640d57450>], [<partitura.score.Part object at 0x7fb640bf2150>, <partitura.score.Part object at 0x7fb640b5ed10>, <partitura.score.Part object at 0x7fb640b5ed90>, <partitura.score.Part object at 0x7fb640b5ee10>], [<partitura.score.Part object at 0x7fb640a257d0>, <partitura.score.Part object at 0x7fb6409fbad0>, <partitura.score.Part object at 0x7fb6409fbb50>, <partitura.score.Part object at 0x7fb6409fbbd0>], [<partitura.score.Part object at 0x7fb6408b3e10>, <partitura.score.Part object at 0x7fb640814950>, <partitura.score.Part object at 0x7fb6408149d0>, <partitura.score.Part object at 0x7fb640814a50>], [<partitura.score.Part object at 0x7fb6406b1d10>, <partitura.score.Part object at 0x7fb640638fd0>, <partitura.score.Part object at 0x7fb6405f8090>, <partitura.score.Part object at 0x7fb6405f8110>], [<partitura.score.Part object at 0x7fb64041a210>, <partitura.score.Part object at 0x7fb640397f10>, <partitura.score.Part object at 0x7fb640397f90>, <partitura.score.Part object at 0x7fb640351050>], [<partitura.score.Part object at 0x7fb6402597d0>, <partitura.score.Part object at 0x7fb640226ad0>, <partitura.score.Part object at 0x7fb640226b50>, <partitura.score.Part object at 0x7fb640226bd0>], [<partitura.score.Part object at 0x7fb640101050>, <partitura.score.Part object at 0x7fb6401178d0>, <partitura.score.Part object at 0x7fb640117950>, <partitura.score.Part object at 0x7fb6401179d0>], [<partitura.score.Part object at 0x7fb63fceac10>, <partitura.score.Part object at 0x7fb63fb79390>, <partitura.score.Part object at 0x7fb63fb793d0>, <partitura.score.Part object at 0x7fb63fb79450>], [<partitura.score.Part object at 0x7fb63f985b90>, <partitura.score.Part object at 0x7fb63f9713d0>, <partitura.score.Part object at 0x7fb63f971450>, <partitura.score.Part object at 0x7fb63f9714d0>], [<partitura.score.Part object at 0x7fb63f86ba50>, <partitura.score.Part object at 0x7fb63f833190>, <partitura.score.Part object at 0x7fb63f833210>, <partitura.score.Part object at 0x7fb63f833290>], [<partitura.score.Part object at 0x7fb63f6c80d0>, <partitura.score.Part object at 0x7fb63f698d10>, <partitura.score.Part object at 0x7fb63f698d90>, <partitura.score.Part object at 0x7fb63f698e10>], [<partitura.score.Part object at 0x7fb63f59a790>, <partitura.score.Part object at 0x7fb63f56ea50>, <partitura.score.Part object at 0x7fb63f56ead0>, <partitura.score.Part object at 0x7fb63f56eb50>], [<partitura.score.Part object at 0x7fb63f41d890>, <partitura.score.Part object at 0x7fb63f3f9110>, <partitura.score.Part object at 0x7fb63f3f9190>, <partitura.score.Part object at 0x7fb63f3f9210>], [<partitura.score.Part object at 0x7fb63f2483d0>, <partitura.score.Part object at 0x7fb63f22f790>, <partitura.score.Part object at 0x7fb63f22f810>, <partitura.score.Part object at 0x7fb63f22f890>], [<partitura.score.Part object at 0x7fb63f085090>, <partitura.score.Part object at 0x7fb63f0a7510>, <partitura.score.Part object at 0x7fb63f0a7590>, <partitura.score.Part object at 0x7fb63f0a7610>], [<partitura.score.Part object at 0x7fb63ef82ad0>, <partitura.score.Part object at 0x7fb63ef47a90>, <partitura.score.Part object at 0x7fb63ef47b10>, <partitura.score.Part object at 0x7fb63ef47b90>], [<partitura.score.Part object at 0x7fb63ed6fed0>, <partitura.score.Part object at 0x7fb63eca6750>, <partitura.score.Part object at 0x7fb63eca67d0>, <partitura.score.Part object at 0x7fb63eca6850>], [<partitura.score.Part object at 0x7fb63ea4ed10>, <partitura.score.Part object at 0x7fb63e982450>, <partitura.score.Part object at 0x7fb63e9824d0>, <partitura.score.Part object at 0x7fb63e982550>], [<partitura.score.Part object at 0x7fb63e805390>, <partitura.score.Part object at 0x7fb63e7f4ed0>, <partitura.score.Part object at 0x7fb63e7f4f50>, <partitura.score.Part object at 0x7fb63e7f4fd0>], [<partitura.score.Part object at 0x7fb63e73f3d0>, <partitura.score.Part object at 0x7fb63e6f0dd0>, <partitura.score.Part object at 0x7fb63e6f0e50>, <partitura.score.Part object at 0x7fb63e6f0ed0>], [<partitura.score.Part object at 0x7fb63e5b36d0>, <partitura.score.Part object at 0x7fb63e519410>, <partitura.score.Part object at 0x7fb63e519490>, <partitura.score.Part object at 0x7fb63e519510>], [<partitura.score.Part object at 0x7fb63e41b410>, <partitura.score.Part object at 0x7fb63e3dbed0>, <partitura.score.Part object at 0x7fb63e3dbf50>, <partitura.score.Part object at 0x7fb63e3dbfd0>], [<partitura.score.Part object at 0x7fb63e231e90>, <partitura.score.Part object at 0x7fb63e1bf250>, <partitura.score.Part object at 0x7fb63e1bf2d0>, <partitura.score.Part object at 0x7fb63e1bf350>], [<partitura.score.Part object at 0x7fb63e056810>, <partitura.score.Part object at 0x7fb63e025550>, <partitura.score.Part object at 0x7fb63e0255d0>, <partitura.score.Part object at 0x7fb63e025650>], [<partitura.score.Part object at 0x7fb63df160d0>, <partitura.score.Part object at 0x7fb63dedf450>, <partitura.score.Part object at 0x7fb63dedf4d0>, <partitura.score.Part object at 0x7fb63dedf550>], [<partitura.score.Part object at 0x7fb63dd8d3d0>, <partitura.score.Part object at 0x7fb63dd71550>, <partitura.score.Part object at 0x7fb63dd715d0>, <partitura.score.Part object at 0x7fb63dd71650>], [<partitura.score.Part object at 0x7fb63dcaded0>, <partitura.score.Part object at 0x7fb63dc6f750>, <partitura.score.Part object at 0x7fb63dc6f7d0>, <partitura.score.Part object at 0x7fb63dc6f850>], [<partitura.score.Part object at 0x7fb63db75990>, <partitura.score.Part object at 0x7fb63db3e9d0>, <partitura.score.Part object at 0x7fb63db3ea50>, <partitura.score.Part object at 0x7fb63db3ead0>], [<partitura.score.Part object at 0x7fb63d9d07d0>, <partitura.score.Part object at 0x7fb63d9a2710>, <partitura.score.Part object at 0x7fb63d9a2790>, <partitura.score.Part object at 0x7fb63d9a2810>], [<partitura.score.Part object at 0x7fb63d834810>, <partitura.score.Part object at 0x7fb63d7a7d50>, <partitura.score.Part object at 0x7fb63d7a7dd0>, <partitura.score.Part object at 0x7fb63d7a7e50>], [<partitura.score.Part object at 0x7fb63d6ce290>, <partitura.score.Part object at 0x7fb63d6eea50>, <partitura.score.Part object at 0x7fb63d6eee10>, <partitura.score.Part object at 0x7fb63d6eee90>], [<partitura.score.Part object at 0x7fb63d500750>, <partitura.score.Part object at 0x7fb63d483750>, <partitura.score.Part object at 0x7fb63d4837d0>, <partitura.score.Part object at 0x7fb63d483850>], [<partitura.score.Part object at 0x7fb63d390390>, <partitura.score.Part object at 0x7fb63d3585d0>, <partitura.score.Part object at 0x7fb63d358650>, <partitura.score.Part object at 0x7fb63d3586d0>], [<partitura.score.Part object at 0x7fb63d254750>, <partitura.score.Part object at 0x7fb63d221ad0>, <partitura.score.Part object at 0x7fb63d221b50>, <partitura.score.Part object at 0x7fb63d221bd0>], [<partitura.score.Part object at 0x7fb63d11b990>, <partitura.score.Part object at 0x7fb63d0e6ed0>, <partitura.score.Part object at 0x7fb63d0e6f50>, <partitura.score.Part object at 0x7fb63d0e6fd0>], [<partitura.score.Part object at 0x7fb63cfe6750>, <partitura.score.Part object at 0x7fb63cfacbd0>, <partitura.score.Part object at 0x7fb63cfacc50>, <partitura.score.Part object at 0x7fb63cfaccd0>], [<partitura.score.Part object at 0x7fb63cdd2e10>, <partitura.score.Part object at 0x7fb63cd66610>, <partitura.score.Part object at 0x7fb63cd66690>, <partitura.score.Part object at 0x7fb63cd66710>], [<partitura.score.Part object at 0x7fb63cbe18d0>, <partitura.score.Part object at 0x7fb63cb47c90>, <partitura.score.Part object at 0x7fb63cb47d10>, <partitura.score.Part object at 0x7fb63cb47d90>], [<partitura.score.Part object at 0x7fb63ca93450>, <partitura.score.Part object at 0x7fb63ca46f90>, <partitura.score.Part object at 0x7fb63ca6d050>, <partitura.score.Part object at 0x7fb63ca6d0d0>], [<partitura.score.Part object at 0x7fb63c918d90>, <partitura.score.Part object at 0x7fb63c88c650>, <partitura.score.Part object at 0x7fb63c88c6d0>, <partitura.score.Part object at 0x7fb63c88c750>], [<partitura.score.Part object at 0x7fb63c7ee310>, <partitura.score.Part object at 0x7fb63c7a9d90>, <partitura.score.Part object at 0x7fb63c7a9e10>, <partitura.score.Part object at 0x7fb63c7a9e90>], [<partitura.score.Part object at 0x7fb63c655fd0>, <partitura.score.Part object at 0x7fb63c63a490>, <partitura.score.Part object at 0x7fb63c63a510>, <partitura.score.Part object at 0x7fb63c63a590>], [<partitura.score.Part object at 0x7fb63c56f550>, <partitura.score.Part object at 0x7fb63c520750>, <partitura.score.Part object at 0x7fb63c5207d0>, <partitura.score.Part object at 0x7fb63c520850>], [<partitura.score.Part object at 0x7fb63c2fa450>, <partitura.score.Part object at 0x7fb63c23aa10>, <partitura.score.Part object at 0x7fb63c23aa90>, <partitura.score.Part object at 0x7fb63c23ab10>], [<partitura.score.Part object at 0x7fb63c0f5790>, <partitura.score.Part object at 0x7fb63c043750>, <partitura.score.Part object at 0x7fb63c0437d0>, <partitura.score.Part object at 0x7fb63c043850>], [<partitura.score.Part object at 0x7fb63bf52f50>, <partitura.score.Part object at 0x7fb63bf333d0>, <partitura.score.Part object at 0x7fb63bf33450>, <partitura.score.Part object at 0x7fb63bf334d0>], [<partitura.score.Part object at 0x7fb63bdece50>, <partitura.score.Part object at 0x7fb63bd59c10>, <partitura.score.Part object at 0x7fb63bd59c90>, <partitura.score.Part object at 0x7fb63bd59d10>], [<partitura.score.Part object at 0x7fb63bc54250>, <partitura.score.Part object at 0x7fb63bc0bc90>, <partitura.score.Part object at 0x7fb63bc0bd10>, <partitura.score.Part object at 0x7fb63bc0bd90>], [<partitura.score.Part object at 0x7fb63bac7e90>, <partitura.score.Part object at 0x7fb63baa7490>, <partitura.score.Part object at 0x7fb63baa7510>, <partitura.score.Part object at 0x7fb63baa7590>], [<partitura.score.Part object at 0x7fb63b982290>, <partitura.score.Part object at 0x7fb63b946110>, <partitura.score.Part object at 0x7fb63b946190>, <partitura.score.Part object at 0x7fb63b946210>], [<partitura.score.Part object at 0x7fb63b8b6890>, <partitura.score.Part object at 0x7fb63b87fdd0>, <partitura.score.Part object at 0x7fb63b87fe50>, <partitura.score.Part object at 0x7fb63b87fed0>], [<partitura.score.Part object at 0x7fb63b7130d0>, <partitura.score.Part object at 0x7fb63b6dc710>, <partitura.score.Part object at 0x7fb63b6dc790>, <partitura.score.Part object at 0x7fb63b6dc810>], [<partitura.score.Part object at 0x7fb63b5f6d50>, <partitura.score.Part object at 0x7fb63b553b90>, <partitura.score.Part object at 0x7fb63b553c10>, <partitura.score.Part object at 0x7fb63b553c90>], [<partitura.score.Part object at 0x7fb643fed950>, <partitura.score.Part object at 0x7fb63b474090>, <partitura.score.Part object at 0x7fb63b4167d0>, <partitura.score.Part object at 0x7fb63b416cd0>], [<partitura.score.Part object at 0x7fb63b232510>, <partitura.score.Part object at 0x7fb63b163610>, <partitura.score.Part object at 0x7fb63b163690>, <partitura.score.Part object at 0x7fb63b163710>], [<partitura.score.Part object at 0x7fb63b02cbd0>, <partitura.score.Part object at 0x7fb63af98610>, <partitura.score.Part object at 0x7fb63af98690>, <partitura.score.Part object at 0x7fb63af98710>], [<partitura.score.Part object at 0x7fb63aea7950>, <partitura.score.Part object at 0x7fb63ae02250>, <partitura.score.Part object at 0x7fb63ae022d0>, <partitura.score.Part object at 0x7fb63ae02350>], [<partitura.score.Part object at 0x7fb63ad2b790>, <partitura.score.Part object at 0x7fb63ac86410>, <partitura.score.Part object at 0x7fb63ac86490>, <partitura.score.Part object at 0x7fb63ac86510>], [<partitura.score.Part object at 0x7fb63aba3290>, <partitura.score.Part object at 0x7fb63ab70850>, <partitura.score.Part object at 0x7fb63ab708d0>, <partitura.score.Part object at 0x7fb63ab70950>], [<partitura.score.Part object at 0x7fb63aaaf610>, <partitura.score.Part object at 0x7fb63aa72090>, <partitura.score.Part object at 0x7fb63aa72110>, <partitura.score.Part object at 0x7fb63aa72190>], [<partitura.score.Part object at 0x7fb63a91f990>, <partitura.score.Part object at 0x7fb63a8849d0>, <partitura.score.Part object at 0x7fb63a884a50>, <partitura.score.Part object at 0x7fb63a884ad0>], [<partitura.score.Part object at 0x7fb63a68f310>, <partitura.score.Part object at 0x7fb63a633d50>, <partitura.score.Part object at 0x7fb63a633dd0>, <partitura.score.Part object at 0x7fb63a633e50>], [<partitura.score.Part object at 0x7fb63a4cdcd0>, <partitura.score.Part object at 0x7fb63a443050>, <partitura.score.Part object at 0x7fb63a4430d0>, <partitura.score.Part object at 0x7fb63a443150>], [<partitura.score.Part object at 0x7fb63a393850>, <partitura.score.Part object at 0x7fb63a35bf10>, <partitura.score.Part object at 0x7fb63a35bf90>, <partitura.score.Part object at 0x7fb63a30d050>], [<partitura.score.Part object at 0x7fb63a24bb50>, <partitura.score.Part object at 0x7fb63a210ed0>, <partitura.score.Part object at 0x7fb63a210f50>, <partitura.score.Part object at 0x7fb63a210fd0>], [<partitura.score.Part object at 0x7fb63a100ed0>, <partitura.score.Part object at 0x7fb63a0c7f90>, <partitura.score.Part object at 0x7fb63a0f3050>, <partitura.score.Part object at 0x7fb63a0f30d0>], [<partitura.score.Part object at 0x7fb639fc0d50>, <partitura.score.Part object at 0x7fb639f94dd0>, <partitura.score.Part object at 0x7fb639f94e50>, <partitura.score.Part object at 0x7fb639f94ed0>], [<partitura.score.Part object at 0x7fb639e5c9d0>, <partitura.score.Part object at 0x7fb639e396d0>, <partitura.score.Part object at 0x7fb639e39750>, <partitura.score.Part object at 0x7fb639e397d0>], [<partitura.score.Part object at 0x7fb639d30f10>, <partitura.score.Part object at 0x7fb639c852d0>, <partitura.score.Part object at 0x7fb639c85350>, <partitura.score.Part object at 0x7fb639c853d0>], [<partitura.score.Part object at 0x7fb639b99510>, <partitura.score.Part object at 0x7fb639b65d10>, <partitura.score.Part object at 0x7fb639b65d90>, <partitura.score.Part object at 0x7fb639b65e10>], [<partitura.score.Part object at 0x7fb639986e50>, <partitura.score.Part object at 0x7fb63991b110>, <partitura.score.Part object at 0x7fb63991b190>, <partitura.score.Part object at 0x7fb63991b210>], [<partitura.score.Part object at 0x7fb6397d6850>, <partitura.score.Part object at 0x7fb6397b1090>, <partitura.score.Part object at 0x7fb6397b1110>, <partitura.score.Part object at 0x7fb6397b1190>], [<partitura.score.Part object at 0x7fb6396b9bd0>, <partitura.score.Part object at 0x7fb63960be90>, <partitura.score.Part object at 0x7fb63960bf10>, <partitura.score.Part object at 0x7fb63960bf90>], [<partitura.score.Part object at 0x7fb63b2f5990>, <partitura.score.Part object at 0x7fb6394ef4d0>, <partitura.score.Part object at 0x7fb6394a0350>, <partitura.score.Part object at 0x7fb6394a0850>], [<partitura.score.Part object at 0x7fb639367b50>, <partitura.score.Part object at 0x7fb6392d8750>, <partitura.score.Part object at 0x7fb6392d87d0>, <partitura.score.Part object at 0x7fb6392d8850>], [<partitura.score.Part object at 0x7fb63918e690>, <partitura.score.Part object at 0x7fb63916f410>, <partitura.score.Part object at 0x7fb63916f490>, <partitura.score.Part object at 0x7fb63916f510>], [<partitura.score.Part object at 0x7fb639078690>, <partitura.score.Part object at 0x7fb638fc3790>, <partitura.score.Part object at 0x7fb638fc3810>, <partitura.score.Part object at 0x7fb638fc3890>], [<partitura.score.Part object at 0x7fb638f25850>, <partitura.score.Part object at 0x7fb638efb090>, <partitura.score.Part object at 0x7fb638efb110>, <partitura.score.Part object at 0x7fb638efb190>], [<partitura.score.Part object at 0x7fb638dda7d0>, <partitura.score.Part object at 0x7fb638d9d910>, <partitura.score.Part object at 0x7fb638d9d990>, <partitura.score.Part object at 0x7fb638d9da10>], [<partitura.score.Part object at 0x7fb638be0bd0>, <partitura.score.Part object at 0x7fb638b00090>, <partitura.score.Part object at 0x7fb638b00110>, <partitura.score.Part object at 0x7fb638b00190>], [<partitura.score.Part object at 0x7fb638a29b10>, <partitura.score.Part object at 0x7fb6389822d0>, <partitura.score.Part object at 0x7fb638982350>, <partitura.score.Part object at 0x7fb6389823d0>], [<partitura.score.Part object at 0x7fb6388ca690>, <partitura.score.Part object at 0x7fb63888a110>, <partitura.score.Part object at 0x7fb63888a150>, <partitura.score.Part object at 0x7fb63888a1d0>], [<partitura.score.Part object at 0x7fb63871ca50>, <partitura.score.Part object at 0x7fb6386a5d90>, <partitura.score.Part object at 0x7fb6386a5e10>, <partitura.score.Part object at 0x7fb6386a5e90>], [<partitura.score.Part object at 0x7fb6385afe50>, <partitura.score.Part object at 0x7fb638574e50>, <partitura.score.Part object at 0x7fb638574ed0>, <partitura.score.Part object at 0x7fb638574f50>], [<partitura.score.Part object at 0x7fb63847d1d0>, <partitura.score.Part object at 0x7fb6383cca90>, <partitura.score.Part object at 0x7fb6383ccb10>, <partitura.score.Part object at 0x7fb6383ccb90>], [<partitura.score.Part object at 0x7fb63824ef90>, <partitura.score.Part object at 0x7fb638236e50>, <partitura.score.Part object at 0x7fb638236ed0>, <partitura.score.Part object at 0x7fb638236f50>], [<partitura.score.Part object at 0x7fb6380d28d0>, <partitura.score.Part object at 0x7fb6380af110>, <partitura.score.Part object at 0x7fb6380af190>, <partitura.score.Part object at 0x7fb6380af210>], [<partitura.score.Part object at 0x7fb637fa5790>, <partitura.score.Part object at 0x7fb637f6ecd0>, <partitura.score.Part object at 0x7fb637f6ed50>, <partitura.score.Part object at 0x7fb637f6edd0>], [<partitura.score.Part object at 0x7fb637e72cd0>, <partitura.score.Part object at 0x7fb637e3bfd0>, <partitura.score.Part object at 0x7fb637de8090>, <partitura.score.Part object at 0x7fb637de8110>], [<partitura.score.Part object at 0x7fb637d4c110>, <partitura.score.Part object at 0x7fb639078850>, <partitura.score.Part object at 0x7fb637cc1390>, <partitura.score.Part object at 0x7fb637cc1450>], [<partitura.score.Part object at 0x7fb637c159d0>, <partitura.score.Part object at 0x7fb637bddb10>, <partitura.score.Part object at 0x7fb637bddb90>, <partitura.score.Part object at 0x7fb637bddc10>], [<partitura.score.Part object at 0x7fb637ad9a50>, <partitura.score.Part object at 0x7fb637ab0650>, <partitura.score.Part object at 0x7fb637ab06d0>, <partitura.score.Part object at 0x7fb637ab0750>], [<partitura.score.Part object at 0x7fb6379b6710>, <partitura.score.Part object at 0x7fb637904d50>, <partitura.score.Part object at 0x7fb637904dd0>, <partitura.score.Part object at 0x7fb637904e50>], [<partitura.score.Part object at 0x7fb637816f10>, <partitura.score.Part object at 0x7fb6377fa310>, <partitura.score.Part object at 0x7fb6377fa390>, <partitura.score.Part object at 0x7fb6377fa410>], [<partitura.score.Part object at 0x7fb6376eba50>, <partitura.score.Part object at 0x7fb6376bf390>, <partitura.score.Part object at 0x7fb6376bf410>, <partitura.score.Part object at 0x7fb6376bf490>], [<partitura.score.Part object at 0x7fb63759d290>, <partitura.score.Part object at 0x7fb637554ed0>, <partitura.score.Part object at 0x7fb637554f50>, <partitura.score.Part object at 0x7fb637554fd0>], [<partitura.score.Part object at 0x7fb63744ee50>, <partitura.score.Part object at 0x7fb637427210>, <partitura.score.Part object at 0x7fb637427290>, <partitura.score.Part object at 0x7fb637427310>], [<partitura.score.Part object at 0x7fb6420d3450>, <partitura.score.Part object at 0x7fb63739ee90>, <partitura.score.Part object at 0x7fb6373207d0>, <partitura.score.Part object at 0x7fb637320890>], [<partitura.score.Part object at 0x7fb63719ff50>, <partitura.score.Part object at 0x7fb637134750>, <partitura.score.Part object at 0x7fb6371347d0>, <partitura.score.Part object at 0x7fb637134850>], [<partitura.score.Part object at 0x7fb636f8eed0>, <partitura.score.Part object at 0x7fb636f77790>, <partitura.score.Part object at 0x7fb636f77810>, <partitura.score.Part object at 0x7fb636f77890>], [<partitura.score.Part object at 0x7fb636e15290>, <partitura.score.Part object at 0x7fb636de0ad0>, <partitura.score.Part object at 0x7fb636de0b50>, <partitura.score.Part object at 0x7fb636de0bd0>], [<partitura.score.Part object at 0x7fb636c87c50>, <partitura.score.Part object at 0x7fb636c5fe90>, <partitura.score.Part object at 0x7fb636c5ff10>, <partitura.score.Part object at 0x7fb636c5ff90>], [<partitura.score.Part object at 0x7fb636bbfdd0>, <partitura.score.Part object at 0x7fb636b03f10>, <partitura.score.Part object at 0x7fb636b03f90>, <partitura.score.Part object at 0x7fb636b2f050>], [<partitura.score.Part object at 0x7fb636a03bd0>, <partitura.score.Part object at 0x7fb6369d8150>, <partitura.score.Part object at 0x7fb6369d81d0>, <partitura.score.Part object at 0x7fb6369d8250>], [<partitura.score.Part object at 0x7fb636883c50>, <partitura.score.Part object at 0x7fb636854f50>, <partitura.score.Part object at 0x7fb636854fd0>, <partitura.score.Part object at 0x7fb636802090>], [<partitura.score.Part object at 0x7fb6367a5190>, <partitura.score.Part object at 0x7fb63675bb90>, <partitura.score.Part object at 0x7fb63675bc10>, <partitura.score.Part object at 0x7fb63675bc90>], [<partitura.score.Part object at 0x7fb63664a290>, <partitura.score.Part object at 0x7fb636613190>, <partitura.score.Part object at 0x7fb636613210>, <partitura.score.Part object at 0x7fb636613290>], [<partitura.score.Part object at 0x7fb636529850>, <partitura.score.Part object at 0x7fb636484510>, <partitura.score.Part object at 0x7fb636484590>, <partitura.score.Part object at 0x7fb636484610>], [<partitura.score.Part object at 0x7fb6363268d0>, <partitura.score.Part object at 0x7fb6362aab90>, <partitura.score.Part object at 0x7fb6362aac10>, <partitura.score.Part object at 0x7fb6362aac90>], [<partitura.score.Part object at 0x7fb636129990>, <partitura.score.Part object at 0x7fb6360a1590>, <partitura.score.Part object at 0x7fb6360a1610>, <partitura.score.Part object at 0x7fb6360a1690>], [<partitura.score.Part object at 0x7fb635ffbb50>, <partitura.score.Part object at 0x7fb635fbc5d0>, <partitura.score.Part object at 0x7fb635fbc650>, <partitura.score.Part object at 0x7fb635fbc6d0>], [<partitura.score.Part object at 0x7fb635e2b790>, <partitura.score.Part object at 0x7fb635da5fd0>, <partitura.score.Part object at 0x7fb635db0090>, <partitura.score.Part object at 0x7fb635db0110>], [<partitura.score.Part object at 0x7fb635c84f90>, <partitura.score.Part object at 0x7fb635c45910>, <partitura.score.Part object at 0x7fb635c45990>, <partitura.score.Part object at 0x7fb635c45a10>], [<partitura.score.Part object at 0x7fb635b43510>, <partitura.score.Part object at 0x7fb635b09710>, <partitura.score.Part object at 0x7fb635b09790>, <partitura.score.Part object at 0x7fb635b09810>], [<partitura.score.Part object at 0x7fb6359c4990>, <partitura.score.Part object at 0x7fb63595c050>, <partitura.score.Part object at 0x7fb63595c0d0>, <partitura.score.Part object at 0x7fb63595c150>], [<partitura.score.Part object at 0x7fb6358accd0>, <partitura.score.Part object at 0x7fb635806350>, <partitura.score.Part object at 0x7fb6358063d0>, <partitura.score.Part object at 0x7fb635806450>], [<partitura.score.Part object at 0x7fb635706450>, <partitura.score.Part object at 0x7fb6356db290>, <partitura.score.Part object at 0x7fb6356db310>, <partitura.score.Part object at 0x7fb6356db390>], [<partitura.score.Part object at 0x7fb6355d2550>, <partitura.score.Part object at 0x7fb63559ae50>, <partitura.score.Part object at 0x7fb63559aed0>, <partitura.score.Part object at 0x7fb63559af50>], [<partitura.score.Part object at 0x7fb6376cce90>, <partitura.score.Part object at 0x7fb6354bcb90>, <partitura.score.Part object at 0x7fb6354bcd90>, <partitura.score.Part object at 0x7fb6354bcd50>], [<partitura.score.Part object at 0x7fb6353a4f90>, <partitura.score.Part object at 0x7fb63537b790>, <partitura.score.Part object at 0x7fb63537b810>, <partitura.score.Part object at 0x7fb63537b890>], [<partitura.score.Part object at 0x7fb635213190>, <partitura.score.Part object at 0x7fb6351e49d0>, <partitura.score.Part object at 0x7fb6351e4a50>, <partitura.score.Part object at 0x7fb6351e4ad0>], [<partitura.score.Part object at 0x7fb635045a90>, <partitura.score.Part object at 0x7fb63502ced0>, <partitura.score.Part object at 0x7fb63502cf50>, <partitura.score.Part object at 0x7fb63502cfd0>], [<partitura.score.Part object at 0x7fb634f49110>, <partitura.score.Part object at 0x7fb634f68d10>, <partitura.score.Part object at 0x7fb634f68d50>, <partitura.score.Part object at 0x7fb634f68dd0>], [<partitura.score.Part object at 0x7fb634e536d0>, <partitura.score.Part object at 0x7fb634e1b8d0>, <partitura.score.Part object at 0x7fb634e1b950>, <partitura.score.Part object at 0x7fb634e1b9d0>], [<partitura.score.Part object at 0x7fb634cd5f90>, <partitura.score.Part object at 0x7fb634c41710>, <partitura.score.Part object at 0x7fb634c41790>, <partitura.score.Part object at 0x7fb634c41810>], [<partitura.score.Part object at 0x7fb634b0f150>, <partitura.score.Part object at 0x7fb634adabd0>, <partitura.score.Part object at 0x7fb634adac50>, <partitura.score.Part object at 0x7fb634adacd0>], [<partitura.score.Part object at 0x7fb6349dd310>, <partitura.score.Part object at 0x7fb6349a51d0>, <partitura.score.Part object at 0x7fb6349a5250>, <partitura.score.Part object at 0x7fb6349a52d0>], [<partitura.score.Part object at 0x7fb634889490>, <partitura.score.Part object at 0x7fb634852c90>, <partitura.score.Part object at 0x7fb634852d10>, <partitura.score.Part object at 0x7fb634852d90>], [<partitura.score.Part object at 0x7fb634753250>, <partitura.score.Part object at 0x7fb63471d690>, <partitura.score.Part object at 0x7fb63471d710>, <partitura.score.Part object at 0x7fb63471d790>], [<partitura.score.Part object at 0x7fb63463ff50>, <partitura.score.Part object at 0x7fb6345a9710>, <partitura.score.Part object at 0x7fb6345a9790>, <partitura.score.Part object at 0x7fb6345a9810>], [<partitura.score.Part object at 0x7fb6344fecd0>, <partitura.score.Part object at 0x7fb6344bf350>, <partitura.score.Part object at 0x7fb6344bf3d0>, <partitura.score.Part object at 0x7fb6344bf450>], [<partitura.score.Part object at 0x7fb6343bff10>, <partitura.score.Part object at 0x7fb634319790>, <partitura.score.Part object at 0x7fb634319810>, <partitura.score.Part object at 0x7fb634319890>], [<partitura.score.Part object at 0x7fb634222990>, <partitura.score.Part object at 0x7fb6341eb990>, <partitura.score.Part object at 0x7fb6341eba10>, <partitura.score.Part object at 0x7fb6341eba90>], [<partitura.score.Part object at 0x7fb6340dabd0>, <partitura.score.Part object at 0x7fb63409abd0>, <partitura.score.Part object at 0x7fb63409ac50>, <partitura.score.Part object at 0x7fb63409acd0>], [<partitura.score.Part object at 0x7fb633f5a350>, <partitura.score.Part object at 0x7fb633f3d710>, <partitura.score.Part object at 0x7fb633f3d790>, <partitura.score.Part object at 0x7fb633f3d810>], [<partitura.score.Part object at 0x7fb63471d750>, <partitura.score.Part object at 0x7fb633d0f750>, <partitura.score.Part object at 0x7fb633d0f810>, <partitura.score.Part object at 0x7fb633d0f850>], [<partitura.score.Part object at 0x7fb633c64410>, <partitura.score.Part object at 0x7fb633c17b50>, <partitura.score.Part object at 0x7fb633c17bd0>, <partitura.score.Part object at 0x7fb633c17c50>], [<partitura.score.Part object at 0x7fb633b0db10>, <partitura.score.Part object at 0x7fb633ae6290>, <partitura.score.Part object at 0x7fb633ae6310>, <partitura.score.Part object at 0x7fb633ae6390>], [<partitura.score.Part object at 0x7fb6339c9cd0>, <partitura.score.Part object at 0x7fb63399d250>, <partitura.score.Part object at 0x7fb63399d2d0>, <partitura.score.Part object at 0x7fb63399d350>], [<partitura.score.Part object at 0x7fb633856250>, <partitura.score.Part object at 0x7fb633833c50>, <partitura.score.Part object at 0x7fb633833cd0>, <partitura.score.Part object at 0x7fb633833d50>], [<partitura.score.Part object at 0x7fb633771bd0>, <partitura.score.Part object at 0x7fb633724f50>, <partitura.score.Part object at 0x7fb63372e350>, <partitura.score.Part object at 0x7fb63372e3d0>], [<partitura.score.Part object at 0x7fb633603a90>, <partitura.score.Part object at 0x7fb6335cd450>, <partitura.score.Part object at 0x7fb6335cd4d0>, <partitura.score.Part object at 0x7fb6335cd550>], [<partitura.score.Part object at 0x7fb63353a910>, <partitura.score.Part object at 0x7fb633482b10>, <partitura.score.Part object at 0x7fb633482b90>, <partitura.score.Part object at 0x7fb633482c10>], [<partitura.score.Part object at 0x7fb63335b890>, <partitura.score.Part object at 0x7fb6332cc7d0>, <partitura.score.Part object at 0x7fb6332cc850>, <partitura.score.Part object at 0x7fb6332cc8d0>], [<partitura.score.Part object at 0x7fb633234910>, <partitura.score.Part object at 0x7fb6331f6b50>, <partitura.score.Part object at 0x7fb6331f6bd0>, <partitura.score.Part object at 0x7fb6331f6c50>], [<partitura.score.Part object at 0x7fb633042650>, <partitura.score.Part object at 0x7fb633027290>, <partitura.score.Part object at 0x7fb633027310>, <partitura.score.Part object at 0x7fb633027390>], [<partitura.score.Part object at 0x7fb632f05690>, <partitura.score.Part object at 0x7fb632ec8210>, <partitura.score.Part object at 0x7fb632ec8290>, <partitura.score.Part object at 0x7fb632ec8310>], [<partitura.score.Part object at 0x7fb632dcbe10>, <partitura.score.Part object at 0x7fb632da5350>, <partitura.score.Part object at 0x7fb632da53d0>, <partitura.score.Part object at 0x7fb632da5450>], [<partitura.score.Part object at 0x7fb632c5dc50>, <partitura.score.Part object at 0x7fb632bce090>, <partitura.score.Part object at 0x7fb632bce110>, <partitura.score.Part object at 0x7fb632bce190>], [<partitura.score.Part object at 0x7fb632aca4d0>, <partitura.score.Part object at 0x7fb632a91a50>, <partitura.score.Part object at 0x7fb632a91ad0>, <partitura.score.Part object at 0x7fb632a91b50>], [<partitura.score.Part object at 0x7fb6329a3790>, <partitura.score.Part object at 0x7fb63296fe90>, <partitura.score.Part object at 0x7fb63296ff10>, <partitura.score.Part object at 0x7fb63296ff90>], [<partitura.score.Part object at 0x7fb632817e10>, <partitura.score.Part object at 0x7fb6327f8a50>, <partitura.score.Part object at 0x7fb6327f8ad0>, <partitura.score.Part object at 0x7fb6327f8b50>], [<partitura.score.Part object at 0x7fb6326edd90>, <partitura.score.Part object at 0x7fb632641350>, <partitura.score.Part object at 0x7fb6326413d0>, <partitura.score.Part object at 0x7fb632641450>], [<partitura.score.Part object at 0x7fb63251aed0>, <partitura.score.Part object at 0x7fb632487650>, <partitura.score.Part object at 0x7fb6324876d0>, <partitura.score.Part object at 0x7fb632487750>], [<partitura.score.Part object at 0x7fb632319310>, <partitura.score.Part object at 0x7fb63229a390>, <partitura.score.Part object at 0x7fb63229a3d0>, <partitura.score.Part object at 0x7fb63229a450>], [<partitura.score.Part object at 0x7fb6321b0310>, <partitura.score.Part object at 0x7fb63217e950>, <partitura.score.Part object at 0x7fb63217e9d0>, <partitura.score.Part object at 0x7fb63217ea50>], [<partitura.score.Part object at 0x7fb63207f210>, <partitura.score.Part object at 0x7fb631fc9750>, <partitura.score.Part object at 0x7fb631fc97d0>, <partitura.score.Part object at 0x7fb631fc9850>], [<partitura.score.Part object at 0x7fb631ec31d0>, <partitura.score.Part object at 0x7fb631e897d0>, <partitura.score.Part object at 0x7fb631e89850>, <partitura.score.Part object at 0x7fb631e898d0>], [<partitura.score.Part object at 0x7fb631d88410>, <partitura.score.Part object at 0x7fb631d534d0>, <partitura.score.Part object at 0x7fb631d53550>, <partitura.score.Part object at 0x7fb631d535d0>], [<partitura.score.Part object at 0x7fb631c51a50>, <partitura.score.Part object at 0x7fb631c26410>, <partitura.score.Part object at 0x7fb631c26490>, <partitura.score.Part object at 0x7fb631c26510>], [<partitura.score.Part object at 0x7fb631b2a9d0>, <partitura.score.Part object at 0x7fb631afe0d0>, <partitura.score.Part object at 0x7fb631afe110>, <partitura.score.Part object at 0x7fb631afe190>], [<partitura.score.Part object at 0x7fb631968b10>, <partitura.score.Part object at 0x7fb6318e0c50>, <partitura.score.Part object at 0x7fb6318e0cd0>, <partitura.score.Part object at 0x7fb6318e0d50>], [<partitura.score.Part object at 0x7fb6317975d0>, <partitura.score.Part object at 0x7fb631765ad0>, <partitura.score.Part object at 0x7fb631765b50>, <partitura.score.Part object at 0x7fb631765bd0>], [<partitura.score.Part object at 0x7fb631610850>, <partitura.score.Part object at 0x7fb6315ee190>, <partitura.score.Part object at 0x7fb6315ee210>, <partitura.score.Part object at 0x7fb6315ee290>], [<partitura.score.Part object at 0x7fb6314e4b10>, <partitura.score.Part object at 0x7fb6314bb290>, <partitura.score.Part object at 0x7fb6314bb310>, <partitura.score.Part object at 0x7fb6314bb390>], [<partitura.score.Part object at 0x7fb6313a8b50>, <partitura.score.Part object at 0x7fb63137f690>, <partitura.score.Part object at 0x7fb63137f710>, <partitura.score.Part object at 0x7fb63137f790>], [<partitura.score.Part object at 0x7fb63121ded0>, <partitura.score.Part object at 0x7fb6311fa910>, <partitura.score.Part object at 0x7fb6311fa990>, <partitura.score.Part object at 0x7fb6311faa10>], [<partitura.score.Part object at 0x7fb6310ff390>, <partitura.score.Part object at 0x7fb631048910>, <partitura.score.Part object at 0x7fb631048990>, <partitura.score.Part object at 0x7fb631048a10>], [<partitura.score.Part object at 0x7fb635020e90>, <partitura.score.Part object at 0x7fb630f3dd90>, <partitura.score.Part object at 0x7fb630ec71d0>, <partitura.score.Part object at 0x7fb630ec7190>], [<partitura.score.Part object at 0x7fb630dc7c10>, <partitura.score.Part object at 0x7fb630d90f10>, <partitura.score.Part object at 0x7fb630d90f90>, <partitura.score.Part object at 0x7fb630dbe050>], [<partitura.score.Part object at 0x7fb630c8da50>, <partitura.score.Part object at 0x7fb630c58910>, <partitura.score.Part object at 0x7fb630c58990>, <partitura.score.Part object at 0x7fb630c58a10>], [<partitura.score.Part object at 0x7fb630b6acd0>, <partitura.score.Part object at 0x7fb630ac7690>, <partitura.score.Part object at 0x7fb630ac7710>, <partitura.score.Part object at 0x7fb630ac7790>], [<partitura.score.Part object at 0x7fb6309f14d0>, <partitura.score.Part object at 0x7fb63094b850>, <partitura.score.Part object at 0x7fb63094b8d0>, <partitura.score.Part object at 0x7fb63094b950>]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate chorales"
      ],
      "metadata": {
        "id": "yphGmsr-NSV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate chorals"
      ],
      "metadata": {
        "id": "v4TJGKiUs086"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_one_choral(model, train_dataloader, part_dic,F1):\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match                                      \n",
        "            \n",
        "            #if idx > 40: # or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "                part_3 = part[3]\n",
        "\n",
        "                #note_array_0 = part_0.note_array\n",
        "                #note_array_1 = part_1.note_array\n",
        "                #note_array_2 = part_2.note_array\n",
        "                #note_array_3 = part_3.note_array                \n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                   \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]                   \n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        \n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "                        else:\n",
        "                            accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                        counting = 0\n",
        "                        ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                        for j in range(len(total_predictions_dict[i])):\n",
        "                            if total_predictions_dict[i][j][0] == gt:\n",
        "                                counting +=1  \n",
        "                        count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                    acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                    print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                    \n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "                    acc_score_dict[\"3\"].append(acc_3)\n",
        "                \n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    print(\"f1_v0 , sample {}:\".format(idx),f1_v0)\n",
        "                    print(\"f1_v1 , sample {}:\".format(idx),f1_v1)\n",
        "                    print(\"f1_v2 , sample {}:\".format(idx),f1_v2)\n",
        "                    \n",
        "                    if len(part)==4:\n",
        "                        pred_3 = accordance_dict[\"3\"]\n",
        "                        truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                        f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                        f_score_dict[\"3\"].append(f1_v3)\n",
        "                        print(\"f1_v3 , sample {}:\".format(idx),f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "UXr2DeiLSyLm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_one_choral(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghAmXcXTTtD1",
        "outputId": "6cb25c44-c623-45b2-8780-65465230b667"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0, sample 0: 0.967741935483871\n",
            "acc 1, sample 0: 0.967479674796748\n",
            "acc 2, sample 0: 0.9357798165137615\n",
            "acc 3, sample 0: 1.0\n",
            "acc 0, sample 1: 1.0\n",
            "acc 1, sample 1: 1.0\n",
            "acc 2, sample 1: 0.9833333333333333\n",
            "acc 3, sample 1: 1.0\n",
            "acc 0, sample 2: 1.0\n",
            "acc 1, sample 2: 1.0\n",
            "acc 2, sample 2: 0.9803921568627451\n",
            "acc 3, sample 2: 1.0\n",
            "acc 0, sample 3: 0.9791666666666666\n",
            "acc 1, sample 3: 0.9387755102040817\n",
            "acc 2, sample 3: 0.9565217391304348\n",
            "acc 3, sample 3: 1.0\n",
            "acc 0, sample 4: 1.0\n",
            "acc 1, sample 4: 0.9\n",
            "acc 2, sample 4: 0.8536585365853658\n",
            "acc 3, sample 4: 1.0\n",
            "acc 0, sample 5: 0.9666666666666667\n",
            "acc 1, sample 5: 0.9885057471264368\n",
            "acc 2, sample 5: 0.9425287356321839\n",
            "acc 3, sample 5: 0.9878048780487805\n",
            "acc 0, sample 6: 1.0\n",
            "acc 1, sample 6: 0.9166666666666666\n",
            "acc 2, sample 6: 0.9702970297029703\n",
            "acc 3, sample 6: 1.0\n",
            "acc 0, sample 7: 0.9824561403508771\n",
            "acc 1, sample 7: 0.8666666666666667\n",
            "acc 2, sample 7: 1.0\n",
            "acc 3, sample 7: 0.9782608695652174\n",
            "acc 0, sample 8: 1.0\n",
            "acc 1, sample 8: 1.0\n",
            "acc 2, sample 8: 0.9583333333333334\n",
            "acc 3, sample 8: 1.0\n",
            "acc 0, sample 9: 0.9523809523809523\n",
            "acc 1, sample 9: 0.925\n",
            "acc 2, sample 9: 0.9473684210526315\n",
            "acc 3, sample 9: 1.0\n",
            "acc 0, sample 10: 1.0\n",
            "acc 1, sample 10: 0.9811320754716981\n",
            "acc 2, sample 10: 0.9795918367346939\n",
            "acc 3, sample 10: 1.0\n",
            "acc 0, sample 11: 0.9873417721518988\n",
            "acc 1, sample 11: 1.0\n",
            "acc 2, sample 11: 0.9692307692307692\n",
            "acc 3, sample 11: 1.0\n",
            "acc 0, sample 12: 0.9444444444444444\n",
            "acc 1, sample 12: 0.9375\n",
            "acc 2, sample 12: 0.9777777777777777\n",
            "acc 3, sample 12: 1.0\n",
            "acc 0, sample 13: 1.0\n",
            "acc 1, sample 13: 0.9682539682539683\n",
            "acc 2, sample 13: 1.0\n",
            "acc 3, sample 13: 1.0\n",
            "acc 0, sample 14: 0.9726027397260274\n",
            "acc 1, sample 14: 0.9571428571428572\n",
            "acc 2, sample 14: 0.9193548387096774\n",
            "acc 3, sample 14: 1.0\n",
            "acc 0, sample 15: 1.0\n",
            "acc 1, sample 15: 0.9811320754716981\n",
            "acc 2, sample 15: 0.9795918367346939\n",
            "acc 3, sample 15: 0.975\n",
            "acc 0, sample 16: 0.9807692307692307\n",
            "acc 1, sample 16: 0.9433962264150944\n",
            "acc 2, sample 16: 1.0\n",
            "acc 3, sample 16: 1.0\n",
            "acc 0, sample 17: 1.0\n",
            "acc 1, sample 17: 0.9117647058823529\n",
            "acc 2, sample 17: 0.9473684210526315\n",
            "acc 3, sample 17: 0.967741935483871\n",
            "acc 0, sample 18: 1.0\n",
            "acc 1, sample 18: 0.9090909090909091\n",
            "acc 2, sample 18: 1.0\n",
            "acc 3, sample 18: 0.90625\n",
            "acc 0, sample 19: 1.0\n",
            "acc 1, sample 19: 0.9636363636363636\n",
            "acc 2, sample 19: 0.9803921568627451\n",
            "acc 3, sample 19: 1.0\n",
            "acc 0, sample 20: 1.0\n",
            "acc 1, sample 20: 0.9090909090909091\n",
            "acc 2, sample 20: 0.8431372549019608\n",
            "acc 3, sample 20: 1.0\n",
            "acc 0, sample 21: 1.0\n",
            "acc 1, sample 21: 0.9230769230769231\n",
            "acc 2, sample 21: 1.0\n",
            "acc 3, sample 21: 1.0\n",
            "acc 0, sample 22: 0.9736842105263158\n",
            "acc 1, sample 22: 0.9545454545454546\n",
            "acc 2, sample 22: 1.0\n",
            "acc 3, sample 22: 0.9791666666666666\n",
            "acc 0, sample 23: 0.9649122807017544\n",
            "acc 1, sample 23: 0.9787234042553191\n",
            "acc 2, sample 23: 0.9803921568627451\n",
            "acc 3, sample 23: 0.9736842105263158\n",
            "acc 0, sample 24: 1.0\n",
            "acc 1, sample 24: 0.9516129032258065\n",
            "acc 2, sample 24: 0.9629629629629629\n",
            "acc 3, sample 24: 0.9811320754716981\n",
            "acc 0, sample 25: 0.9433962264150944\n",
            "acc 1, sample 25: 1.0\n",
            "acc 2, sample 25: 1.0\n",
            "acc 3, sample 25: 1.0\n",
            "acc 0, sample 26: 1.0\n",
            "acc 1, sample 26: 0.9743589743589743\n",
            "acc 2, sample 26: 0.9736842105263158\n",
            "acc 3, sample 26: 0.9473684210526315\n",
            "acc 0, sample 27: 1.0\n",
            "acc 1, sample 27: 0.972972972972973\n",
            "acc 2, sample 27: 1.0\n",
            "acc 3, sample 27: 1.0\n",
            "acc 0, sample 28: 1.0\n",
            "acc 1, sample 28: 0.975609756097561\n",
            "acc 2, sample 28: 0.9523809523809523\n",
            "acc 3, sample 28: 1.0\n",
            "acc 0, sample 29: 0.975609756097561\n",
            "acc 1, sample 29: 0.9534883720930233\n",
            "acc 2, sample 29: 0.8888888888888888\n",
            "acc 3, sample 29: 0.972972972972973\n",
            "acc 0, sample 30: 1.0\n",
            "acc 1, sample 30: 0.975\n",
            "acc 2, sample 30: 1.0\n",
            "acc 3, sample 30: 1.0\n",
            "acc 0, sample 31: 1.0\n",
            "acc 1, sample 31: 1.0\n",
            "acc 2, sample 31: 1.0\n",
            "acc 3, sample 31: 1.0\n",
            "acc 0, sample 32: 0.9928571428571429\n",
            "acc 1, sample 32: 0.9622641509433962\n",
            "acc 2, sample 32: 0.9280575539568345\n",
            "acc 3, sample 32: 0.9568965517241379\n",
            "acc 0, sample 33: 0.9705882352941176\n",
            "acc 1, sample 33: 1.0\n",
            "acc 2, sample 33: 0.9761904761904762\n",
            "acc 3, sample 33: 1.0\n",
            "acc 0, sample 34: 0.9753086419753086\n",
            "acc 1, sample 34: 0.8918918918918919\n",
            "acc 2, sample 34: 1.0\n",
            "acc 3, sample 34: 0.9846153846153847\n",
            "acc 0, sample 35: 0.9821428571428571\n",
            "acc 1, sample 35: 0.8979591836734694\n",
            "acc 2, sample 35: 0.9607843137254902\n",
            "acc 3, sample 35: 1.0\n",
            "acc 0, sample 36: 0.9692307692307692\n",
            "acc 1, sample 36: 0.9827586206896551\n",
            "acc 2, sample 36: 0.9464285714285714\n",
            "acc 3, sample 36: 1.0\n",
            "acc 0, sample 37: 1.0\n",
            "acc 1, sample 37: 0.96\n",
            "acc 2, sample 37: 1.0\n",
            "acc 3, sample 37: 1.0\n",
            "acc 0, sample 38: 1.0\n",
            "acc 1, sample 38: 0.8679245283018868\n",
            "acc 2, sample 38: 0.9591836734693877\n",
            "acc 3, sample 38: 0.9767441860465116\n",
            "acc 0, sample 39: 0.9523809523809523\n",
            "acc 1, sample 39: 0.9696969696969697\n",
            "acc 2, sample 39: 1.0\n",
            "acc 3, sample 39: 1.0\n",
            "acc 0, sample 40: 0.9090909090909091\n",
            "acc 1, sample 40: 0.8214285714285714\n",
            "acc 2, sample 40: 0.92\n",
            "acc 3, sample 40: 0.9787234042553191\n",
            "acc 0, sample 41: 0.9871794871794872\n",
            "acc 1, sample 41: 0.9512195121951219\n",
            "acc 2, sample 41: 0.9714285714285714\n",
            "acc 3, sample 41: 1.0\n",
            "acc 0, sample 42: 1.0\n",
            "acc 1, sample 42: 1.0\n",
            "acc 2, sample 42: 1.0\n",
            "acc 3, sample 42: 0.9787234042553191\n",
            "acc 0, sample 43: 1.0\n",
            "acc 1, sample 43: 0.9487179487179487\n",
            "acc 2, sample 43: 1.0\n",
            "acc 3, sample 43: 0.9743589743589743\n",
            "acc 0, sample 44: 1.0\n",
            "acc 1, sample 44: 0.9253731343283582\n",
            "acc 2, sample 44: 0.95\n",
            "acc 3, sample 44: 1.0\n",
            "acc 0, sample 45: 1.0\n",
            "acc 1, sample 45: 1.0\n",
            "acc 2, sample 45: 0.8846153846153846\n",
            "acc 3, sample 45: 1.0\n",
            "acc 0, sample 46: 1.0\n",
            "acc 1, sample 46: 0.95\n",
            "acc 2, sample 46: 0.9459459459459459\n",
            "acc 3, sample 46: 1.0\n",
            "acc 0, sample 47: 0.9642857142857143\n",
            "acc 1, sample 47: 0.9636363636363636\n",
            "acc 2, sample 47: 1.0\n",
            "acc 3, sample 47: 1.0\n",
            "acc 0, sample 48: 0.9830508474576272\n",
            "acc 1, sample 48: 0.9166666666666666\n",
            "acc 2, sample 48: 0.9655172413793104\n",
            "acc 3, sample 48: 1.0\n",
            "acc 0, sample 49: 0.9782608695652174\n",
            "acc 1, sample 49: 0.9534883720930233\n",
            "acc 2, sample 49: 0.9714285714285714\n",
            "acc 3, sample 49: 1.0\n",
            "acc 0, sample 50: 1.0\n",
            "acc 1, sample 50: 0.9629629629629629\n",
            "acc 2, sample 50: 0.9433962264150944\n",
            "acc 3, sample 50: 1.0\n",
            "acc 0, sample 51: 1.0\n",
            "acc 1, sample 51: 0.9811320754716981\n",
            "acc 2, sample 51: 1.0\n",
            "acc 3, sample 51: 1.0\n",
            "acc 0, sample 52: 0.9696969696969697\n",
            "acc 1, sample 52: 0.8205128205128205\n",
            "acc 2, sample 52: 0.9166666666666666\n",
            "acc 3, sample 52: 1.0\n",
            "acc 0, sample 53: 0.9795918367346939\n",
            "acc 1, sample 53: 0.9433962264150944\n",
            "acc 2, sample 53: 1.0\n",
            "acc 3, sample 53: 1.0\n",
            "acc 0, sample 54: 0.9358974358974359\n",
            "acc 1, sample 54: 0.8674698795180723\n",
            "acc 2, sample 54: 0.9305555555555556\n",
            "acc 3, sample 54: 1.0\n",
            "acc 0, sample 55: 1.0\n",
            "acc 1, sample 55: 0.9736842105263158\n",
            "acc 2, sample 55: 1.0\n",
            "acc 3, sample 55: 1.0\n",
            "total_predictions_dict dict_keys(['0', '1', '2', '3'])\n",
            "0.9846559944851886 0.9488715572537995 0.9652351056782041 0.9914186416972107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    plt.plot(acc_score_dict[\"0\"],'-o')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend(['Accuracy0'])\n",
        "    plt.title('Accuracy vs Epochs')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "J92mdaQG0MXZ",
        "outputId": "cd015a47-a9a8-4af6-f506-c5012ea4004a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd7wcZ3nvf8/2evpRO5IluSAQyMFgMAYTDLnBdIRJKCEEEmLgUq5vsWOc3NASrrhx6g2kkAQcEmIHjBHGNpZxo8QQLHNsy7ItWy6Szh6V08v28tw/Zt7Z2dmZ2ZndndnV0fv9fM7n7O7M7r475X3epxMzQyKRSCQSI4FeD0AikUgk/YkUEBKJRCIxRQoIiUQikZgiBYREIpFITJECQiKRSCSmSAEhkUgkElOkgJBIJJYQERPRub0eh6Q3SAEh6TlEdB8RLRBRtNdj6WeI6DkiyhPRqu7vS70el2TtIgWEpKcQ0TYArwbAAN7m83eH/Py+LvFWZk7p/j7R6wFJ1i5SQEh6zW8B+BmA6wF8QL+BiLYQ0c1ENENEc/rVMhFdQUSPE9EKET1GRC9RX28wiRDR9UT0x+rjS4loioiuIaITAL5GRMNEdKv6HQvq4826948Q0deIaFrdvld9/VEieqtuvzARzRLRBcYfqI7zLbrnIfX7XkJEMSL6V/X3LRLRA0S03u1BJKIPEtF/ENGXiGiJiJ4gol/Rbd9ERLcQ0TwRHSaiK3TbgkT0+0T0tHo8HySiLbqP/y9E9JQ6vi8TEanvO5eIfqh+3ywR/bvbcUv6GykgJL3mtwB8Q/27TEyORBQEcCuAIwC2AZgAcKO67dcBfFZ97wAUzWPO4fdtADACYCuAD0O5B76mPj8LQB6A3mzzLwASAF4IYB2Av1Bf/zqA39Tt9yYAx5l50uQ7bwDwXt3zywDMMvMvoAjFQQBbAIwC+Kg6hna4CMDTAMYAfAbAzUQ0om67EcAUgE0Afg3A/yGi16nb/qc6vjdBOZ6/AyCn+9y3AHgZgPMBvEsdPwD8EYA7AQwD2Azgr9sct6RfYWb5J/968gfgEgBlAGPq8ycA/A/18cUAZgCETN63D8CVFp/JAM7VPb8ewB+rjy8FUAIQsxnTiwEsqI83AqgBGDbZbxOAFQAD6vObAPyexWeeq+6bUJ9/A8Cn1ce/A+B+AOc7OF7PAVgFsKj7u0Ld9kEA0wBIt//PAbwfivCpAkjrtu0BcL36+BCAt9scz0t0z78J4FPq468D+AqAzb2+luSfN39Sg5D0kg8AuJOZZ9Xn/4a6mWkLgCPMXDF53xYoK+V2mGHmgnhCRAki+nsiOkJEywB+BGBI1WC2AJhn5gXjhzDzNID/APBOIhoC8EYoE38TzHwYwOMA3kpECSgaz7+pm/8FisC7UTVj/QkRhW3Gv5uZh3R//6DblmFmffXNI1AE2Sb1d6wYtk2oj1sdzxO6xzkAKfXx7wEgAD8nooNE9Ds2nyE5DTkdnXSSNQARxaGYK4KqPwAAolAm518CcAzAWUQUMhESxwCcY/HROSgmIcEGKKYVgbF88f8CsAPARcx8goheDGASysR3DMAIEQ0x86LJd/0zgN+Fch/9lJkz1r9YMzMFADymCg0wcxnA5wB8TnXY3w5lRf9PNp9lxQQRkU5InAXgFiiaxQgRpXVC4iwAYrzieD7q5suY+QSAKwCAiC4BcBcR/Uj8Nsnpj9QgJL1iNxSzx04oZp0XA3gBgB9D8S38HMBxAF8koqTqzH2V+t5/BHAVEb2UFM4loq3qtocA/IbqeH0DgNe0GEcais1/UbXXf0ZsYObjAL4P4G9UZ3aYiH5Z9969AF4C4Eoo5hY7bgTwegD/FXXtAUT0WiLapWosy1BMbrUWn2XFOgD/TR3nr0M5nrcz8zEoZqw96nE8H8CHAPyr+r5/BPBHRHSeejzPJ6LRVl9GRL+uc+gvQBG+7Y5d0odIASHpFR8A8DVmPsrMJ8QfFAfx+6Cs4N8KxX5/FIoW8G4AYOZvAfgClIl2BcpELZyxV6rvW1Q/Z2+LcfwlgDiAWSjRVHcYtr8fyqT9BIBTAP672MDMeQDfBrAdwM12X6IKm58CeCUAfbTPBij+i2UoZqgfQjE7WfE9asyD+I5u238COE/9LV8A8GvMLJz374Xi7J8G8B0An2Hmu9Rtfw7Ft3CnOo5/gnJMWvEyAP9JRKtQNJUrmfkZB++TnCZQo8lSIpG4gYg+DeB5zPybLXf2dhwfBPC7zHxJL8chWVtIH4RE0iaqSepDULQMiWTNIU1MEkkbqIlmxwB8n5l/1OvxSCReIE1MEolEIjFFahASiUQiMWXN+CDGxsZ427ZtvR6GRCKRnFY8+OCDs8w8brZtzQiIbdu2Yf/+/b0ehkQikZxWENERq23SxCSRSCQSU6SAkEgkEokpUkBIJBKJxBQpICQSiURiihQQEolEIjHFsygmIvoqlE5Up5j5RSbbCcBfQelilQPwQVY6bIGIPgDgf6u7/jEz/7NX4zRj72QG1+07hOnFPDYNxXH1ZTuw+4IJy9fdfEa/7uv2WHixLwDHx93Nvm5+s9v3ezU2N5/rJ924N7z6PqDz68fP8Xr5fd3Cs0xqtSzyKoCvWwiINwH4JBQBcRGAv2Lmi9T6NvsBXAilfPCDAF5q1rRFz4UXXsjdCHPdO5nBtTcfQL5c1V6Lh4N450sn8O0HM02v77l8V9MJsfqMft3X7bHwYmzhAAEElKvc8H6z4+5mXze/2e0x6/R3dOP4uDmfndKNe8Or7+vG9ePneL38PrefS0QPMvOFZts8MzGp9WnmbXZ5OxThwcz8MyiNYjZC6Xf7A2YWnbx+AOANXo3TyHX7DjUccADIl6v4xs+Omr5+3b5Djj+jX/e1ws+xlWvccMOK95sddzf7uvnNbo9Zp7+jG8fHzfnslG7cG159XzeuHz/H6+X3dfOa6KUPYgJKsTPBlPqa1etNENGHiWg/Ee2fmZnpyqCmF837xVvpWWb7W31Gv+5rhd9jM8ONfuvmHLnZrxvHshvXT6dj6JRu3Btefl+n+/o9Xq++r5vXxGntpGbmrzDzhcx84fi4aaa4azYNmfdJCRI53t/qM/p1Xyv8HpsZVsfdzb5Ov8/tMevG7+j0+LjZt1O6cW94+X2d7uv3eL36vm5eE70UEBkozdIFm9XXrF73hasv24F4ONjwWjwcxHsv2mL6unB4GT8jEgo43jcWdr5v1MXnOh2vFW5/RyToZt/GmyYcIIQNr1kddzf7uvnNbo+Z2W/uxtjcHB8357NTunFvuP0+s+u902vCq2PZi+Pjxefq6aWAuAXAb6k9cF8BYElty7gPwOvVHsDDUPr47vNrULsvmMCey3chFFAuoImhOPZcvgt/vHsX9ly+Szsh4nUzZ9DuCybwrgs3a89b7Xv16+sndMNAzHbf91+8VXve6nP3XL4L6s/AYDzk2nm1+4IJfMDF913+kgnH+75p10YASl/RiaE4rvv1X8J1v/ZLmBiKa6/pj7v+9Vb7JqOtz5HV791z+S6ko0pw31AibPv+3RdM4N0v2+zqd4iJqdXxedsvbbL8XMEnf+VcX6OYxPFJRBqPr/htxtc7HdvuCybwGy9X1oqdXhNm+44mIwCA8VS0a+M1O8fadenB8dlz+S4EA62vqbZhZk/+ANwApel8GYof4UMAPgrgo+p2AvBlAE8DOADgQt17fwfAYfXvt51830tf+lLuJhd94S6+6psPNb3+Z3ce4m2fupWL5art+790z1O89Zpb+ZV77m75XQ8dXeCt19zKW6+5lX/69Kztvv/wo6d56zW38ks+f2fLz2VmPu/3b+et19zK//Pfm3+LE255KMNbr7mVf/3v7m+5r/jNZ197G5cr9sfnD/ce4Bd95g6u1WptjcuOv7n3MG+95lbOFsttvf9/f+cAb73mVr7+P55tue+//ecR3nrNrXx8Me/osz90/c/5sr/4Ycv9/uSOx/nsa2/jSrX5+MyuFPjsa2/jPbc/7ug7u80V//wAv/7Pm3/DZ777KJ//2X1d/a7/+/3H+Zxrb+N8qdLVz2Vm3v/cHG+95la+94mTXf3cl3/hB6Zzx5/feYi3XnNry3vDLRd94S6++lvt3d/MzAD2s8W86mUU03uZeSMzh5l5MzP/EzP/HTP/nbqdmfnjzHwOM+9i5v26936Vmc9V/77m1Rhtxo75XAkj6gpDz+ahOJiBE0sF28+YWlAcRfPZUsvvm8/V93luNmu773NzyvaVYqXl5xYrVZSqNQDAgcxiy/3NyJWU75ldKbbcd0bdp1pjHG9xfI7O53DWSALkwkbslHXpKADg1HLrMZuxmC8DALKl1sc4q56HeCTYYk+F9QMxnHJwLE8uF7EuHdVWh3pGU1H88nljuOWhDGo1/xt+zWVLGE013xupaAirxYpY5HWFA5klnLc+jVjY2fF1w1BC+Q1L6vnuBqVKDadWiqZ+AHHMFnLd+z5AiVwympq6xWntpPaKXKmKUqVmKiAmhpUTP7WYs/2MjBpJkC9XUTCEohlZ1AmIZ+daCIhZ5XtLlRpKlZrtvqsFZfIaToRx+NSqNtm7IVdSxj7jRECs1vc5Om9/fISA8IJxISAcjNkMcT7yJfvzpt8n4UJAzGdLKFbsP/vkcgHrBmKW23dfMIHppQIeeM4uktwb5rPmi6dULIRqjVEo21+XTmFmHJxexq6Jga58npGheBgAsOBgEeeUk8sFMKPBFCgYTSrX5Vy2vevSiny5ipjD688tUkCYIFb9w2YahBAQC/ahZFML9QlyIWd/Ac5nlRXFWCraUoN4Vrc920KLyBaVSegVZ4+ixsBj08u2+5shBMRKsdJS0M2sFLUbw05A1GqMqfm8ZwJi3UBUG087iPMljp8d2VIVkWAA4aCzW2mDOum30m5OLBWwQf0dZvzqzvVIRILY+9C0o+/tJnOrRc1+ryep+m5Wit1ZIU8vFTCfLWHXxGBXPs/IoCogFruoQYiFoZkGIYTq/Gr3BFK1xihVakiEvSmKIQWECWKCGEk03wQbB+MgAjI2AoKZMb2Yx5YR5SJZyNpfgIu5EgIE/NLmwQYBYKRQrmJ6Ka8JqdUWAkLcqBefMwpAUdfdohdCrSbc2ZUidk0MIhQgWwFxcqWAUrWGLV4JiLQ6Ca/Ym7msWFRNAPlya40rX6ogEXW+els/qIztxLL92E4uFzRhYkYiEsLrd67HbY9Mt9RGukm5WsNyoYKRZLPwEs59J4LVCQemlOv1RR4JiFAwgHQ0pJ3vbjCtCYjmczemmphmu6ixiEVbPOLNVC4FhAlzNhpEJBTAunRUWylYvb9QruFFm5QLu7UGUcJwIoKzx5M4MpeztCsfm8+BGdqKaqVgP4EJE9M54ymsS0e1G84NOZ2ZRW9CMmNmtYgNgzFsHo7jmI2AODavHDuvNIjhRBjhIHVgYlJ9EA41iIQL++96VSs4aSMg8qUqlgsVWxMToJiZlgsV3HeoO0miThDmmBELHwRQv+465dHMEoIBwgs2emNiAoChZLirPgghIDYO2mkQ3TMxiftT+iB8RLsJTAQEoNgX7TQIsU2sfFoJiMVcGUOJMLaNJVGs1HDcYvJ4RtUuxOe20iDE9mQ0hPM3D7alQej9FnYaRKFcxUqhgvF0FFtGErYCQmgXXgkIIsJ4KtqWk7pcrWnHLefQB5GIOlfvhVZgF+QgtAs7DQIALjl3DGOpCL77kG9pQtriyQ8T04HMEs5bl/LEQS0Yikda3p9umF4qYCQZMQ1aGEpEEKD6MewGQoPw6hhJAWHCfAsBsXk4YeukFv4JsdJv5QQTTr/to0kA1pFMzxkERCsfhJjoUtEQXjQxiMMzqy3fYyRbqmorQzsBIbaNp6I4ayRha2I6Op9DgLzNAh5PR1tqPGbozQ1OnPrZUsWxgxpQ7N7RUMBWgxDbNgzaC4hQMIC3nL8Jdz1+CsuF7kbGWDG3ai0g0rHumZiYGY9mljwzLwmGEuGum5jMzEsAEAwQhhORrgqIvGZikgLCNxZyJQQDhIGY+cpwYjiO44sFVC1MQRlVeNQ1CPsLcCGnmJi2jSkCwsoP8dxcFiPJCCbUC7BVqKsQEOmYokEwA48dd+eozhUr2Dys+F1sBYQ6GY+lIzhrJIGFXNly0jo2n8PGwXhTlnY3GU/HcKqFnd+MpXz95nWiQeRKVVcCgoiwYTCGkzbajRAQ622c1IK3v3gTSpUa7nj0hOMxdIKIwLEKcwWA1S5oECeWC5jz0EEtGIx338S0ycS8JBhNRTDXRRNTXpqY/Gc+W8ZwImIZoz8xFEelxpZO0MxCHuloCCPJCNKxUMtcCCEgNgzEEA0FLDWIZ2ez2DaaQCqqRF+0svWK7UlVgwCAR1z6IbKlKgbjYYwkIpi1ubDrGkRMMx1ZmZmOzuc0B75XrBuIthXFJIR5MhJ0pEHkShUkIu4iSNanY7ZOamF+Wt/CxAQAL94yhNFkGH+491Fs/9RteNUX78HeSe9MTnXtull4Jbvog/DaQS0YTkQawsw7gZmRWcjbasYjyYij3CinSA2iB8xnixhJhi23twp1nVrIa/kSrS5AZsZCtozhZASBAGHbaFJLhjPy3GwO28aSWimJ1mGuFRABiXAQ69IxrB+I4lGXfoi8ukIeT9tPuEJ4CB8EYC8gvPI/CNalo5jLllCuuovJF+aGieG4JxoEoEQy2ZmYTiwXkIwEkY5ZX4OC7z40jaV8BcVKDQwlzPLamw94JiTms0rEncgh0CNMTKtdMDE9mllCgICdHjqoAcXEtJQvdyXhcLlQQbZUNc2BEIymopqZrhtoAkJqEP6xoGoQVggBYeWozizWQ1GHkxHM25iYciUl23k4odxw28eSmjNaT75UxYnlAraPJpGMCGdgqzDXClKREAJqNu6uiSE8MuUuozpbqiARDWEsZW/TF8JjNBXRBISZHyJfqmJmpei5gBDJcm5vRuGw3DTkUEAU3QuIDQNRnFgqWGYcn1ouOtIeAKUnQKXmX5+IOTXiLmCS4R0NBRAKUFdMTAcySzh3XcqzlbFgMB5GjVtHBDph2iYHQjCa7LIPoiQ1CN+Zz5mXEhCIC8Aq1DWzkNdWEcOJsK2T2piUt20siWPzOVQMK1+hVWwbSyIQIKWsgQMTU1IXYbNrYhDPzGZbRj/pyRWrSDrQIGZWihhJRhAOBjAYD2MwHtbCWfWIBEKvciAE7eZCLDYICI9MTAMxFCs1LOfNP//EcsGxgPCjJ4CeudWiZfAGESEVa31dtoKZcSCz7Ll5CaiX21jMdz5p2+VACEaTUSzly641WyukD6IHLKirJCsSEcW/YGZiWsqXsVKsaCamkYR9GJ0waYjv2z6WQLnKmF5snNiEX2K76shORoOtTUylClI6R7twVB90YWYSE+B4OorZ1aLlqndmpaglAgGwjGTyOsRV0G49psVcGaGAEiZbKNcsAxEEbZmYBuyT5U4sFVpGMAn86AmgZ96iDpMgGQl1bGI6uVzE7GrRcwc1AE1z70YkkxAQdiYmkT/SrfIe0sTkM7UaY8GiUJ+ezcPxhnIaAvHa5mFlAhxORuw1CJG1rfo8tqmhrsaaTMLsJCKdRGE0O1YKFS2yBKg7/JzmQzCzNgGKCdPqO2dWi5pZB1AEgJkPwjcBMdBePaaFXBlDiYjm5zG2dNRTqtRQqXGDluaEDTbZ1MxK8INTDcKPngB65rIlraaQGelYqGMTk/CT+SEghhLdK7eRWSwgHCSMpayPz5g6r8x2yQ+h5UFIE5M/LOXLqDFsNQhATZYzUeOFX0JvYsqWqpblEIRJY0jTIMxzIZ6bzWI8HdUm/FQs7CjMVS8gxtNRbByMORYQpWp9AhxLK+OzMjPNrhYxrrsxtowkMLWQb1qBH53PIRkJthTAnSImMbeRTEv5EoYSYc1slLM5xsIE5Xb1JhLgzBzV89kSylV2FOIK1HsCdLvXgBVWhfoEThYurTiQWQIRsHOTtw5qABiMqyamLkQyTS/msXEwbuqfEWjZ1N3SIKSJyV/qK/rWAmJ6Md9kchFCY0LnpAasVVgtbFAVEOPpKJKRYFMuxHNzWS2RDgBSTkxMBgEBKFqEUwGRK9YrlY6nlEnNbMJlZsysNGsQpWqtaRI8Np/DFo/KfOuJhAIYSUZc+yCUAIWwZjayc1SLbUkXtZiAugP9pEk2tdMsaj27L5jAx193LgDgzv/xy54Jh0q1hsVc2fbeSEY7NzE9mlnCOeMp176ddhjqsonJzv8AKFFMQPcquubLVYSD5LhYpFukgDDQqsyGYGI4jkK51hSRkFnIIxYOaJmmQhOx8kMsZEsgAgbUsEEiwtbRZJOAeHY2h21jdbOMUyd1ypDsd/7EIJ6ZyWLFQeZtTlVfk6oPAjCvx7RarKBQrjUJCKA5kumoKiD8YF066trEtJgvYzAe0SYnu54QmgbhciKLhYMYToRNTUzCZ7LeoQ9CIDTW40veOKeB+uLJzgehOKk7m2wPZJZ8MS8B9XDd7gkIe9+PmBe6FeqaK1U9LUUiBYSBVmU2BMLHYHRUT6kRTGKFLASElUq5kCtjKB5uaAyzfbwxF2KlUMbsalHzPwBAKhp2UM3VRIPYrNx4Bx2U/hbmlbgaxQSYNw4SWsVYg4mpuew3M/uSAyEYb0dA5EoNGoRdTwhNg2jD/rt+wDwX4sSy8yQ5PaI4nDG4oZs4uTfSHZqYTi0XcGql6EsEE6Cr6NphFFOlWsOJ5YKtgxpQwmqDAeqaBlHwsFkQIAVEE2Klb1bJVY+4EIy5EJnFPCaG6xPgcNJ+hTKfa46Y2j6axNRCXguFOzKX014XKM5A6xuRmU1NTGJl5qSya1ZnQhFCzEyD0LKodRrEpqE4AtSYLDezWkShXPNVQDjphKdnIVfCcLLupM7aCAhRc6idGHRFQDSP7cRSAUT1KCynbFQ1Dk81CK0Ok/XYktFQR7WYDvjooBYMdqEe08mVImrcOnosoNZj6poPolz1NFdECggDonmPWS8IPcLHkDEU7css5htWESMtNIhFdULSs20siWqNtcn1WUMEE6BM2nbtHfPlKmqMJhPTWCqKTQ4d1UKDSKjJdmOpiKkPQkRk6AVEOBjApqHGst9el/k2si4dw8yKdWiukUK5ikK5hsF4GHG1AUvexsQk+kUk27CVbxgwL7dxcrmA0WTUtU15w2BM6VPioQahVXK1MzGpGkS7mcmPZpZBBLzQBwe1QCnY19mE7SRJTjCWinQtiilfkhqEr8xni4iFAy2l8mA8jHQs1KBB5EoVzGdLWhY1oEvEsbgA51WnqJ7tqq9BmJlERNO20UYTk117R30dJiOjqQhuO3C8Ze0eYUIR5harZLkZ1RE8bgjvM+ZCCGHhpw+ipDpWnaDPSamXM2mtQbjNgwAUH8PsarEpYerkcsFxBJOecFDpU3LcowQ5wKGJKdbad2PF3skM/va+w2AGXv8XP/K0ppSeoXik4zDXeg5Ea9NgN+sxSQ3CZ+azZVsVWs/EULzBByGEhV5AREIBpKIhTTMxsmhiYtJyIdT+08/OZbFxMNZwIaS0ujfmN6JWydUgIPZOZvD48RVUa9yydo+4yYXDdjwVNV35zKwWtVLGehQBUT8+Qljoj4+XuM2FEHbooURYO9Y5mzwIrR+1yzwIQNEgmJujwk4sF11FMOnZOBjHcZs+E50yt1oEkX0IuFawz6UfYu9kBtfefAAFtc+61zWl9Awlwljq0MSUsWkUZESpxyR9EKclig26dZE0QJno9LkQUxaZlHYqrFlcuagCKzQHpYprsmGflLrCbSUgjD4IN7V7jGGc1hqE0qPYGP+9ZSSB2dWiFu1zdD6HDQMxT6Mu9AiNxmkuhGgNO5QIa2YjuzwITYC28XusOsudXC64jmASbBqKeVZiA1BMTMaACiPienPbd+S6fYeakhK9rCmlZygR7rhp0PRiXrluHCwWulmPKSdNTP4y36LMhh7RWU7YuLUkOcMKeSQZ0UIE9eRLVRQrNc0MJSAinD1WD3V9bjbb4H8A0LLkt3jd6INwU7snW2zUIMZSSrkNo315drXU4H8Q1Mt+K5/tZwQTAK1lp9NcCNELYige0W46J3kQbnpSC9abJMsVK1XMZ0ttaxCbBuOYXmrOzekWrZLkgPr15rb4nd81pfQMxSMdV3SdXizY9oHQM5qMYKVQ6Uov8Xy56lkWNSAFRBNOymwINg8nsFKsaEXXphbyCAdJKxQnGE6Yl9swltnQs00VEEu5MhZyZc0vIRArNav2jisWGoSb2j15Ex9EpcZN9lpjkpzAmAtxzMccCEBXj8mpBiF8EMkwAgFCPGzfEyJXqiAYIETaSFLSym3oTEJaDkQbPggA2Dik5OZ0s0OanrlsSUv0siLVponJ75pSeoYSakXXDsJzneRACOr1mDo/TwWpQfjL/KoLDUL0hVAjmTJqqr1RBR9OhE27ygmhYdQgAMUPMb2Ux6GTK9pzPXVV3nwVkrUQEFdftgOxcONpt6rdky1VEQkGtIgaLRfCYD+dWSk2OaiBRgFRKCvlyr1uFKQnGQ0hGQk6LtgnzAxDavmFRCTYUoNIRIJtZYWPJCIIBwkndcLrZJs5EIJNqtCZ9ijUdT5bMm01qqddE9PVl+1oErRe1pTSI+6/TvwQSvSis/MmfJx2Dbickpc+CP8oVWpYKVZa3gQCYy5EZiFnmihjVbBvwaasx/axJJiBHz05oz3XU3dSm1/Umg/CYGLafcEE9rxjV8NvsKrdkytVGswnZjb9Wo0xu1rEmIkGMZQIIxUN4dh8DpnFPJj9C3EVKMlyDk1MuTKioXoEWyLaQkC00QtCEAgomqa+3IbIi3BaydXIxiFvk+UcmZii7ZmYdl8wgfdfvBUAQPC+ppQeLZu6zWS55UIZK4WKYw1ChAl3I5IpX27/GnSC98VOTiMWHSbJCeq5EHnt/6vPG2/abzgRwUqxgnK11hDfrvWCSJibmADg3kOnQNQcGqqp8hY3orhBjRoEALzjJZvxmVsO4vKXbMZn3/ZCy9+XK1UbYvyFENALiMV8GZUam2oQRIQtalXXYz5VcTUiciGcsGCIKEuEQ/YmpnK1rRwIwfqBaEMuhJZFnW7fSQ14kyxXVascOw02hUgAACAASURBVNUg2smmvuCsIQBKPanz1qfdD7JNRD2mVr3jrTiuCmTHAkKU2+gwm7qmhrnLUhs+4bRQn2A0GUEsHMDUQh7FShUnl4umIZxC4BgjJYy9IPSIrOmD08vYNBhvugjqN6K1iSkUIERD5qd4IB7GcovY71yp0hBaO24iIPStRs04aySOoz0UEOMuelMv5sraZAE40SAqHcWgbxhsTJY7uVxAJBRoGIMbxpJRhIPkiQaxkCuBufW9kWzTxARAy+mJhvyJchPUC/a1t6J3kyQH1E1MndZjKlS87SYHSAHRQH1F70xAEJEWySRWEaYmJouKkeL7Bk36+w4mwtr7zh5PNm2PhQMI2rR3XC0qhfqs7OPpWBjLLcwAWbWbnPaeaAjRUKCh3IZZmQ09IlnuyFwO0VDAcj+vcFOwr0lAOPBBdKJBrEvHGvwjJ5cL2DAQa7vSbSBA2DAY80SDmNeyqO3PXyQUQDQUaMvhK6J6jD4yr9F8EBYLpr2TGbzqi/dYJpZmHDQK0jMQDyEUoI5DXb0u9Q1IAdGAiCpw06tgYjiBzGK+qcy3HqtyG4u5EgbjYYRMomD2TmY0Nf0XRxaaLkoi+7ajq4WK7eSVjoVaVnQ1ttMkoqb6RmaF+vScNZJAsVLDg0cXcJYPZb6NrEvHsFqsOGofupgvaQ5qQAnvtVsJGzUst2wYVMYmzvOJpfayqPVsGox7Eho6p9Vhan1vOKk0bEaxRxrEoE1FV5HAl1nMWyaWTi/mlS6EDhc/RKSEvneoQXjdTQ6QAqKBedUm6EZAiGQ54ajeMtxsQrEqtzGfay6zAdQvynJVicvOlqqmWaUpm9r7q8WKVvbAjIFYqKUj0ayd5lgq6kqDEL6Th48t+m5eAszNYlYs5MoNSZKJSNC2o1yuVHXdC0KPyHcQoa4nXfSitmLTUNy1ianVChnQldmwqcMkSMXsBasVwmQS9VmDCAeVagdmyXJOEvimF/PYMBizTSA0MpqKduyD8LqbHCAFRAPzukxap0wMxTGfLeHJkysIkHkESr2LVOMKxaxQH+A8q1QREDYmJpusznQsbJlDIciVqk1lJIzZ1DOrRURCAQxYCCMhFGrsXw0mPU5zIZgZS7my1mEMEBqEvYAQRf3aQQiDU8sFMDNOqCamTtg4qJQRb9VLW+BkhQzUHapOFk/tdpWraxD+T0uDcfNyG04S+KYXC67zNbqRTZ0vKcernUx+p0gBoWMhV8JALOSqkqZwSj/w3DzWD8RM31uPkjBoEBZZ206zSlM2Jb9XixXbtP+BWEhL8LMiW6w09TowCohZNQfCynQ0MRyH2NQLDUKrx9QiFyJXqqJUrTVodIoPwr7URicahDAnnVguYLmgNF1qN8RVsHEojkqNHTvmnS5GhInJiX8uFW2tnZpRrNQQCQV8N0MCSnKkWcE+Jwl8xgrOThhNRTp2UmsmJqlB+IOTOG8j4sJ4dHrZ8iKJhYNIRIJNuRALFgLCaVapXXtH4aS2Ih0L25YLB4SJyaBBpKKYz5VQUauQzqyaZ1ELvn/gBMTt/qV7D/tWoVMgstpb5UJofUB05yOpmpisSjDkSp1V0tSyqZcLWpLcug41CJGs5TRZzuliZD6r+MucLJ5S0VBb1VwL5SpiPdAeALWiq4mJ6erLdjTZ+MNB0hL4qjVF82vVatRINyq6isWLDHP1iQULk48dorNctca2VUqHE831mBZyZdMyG2YXpVlWaTpq3d5xtVBpquTa8N5YCNUaW0bpMLPqpG7WIJjrNumZlaKlg1qYL8T8Op8t+VahUzAUDyMUoJYrauGgHNRpEPFICMx127ieSrWGUqXWURRTIhJCOhbCyaWC5ofo3MSkth516IdwuhiZz5Zs+0DoUdqOtqdBRH0q5GjEqmnQ7gsmsOfyXZoWHAkqUVq/unM9AOX6r9bYtYlpLBVVW/W2X4+pcLo7qYnoDUR0iIgOE9GnTLZvJaK7iegRIrqPiDbrtv0JER0koseJ6P+RD3rn3GrrRCAj69JK7DlgHsEkGE42XoCFchX5ctW0zIa4KCeG4rZZpXa23lYmpnRMmQitTAHFSg01bi5EJ4SBsOlb1WECeluhUxBQo0ta+SDMclKE+chMiIoy4J1msYrOckKD6FRAiIJxTkNdr75sh3b9CswWI3PZouN7o30fRLUn/gdAWUhY9YR47Y51YAaufePzceNHXoHVYhVf+49nAdRDXN0KiLpfsn0twg8Tk2eZ1EQUBPBlAL8KYArAA0R0CzM/ptvtTwF8nZn/mYheB2APgPcT0SsBvArA+ep+PwHwGgD3eTVeQNEg3HayCgQIGweVZLCJIWsbu7HNoF2ZDUAREq3KDFi1dxSagb2TWpREKJvavUUUinGFrEUFrRZRqdYwnzOv5Ar0tkKnHie5EPpeEAKtomuxCqQa989pzYI6u4VEZ7m6iamzMNeBuFJ/KuPwGO++YAJ/e99hHD6VRZUZ4SCZLkbmVkum+ThmtC0gKt5mBdsxnFBMTLUaN5WtF8Umt44m8JKzhvH6nevx9z98Bu+7aKuuUZB7JzWgHFejcNk7mcF1+w5pBQCvvmyH6VwgnNSnqwbxcgCHmfkZZi4BuBHA2w377ARwj/r4Xt12BhADEAEQBRAGcNLDsYKZ2/JB7J3MaNmwf3bnIUvzibgABXZlNpwinNRGG7mw/9qFuYptVslyxm5ygnW6sNH5rJJdayUgelmhU894OoZTJu099YgyC3oBITSwXLn5GOW0Zkrd0CAKOLFcwFAi3PEESUTYOBR3bGI6vpTHk6dW8fHXnoOP/PLZIBDefP7Gpv2Ue8OZ8EpFQyiUa03d8lpRrPRQg1Aruq6a+E6OzCtl988aUQTkVZftQLZUwd/+8GlNQGx0GVwgzHXGUFenUWXA6e+kngBwTPd8Sn1Nz8MALlcfvwNAmohGmfmnUATGcfVvHzM/bvwCIvowEe0nov0zMzMdDTZfVnozuPFBiJNZUrtgzdnY2I1OKbsyG04RPgajQ9Cu3ahgQE0OWrbwYdQFRHNPa0ApsSFW5eMWtmmnvhSvGU9HW1bOXDJUcgXqN56ZlmYlQN2yYVDRbk4sdR7iKtjoIpv6O5MZMAPvfOlmvHBiEKVqDU+qFYQFNYd1mAQiOMJtLkShXOuZgNCS5UxKcAsN4qxRxULwvPVpXP6Szbj+/ufw4JEFDMRCmsnWKVblNtyYZU97H4QDrgLwGiKahGJCygCoEtG5AF4AYDMUofI6Inq18c3M/BVmvpCZLxwfby6S5wYtEcjFhO3mZA4llNIWIvpH0yBcaix66nVvGsdgVepbz0CLxi5atzSDDyIeCSIVDWFmpaglzFlpEE59KV6zLh3FXLYeeWXGQq6MZCSIiG6CEua1vJkPwkKAumX9QAzVGuOx6eWOI5gESuOg1hoEM+OmB6fwsm3D2DqaxK6JQQDAwcxyw36L+TJqDuowCcR16TbUtVip9szEpCWzmlR0PTqXw2gy0nA//ff/ch6q1RrufOwklgsV297uZoxYVHR1Y5YV/UiMPqRu4mU11wyALbrnm9XXNJh5GqoGQUQpAO9k5kUiugLAz5h5Vd32fQAXA/ixV4N10pDdiJuTKT53MV/GWCpqGlbplsaS3/XJZcWi1LeeupPaQoNQhY5ZlI7IhdCyqFPWE5sTX4rXrBtQIq9mV0uWeQYLuVJTwIDQDsxCNq0EqFtEstz0UgGXnDfW0WcJNg3FMbNSVE021uN76NginpnJ4sOvPhsAsHUkgVQ0hAOZJbzrZfVbV1QYcBrFZKXZtqJYqdkuarzEql4aAByZy2nag2D/cwsAESC6SaqmIACOrvd0NIRIMIBZg4lp01Dc1H9k3tCrhni4vX4kTvFSg3gAwHlEtJ2IIgDeA+AW/Q5ENEZEYgzXAviq+vgoFM0iRERhKNpFk4mpm7SzondjYzeW21hoI2vbSNpipSZMTK3CXM3eK7CzsY+nGgXEWLp9IecHTnIhlgyF+oD6bzfTIIzd9tpFb1bqmolJjck/uWRvVvv2L6YQCwfwJtXnEAgQXrhpAI9OLzXsV6/D5NAHIRYuLjWIQtleoHmJVtHVJJLp6HwOWw1JntftO9SUre4mQs+qHtPVl+1oKtlhZZbNl73XuDwTEMxcAfAJAPugTO7fZOaDRPR5InqbutulAA4R0ZMA1gP4gvr6TQCeBnAAip/iYWb+nldjBVpHFZnhxsZeL9hX1r4v7TJr20grE5OdDyIeDiIYIGsNwmYCFDb92dUiUtFQx2YWr3FSj8nYCwLQHV8zDcIiysst+tpL6zvMohaIUFe7ZLlCuYpbHprGZS/cgAGd/fxFE4N4/PhygzluzqV2rZmYXPoglCimXvkgRFe5xgm7VKlheinfVAWgGxF6o6nmchu7L5jAep3J1s4sWyhXEY94e7w8vbOZ+XYAtxte+7Tu8U1QhIHxfVUAH/FybEbExO3GByFOmpOQNGO5DbMJyS31nhCNk7xVP2o9RIS0TbkNMSmaCZmxVETTIMYcmh16iZN6TIv5staRTRC30yC6FEEyloogQEqtqnYbBRkRGoTdZHX346ewXKjgnS/Z3PD6rolBFMo1PD2TxY4NStOeOa3Ut0sTk1sBUa71TIMQTmpj06CphZzSCdHQ8teNKciKEZN6TAvZEqaXCoiHgyhUqvjx7722KexWkC9VkeigFpgT+nvp5yML2RKCAbINDTXDqY1drL5EuY35rPusbSNWZiLNxNTitwzEwi19EFYaxHKhgqmFvO/9HdpBS+6zqce0aFJZVxRBM4tiytr4aNxw6yPHtcfXfucAVouVjn029WQ5a5Pat38xhQ0DMbzq3Ea/x4smlDygRzNLmoCYd1GHCejAxFSp+l7JVRAJKRVdjT6II7ocCD1XX7YD1958oCFIxW2E3lgqimdnsw2v/eyZOQDApTvG8f1HTyBbqlhGSOXLVU8ruQK9j2LqG+bVFb2VtO4UcXOJchtmE5JbrNo7rjowMQGiJ0SrPAhzJzUAHDqxcloIiEgogJFkxNIHUasxFnONvSAAIBQMIBIKmOZB5EsVEHXW3MZYimRmpdiVUiTxSBDDibCpBrF3MoNX/J+7cc8Tp7BarOB7D083bN8+lkIiEsSBTN0PMZ8tIh0LNUR42ZG0uC5bUfS4fWYrBuPhpigm0QnR6IPoRoSeWT2m+5+eQyIS1AS3XSRYvlRF3GOBKjUIlfnVkmldpG4RjwQRCwe0Fcp8toTz1qVavMseq/aO2WIF0VCgpX/DXkAon2FW414IhXy5atqLuh8RjnUzVooV1Ng8YCAZCWralJ5sqYpEhxEkdmHSnWoRGwfjTRqEEEjiO1eLlabIm2CAsHPjAA7qHNVz2ZJlvS0zhFblRkAwc08T5QDl/BtLfh+ZyyEWNu+E2GmE3mgqglypqkz0qiZw/9OzePn2EW1BaSsgylXPTbxSg1CZ74JPoBUjunIbVr0g3BAJKStcozNwpUWzIIHSdtTcxKSUsjb/DH1Y6+mgQQBKqKuVD0JElpnVxUpEQua1mEpVxDs0L3lZikRpHNT4OU7zdl40MYiD08talM7cqrsKA8EAIRkJujIxlauMGvemF4RgKBFuKsl/ZC7nWSdErdyGGup6armAp2eyeOU5ow2lcKzIlzurJuwEKSBUFtoos+GWIbXcRqFcRbZU7djEBIiKrs0+iFbmJaCFBlFs7iYn0Ie1ullZ9oq9kxnsf24BDx1bNE1oqme1N58Pq54QuQ57QQDeliLZNBRrEhBOBdKLJgaRK1U1+3g7JWiSLusx1ftR987ENJSINIW5Hp3PaiU2uo0xm/qnqv/hleeMtQxDBxQT02kb5nq60U6pb7cIm6M2IXXh+5SCfc0mJicJRwM2GoRZu1GBPh6+3zUIo1nFrLbNgq0GEbTWIDq8Ob0sRbJxMI7lQqVhkraqF2QUSHpHNaCYmNxWObZrZmVGsdK7bnKCIUNXOWZWciBGvWl0Zcymvv/wHAbjYbxg44DmmLa6PwElzLXTPJxWSAEBUWum7PomcMuQWnO+G1nUArPKmSuOBYR5sT9AMTFZ5TdEQgFttd3vAsKJWWUpb520qJiYrDSIzkxMXpYiEQ1sjuu0g4vPGW3az0wgnTueQjQUwKOZJa0Ok1sNIu1SgxB1hXoV5gqo92e+rDXRmlkpolCuedYJcSxZr2sGAPc/M4uLto8gGKCWpXAA1cTksQYhndRQpHS1xt77IJJK06CFbBcFhImZaLVQcVRdMh0LgxmmoXS5UtXShLJ3MqN954e/vh+feuMLel5OwwonZhW785GIBHFiuXkV16qculO8KkUitILppQLOW5/GarGCHz45g3PGkyiUq1ofZbO8nVAwgBdsHMCBzJJ2b7RlYnLhg9A0iB6FuQJKocZqjbFSrGAgFtZCXI1lNrqFXoM4Np/Dsfk8PvSq7QBa92thZikg/KKdOkztMJSIYClfxqxW1qNzH0QqGmoK38yW7NuNCvR2TjMBYTZhCpNNRdU6TiwXXdWg8RsnCU3C7jxgcswSUQsndbGqJeD1I2KBIDSIf/zxM5hdLeEfP/AyvHjLUMv375oYxN7JjLa6detrSkVDOJrNOd6/WBYmph6Guaoa5FKujIFYGEfnzENcu0UyEkQ0FMBctoSfPq36H9Tw1lg4gJBNpYNipQZmyDwIP9BMPl77IBLKiv2I6vxzk7VtRcrCSe1kdWu3SrFywvZDlzg3OLHzL+bKGIiFEDIJC06ELZzUZWsTXD+wfiAGIkWDmFkp4h9+9AzetGuDI+EAKH6IlWIFvzi6CMD94smtD0K0de2lBjGs1UtTJuUj8zkQ1dsKdxsiwmgygrnVEn76zBzGUhEt9F1UOrDSIER2v9QgfKCdMhvtIATQM6qAMHOKusXsRnTqg6g3DWpepWSLVdMJsF+6xDlFXw5FaBJ/8ObnN2g7iyaVXAWJqHkehF2UVz8QDgawLh3F9GIeX7rnKRQqNVz1eufO7xeppb9/9KTSZ8VrH4TQIGI99kEA9ZLfR+ey2DQYd5wg2A6jqSjmskU8fnwZF58z1hBOm46FLY9h3odeEIAUEADqNugRj5NOxArlmZlVpKLOM1PtMDqpS5UaSg7LJoumQWZqbK5UQdJkAuxGDRq/EXb+A1NLeOuXftIk+BZsstoTkSBy5SqYueHmtYvy6gf2TmYwny3hpgenAACvPGcEZ487T8w8b10akWAAP35qFoDzOkwC4YMwHjcrin2gQQwZ6jEdmc955qAWjCQjeOjYIhZzZbzSEESgaBDmJiY/uskB0sQEQFet0msNQhMQ2Y7KfOsxtnfUmgW59EHoqdUUB5jZBNgvXeLa4YWbBjCWiuLeQ43dB201iEgI1RprTlRAf3z6c30l/ETlaj067RdHFl2V8IiEAtixIa1FeLVjYqoYjpsdhXLvw1zrPghlPjjmYYirYDQV0UxaZgLCqiWwXyYmKSCg+CBi4YDn0lg4pVeKla45xFOGchtO6zAB1n2pC5UqmBUHrZF+6RLXDoEA4dId4/jhoVMN5awX8829IARmPSHE6q3TRDmvMPMTFSo1134iYWZKR0Ouncdpl/WY+iJRLl73QawWK5hdLWGLhxrE3skM7jx4AgAQJMIvjiw0bE9Fw9Y+CJ80iP5cAvnMfLbkufYANIZRdsP/ADQW7BtKRLQb0q5ZkGDAoqtcvVKp+cXXD13i2uV1z1+Hmx6cwuSxRbxs2wgAxcRoFXIs6gplSxXNhyRKoXdaasMruuUnqtQUIbpSVFpqWpWyN0Mr2FeoOIqA6odEuUgogGQkiMV8uR7B5JEGYUzgrDLj97/zKIhIO8YDsRCesDIxSQ3CPxa6UHrbCQldz+ORbpmYYo0rtVUXJialoB81rVLq3eT6cwLshEvOG0MwQLj3iVMAgGqNsVyoaP0AjJj1hBCPrQRor+lGCY+9kxnc8lC90qtZBrodVpWGrSj2QaIcIMrhlHFUq+LqTZkNJ9GAtlFMZX80LikgoBTq8zoHAlDbDKor1W4JpJRupab/78RJTURKuQ1D/Rm7bnKnOwOxMC7cOox7VAEhbOxWTmphRsrqBETWpldGP9ANP9F1+w41+Q/chDMbFy6tEN/Vq45ygsF4GIu5Eo7OK5GGXiXJOdHyRBSTyOzWU5BOan/YO5nBI1NL+PFTs6aF3LqNsHV3K2vbWHtf0yAcZvmarVI0DaJHDeS95rXPX4cnTqzg+FLetpIrAMTVjl36XIh8ub81rG74iTo1UxkXLq2om5h6K3SHk0q5jSNzOQzGw5aaZac40fLSMSVAwixRs1s90VvRn1e4Twg7oChrLNRowLusYKGpdEuDSHdgYlLe39xVrpUP4nTndc9fhy9+/wncd2gGz1uvdE2zclILDUKfC9HvGgTQuZ+o03BmLXjCJMnQjEK5CiIgHPSmYZdThuIRPLG07GmRPsBZRzp9Iqsx6MSvPIgzWoPoRVaw0By6Ueob6MzEBNhrEF6rr73ivHUpTAzFcc8TpzQNwkqjE0Igp7tO7LrtrRU6NVOlHBSb01Os1BALddaAqRsMJsJYyis+CC9zIJxoeXY9IcQ16LUPYu1e4Q7wOyt472QG9x1SbN+fveUgKlXuWFOxMjE57ZWcjoXw3GxjzZyc5oRdm5cHkRLu+p3JDF73/HUArDUIIQRyOlt63Ym/NgUo0JiBPr2YtyzsZ4VbJ3Wh3Lt+1HqG4mEs5MpYzJXxlvM3evpdrbQ8qzB0oK5xeR31tTZnAIf4mRVsDGubXS11xZxlvBFXi0oGtNPe2mZd5YRDNtGncf7d4HXPX4dv/OdRLQ7dOlFO1SBKJhrEGj4+QGdmqng4iAA1t8O1oliu9TTEVTCciGgmZ6+zqFuRtghDB0Q/au81rt6fkR7iZ1awV+asYICQ0LV3XC04q+QqGIg1J+PkXGohpyMXnzOKSCiAHz01iwBZ541oGkTJTINYu8enU4gIqah1mKaRYsX77mhOGNRpkl51knPKgE0kmFWlg25zRgsIP7OCvTRn6ds7rhbdNbJJq8X+qrqmQTmfknB6SSISwivOHkW1xhhKRCw1rkhIKbtspkGs5ePTDcyaWVlR6BMNYkgXteR1mY1W2Plx8mV/BOoZvwTyKyvYS3NW2iAgnGRRa+/VrVJESF+uVFFMBA7NVKcrY2rpk/lsyTZT2Nh2NFeqIhYOILjGj0+npGLN7XCt6BcNQpgaI8EANgy0brrlJXYmpoIPzYKAM1yD8BMvzVn6kt+rRfcmJqDxIszadJNbK+ydzOC2Aye053aZwsa2o0ql2zN+bdUSNxpEsdIfGsQvjswDAErVGl79J/d6nhdlRzKi+HHM+7VUfYky7P0ZOUPw0pylbxq0WnA3eWmREnndBFjs72Y43cBNpnAiGmzIpM4V/bk5T3eSLnwQhXK150lyeycz+Mu7ntKeuy0v0m3s/Dj5kjQxrTm8MmcloyHMq+0d3WoQZmpsv/c66AZufEKJSLChFlOuVJUahAPSsRBOLBVa7whFgxhJ9na9et2+QyhYLBp6VZzSLMoQUARqtwp+2iE1iDVAJz6IgXizI+xMEBBuCtolIo229GypIjUIB7g3MfX2mPZjt0Srgn19FcVERDcT0ZuJSAqUPkT4IJi5jSgm0aNC74Nw9xmnI258QolIsCFEOX8G+Gi6QTLa3C/dCsXE1NvppRtVcLvNgEkpHEAREP3kpP4bAL8B4Cki+iIR9X/7sDOIZFRZ4RbKNVRr7NLEZKJBFP25+HqJG59QskmDqGpF/CTWpKMhrJbMq5EaKVZqiPb4muvHbomWGkSphpgPGoSjq5yZ7wJwFxENAniv+vgYgH8A8K/MbN7VQuILqWgI5SpjLlsE4KxZkMBUQJTXvgYBOPcJxZvCXCtSg3BAKhYCs+qzaXE9FftAg+i0vIgXpGMhPHXKTEBUfFnEOZ4FiGgUwG8CeD+ASQDfAHAJgA8AuNSLwUmcISb5k8uKQ9DN5B4NKU2M9D0hcsW174NwQ9IkD0Ien9akoor50onZs1Cp9UUtpn7rlmhWbZmZfTMxOZpJiOg7AHYA+BcAb2Xm4+qmfyei/V4NTuIMEVFzYknRIJxWchUMxMINBcHOBB+EG+LGPIgzIAy4Gwgta6VQwfoB6/2YGSW1mqukEWFiYmat7lKpWkON/am27FRk/z9m3snMe3TCAQDAzBdavYmI3kBEh4joMBF9ymT7ViK6m4geIaL7iGizbttZRHQnET1ORI8R0TaHYz3jED6H40v5hudOGYiFtFVKtcYolGtr3gfhhmQkiHJVmcSYGTmfIkhOd4Rm2yqbWmsW1AcaRL+RioVQqXFDzk6hpDzuJyf1TiIaEk+IaJiIPmb3BiIKAvgygDcC2AngvUS007DbnwL4OjOfD+DzAPbotn0dwHXM/AIALwdwyuFYzziEz0GYmNJRd70m9I4wEa0jbex19H2pC+UamGWhPifoTUx2FMv90U2uHxFRhvpciLxP7UYB5wLiCmZeFE+YeQHAFS3e83IAh5n5GWYuAbgRwNsN++wEcI/6+F6xXRUkIWb+gfp9q8ycg8QUYQ46sVxUn7u7cPR2TlHJVU6AdcTxzZUrZ0QviG6hNzHZUayI5jdSgzAyYBJE4lc3OcC5gAiSrvC4qh20SuObAHBM93xKfU3PwwAuVx+/A0BadYY/D8Cimn8xSUTXqd/ZABF9mIj2E9H+mZkZhz9l7SFMSifaNDGlYyHNByFKSkgNoo4QBtliVddNTh6fVghN1rGJSWoQTZhGGaqLFD9KbTgVEHdAcUj/ChH9CoAb1Nc65SoAryGiSQCvAZABUIXiPH+1uv1lAM4G8EHjm5n5K8x8ITNfOD4+3oXhnJ6kNQ2iExOTqkGIdqMyzl9DaFP5UvWMaDfaLVI2/Qz0FNQVca/DXPsRs1I4BR9NTE6v8msAfATAf1Wf/wDAP7Z4TwbAFt3zzeprGsw8c6VPkAAAHExJREFUDVWDIKIUgHcy8yIRTQF4iJmfUbftBfAKAP/kcLxnFMIEcnKpiAC5V9X1TYNyUoNoIik0iFIF5ZpybNd6N7luIK6hlj4IVYPoh3Lf/YaZBpH30UntNFGuBuBv1T+nPADgPCLaDkUwvAdKNrYGEY0BmFc//1oAX9W9d4iIxpl5BsDrAMhwWgsSkSCIlPC3gVjIdRvCdCyMXKmKSrWmmQPkCrmO3kktGisl5GTWkmgoiEgw4EBASA3CCjMNQvgg+qkW03lEdJMabvqM+LN7DzNXAHwCwD4AjwP4JjMfJKLPE9Hb1N0uBXCIiJ4EsB7AF9T3VqGYl+4mogMACErWtsQEURYYqF9QbtA3DZI29maEhpYtVTQBKvNEnJGKta7HVNCimKSAMGKqQZSFU79PNAgAXwPwGQB/AeC1AH4bDoQLM98O4HbDa5/WPb4JwE0W7/0BgPMdju+MR9SNb8c0pL8INROT1CA0hCqfK1VRDSkahKzm6oxkNOhYg5AmpmZSkRCI0JDIWij1X5hrnJnvBkDMfISZPwvgzd4NS+IWoUG4zaIG6lrHUr5cD+OUNnYNLcy1WEG2KAWoG1LRsPM8CBnm2kQgQEhFQoZ+LSKQpH80iKJa6vspIvoEFJ9CyrthSdwiIkZSbZiY9D0h5ATYjDC35cpVVGpSg3BD2kHJ74Lmg5DH1AxjRdd8uf8yqa8EkADw3wC8FErRvg94NSiJe+oahPuLRt+XOleqgNqIhFrLREMBBEgpYih9NO5wZGIqiygmec2ZYfTj5H0MC265TFQT1N7NzFcBWIXif5D0GZ2ZmBp9EIlw0HUk1FqGiJCIhJRIrxojEgwgHJSTmRNSsTCOzNkXQZCJcvakY+GGhl4FtZJrIOD9PerE0VyFUtZb0sfUBUQ7UUyNGkRCRug0kYgEkStV1OMjJzKnpKIhrMhEuY5oMjGVqr6ZOJ3OBJNEdAuAbwHIiheZ+WZPRiVxTbIDE5PQIJZVH0RSmk+aSKg9ISo1ljkQLkhFgy19EHUNQgoIM9IGLcyvXhCAcwERAzAHJWFNwACkgOgT0pqT2v3qPxwMIBYOqBpEFXHpoG4iofaEqNQCUsNyQSoaRr6sJGGGLMxyhXIVoQBZbj/T0ZfCARQNwi9/jdNMaul36HM6MTEB9XIbuVJFahAmNGgQ8vg4RixYsqUqBuPmk1qxUpPagw36YpqAqkH0k4mJiL4GRWNogJl/p+sjkrhm72QGf3Pf0wCA6/Y9gUQk6LptorBzKjdye0JmLZOIhrCUL6NSlQLCKXsnM/jru58CALz+L36Ia9/4AtPrslipyiQ5GwZiYZQqNRQrVURDQcUH0Wcmplt1j2NQSnNPd384Erfsnczg2psPaKFvC7kyrr35AAC4EhLpWBjLhTJyxQo2DsQ8GevpTCIcxImlPKqhANal5fFphfG6PLlctLwuC2WpQdihjzKMpoLIl6sY8GkR5+isMPO3dX/fAPAuAJatRiX+cd2+Q9pNKMiXq7hu3yFXnyM0iFypKqN0TEhEg0o/iKJsN+oEN9dlsVJDVGoQlhjrMSlhrv4I1Ha/5TwA67o5EEl7TC/mXb1uxYDQIEoVmUVtQiKirNxyJSkgnODmuiyWq1KDsEH0dxGO6r6LYiKiFTT6IE5A6REh6TGbhuLImNx0m4birj5H74OQGkQzyUgI2WIF5VBAlkJ3gJvrsiA1CFuMGkTOxzwIpyamNDMP6P6ex8zf9npwktZcfdmOptVEPBzE1ZftcPU5A/EwlnJllCo1JGQ3uSbikSCKlZrUIBzi5rqUGoQ9KU1AKBpEoeSfU99pP4h3ENGg7vkQEe32blgSp+y+YAJ7Lt+FiaE4CMDEUBx7Lt/lPoopGkKpqiQsyW5yzQizW7XGsheEA8R1uXFQcegPxEKW12WxUpNRTDbUa6UpGkTfmZgAfIaZvyOeqG1BPwNgrzfDkrhh9wUTrgWCkbQuwU6aUJrRq/R+3ZynO7svmMDbX7wJ5/7B9/H+i7daXqOFchXj6ajPozt90JuYytWar7k4TvU6s/3kLLKG0HeikxpEM/pjIo+Pc0S3wxWbchslqUHYIpJgVwoVX7vJAc4FxH4i+nMiOkf9+3MAD3o5MIm/6DUIuUJuJq7zy8hSJO4wFpszIjOp7QkFA0hEglgplH3tJgc4FxCfBFAC8O8AbgRQAPBxrwYl8R994o20sTfToEFIJ7Ur0rFwQy0hIwXppG6JELJCg+grHwQzZwF8yuOxSHpIow9CToBG9MdEdpNzh7GWkBHppG6N6AkhGlb5JSCcRjH9gIiGdM+HiWifd8OS+M2AzgchndTN6I+JTCR0hygEaYVSY0hqEHYYNYhYn5mYxph5UTxh5gXITOo1hdQg7NEfE3l83DFgKFetp1pjlKssu8m1QKmVVtF8EH71JHEqIGpEdJZ4QkTbYFLdVXL6om9VKn0Qzeg1CNkPwh12TupiRUTlSA3CDtETQvNB9FO5bwB/AOAnRPRDAATg1QA+7NmoJL4jIiVkprA5DRqEtJe7Ih0LY7VYATM39TovlmU3OScM9MhJ7bTUxh1QqrceAnADgP8FwF01OEnfMxALI0DyZjVDf0PKWlXuSMdCqNZYc7DqKagahKzFZI+SS1JGvtSHeRBE9LsA7oYiGK4C8C8APuvdsCR+s3cyg9nVImoMXPJ/78XeyUyvh9RXBAKEeDiIYIAQka0xXZE2lIrQIzQIaWKyJx0Lo1CuadFg/ZYHcSWAlwE4wsyvBXABgEX7t0hOF0Rzl0pNcStlFvO49uYDUkgYSEaDSESCTWYSiT1pQ7E5PcWKMDFJDcIOcQxnVooA+szEBKDAzAUAIKIoMz8BwF25UEnf0q2mQ2uZvZMZLOTKWClU8Kov3iOFpwvE5GaWC1FQrztp1rRHaGGnVgoA+ixRDsCUmgexF8APiGgBwBHvhiXxk241HVqrCA2ratCwAHdtXc9U6iYmaw1CJsrZo9cgoqEAAgF/tFinTup3MPMiM38WwB8C+CcAstz3GsGquZDbpkNrFalhdcaAoeGNHhHmKjUIe4SAOLVc9DWT3/VZYeYfMvMtzFzyYkAS/+lW06G1itSwOsPOSV0oSx+EEwZ0JiY/i2lKsS3pWtOhtYrUsDrD3kktE+WcII7hQq7sq4CQKaESAN1pOrRWufqyHbj25gMNZiapYTknEVHCg+3CXKUGYY++X4uf/hopICSSFgjBed2+Q5hezGPTUBxXX7ZDClSH1JsGNWsQ9UQ5qUHY0ataaVJASCQOkBpWZ1jVY9IS5aQGYUs4GEAsHEChXOtvJ7UbiOgNRHSIiA4TUVM/CSLaSkR3E9EjRHQfEW02bB8goiki+pKX45RIJN6iVCOVGkQnpKKKmclPE5NnZ4WIggC+DOCNAHYCeC8R7TTs9qcAvs7M5wP4PIA9hu1/BOBHXo1RIpH4g1XTIKFByPIlrRHhwmsliunlAA4z8zNqSOyNAN5u2GcngHvUx/fqtxPRSwGsB3Cnh2OUSCQ+MGBlYqrUEPEx8et0Jr3GBMQEgGO651Pqa3oeBnC5+vgdANJENEpEAQB/BqUwoCVE9GEi2k9E+2dmZro0bIlE0m2s+lLLftTOEZFMa8YH4YCrALyGiCYBvAZABkAVwMcA3M7MU3ZvZuavMPOFzHzh+Pi496OVSCRtYemkrtRkiKtDNA1ijUQxZQBs0T3frL6mwczTUDUIIkoBeCczLxLRxQBeTUQfA5ACECGiVWZucnRLJJL+Jx0LmTYNKlaqMknOIb0wMXkpIB4AcB4RbYciGN4D4Df0OxDRGIB5Zq4BuBbAVwGAmd+n2+eDAC6UwkEiOX1Jx8Ja0yB9S9tiuSZNTA7RTExrwQfBzBUAnwCwD8DjAL7JzAeJ6PNE9DZ1t0sBHCKiJ6E4pL/g1XgkEknvSFsU7CtWqtLE5BBxDGNrxMQEZr4dwO2G1z6te3wTgJtafMb1AK73YHgSicQn9CW/NwzGtNeLlZo0MTlkTWkQEolEIhiwaBqkRDFJDaIVeycz+Ot7ngIAfOG2x3xrWCVLbUgkEs+xahpUrNSQTMppyA7RsEoUi1zIlX1rWCU1CIlE4jlWTYOK5Zqsw9SCXjaskgJCIpF4jlXToEKlKuswtaCXDavkmZFIJJ5j1TRIhrm2ppcNq+SZkUgknmPVNEhJlJMmJjt62RJYeockEonnWDUNKkgNoiW9bFglBYREIvEFYz0mZpaJcg7pVcMqKbolEokvKE2D6gKiUmPUGDJRro+RZ0YikfiCokHUTUwFNXRTahD9ixQQEonEF4xNg4oVtR+11CD6FnlmJBKJLxj7UgsBITWI/kUKCIlE4gtGJ7VmYpIaRN8iz4xEIvEFfdMgQEmSA6QG0c9IASGRSHxB3zQIUJLkAKlB9DPyzEgkEl8wNg0qaBqEnIb6FXlmJBKJLxhLfgsNQpba6F+kgJBIJL6QNjQNqkcxyWmoX5FnRiKR+MKAoaKrTJTrf6SAkEgkvmDsCSET5fofeWYkEokvGJ3URalB9D1SQEgkEl9odlKrPgipQfQt8sxIJBJfSEaCCJCJiUlqEH2LFBASicQXiAjpWLjBSU0EhIPU45FJrJACQiKR+Ia+HlOxonSTI5ICol+RAkIikfiGvmlQsSz7Ufc7UkBIJBLf0DcNkv2o+x95diQSiW8MNJiYZD/qfkcKCIlE4hvpWBgrxXqYq0yS62/k2ZFIJL6hd1IXylKD6HekgJBIJL4hBAQza1FMkv5Fnh2JROIb+qZBiolJahD9jBQQEonEN/T1mBQTk5yC+hl5diQSiW/o6zEVKzVZh6nP8fTsENEbiOgQER0mok+ZbN9KRHcT0SNEdB8RbVZffzER/ZSIDqrb3u3lOCUSiT/omwYVK1VZh6nP8UxAEFEQwJcBvBHATgDvJaKdht3+FMDXmfl8AJ8HsEd9PQfgt5j5hQDeAOAviWjIq7FKJBJ/0DcNKpSlBtHveHl2Xg7gMDM/w8wlADcCeLthn50A7lEf3yu2M/OTzPyU+ngawCkA4x6OVSKR+IC+aVBRhrn2PV4KiAkAx3TPp9TX9DwM4HL18TsApIloVL8DEb0cQATA08YvIKIPE9F+Ito/MzPTtYFLJBJv0DuppQ+i/+n12bkKwGuIaBLAawBkAFTFRiLaCOBfAPw2M9eMb2bmrzDzhcx84fi4VDAkkn5HaBDLwkktNYi+JuThZ2cAbNE936y+pqGajy4HACJKAXgnMy+qzwcA3AbgD5j5Zx6OUyKR+IRoGjS3WgQAGeba53h5dh4AcB4RbSeiCID3ALhFvwMRjRGRGMO1AL6qvh4B8B0oDuybPByjRCLxESJCKhrC7GoJAGSiXJ/jmYBg5gqATwDYB+BxAN9k5oNE9Hkiepu626UADhHRkwDWA/iC+vq7APwygA8S0UPq34u9GqtEIvGPdCyMWalBnBZ4aWICM98O4HbDa5/WPb4JQJOGwMz/CuBfvRybRCLpDelYCDMrUkCcDsizI5FIfGVAp0FIE1N/IwWERCLxlXQshLms4oOQGkR/I8+ORCLxlXQsBGblsdQg+hspICQSia+IXAhAahD9jjw7EonEVwbi9diYqNQg+hpPo5h6TblcxtTUFAqFQq+HsuaIxWLYvHkzwuFw650lEh16DUL2pO5v1rSAmJqaQjqdxrZt20BEvR7OmoGZMTc3h6mpKWzfvr3Xw5GcZoh6TABkqY0+Z02L70KhgNHRUSkcugwRYXR0VGpmkraQPojThzV/dqRw8AZ5XCXtotcgZBRTf7PmBYREIukvBhpMTHIK6mfk2dGxdzKDV33xHmz/1G141Rfvwd7JTOs3Of3svXtBRHjiiSe69plesmfPHpx77rnYsWMH9u3b1+vhSNYQ0sR0+iDPjsreyQyuvfkAMot5MIDMYh7X3nyga0LihhtuwCWXXIIbbrihK59nRrVabb2TAx577DHceOONOHjwIO644w587GMf69pnSyTCxBQKEEJBOQX1M2s6iknP5753EI9NL1tunzy6iFK1sSdRvlzF7930CG74+VHT9+zcNIDPvPWFLb97dXUVP/nJT3DvvffirW99Kz73uc+hWq3immuuwR133IFAIIArrrgCn/zkJ/HAAw/gyiuvRDabRTQaxd13341vf/vb2L9/P770pS8BAN7ylrfgqquuwqWXXopUKoWPfOQjuOuuu/DlL38Z99xzD773ve8hn8/jla98Jf7+7/8eRITDhw/jox/9KGZmZhAMBvGtb30Ln/vc53D55Zdj9+7dAID3ve99eNe73oXHHnsM73nPexCNRrF9+3ace+65+PnPf46LL77Y6eGWSCwRGoTUHvofeYZUjMKh1etu+O53v4s3vOENeN7znofR0VE8+OCD+MpXvoLnnnsODz30EB555BG8733vQ6lUwrvf/W781V/9FR5++GHcddddiMfjtp+dzWZx0UUX4eGHH8Yll1yCT3ziE3jggQfw6KOPIp/P49ZbbwWgTP4f//jH8fDDD+P+++/Hxo0b8aEPfQjXX389AGBpaQn3338/3vzmNyOTyWDLlnqvp82bNyOT6Z65TXJmI5oGySS5/ueM0SBarfRf9cV7kFnMN70+MRTHv3+ks5XzDTfcgCuvvBIA8J73vAc33HADnn32WXz0ox9FKKScgpGRERw4cAAbN27E/2/v/oOsrOo4jr8/LWu7KbIphrkLsqRDu8gvQ8bCRiJLSwfRNfqhqE1DzqCMzpShTZk542g6k/QHM9GURmIsAkJO/WHyYyz/SFiRwlwZTGB2iYREE1zc3OXbH8+5sNBVF3aXu/fu5zXD3Puc++zd813Ovd/nnOd5zrngggsAOPXUUz/wvcvKymhoaDi0vW7dOh544AHa2trYu3cvY8aMYerUqezcuZOrrroKyG5yA7j44ouZM2cOe/bsYcWKFTQ0NByqj1lfyS0aVOEeRL/nb4Pk9ktHc+cTmznw7uGx9sryMm6/dHSP3nfv3r2sXbuWzZs3I4nOzk4kHUoC3TFo0CAOHjzck+l6/0FFRQVlZWWHyufMmUNTUxPDhw/n7rvv/sB7Fa6//noWL15MY2MjjzzyCADV1dW0tLQc2qe1tZXq6upu19fs/ax6YSdvt3fy1jsdTLl/LbdfOpoZE92++iOn8GTGxGruu3os1VWViKzncN/VY3vccJcvX86sWbPYsWMH27dvp6WlhdraWsaPH8/ChQvp6OgAskQyevRodu3axYYNGwDYt28fHR0djBw5kk2bNnHw4EFaWlpYv3593t+VSwZDhw5l//79LF+ercU0ePBgampqWLVqFQDt7e20tbUBcOONNzJ//nwA6uvrAZg+fTqNjY20t7ezbds2tm7dyuTJk3v0dzCDwxeDdKbpXHv7YhDrXe5BdDFjYnWvH8ksWbKEefPmHVHW0NBAc3MzI0aMYNy4cZSXlzN79mxuueUWli5dyty5czlw4ACVlZWsXr2aKVOmUFtbS319PXV1dZx//vl5f1dVVRWzZ8/mvPPO48wzzzyil/Loo49y0003cdddd1FeXs6yZcsYNWoUw4YNo66u7tCJaoAxY8Ywc+ZM6uvrGTRoEAsWLDjUSzHriQef2nJELx2yi0EefGqLexH9kCI3MXuRmzRpUjQ1NR1R1tzcTF1dXYFqVBza2toYO3YsGzduZMiQIcf0s/772rGqveMP5PvGEbDt/stPdHUMkPR8REzK95qHmAaw1atXU1dXx9y5c485OZgdj7Oq8l+V917lVlgeYhrALrnkEnbs2FHoatgA0lcXg1jfKPkEERGeWK4PlMrQpJ1YufMMDz61hX++eYCzqip9FVM/VtIJoqKigtdff91Tfvey3HoQufspzI5FX1wMYn2jpBNETU0Nra2t7Nmzp9BVKTm5FeXMrHSVdIIoLy/3imdmZsfJVzGZmVleThBmZpaXE4SZmeVVMndSS9oD9OSi/qHAv3upOv2NYytepRyfY+sfzo6IM/K9UDIJoqckNb3X7ebFzrEVr1KOz7H1fx5iMjOzvJwgzMwsLyeIw35R6Ar0IcdWvEo5PsfWz/kchJmZ5eUehJmZ5eUEYWZmeQ34BCHpMklbJL0i6Y5C16enJD0sabekF7uUnSbpaUlb0+NHC1nH4yVpuKR1kl6S9HdJt6byoo9PUoWk9ZL+mmL7cSqvlfRcap9LJZ1U6LoeL0llkl6Q9Pu0XUqxbZe0WdImSU2prOjb5YBOEJLKgAXAl4B64OuS6gtbqx77NXDZUWV3AGsi4lxgTdouRh3AdyKiHrgQuDn9f5VCfO3AtIgYD0wALpN0IfAT4KGIOAd4A/hWAevYU7cCzV22Syk2gM9FxIQu9z8Ufbsc0AkCmAy8EhGvRsR/gUbgygLXqUci4k/A3qOKrwQWpeeLgBkntFK9JCJ2RcTG9Hwf2ZdNNSUQX2T2p83y9C+AacDyVF6UsQFIqgEuB36ZtkWJxPY+ir5dDvQEUQ20dNluTWWlZlhE7ErP/wUMK2RleoOkkcBE4DlKJL40BLMJ2A08DfwDeDMiOtIuxdw+5wPfAw6m7dMpndggS+Z/lPS8pG+nsqJvlyW9HoT9v4gISUV9bbOkU4AVwG0R8VbX1QKLOb6I6AQmSKoCVgKfLHCVeoWkK4DdEfG8pKmFrk8fuSgidkr6GPC0pJe7vlis7XKg9yB2AsO7bNekslLzmqSPA6TH3QWuz3GTVE6WHB6LiCdSccnEBxARbwLrgE8DVZJyB3LF2j6nANMlbScbxp0G/IzSiA2AiNiZHneTJffJlEC7HOgJYgNwbrqa4iTga8CTBa5TX3gSuCE9vwH4XQHrctzSuPWvgOaI+GmXl4o+PklnpJ4DkiqBL5CdY1kHXJN2K8rYIuLOiKiJiJFkn7G1EXEtJRAbgKSTJQ3OPQe+CLxIKbTLgX4ntaQvk42PlgEPR8S9Ba5Sj0haAkwlm274NeBHwCrgcWAE2ZToMyPi6BPZ/Z6ki4A/A5s5PJb9fbLzEEUdn6RxZCcyy8gO3B6PiHskjSI76j4NeAG4LiLaC1fTnklDTN+NiCtKJbYUx8q0OQj4bUTcK+l0ir1dDvQEYWZm+Q30ISYzM3sPThBmZpaXE4SZmeXlBGFmZnk5QZiZWV5OEGb9gKSpuVlOzfoLJwgzM8vLCcLsGEi6Lq3bsEnSwjTB3n5JD6V1HNZIOiPtO0HSXyT9TdLK3HoAks6RtDqt/bBR0ifS258iabmklyU9pq6TTJkVgBOEWTdJqgO+CkyJiAlAJ3AtcDLQFBFjgGfI7l4H+A0wLyLGkd39nSt/DFiQ1n74DJCb8XMicBvZ2iSjyOYwMisYz+Zq1n2fBz4FbEgH95VkE7AdBJamfRYDT0gaAlRFxDOpfBGwLM3ZUx0RKwEi4h2A9H7rI6I1bW8CRgLP9n1YZvk5QZh1n4BFEXHnEYXSD4/a73jnr+k6D1En/nxagXmIyaz71gDXpDn/c2sOn032OcrNSvoN4NmI+A/whqTPpvJZwDNpJbxWSTPSe3xY0kdOaBRm3eQjFLNuioiXJP2AbOWwDwHvAjcDbwOT02u7yc5TQDbF889TAngV+GYqnwUslHRPeo+vnMAwzLrNs7ma9ZCk/RFxSqHrYdbbPMRkZmZ5uQdhZmZ5uQdhZmZ5OUGYmVleThBmZpaXE4SZmeXlBGFmZnn9D7Lur9rmMKgTAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate on F1 meassure"
      ],
      "metadata": {
        "id": "zYMy2JARxEBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    acc_0 , acc_1, acc_2, acc_3 = evaluate_one_choral(model,val_dataloader,part_dic,F1=True)\n",
        "    acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "RWxVG3XAYTcC",
        "outputId": "ce5d25ad-1847-4c2b-93f7-eaea578b0d20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1_v0 , sample 0: 0.9666666666666666\n",
            "f1_v1 , sample 0: 0.9663865546218486\n",
            "f1_v2 , sample 0: 0.9313725490196079\n",
            "f1_v3 , sample 0: 1.0\n",
            "f1_v0 , sample 1: 1.0\n",
            "f1_v1 , sample 1: 1.0\n",
            "f1_v2 , sample 1: 0.983050847457627\n",
            "f1_v3 , sample 1: 1.0\n",
            "f1_v0 , sample 2: 1.0\n",
            "f1_v1 , sample 2: 1.0\n",
            "f1_v2 , sample 2: 0.98\n",
            "f1_v3 , sample 2: 1.0\n",
            "f1_v0 , sample 3: 0.9787234042553191\n",
            "f1_v1 , sample 3: 0.9347826086956522\n",
            "f1_v2 , sample 3: 0.9545454545454545\n",
            "f1_v3 , sample 3: 1.0\n",
            "f1_v0 , sample 4: 1.0\n",
            "f1_v1 , sample 4: 0.9189189189189189\n",
            "f1_v2 , sample 4: 0.9487179487179488\n",
            "f1_v3 , sample 4: 1.0\n",
            "f1_v0 , sample 5: 0.9655172413793104\n",
            "f1_v1 , sample 5: 0.9883720930232558\n",
            "f1_v2 , sample 5: 0.9390243902439025\n",
            "f1_v3 , sample 5: 0.9876543209876543\n",
            "f1_v0 , sample 6: 1.0\n",
            "f1_v1 , sample 6: 0.9306930693069307\n",
            "f1_v2 , sample 6: 0.9693877551020408\n",
            "f1_v3 , sample 6: 1.0\n",
            "f1_v0 , sample 7: 0.9821428571428572\n",
            "f1_v1 , sample 7: 0.8750000000000001\n",
            "f1_v2 , sample 7: 1.0\n",
            "f1_v3 , sample 7: 0.9777777777777777\n",
            "f1_v0 , sample 8: 1.0\n",
            "f1_v1 , sample 8: 1.0\n",
            "f1_v2 , sample 8: 0.9565217391304348\n",
            "f1_v3 , sample 8: 1.0\n",
            "f1_v0 , sample 9: 0.975609756097561\n",
            "f1_v1 , sample 9: 0.9333333333333333\n",
            "f1_v2 , sample 9: 0.972972972972973\n",
            "f1_v3 , sample 9: 1.0\n",
            "f1_v0 , sample 10: 1.0\n",
            "f1_v1 , sample 10: 0.9807692307692307\n",
            "f1_v2 , sample 10: 0.9791666666666666\n",
            "f1_v3 , sample 10: 1.0\n",
            "f1_v0 , sample 11: 0.9871794871794872\n",
            "f1_v1 , sample 11: 1.0\n",
            "f1_v2 , sample 11: 0.9682539682539683\n",
            "f1_v3 , sample 11: 1.0\n",
            "f1_v0 , sample 12: 0.9411764705882353\n",
            "f1_v1 , sample 12: 0.9333333333333333\n",
            "f1_v2 , sample 12: 0.9772727272727273\n",
            "f1_v3 , sample 12: 1.0\n",
            "f1_v0 , sample 13: 1.0\n",
            "f1_v1 , sample 13: 0.9672131147540983\n",
            "f1_v2 , sample 13: 1.0\n",
            "f1_v3 , sample 13: 1.0\n",
            "f1_v0 , sample 14: 0.971830985915493\n",
            "f1_v1 , sample 14: 0.955223880597015\n",
            "f1_v2 , sample 14: 0.9122807017543859\n",
            "f1_v3 , sample 14: 1.0\n",
            "f1_v0 , sample 15: 1.0\n",
            "f1_v1 , sample 15: 0.9807692307692307\n",
            "f1_v2 , sample 15: 0.9791666666666666\n",
            "f1_v3 , sample 15: 0.9743589743589743\n",
            "f1_v0 , sample 16: 0.9803921568627451\n",
            "f1_v1 , sample 16: 0.9400000000000001\n",
            "f1_v2 , sample 16: 1.0\n",
            "f1_v3 , sample 16: 1.0\n",
            "f1_v0 , sample 17: 1.0\n",
            "f1_v1 , sample 17: 0.9206349206349206\n",
            "f1_v2 , sample 17: 0.9444444444444444\n",
            "f1_v3 , sample 17: 0.9666666666666666\n",
            "f1_v0 , sample 18: 1.0\n",
            "f1_v1 , sample 18: 0.9\n",
            "f1_v2 , sample 18: 1.0\n",
            "f1_v3 , sample 18: 0.896551724137931\n",
            "f1_v0 , sample 19: 1.0\n",
            "f1_v1 , sample 19: 0.9622641509433962\n",
            "f1_v2 , sample 19: 0.98\n",
            "f1_v3 , sample 19: 1.0\n",
            "f1_v0 , sample 20: 1.0\n",
            "f1_v1 , sample 20: 0.9622641509433962\n",
            "f1_v2 , sample 20: 0.891304347826087\n",
            "f1_v3 , sample 20: 1.0\n",
            "f1_v0 , sample 21: 1.0\n",
            "f1_v1 , sample 21: 0.9494949494949495\n",
            "f1_v2 , sample 21: 1.0\n",
            "f1_v3 , sample 21: 1.0\n",
            "f1_v0 , sample 22: 0.9798657718120806\n",
            "f1_v1 , sample 22: 0.9523809523809523\n",
            "f1_v2 , sample 22: 1.0\n",
            "f1_v3 , sample 22: 0.9787234042553191\n",
            "f1_v0 , sample 23: 0.9636363636363636\n",
            "f1_v1 , sample 23: 0.9782608695652174\n",
            "f1_v2 , sample 23: 0.98\n",
            "f1_v3 , sample 23: 0.972972972972973\n",
            "f1_v0 , sample 24: 1.0\n",
            "f1_v1 , sample 24: 0.9491525423728813\n",
            "f1_v2 , sample 24: 0.9615384615384615\n",
            "f1_v3 , sample 24: 0.9807692307692307\n",
            "f1_v0 , sample 25: 0.9400000000000001\n",
            "f1_v1 , sample 25: 1.0\n",
            "f1_v2 , sample 25: 1.0\n",
            "f1_v3 , sample 25: 1.0\n",
            "f1_v0 , sample 26: 1.0\n",
            "f1_v1 , sample 26: 0.9736842105263158\n",
            "f1_v2 , sample 26: 0.972972972972973\n",
            "f1_v3 , sample 26: 0.958904109589041\n",
            "f1_v0 , sample 27: 1.0\n",
            "f1_v1 , sample 27: 0.9722222222222222\n",
            "f1_v2 , sample 27: 1.0\n",
            "f1_v3 , sample 27: 1.0\n",
            "f1_v0 , sample 28: 1.0\n",
            "f1_v1 , sample 28: 0.975\n",
            "f1_v2 , sample 28: 0.962962962962963\n",
            "f1_v3 , sample 28: 1.0\n",
            "f1_v0 , sample 29: 0.975\n",
            "f1_v1 , sample 29: 0.951219512195122\n",
            "f1_v2 , sample 29: 0.888888888888889\n",
            "f1_v3 , sample 29: 0.9722222222222222\n",
            "f1_v0 , sample 30: 1.0\n",
            "f1_v1 , sample 30: 0.9873417721518987\n",
            "f1_v2 , sample 30: 1.0\n",
            "f1_v3 , sample 30: 1.0\n",
            "f1_v0 , sample 31: 1.0\n",
            "f1_v1 , sample 31: 1.0\n",
            "f1_v2 , sample 31: 1.0\n",
            "f1_v3 , sample 31: 1.0\n",
            "f1_v0 , sample 32: 0.9928057553956835\n",
            "f1_v1 , sample 32: 0.9607843137254902\n",
            "f1_v2 , sample 32: 0.9389312977099237\n",
            "f1_v3 , sample 32: 0.9549549549549551\n",
            "f1_v0 , sample 33: 0.9696969696969697\n",
            "f1_v1 , sample 33: 1.0\n",
            "f1_v2 , sample 33: 0.975609756097561\n",
            "f1_v3 , sample 33: 1.0\n",
            "f1_v0 , sample 34: 0.9875\n",
            "f1_v1 , sample 34: 0.9197080291970803\n",
            "f1_v2 , sample 34: 1.0\n",
            "f1_v3 , sample 34: 0.9843749999999999\n",
            "f1_v0 , sample 35: 0.9818181818181818\n",
            "f1_v1 , sample 35: 0.9111111111111111\n",
            "f1_v2 , sample 35: 0.9591836734693878\n",
            "f1_v3 , sample 35: 1.0\n",
            "f1_v0 , sample 36: 0.9682539682539683\n",
            "f1_v1 , sample 36: 0.9824561403508771\n",
            "f1_v2 , sample 36: 0.9433962264150945\n",
            "f1_v3 , sample 36: 1.0\n",
            "f1_v0 , sample 37: 1.0\n",
            "f1_v1 , sample 37: 0.9583333333333334\n",
            "f1_v2 , sample 37: 1.0\n",
            "f1_v3 , sample 37: 1.0\n",
            "f1_v0 , sample 38: 1.0\n",
            "f1_v1 , sample 38: 0.8958333333333334\n",
            "f1_v2 , sample 38: 0.9574468085106383\n",
            "f1_v3 , sample 38: 0.9761904761904763\n",
            "f1_v0 , sample 39: 0.9586776859504132\n",
            "f1_v1 , sample 39: 0.96875\n",
            "f1_v2 , sample 39: 1.0\n",
            "f1_v3 , sample 39: 1.0\n",
            "f1_v0 , sample 40: 0.9215686274509803\n",
            "f1_v1 , sample 40: 0.88\n",
            "f1_v2 , sample 40: 0.9361702127659575\n",
            "f1_v3 , sample 40: 0.9782608695652174\n",
            "f1_v0 , sample 41: 0.9870129870129869\n",
            "f1_v1 , sample 41: 0.9554140127388536\n",
            "f1_v2 , sample 41: 0.9705882352941176\n",
            "f1_v3 , sample 41: 1.0\n",
            "f1_v0 , sample 42: 1.0\n",
            "f1_v1 , sample 42: 1.0\n",
            "f1_v2 , sample 42: 1.0\n",
            "f1_v3 , sample 42: 0.9782608695652174\n",
            "f1_v0 , sample 43: 1.0\n",
            "f1_v1 , sample 43: 0.945945945945946\n",
            "f1_v2 , sample 43: 1.0\n",
            "f1_v3 , sample 43: 0.9736842105263158\n",
            "f1_v0 , sample 44: 1.0\n",
            "f1_v1 , sample 44: 0.9193548387096774\n",
            "f1_v2 , sample 44: 0.9473684210526316\n",
            "f1_v3 , sample 44: 1.0\n",
            "f1_v0 , sample 45: 1.0\n",
            "f1_v1 , sample 45: 1.0\n",
            "f1_v2 , sample 45: 0.8817204301075269\n",
            "f1_v3 , sample 45: 1.0\n",
            "f1_v0 , sample 46: 1.0\n",
            "f1_v1 , sample 46: 0.9743589743589743\n",
            "f1_v2 , sample 46: 0.9428571428571428\n",
            "f1_v3 , sample 46: 1.0\n",
            "f1_v0 , sample 47: 0.962962962962963\n",
            "f1_v1 , sample 47: 0.9622641509433962\n",
            "f1_v2 , sample 47: 1.0\n",
            "f1_v3 , sample 47: 1.0\n",
            "f1_v0 , sample 48: 0.9827586206896551\n",
            "f1_v1 , sample 48: 0.9285714285714286\n",
            "f1_v2 , sample 48: 0.9642857142857143\n",
            "f1_v3 , sample 48: 1.0\n",
            "f1_v0 , sample 49: 0.9777777777777777\n",
            "f1_v1 , sample 49: 0.951219512195122\n",
            "f1_v2 , sample 49: 0.9705882352941176\n",
            "f1_v3 , sample 49: 1.0\n",
            "f1_v0 , sample 50: 1.0\n",
            "f1_v1 , sample 50: 0.9615384615384615\n",
            "f1_v2 , sample 50: 0.9607843137254902\n",
            "f1_v3 , sample 50: 1.0\n",
            "f1_v0 , sample 51: 1.0\n",
            "f1_v1 , sample 51: 0.9807692307692307\n",
            "f1_v2 , sample 51: 1.0\n",
            "f1_v3 , sample 51: 1.0\n",
            "f1_v0 , sample 52: 0.96875\n",
            "f1_v1 , sample 52: 0.8695652173913044\n",
            "f1_v2 , sample 52: 0.9253731343283582\n",
            "f1_v3 , sample 52: 1.0\n",
            "f1_v0 , sample 53: 0.9791666666666666\n",
            "f1_v1 , sample 53: 0.9400000000000001\n",
            "f1_v2 , sample 53: 1.0\n",
            "f1_v3 , sample 53: 1.0\n",
            "f1_v0 , sample 54: 0.9600000000000001\n",
            "f1_v1 , sample 54: 0.9078947368421052\n",
            "f1_v2 , sample 54: 0.9411764705882353\n",
            "f1_v3 , sample 54: 1.0\n",
            "f1_v0 , sample 55: 1.0\n",
            "f1_v1 , sample 55: 0.972972972972973\n",
            "f1_v2 , sample 55: 1.0\n",
            "f1_v3 , sample 55: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate fugues"
      ],
      "metadata": {
        "id": "TzTpXHznL02j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import statistics\n",
        "\n",
        "\n",
        "def evaluate_accuracy_for_all(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    note_counter_0 = 0\n",
        "    note_counter_1 = 0\n",
        "    note_counter_2 = 0\n",
        "    note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "                                      \n",
        "                print(\"nbr_voices:\",nbr_voices)\n",
        "            #if idx == 0 or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "\n",
        "                note_counter_0 += len(note_array_0)\n",
        "                note_counter_1 += len(note_array_1)\n",
        "                note_counter_2 += len(note_array_2)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "                if len(part)== 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = part_3.note_array\n",
        "                    note_counter_3 += len(note_array_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                   \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        \n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "                            \n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                      counting = 0\n",
        "                      ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                      for j in range(len(total_predictions_dict[i])):\n",
        "                          if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                      count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "\n",
        "                    if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    print(\"f1_v0 , sample {}:\".format(idx),f1_v0)\n",
        "                    print(\"f1_v1 , sample {}:\".format(idx),f1_v1)\n",
        "                    print(\"f1_v2 , sample {}:\".format(idx),f1_v2)\n",
        "                    if len(part)==4:\n",
        "                      pred_3 = accordance_dict[\"3\"]\n",
        "                      truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                      f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                      f_score_dict[\"3\"].append(f1_v3)\n",
        "                      print(\"f1_v3 , sample {}:\".format(idx),f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])\n",
        "        #return total_predictions_dict, total_truth_dict"
      ],
      "metadata": {
        "id": "0xbN5YU8nGT0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, train_dataloader, part_dic,F1):\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader): \n",
        "        print(\"nbr_voices:\",nbr_voices)\n",
        "\n",
        "test(model,val_dataloader,part_dic,F1=False)"
      ],
      "metadata": {
        "id": "p0HZ9d5TO_Aj",
        "outputId": "a97f53c3-40ba-4606-c818-0b0cabc75f71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True:\n",
        "    dict_pred , dict_gt, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "20MP5Gk5kc2X"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "H9iWhIkMNjmp",
        "outputId": "b1acb561-02f4-43ce-c465-5ed8f2495e51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9858302029502208, 0.9550993100996932, 0.968737973909645, 0.991291567581071)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss3 * 1.5 = (0.8989219661919758, 0.7166572856993837, 0.8185917288474246, 0.0)"
      ],
      "metadata": {
        "id": "6nwnRTADQvQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 epoch, no loss modifier:\n",
        "#ACC:(0.9165209182020722,\n",
        "# 0.7864434689151618,\n",
        "# 0.8130949796045199,\n",
        "# 0.003652274754166715)\n",
        "\n",
        "# 20 epoch, no loss modifier:\n",
        "#(0.7962210840410273, 0.8669639629052727, 0.751302181991106, 0.0)\n",
        "\n",
        "# 20 ep, loss3 *1,5\n",
        "#(0.8721136343927623,\n",
        "# 0.8319586824413445,\n",
        "# 0.7563218924966578,\n",
        "# 0.09029535181592076)"
      ],
      "metadata": {
        "id": "6mCYnJLnHfYB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate fugues F1 score"
      ],
      "metadata": {
        "id": "52P6em3ANb1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True:\n",
        "    f1_v0, f1_v1, f1_v2, f1_v3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=True)\n",
        "    print(f1_v0, f1_v1, f1_v2, f1_v3)"
      ],
      "metadata": {
        "id": "FLLmyO6o5vW5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### take 0 -> compare to truth of 0,1,2,3 -> overall voice\n",
        "\n",
        "count_list = []\n",
        "\n",
        "count_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "truth_dic = {'0': 0, '1': 1, '2': 2, '3': 3 }\n",
        "\n",
        "voice_entry_list = [\"0\", \"1\", \"2\", \"3\"]\n",
        "for voice_entry_one in voice_entry_list:\n",
        "    for voice_entry_two in voice_entry_list:\n",
        "        count_list = []\n",
        "        #print(\"voices:\",voice_entry_one,voice_entry_two)\n",
        "        for i in range(len(dict_pred[voice_entry_one])):\n",
        "            if dict_pred[voice_entry_one][i][0] == truth_dic[voice_entry_two]:      #dict_truth[voice_entry_two][i][0]:\n",
        "                count_list.append(1)\n",
        "            else:\n",
        "                count_list.append(0)\n",
        "        count_dict[voice_entry_one].append(count_list)\n",
        "\n",
        "dictionary_sum={}\n",
        "for i in voice_entry_list:\n",
        "    v0_match,v1_match,v2_match,v3_match = count_dict[i]\n",
        "    sum_v0 = np.sum(v0_match)\n",
        "    sum_v1 = np.sum(v1_match)\n",
        "    sum_v2 = np.sum(v2_match)\n",
        "    sum_v3 = np.sum(v3_match)\n",
        "    dictionary_sum[\"v0\"] = sum_v0\n",
        "    dictionary_sum[\"v1\"] = sum_v1\n",
        "    dictionary_sum[\"v2\"] = sum_v2\n",
        "    dictionary_sum[\"v3\"] = sum_v3\n",
        "\n",
        "    val_list = list(dictionary_sum.values())\n",
        "    \n",
        "    print(\"voice{} matches with\".format(i))\n",
        "    print(\"dict\",dictionary_sum)\n",
        "\n",
        "    max_sum = max(sum_v0,sum_v1,sum_v2,sum_v3)\n",
        "\n",
        "\n",
        "    print(\"max_sum\", val_list.index(max_sum) )\n",
        "\n",
        "    print(\"accuracy voice{}:\".format(i), max_sum/(sum_v0+sum_v1+sum_v2+sum_v3) )\n",
        "    print(\"________________\")\n",
        "    print(\" \")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BoQcV_i038DD",
        "outputId": "51322889-05f2-4c46-aa45-532f0721a003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n### take 0 -> compare to truth of 0,1,2,3 -> overall voice\\n\\ncount_list = []\\n\\ncount_dict = {\\'0\\': [], \\'1\\': [], \\'2\\': [], \\'3\\': [] }\\ntruth_dic = {\\'0\\': 0, \\'1\\': 1, \\'2\\': 2, \\'3\\': 3 }\\n\\nvoice_entry_list = [\"0\", \"1\", \"2\", \"3\"]\\nfor voice_entry_one in voice_entry_list:\\n    for voice_entry_two in voice_entry_list:\\n        count_list = []\\n        #print(\"voices:\",voice_entry_one,voice_entry_two)\\n        for i in range(len(dict_pred[voice_entry_one])):\\n            if dict_pred[voice_entry_one][i][0] == truth_dic[voice_entry_two]:      #dict_truth[voice_entry_two][i][0]:\\n                count_list.append(1)\\n            else:\\n                count_list.append(0)\\n        count_dict[voice_entry_one].append(count_list)\\n\\ndictionary_sum={}\\nfor i in voice_entry_list:\\n    v0_match,v1_match,v2_match,v3_match = count_dict[i]\\n    sum_v0 = np.sum(v0_match)\\n    sum_v1 = np.sum(v1_match)\\n    sum_v2 = np.sum(v2_match)\\n    sum_v3 = np.sum(v3_match)\\n    dictionary_sum[\"v0\"] = sum_v0\\n    dictionary_sum[\"v1\"] = sum_v1\\n    dictionary_sum[\"v2\"] = sum_v2\\n    dictionary_sum[\"v3\"] = sum_v3\\n\\n    val_list = list(dictionary_sum.values())\\n    \\n    print(\"voice{} matches with\".format(i))\\n    print(\"dict\",dictionary_sum)\\n\\n    max_sum = max(sum_v0,sum_v1,sum_v2,sum_v3)\\n\\n\\n    print(\"max_sum\", val_list.index(max_sum) )\\n\\n    print(\"accuracy voice{}:\".format(i), max_sum/(sum_v0+sum_v1+sum_v2+sum_v3) )\\n    print(\"________________\")\\n    print(\" \")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR MONOPHONIC F1\n",
        "\n",
        "# start with GT\n",
        "# look at first note in pred-> save note label\n",
        "# look at second note in pred-> if same note as before : SUCESS if it is not: FAIL\n",
        " # DO This for all 4 voices\n",
        " ## in GT there is always the same voice following -> would always be an array of 1\n",
        "\n",
        "## POLYPHONIC \n",
        "\n",
        "# prbl after 1 note there can be multiple diff voices .. chords"
      ],
      "metadata": {
        "id": "-bR7gcej90qh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you have the ground truth on the different parts that you get when you import your score. Each part correspond to a voice. So if your note array contains all notes of all voices, you have for each note in your note array a number that is the ground truth voice (that you take from the part) and a number that is the predicted voice (that you take from the maximum vote)."
      ],
      "metadata": {
        "id": "Z5q305YzvjMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start time, duration , pitch to separate \n",
        "\n",
        "use the onset_beat and duration_beat\n",
        "\n",
        "multiply them according to the values set when producing the pianorolls \n",
        "\n",
        "-> get the position in the pianoroll\n",
        "\n",
        "time_div = 12\n",
        "\n"
      ],
      "metadata": {
        "id": "EmvxtyaVKG27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "A94mchm4LV6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy_v0\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v1\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v2\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v3\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Accuracy0','Accuracy1','Accuracy2','Accuracy3'])\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TgJDHaxAgYN",
        "outputId": "382b6ebc-45a2-4bca-9c35-d7031e2bdcc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5gV1Znv8e9PQBpEQUAFaQgohnARFFtI1DnxrhmDYjCRxChegswY1FycgYwzSDAzYiaJnkQzoyceRU0alCjeEhVEnTEmclGiIhrwQmiEEbkJSoPAO3/s6nbTNt2b6t69eze/z/PU01WrVlW9a6P99qpVe5UiAjMzsz21T6EDMDOz4uQEYmZmqTiBmJlZKk4gZmaWihOImZml4gRiZmapOIGYWYNICkl9Cx2HNT0nECsKkp6RtF5S20LH0pxJekfSFkmbs5ZbCh2XtUxOINbsSeoN/A0QwNlNfO3WTXm9RjIiIjpkLeMLHZC1TE4gVgwuAv4E3AWMyd4hqaekByStkbQ2+69tSWMlLZG0SdJrkoYm5bvccpF0l6QfJesnSqqQNEHSauBOSQdKejS5xvpkvTTr+M6S7pT0brJ/VlL+qqQRWfXaSHpf0tE1G5jE+eWs7dbJ9YZKKpF0b9K+DZLmSzpkTz9ESRdL+oOkWyRtlPS6pFOy9h8q6WFJ6yQtkzQ2a18rSf8k6c3k81woqWfW6U+VtDSJ71ZJSo7rK+nZ5HrvS5qxp3Fb8+UEYsXgIuDXyXJG1S9PSa2AR4HlQG+gBzA92fdVYHJy7AFkei5rc7xeN6Az8BngcjL/n9yZbPcCtgDZt4XuAdoDA4GDgZuS8ruBb2bV+1tgVUS8VMs1y4GvZ22fAbwfES+SSZodgZ5AF+DvkhjSGA68CXQFrgMekNQ52TcdqAAOBc4D/k3Sycm+7yXx/S2Zz/NS4KOs834ZOBYYDHwtiR/geuBJ4ECgFPhFyritOYoIL16a7QKcAHwMdE22Xwe+m6x/AVgDtK7luCeAq3dzzgD6Zm3fBfwoWT8R2AaU1BHTUcD6ZL07sBM4sJZ6hwKbgAOS7ZnAP+7mnH2Tuu2T7V8Dk5L1S4HngcE5fF7vAJuBDVnL2GTfxcC7gLLqzwMuJJOcdgD7Z+27AbgrWX8DOKeOz/OErO37gInJ+t3A7UBpof9b8tL4i3sg1tyNAZ6MiPeT7d/wyW2snsDyiNhey3E9yfylncaaiKis2pDUXtJtkpZL+gD4L6BT0gPqCayLiPU1TxIR7wJ/AEZJ6gR8iUxi+JSIWAYsAUZIak+mx/SbZPc9ZBLi9OQ22Y8ltakj/pER0Slr+X9Z+1ZGRPYMqsvJJLpDk3ZsqrGvR7Je3+e5Omv9I6BDsv6PgIB5khZLurSOc1iRKcYBQttLSGpH5nZIq2Q8AqAtmV/eQ4AVQC9JrWtJIiuAw3dz6o/I3HKq0o3MrZsqNaeo/j7QDxgeEaslHQW8ROYX4wqgs6ROEbGhlmtNA75F5v+1P0bEyt23uPo21j7Aa0lSISI+Bn4I/DB5oOB3ZHoEd9Rxrt3pIUlZSaQX8DCZnklnSftnJZFeQFW8VZ/nq3tysYhYDYwFkHQCMEfSf1W1zYqbeyDWnI0kc1tlAJnbRkcB/YH/JjO2MQ9YBUyVtF8y2Hx8cuyvgGskHaOMvpI+k+xbBHwjGRg+E/hiPXHsT2bMYUMyXnBd1Y6IWAX8HvhlMtjeRtL/yTp2FjAUuJrM7Zy6TAdOB/6eT3ofSDpJ0pFJj+cDMrf0dtZzrt05GLgqifOrZD7P30XECjK3yW5IPsfBwGXAvclxvwKul3RE8nkOltSlvotJ+mrWAwfrySTntLFbM+MEYs3ZGODOiPhrRKyuWsgMYF9Apgcwgsz4wV/J9CLOB4iI+4F/JfOLeBOZX+RVg8VXJ8dtSM4zq544bgbaAe+TeRrs8Rr7LyTzS/114D3gO1U7ImIL8FugD/BAXRdJktEfgeOA7KeVupEZP/mAzG2uZ8nc1tqdR7Tr90AezNr3AnBE0pZ/Bc6LiKqHC75O5mGEd4EHgesiYk6y72dkxjaeTOK4g8xnUp9jgRckbSbT07k6It7K4TgrAtr1dqiZNTZJk4DPRsQ3662c3zguBr4VEScUMg5rOTwGYpZHyS2vy8j0UsxaFN/CMsuT5It4K4DfR8R/FToes8bmW1hmZpaKeyBmZpbKXjUG0rVr1+jdu3ehwzAzKyoLFy58PyIOqlm+VyWQ3r17s2DBgkKHYWZWVCQtr63ct7DMzCwVJxAzM0vFCcTMzFLZq8ZAzKy4ffzxx1RUVFBZWVl/ZdtjJSUllJaW0qZNXZM9f8IJxMyKRkVFBfvvvz+9e/cmeemhNZKIYO3atVRUVNCnT5+cjvEtLDMrGpWVlXTp0sXJIw8k0aVLlz3q3TmBmFlRcfLInz39bJ1AzMwsFScQM7M9NGvWLCTx+uuvFzqUnNxwww307duXfv368cQTTzTaeZ1AzKzFmvXSSo6fOpc+Ex/j+KlzmfVSXW8Uzl15eTknnHAC5eXljXK+2uzYsaNRzvPaa68xffp0Fi9ezOOPP84VV1zRaOd2AjGzFmnWSyv5wQOvsHLDFgJYuWELP3jglQYnkc2bN/Pcc89xxx13MH36dCDzy/6aa65h0KBBDB48mF/84hcAzJ8/n+OOO44hQ4YwbNgwNm3axF133cX48eOrz/flL3+ZZ555BoAOHTrw/e9/nyFDhvDHP/6RKVOmcOyxxzJo0CAuv/xyqmZPX7ZsGaeeeipDhgxh6NChvPnmm1x00UXMmvXJyzUvuOACHnroIR566CFGjx5N27Zt6dOnD3379mXevHkN+gyq+DFeMytKP3xkMa+9+8Fu97/01w1s27Hr69e3fLyDf5z5MuXz/lrrMQMOPYDrRgys87oPPfQQZ555Jp/97Gfp0qULCxcuZN68ebzzzjssWrSI1q1bs27dOrZt28b555/PjBkzOPbYY/nggw9o167utwB/+OGHDB8+nJ/+9KeZeAYMYNKkSQBceOGFPProo4wYMYILLriAiRMncu6551JZWcnOnTu57LLLuOmmmxg5ciQbN27k+eefZ9q0acyePZvPf/7z1dcoLS1l5crG6Ym5B2JmLVLN5FFfea7Ky8sZPXo0AKNHj6a8vJw5c+Ywbtw4WrfO/E3euXNn3njjDbp3786xxx4LwAEHHFC9f3datWrFqFGjqreffvpphg8fzpFHHsncuXNZvHgxmzZtYuXKlZx77rlA5st/7du354tf/CJLly5lzZo1lJeXM2rUqHqv11DugZhZUaqvp3D81Lms3LDlU+U9OrVjxrgvpLrmunXrmDt3Lq+88gqS2LFjB5Kqk0QuWrduzc6dnySx7O9dlJSU0KpVq+ryK664ggULFtCzZ08mT55c73c0LrroIu69916mT5/OnXfeCUCPHj1YsWJFdZ2Kigp69OiRc7x1cQ/EzFqkfzijH+3atNqlrF2bVvzDGf1Sn3PmzJlceOGFLF++nHfeeYcVK1bQp08fhgwZwm233cb27duBTKLp168fq1atYv78+QBs2rSJ7du307t3bxYtWsTOnTtZsWLFbscjqpJF165d2bx5MzNnzgRg//33p7S0tHq8Y+vWrXz00UcAXHzxxdx8881A5vYXwNlnn8306dPZunUrb7/9NkuXLmXYsGGpP4Ns7oGYWYs08ujMX9n//sQbvLthC4d2asc/nNGvujyN8vJyJkyYsEvZqFGjWLJkCb169WLw4MG0adOGsWPHMn78eGbMmMGVV17Jli1baNeuHXPmzOH444+nT58+DBgwgP79+zN06NBar9WpUyfGjh3LoEGD6Nat2y69nHvuuYdx48YxadIk2rRpw/33389hhx3GIYccQv/+/Rk5cmR13YEDB/K1r32NAQMG0Lp1a2699dbqXk5D7VXvRC8rKwu/UMqseC1ZsoT+/fsXOoxm66OPPuLII4/kxRdfpGPHjqnOUdtnLGlhRJTVrOtbWGZmLcCcOXPo378/V155Zerksad8C8vMrAU49dRTWb681jfP5o17IGZmlooTiJmZpeIEYmZmqTiBmJlZKk4gZmZ7qJimc1+7di0nnXQSHTp02GUSx8bgBGJmLdfL98FNg2Byp8zPl+9rlNMW03TuJSUlXH/99fzkJz9plPNlK2gCkXSmpDckLZM0sZb9bSXNSPa/IKl3jf29JG2WdE1TxWxmReLl++CRq2DjCiAyPx+5qsFJpNimc99vv/044YQTKCkpaVC7a1Ow74FIagXcCpwGVADzJT0cEa9lVbsMWB8RfSWNBm4Ezs/a/zPg900Vs5k1I7+fCKtf2f3+ivmwY+uuZR9vgYfGw8JptR/T7Uj40tQ6L1ts07nnUyF7IMOAZRHxVkRsA6YD59Socw5Q9QnMBE5R8tZ3SSOBt4HFTRSvmRWTmsmjvvIceTr3TxTym+g9gBVZ2xXA8N3ViYjtkjYCXSRVAhPI9F7qvH0l6XLgcoBevXo1TuRmVnj19BS4aVBy+6qGjj3hksdSXbIYp3PPp2IdRJ8M3BQRm+urGBG3R0RZRJQddNBB+Y/MzJqHUyZBmxq3jNq0y5SnVIzTuedTIXsgK4GeWdulSVltdSoktQY6AmvJ9FTOk/RjoBOwU1JlRNyS/7DNrCgM/lrm51NTYGMFdCzNJI+q8hSKcTp3gN69e/PBBx+wbds2Zs2axZNPPtkoCaZg07knCeEvwClkEsV84BsRsTirzreBIyPi75JB9K9ExNdqnGcysDki6n1GzdO5mxU3T+det71mOveI2A6MB54AlgD3RcRiSVMknZ1Uu4PMmMcy4HvApx71NTOzvXA694j4HfC7GmWTstYrga/Wc47JeQnOzKyIeDp3MzMrGk4gZmaWihOImZml4gRiZmapOIGYme2hYprOffbs2RxzzDEceeSRHHPMMcydO7fRzu0EYmYt1mNvPcbpM09n8LTBnD7zdB57K90UJjUV03TuXbt25ZFHHuGVV15h2rRpXHjhhY1yXnACMbMW6rG3HmPy85NZ9eEqgmDVh6uY/PzkBieRYpvO/eijj+bQQw8FYODAgWzZsoWtWxs2oWSVgn4PxMwsrRvn3cjr63Z/C+nlNS+zbee2Xcoqd1Qy6Q+TmPmXmbUe87nOn2PCsAm17qtSzNO5//a3v2Xo0KG0bdu2zjhy5R6ImbVINZNHfeW5Ktbp3BcvXsyECRO47bbbGtT+bO6BmFlRqq+ncPrM01n14apPlXffrzt3npluqvNinc69oqKCc889l7vvvpvDDz8851jr4x6ImbVIVw+9mpJWu77GtaRVCVcPvTr1OYtxOvcNGzZw1llnMXXqVI4//vjUba+NE4iZtUhnHXYWk4+bTPf9uiNE9/26M/m4yZx12Fmpz1leXl5966jKqFGjWLVqVfV07kOGDOE3v/kN++67b/V07kOGDOG0006jsrJyl+ncr7rqqpymcz/jjDM+NZ37z3/+cwYPHsxxxx3H6tWrAaqnc7/kkkuq695yyy0sW7aMKVOmcNRRR3HUUUfx3nvvpf4MshVsOvdC8HTuZsXN07nXba+Zzt3MzBrPXjedu5mZNQ5P525mZkXDCcTMzFJxAjEzs1ScQMzMLBUnEDOzPVRM07nPmzev+vsfQ4YM4cEHH2y0czuBmFmLtfGRR1h68iks6T+ApSefwsZHHmmU8xbTdO6DBg1iwYIFLFq0iMcff5xx48ZVf2O+oZxAzKxF2vjII6z6l0lsf/ddiGD7u++y6l8mNTiJFNt07u3bt6+eVLGyshJJDWp/Nn8PxMyK0up/+ze2Ltn9LaQtf/4zsW3XmXejspJV1/4zG+67v9Zj2vb/HN3+6Z/qvG4xTuf+wgsvcOmll7J8+XLuueeeemcFzpV7IGbWItVMHvWV56oYp3MfPnw4ixcvZv78+dxwww31zuqbK/dAzKwo1ddTWHryKZnbVzW0PvRQPnPP3amuWazTuVfp378/HTp04NVXX6Ws7FNTW+0x90DMrEU6+LvfQSW7TueukhIO/u53Up+zGKdzf/vtt6vjWr58Oa+//jq9e/dO/Rlkcw/EzFqkjiNGAPDeTTezfdUqWnfvzsHf/U51eRrl5eVMmLDri6xGjRrFkiVLqqdzb9OmDWPHjmX8+PHV07lv2bKFdu3aMWfOnF2mc+/fv39O07l369btU9O5jxs3jkmTJtGmTRvuv/9+DjvssOrp3EeOHFld97nnnmPq1Km0adOGffbZh1/+8pd07do19WeQzdO5m1nR8HTudfN07mZmtsc8nbuZmaWy103nLulMSW9IWiZpYi3720qakex/QVLvpPw0SQslvZL8PLmpYzezwtibbrs3tT39bAuWQCS1Am4FvgQMAL4uaUCNapcB6yOiL3ATcGNS/j4wIiKOBMYA9zRN1GZWSCUlJaxdu9ZJJA8igrVr11JS48m1uhTyFtYwYFlEvAUgaTpwDvBaVp1zgMnJ+kzgFkmKiJey6iwG2klqGxFb8x+2mRVKaWkpFRUVrFmzptChtEglJSWUlpbmXL+QCaQHsCJruwIYvrs6EbFd0kagC5keSJVRwItOHmYtX5s2bejTp0+hw7BEUQ+iSxpI5rbW6XXUuRy4HKBXr15NFJmZWctXyEH0lUDPrO3SpKzWOpJaAx2Btcl2KfAgcFFEvLm7i0TE7RFRFhFlBx10UCOGb2a2dytkApkPHCGpj6R9gdHAwzXqPExmkBzgPGBuRISkTsBjwMSI+EOTRWxmZtUKlkAiYjswHngCWALcFxGLJU2RdHZS7Q6gi6RlwPeAqkd9xwN9gUmSFiXLwU3cBDOzvZqnMjEzszp5KhMzM2tUTiBmZpaKE4iZmaXiBGJmZqk4gZiZWSpOIGZmlooTiJmZpeIEYmZmqTiBmJlZKk4gZmaWihOImZml4gRiZmapOIGYmVkqTiBmZpZKTglE0gOSzpLkhGNmZkDuPZBfAt8AlkqaKqlfHmMyM7MikFMCiYg5EXEBMBR4B5gj6XlJl0hqk88Azcysecr5lpSkLsDFwLeAl4D/SyahzM5LZGZm1qy1zqWSpAeBfsA9wIiIWJXsmiHJ74g1M9sL5ZRAgJ9HxNO17ajtPblmZtby5XoLa4CkTlUbkg6UdEWeYjIzsyKQawIZGxEbqjYiYj0wNj8hmZlZMcg1gbSSpKoNSa2AffMTkpmZFYNcx0AeJzNgfluyPS4pMzOzvVSuCWQCmaTx98n2bOBXeYnIzMyKQk4JJCJ2Av+RLGZmZjl/D+QI4AZgAFBSVR4Rh+UpLjMza+ZyHUS/k0zvYztwEnA3cG++gjIzs+Yv1wTSLiKeAhQRyyNiMnBW/sIyM7PmLtdB9K3JVO5LJY0HVgId8heWmZk1d7n2QK4G2gNXAccA3wTG5CsoMzNr/upNIMmXBs+PiM0RURERl0TEqIj4U0MvLulMSW9IWiZpYi3720qakex/QVLvrH0/SMrfkHRGQ2MxM7M9U28CiYgdwAmNfeEkMd0KfInM011flzSgRrXLgPUR0Re4CbgxOXYAMBoYCJwJ/DI5n5mZNZFcx0BekvQwcD/wYVVhRDzQgGsPA5ZFxFsAkqYD5wCvZdU5B5icrM8EbkmmVDkHmB4RW4G3JS1LzvfHBsRjZmZ7INcEUgKsBU7OKgugIQmkB7Aia7sCGL67OhGxXdJGoEtS/qcax/ao7SKSLgcuB+jVq1cDwjUzs2y5fhP9knwHki8RcTtwO0BZWVkUOBwzsxYj12+i30mmx7GLiLi0AddeCfTM2i5NymqrUyGpNdCRTE8ol2PNzCyPcn2M91HgsWR5CjgA2NzAa88HjpDUR9K+ZAbFH65R52E+eVz4PGBuRERSPjp5SqsPcAQwr4HxmJnZHsj1FtZvs7cllQPPNeTCyZjGeOAJoBXw/yNisaQpwIKIeBi4A7gnGSRfRybJkNS7j8yA+3bg28nTYmZm1kSU+YN+Dw+S+gGPJY/XFo2ysrJYsGBBocMwMysqkhZGRFnN8lzHQDax6xjIajLvCDEzs71Urrew9s93IGZmVlxyGkSXdK6kjlnbnSSNzF9YZmbW3OX6FNZ1EbGxaiMiNgDX5SckMzMrBrkmkNrq5fotdjMza4FyTSALJP1M0uHJ8jNgYT4DMzOz5i3XBHIlsA2YAUwHKoFv5ysoMzNr/nJ9CutD4FPv6zAzs71Xrk9hzZbUKWv7QElP5C8sMzNr7nK9hdU1efIKgIhYDxycn5DMzKwY5JpAdkqqfplG8mpZT41uZrYXy/VR3GuB5yQ9Cwj4G5KXNJmZ2d4p10H0xyWVkUkaLwGzgC35DMzMzJq3XCdT/BZwNZkXNy0CPk/m/eMn13WcmZm1XLmOgVwNHAssj4iTgKOBDXUfYmZmLVmuCaQyIioBJLWNiNeBfvkLy8zMmrtcB9Erku+BzAJmS1oPLM9fWGZm1tzlOoh+brI6WdLTQEfg8bxFZWZmzd4ez6gbEc/mIxAzMysuuY6BmJmZ7cIJxMzMUnECMTOzVJxAzMwsFScQMzNLxQnEzMxScQIxM7NUnEDMzCwVJxAzM0vFCcTMzFJxAjEzs1QKkkAkdZY0W9LS5OeBu6k3JqmzVNKYpKy9pMckvS5psaSpTRu9mZlB4XogE4GnIuII4KlkexeSOgPXAcOBYcB1WYnmJxHxOTIvtjpe0peaJmwzM6tSqARyDjAtWZ8GjKylzhnA7IhYFxHrgdnAmRHxUUQ8DRAR24AXybxq18zMmlChEsghEbEqWV8NHFJLnR7AiqztiqSsWvKSqxFkejFmZtaE9vh9ILmSNAfoVsuua7M3IiIkRYrztwbKgZ9HxFt11LscuBygV69ee3oZMzPbjbwlkIg4dXf7JP2PpO4RsUpSd+C9WqqtBE7M2i4Fnsnavh1YGhE31xPH7UldysrK9jhRmZlZ7Qp1C+thYEyyPgZ4qJY6TwCnSzowGTw/PSlD0o/IvFb3O00Qq5mZ1aJQCWQqcJqkpcCpyTaSyiT9CiAi1gHXA/OTZUpErJNUSuY22ADgRUmLJH2rEI0wM9ubKWLvuatTVlYWCxYsKHQYZmZFRdLCiCirWe5vopuZWSpOIGZmlooTiJmZpeIEYmZmqTiBmJlZKk4gZmaWihOImZml4gRiZmapOIGYmVkqTiBmZpaKE4iZmaXiBGJmZqk4gZiZWSpOIGZmlooTiJmZpeIEYmZmqTiBmJlZKk4gZmaWihOImZml4gRiZmapOIGYmVkqTiBmZpaKE4iZmaXiBGJmZqk4gZiZWSpOIGZmlooTiJmZpeIEYmZmqTiBmJlZKk4gZmaWihOImZmlUpAEIqmzpNmSliY/D9xNvTFJnaWSxtSy/2FJr+Y/YjMzq6lQPZCJwFMRcQTwVLK9C0mdgeuA4cAw4LrsRCPpK8DmpgnXzMxqKlQCOQeYlqxPA0bWUucMYHZErIuI9cBs4EwASR2A7wE/aoJYzcysFoVKIIdExKpkfTVwSC11egArsrYrkjKA64GfAh/VdyFJl0taIGnBmjVrGhCymZlla52vE0uaA3SrZde12RsREZJiD857FHB4RHxXUu/66kfE7cDtAGVlZTlfx8zM6pa3BBIRp+5un6T/kdQ9IlZJ6g68V0u1lcCJWdulwDPAF4AySe+Qif9gSc9ExImYmVmTKdQtrIeBqqeqxgAP1VLnCeB0SQcmg+enA09ExH9ExKER0Rs4AfiLk4eZWdMrVAKZCpwmaSlwarKNpDJJvwKIiHVkxjrmJ8uUpMzMzJoBRew9wwJlZWWxYMGCQodhZlZUJC2MiLKa5f4mupmZpeIEYmZmqTiBmJlZKk4gZmaWihOImZml4gRiZmapOIGYmVkqTiBmZpaKE4iZmaXiBGJmZqk4gZiZWSpOIGZmlooTiJmZpeIEYmZmqTiBmJlZKk4gZmaWihOImZml4gRiZmapOIGYmVkqTiBmZpaKE4iZmaXiBGJmZqk4gZiZWSpOIGZmlooiotAxNBlJa4DlhY5jD3UF3i90EE3Mbd47uM3F4zMRcVDNwr0qgRQjSQsioqzQcTQlt3nv4DYXP9/CMjOzVJxAzMwsFSeQ5u/2QgdQAG7z3sFtLnIeAzEzs1TcAzEzs1ScQMzMLBUnkGZAUmdJsyUtTX4euJt6Y5I6SyWNqWX/w5JezX/EDdeQNktqL+kxSa9LWixpatNGv2cknSnpDUnLJE2sZX9bSTOS/S9I6p217wdJ+RuSzmjKuBsibZslnSZpoaRXkp8nN3XsaTTk3zjZ30vSZknXNFXMjSIivBR4AX4MTEzWJwI31lKnM/BW8vPAZP3ArP1fAX4DvFro9uS7zUB74KSkzr7AfwNfKnSbdtPOVsCbwGFJrH8GBtSocwXwn8n6aGBGsj4gqd8W6JOcp1Wh25TnNh8NHJqsDwJWFro9+Wxv1v6ZwP3ANYVuz54s7oE0D+cA05L1acDIWuqcAcyOiHURsR6YDZwJIKkD8D3gR00Qa2NJ3eaI+CgingaIiG3Ai0BpE8ScxjBgWUS8lcQ6nUzbs2V/FjOBUyQpKZ8eEVsj4m1gWXK+5i51myPipYh4NylfDLST1LZJok6vIf/GSBoJvE2mvUXFCaR5OCQiViXrq4FDaqnTA1iRtV2RlAFcD/wU+ChvETa+hrYZAEmdgBHAU/kIshHU24bsOhGxHdgIdMnx2OaoIW3ONgp4MSK25inOxpK6vckffxOAHzZBnI2udaED2FtImgN0q2XXtdkbERGScn62WtJRwOER8d2a91ULLV9tzjp/a6Ac+HlEvJUuSmuOJA0EbgROL3QseTYZuCkiNicdkqLiBNJEIuLU3e2T9D+SukfEKkndgfdqqbYSODFruxR4BvgCUCbpHTL/ngdLeiYiTqTA8tjmKrcDSyPi5kYIN19WAj2ztkuTstrqVCRJsSOwNsdjm6OGtBlJpcCDwEUR8Wb+w22whrR3OHCepB8DnYCdkioj4pb8h90ICj0I4yUA/p1dB5R/XEudzmTukx6YLG8DnWvU6U3xDKI3qM1kxnt+C+xT6LbU087WZAb/+/DJAOvAGnW+za4DrPcl6wPZdRD9LYpjEL0hbe6U1P9KodvRFO2tUWcyRTaIXvAAvARk7kkOPG4AAAI4SURBVP0+BSwF5mT9kiwDfpVV71IyA6nLgEtqOU8xJZDUbSbzF14AS4BFyfKtQrepjrb+LfAXMk/qXJuUTQHOTtZLyDyBswyYBxyWdey1yXFv0EyfNGvMNgP/DHyY9e+6CDi40O3J579x1jmKLoF4KhMzM0vFT2GZmVkqTiBmZpaKE4iZmaXiBGJmZqk4gZiZWSpOIGZFQNKJkh4tdBxm2ZxAzMwsFScQs0Yk6ZuS5klaJOk2Sa2S9zzclLy75ClJByV1j5L0J0kvS3qw6p0okvpKmiPpz5JelHR4cvoOkmYm70H5ddVsrmaF4gRi1kgk9QfOB46PiKOAHcAFwH7AgogYCDwLXJcccjcwISIGA69klf8auDUihgDHAVWzFh8NfIfMe0IOA47Pe6PM6uDJFM0azynAMcD8pHPQjswkkTuBGUmde4EHJHUEOkXEs0n5NOB+SfsDPSLiQYCIqARIzjcvIiqS7UVkpq55Lv/NMqudE4hZ4xEwLSJ+sEuh9C816qWdPyj7vRg78P+/VmC+hWXWeJ4iMzX3wVD93vfPkPn/7LykzjeA5yJiI7Be0t8k5RcCz0bEJjJTfo9MztFWUvsmbYVZjvwXjFkjiYjXJP0z8KSkfYCPyUzj/SEwLNn3HplxEoAxwH8mCeIt4JKk/ELgNklTknN8tQmbYZYzz8ZrlmeSNkdEh0LHYdbYfAvLzMxScQ/EzMxScQ/EzMxScQIxM7NUnEDMzCwVJxAzM0vFCcTMzFL5X/9BHo/lqvQVAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend('Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxMs8GEfMvPE",
        "outputId": "c75914d5-113a-4fde-f893-c5aa73d4fc01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"train_loss\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqMcJT5aFL01",
        "outputId": "05c2531c-3e79-42e2-90e8-0ea5ef2433d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chew chorals\n",
        "\n"
      ],
      "metadata": {
        "id": "fS4tzYkxr06Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_chew_pr (file_name, sentences):\n",
        "    path = \"AI-MA_project/chorales_converted/\"\n",
        "    fullname = os.path.join(path, \"chor\"+ file_name +\".xml\")\n",
        "    part = partitura.load_musicxml(fullname)\n",
        "        \n",
        "\n",
        "    ### apply chews method ### \n",
        "    chew_sep = partitura.musicanalysis.estimate_voices(part, monophonic_voices=True)\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_zero = np.where(chew_sep==1)\n",
        "    pos_one = np.where(chew_sep==2)\n",
        "    pos_two = np.where(chew_sep==3)\n",
        "    pos_three = np.where(chew_sep==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    part_zero = partitura.utils.ensure_notearray(part)[pos_zero]\n",
        "    part_one = partitura.utils.ensure_notearray(part)[pos_one]\n",
        "    part_two = partitura.utils.ensure_notearray(part)[pos_two]\n",
        "    part_three = partitura.utils.ensure_notearray(part)[pos_three]\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    pr_zero = partitura.utils.compute_pianoroll(part_zero, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(part_one, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(part_two, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    pr_three = partitura.utils.compute_pianoroll(part_three, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_three = pr_three.toarray()\n",
        "    \n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    #print(\"scores_comb.shape\",scores_comb.shape)\n",
        "    #print(\"sentences[:,None,:,:].shape\",sentences[:,None,:,:].shape)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "    \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "55-Dy39Cwg5v"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_chew(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    #note_counter_0 = 0\n",
        "    #note_counter_1 = 0\n",
        "    #note_counter_2 = 0\n",
        "    #note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx != 26 and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                #note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                #note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                #note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                #note_array_3 = part_3.note_array\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "                #note_counter_0 += len(note_array_0)\n",
        "                #note_counter_1 += len(note_array_1)\n",
        "                #note_counter_2 += len(note_array_2)\n",
        "                #note_counter_3 += len(note_array_3)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################            \n",
        "                    prediction = calculate_chew_pr(file_name,voices[:,:,:,-1]) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    #print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "NVvLm9pdryYJ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sampel 26&27 dont work "
      ],
      "metadata": {
        "id": "yD0w6nJQ82iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_chew(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "UD11ziChvL_-",
        "outputId": "6c86d405-ad96-4ecc-a146-a092d2605c1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmusicxml.py:302: UserWarning: Found repeat without start\n",
            "Starting point 0 is assumend\n",
            "  \"Starting point {} is assumend\".format(start_times[start_time_id]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0, sample 0: 1.0\n",
            "acc 1, sample 0: 0.9349593495934959\n",
            "acc 2, sample 0: 0.9724770642201835\n",
            "acc 3, sample 0: 0.9770114942528736\n",
            "acc 0, sample 1: 1.0\n",
            "acc 1, sample 1: 1.0\n",
            "acc 2, sample 1: 0.9833333333333333\n",
            "acc 3, sample 1: 1.0\n",
            "acc 0, sample 2: 0.9811320754716981\n",
            "acc 1, sample 2: 1.0\n",
            "acc 2, sample 2: 1.0\n",
            "acc 3, sample 2: 0.9767441860465116\n",
            "acc 0, sample 3: 0.9791666666666666\n",
            "acc 1, sample 3: 0.9183673469387755\n",
            "acc 2, sample 3: 1.0\n",
            "acc 3, sample 3: 0.9666666666666667\n",
            "acc 0, sample 4: 1.0\n",
            "acc 1, sample 4: 0.9\n",
            "acc 2, sample 4: 0.8536585365853658\n",
            "acc 3, sample 4: 1.0\n",
            "acc 0, sample 5: 1.0\n",
            "acc 1, sample 5: 0.9770114942528736\n",
            "acc 2, sample 5: 0.9655172413793104\n",
            "acc 3, sample 5: 0.926829268292683\n",
            "acc 0, sample 6: 1.0\n",
            "acc 1, sample 6: 0.9444444444444444\n",
            "acc 2, sample 6: 0.9702970297029703\n",
            "acc 3, sample 6: 0.9775280898876404\n",
            "acc 0, sample 7: 0.9298245614035088\n",
            "acc 1, sample 7: 0.9555555555555556\n",
            "acc 2, sample 7: 0.9607843137254902\n",
            "acc 3, sample 7: 0.9782608695652174\n",
            "acc 0, sample 8: 1.0\n",
            "acc 1, sample 8: 1.0\n",
            "acc 2, sample 8: 1.0\n",
            "acc 3, sample 8: 0.9565217391304348\n",
            "acc 0, sample 9: 0.9761904761904762\n",
            "acc 1, sample 9: 0.9\n",
            "acc 2, sample 9: 0.9473684210526315\n",
            "acc 3, sample 9: 1.0\n",
            "acc 0, sample 10: 1.0\n",
            "acc 1, sample 10: 0.9811320754716981\n",
            "acc 2, sample 10: 1.0\n",
            "acc 3, sample 10: 1.0\n",
            "acc 0, sample 11: 0.9873417721518988\n",
            "acc 1, sample 11: 0.9878048780487805\n",
            "acc 2, sample 11: 1.0\n",
            "acc 3, sample 11: 0.9615384615384616\n",
            "acc 0, sample 12: 1.0\n",
            "acc 1, sample 12: 0.875\n",
            "acc 2, sample 12: 1.0\n",
            "acc 3, sample 12: 1.0\n",
            "acc 0, sample 13: 1.0\n",
            "acc 1, sample 13: 0.9682539682539683\n",
            "acc 2, sample 13: 1.0\n",
            "acc 3, sample 13: 1.0\n",
            "acc 0, sample 14: 0.9863013698630136\n",
            "acc 1, sample 14: 0.9571428571428572\n",
            "acc 2, sample 14: 0.9516129032258065\n",
            "acc 3, sample 14: 0.9411764705882353\n",
            "acc 0, sample 15: 1.0\n",
            "acc 1, sample 15: 0.9811320754716981\n",
            "acc 2, sample 15: 1.0\n",
            "acc 3, sample 15: 0.975\n",
            "acc 0, sample 16: 0.9807692307692307\n",
            "acc 1, sample 16: 0.9433962264150944\n",
            "acc 2, sample 16: 0.9830508474576272\n",
            "acc 3, sample 16: 1.0\n",
            "acc 0, sample 17: 0.972972972972973\n",
            "acc 1, sample 17: 0.9411764705882353\n",
            "acc 2, sample 17: 0.9473684210526315\n",
            "acc 3, sample 17: 0.9354838709677419\n",
            "acc 0, sample 18: 1.0\n",
            "acc 1, sample 18: 0.9090909090909091\n",
            "acc 2, sample 18: 1.0\n",
            "acc 3, sample 18: 1.0\n",
            "acc 0, sample 19: 1.0\n",
            "acc 1, sample 19: 0.9818181818181818\n",
            "acc 2, sample 19: 0.9803921568627451\n",
            "acc 3, sample 19: 1.0\n",
            "acc 0, sample 20: 1.0\n",
            "acc 1, sample 20: 0.8909090909090909\n",
            "acc 2, sample 20: 0.8823529411764706\n",
            "acc 3, sample 20: 0.975\n",
            "acc 0, sample 21: 1.0\n",
            "acc 1, sample 21: 0.9423076923076923\n",
            "acc 2, sample 21: 0.9787234042553191\n",
            "acc 3, sample 21: 0.975609756097561\n",
            "acc 0, sample 22: 1.0\n",
            "acc 1, sample 22: 0.0\n",
            "acc 2, sample 22: 1.0\n",
            "acc 3, sample 22: 0.9791666666666666\n",
            "acc 0, sample 23: 0.9824561403508771\n",
            "acc 1, sample 23: 0.9574468085106383\n",
            "acc 2, sample 23: 1.0\n",
            "acc 3, sample 23: 0.9473684210526315\n",
            "acc 0, sample 24: 1.0\n",
            "acc 1, sample 24: 0.967741935483871\n",
            "acc 2, sample 24: 0.9629629629629629\n",
            "acc 3, sample 24: 0.9433962264150944\n",
            "acc 0, sample 25: 1.0\n",
            "acc 1, sample 25: 0.94\n",
            "acc 2, sample 25: 1.0\n",
            "acc 3, sample 25: 1.0\n",
            "acc 0, sample 28: 0.9777777777777777\n",
            "acc 1, sample 28: 0.926829268292683\n",
            "acc 2, sample 28: 0.9285714285714286\n",
            "acc 3, sample 28: 0.9722222222222222\n",
            "acc 0, sample 29: 0.9512195121951219\n",
            "acc 1, sample 29: 0.9534883720930233\n",
            "acc 2, sample 29: 0.9111111111111111\n",
            "acc 3, sample 29: 0.918918918918919\n",
            "acc 0, sample 30: 0.030303030303030304\n",
            "acc 1, sample 30: 0.0\n",
            "acc 2, sample 30: 0.9736842105263158\n",
            "acc 3, sample 30: 0.967741935483871\n",
            "acc 0, sample 31: 1.0\n",
            "acc 1, sample 31: 1.0\n",
            "acc 2, sample 31: 1.0\n",
            "acc 3, sample 31: 1.0\n",
            "acc 0, sample 32: 0.9857142857142858\n",
            "acc 1, sample 32: 0.9622641509433962\n",
            "acc 2, sample 32: 0.9640287769784173\n",
            "acc 3, sample 32: 0.9224137931034483\n",
            "acc 0, sample 33: 1.0\n",
            "acc 1, sample 33: 0.975\n",
            "acc 2, sample 33: 1.0\n",
            "acc 3, sample 33: 0.9705882352941176\n",
            "acc 0, sample 34: 0.9753086419753086\n",
            "acc 1, sample 34: 0.918918918918919\n",
            "acc 2, sample 34: 0.9866666666666667\n",
            "acc 3, sample 34: 0.9846153846153847\n",
            "acc 0, sample 35: 0.9642857142857143\n",
            "acc 1, sample 35: 0.9387755102040817\n",
            "acc 2, sample 35: 0.9607843137254902\n",
            "acc 3, sample 35: 0.9777777777777777\n",
            "acc 0, sample 36: 1.0\n",
            "acc 1, sample 36: 0.9482758620689655\n",
            "acc 2, sample 36: 0.9464285714285714\n",
            "acc 3, sample 36: 1.0\n",
            "acc 0, sample 37: 1.0\n",
            "acc 1, sample 37: 0.96\n",
            "acc 2, sample 37: 1.0\n",
            "acc 3, sample 37: 1.0\n",
            "acc 0, sample 38: 0.9803921568627451\n",
            "acc 1, sample 38: 0.8867924528301887\n",
            "acc 2, sample 38: 0.9795918367346939\n",
            "acc 3, sample 38: 0.9302325581395349\n",
            "acc 0, sample 39: 0.09523809523809523\n",
            "acc 1, sample 39: 0.8636363636363636\n",
            "acc 2, sample 39: 0.9242424242424242\n",
            "acc 3, sample 39: 1.0\n",
            "acc 0, sample 40: 0.9090909090909091\n",
            "acc 1, sample 40: 0.7857142857142857\n",
            "acc 2, sample 40: 0.92\n",
            "acc 3, sample 40: 0.9787234042553191\n",
            "acc 0, sample 41: 0.08974358974358974\n",
            "acc 1, sample 41: 0.0\n",
            "acc 2, sample 41: 0.9571428571428572\n",
            "acc 3, sample 41: 1.0\n",
            "acc 0, sample 42: 1.0\n",
            "acc 1, sample 42: 1.0\n",
            "acc 2, sample 42: 1.0\n",
            "acc 3, sample 42: 0.9787234042553191\n",
            "acc 0, sample 43: 0.9777777777777777\n",
            "acc 1, sample 43: 0.9743589743589743\n",
            "acc 2, sample 43: 1.0\n",
            "acc 3, sample 43: 0.9743589743589743\n",
            "acc 0, sample 44: 1.0\n",
            "acc 1, sample 44: 0.9253731343283582\n",
            "acc 2, sample 44: 0.9666666666666667\n",
            "acc 3, sample 44: 0.9827586206896551\n",
            "acc 0, sample 45: 1.0\n",
            "acc 1, sample 45: 1.0\n",
            "acc 2, sample 45: 0.9615384615384616\n",
            "acc 3, sample 45: 0.9130434782608695\n",
            "acc 0, sample 46: 1.0\n",
            "acc 1, sample 46: 1.0\n",
            "acc 2, sample 46: 1.0\n",
            "acc 3, sample 46: 0.9354838709677419\n",
            "acc 0, sample 47: 1.0\n",
            "acc 1, sample 47: 0.9454545454545454\n",
            "acc 2, sample 47: 1.0\n",
            "acc 3, sample 47: 1.0\n",
            "acc 0, sample 48: 0.9661016949152542\n",
            "acc 1, sample 48: 0.9333333333333333\n",
            "acc 2, sample 48: 0.9827586206896551\n",
            "acc 3, sample 48: 0.9803921568627451\n",
            "acc 0, sample 49: 1.0\n",
            "acc 1, sample 49: 0.9302325581395349\n",
            "acc 2, sample 49: 1.0\n",
            "acc 3, sample 49: 0.9705882352941176\n",
            "acc 0, sample 50: 1.0\n",
            "acc 1, sample 50: 0.9629629629629629\n",
            "acc 2, sample 50: 0.9622641509433962\n",
            "acc 3, sample 50: 0.975609756097561\n",
            "acc 0, sample 51: 1.0\n",
            "acc 1, sample 51: 0.9811320754716981\n",
            "acc 2, sample 51: 0.9743589743589743\n",
            "acc 3, sample 51: 1.0\n",
            "acc 0, sample 52: 0.9090909090909091\n",
            "acc 1, sample 52: 0.8205128205128205\n",
            "acc 2, sample 52: 0.9166666666666666\n",
            "acc 3, sample 52: 1.0\n",
            "acc 0, sample 53: 1.0\n",
            "acc 1, sample 53: 0.9245283018867925\n",
            "acc 2, sample 53: 1.0\n",
            "acc 3, sample 53: 1.0\n",
            "acc 0, sample 54: 0.9358974358974359\n",
            "acc 1, sample 54: 0.8554216867469879\n",
            "acc 2, sample 54: 0.9583333333333334\n",
            "acc 3, sample 54: 0.9682539682539683\n",
            "acc 0, sample 55: 1.0\n",
            "acc 1, sample 55: 0.9736842105263158\n",
            "acc 2, sample 55: 1.0\n",
            "acc 3, sample 55: 0.9696969696969697\n",
            "total_predictions_dict dict_keys(['0', '1', '2', '3'])\n",
            "0.9356314221612648 0.8907663175689277 0.9724951601540366 0.9752860342910543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# McLeod chorals"
      ],
      "metadata": {
        "id": "9ejZujau-TXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mcleod_pr (file_name, sentences):\n",
        "    path = \"AI-MA_project/hmm_sep/\"\n",
        "    fullname = os.path.join(path, \"chor\"+ file_name +\".mid\")\n",
        "        \n",
        "\n",
        "    ### apply chews method ### \n",
        "    part_hmm = partitura.load_score_midi(fullname,part_voice_assign_mode=2)\n",
        "    voice_info = partitura.utils.note_array_from_part(part_hmm)[\"voice\"]\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(voice_info==1)\n",
        "    pos_1 = np.where(voice_info==2)\n",
        "    pos_2 = np.where(voice_info==3)\n",
        "    pos_3 = np.where(voice_info==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    note_array_0= partitura.utils.ensure_notearray(part_hmm)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part_hmm)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part_hmm)[pos_2]\n",
        "    note_array_3 = partitura.utils.ensure_notearray(part_hmm)[pos_3]\n",
        "\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_three = pr_three.toarray()\n",
        "    \n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    #print(\"scores_comb.shape\",scores_comb.shape)\n",
        "    #print(\"sentences[:,None,:,:].shape\",sentences[:,None,:,:].shape)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "    \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "pQP8pq9a-S6V"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mc_leod(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx not in [16,17,18,27,28,32,44,45,48,49,50]: # and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                #note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                #note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                #note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                #note_array_3 = part_3.note_array\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################                   \n",
        "                    prediction = calculate_mcleod_pr(file_name,voices[:,:,:,-1]) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "2igkR5SI-Z1L"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "does not work for samples 16,17,18,27,28,32,44,45,48,49,50"
      ],
      "metadata": {
        "id": "g1JC5yeMBnnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_mc_leod(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "oxRr270dC_mj",
        "outputId": "ab650931-9a58-441a-c55e-3bd6cda5d9de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:419: UserWarning: pitch spelling\n",
            "  warnings.warn(\"pitch spelling\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: voice estimation\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: create_part\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add time sigs and measures\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: tie notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: find tuplets\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: done create_part\n",
            "  part_name=part_names.get(part_nr, None),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0, sample 0: 1.0\n",
            "acc 1, sample 0: 0.9512195121951219\n",
            "acc 2, sample 0: 0.963302752293578\n",
            "acc 3, sample 0: 0.9770114942528736\n",
            "acc 0, sample 1: 1.0\n",
            "acc 1, sample 1: 1.0\n",
            "acc 2, sample 1: 0.9833333333333333\n",
            "acc 3, sample 1: 1.0\n",
            "acc 0, sample 2: 1.0\n",
            "acc 1, sample 2: 0.9791666666666666\n",
            "acc 2, sample 2: 1.0\n",
            "acc 3, sample 2: 0.9767441860465116\n",
            "acc 0, sample 3: 1.0\n",
            "acc 1, sample 3: 0.9183673469387755\n",
            "acc 2, sample 3: 0.9782608695652174\n",
            "acc 3, sample 3: 0.9666666666666667\n",
            "acc 0, sample 4: 1.0\n",
            "acc 1, sample 4: 0.9\n",
            "acc 2, sample 4: 0.8536585365853658\n",
            "acc 3, sample 4: 1.0\n",
            "acc 0, sample 5: 0.9777777777777777\n",
            "acc 1, sample 5: 0.9655172413793104\n",
            "acc 2, sample 5: 0.9885057471264368\n",
            "acc 3, sample 5: 0.9024390243902439\n",
            "acc 0, sample 6: 1.0\n",
            "acc 1, sample 6: 0.9351851851851852\n",
            "acc 2, sample 6: 0.9801980198019802\n",
            "acc 3, sample 6: 0.9775280898876404\n",
            "acc 0, sample 7: 0.9824561403508771\n",
            "acc 1, sample 7: 0.8888888888888888\n",
            "acc 2, sample 7: 0.9803921568627451\n",
            "acc 3, sample 7: 0.9565217391304348\n",
            "acc 0, sample 8: 1.0\n",
            "acc 1, sample 8: 1.0\n",
            "acc 2, sample 8: 1.0\n",
            "acc 3, sample 8: 0.9565217391304348\n",
            "acc 0, sample 9: 0.9761904761904762\n",
            "acc 1, sample 9: 0.9\n",
            "acc 2, sample 9: 0.9473684210526315\n",
            "acc 3, sample 9: 1.0\n",
            "acc 0, sample 10: 1.0\n",
            "acc 1, sample 10: 0.9811320754716981\n",
            "acc 2, sample 10: 1.0\n",
            "acc 3, sample 10: 1.0\n",
            "acc 0, sample 11: 0.9873417721518988\n",
            "acc 1, sample 11: 0.9878048780487805\n",
            "acc 2, sample 11: 1.0\n",
            "acc 3, sample 11: 0.9615384615384616\n",
            "acc 0, sample 12: 1.0\n",
            "acc 1, sample 12: 0.875\n",
            "acc 2, sample 12: 1.0\n",
            "acc 3, sample 12: 1.0\n",
            "acc 0, sample 13: 1.0\n",
            "acc 1, sample 13: 0.9523809523809523\n",
            "acc 2, sample 13: 1.0\n",
            "acc 3, sample 13: 1.0\n",
            "acc 0, sample 14: 0.9863013698630136\n",
            "acc 1, sample 14: 0.9285714285714286\n",
            "acc 2, sample 14: 0.9838709677419355\n",
            "acc 3, sample 14: 0.9019607843137255\n",
            "acc 0, sample 15: 1.0\n",
            "acc 1, sample 15: 0.9811320754716981\n",
            "acc 2, sample 15: 1.0\n",
            "acc 3, sample 15: 0.95\n",
            "acc 0, sample 19: 1.0\n",
            "acc 1, sample 19: 0.9818181818181818\n",
            "acc 2, sample 19: 0.9803921568627451\n",
            "acc 3, sample 19: 1.0\n",
            "acc 0, sample 20: 1.0\n",
            "acc 1, sample 20: 0.9272727272727272\n",
            "acc 2, sample 20: 0.8823529411764706\n",
            "acc 3, sample 20: 0.95\n",
            "acc 0, sample 21: 1.0\n",
            "acc 1, sample 21: 0.9615384615384616\n",
            "acc 2, sample 21: 0.9787234042553191\n",
            "acc 3, sample 21: 0.975609756097561\n",
            "acc 0, sample 22: 1.0\n",
            "acc 1, sample 22: 0.9318181818181818\n",
            "acc 2, sample 22: 1.0\n",
            "acc 3, sample 22: 0.9791666666666666\n",
            "acc 0, sample 23: 1.0\n",
            "acc 1, sample 23: 0.9361702127659575\n",
            "acc 2, sample 23: 1.0\n",
            "acc 3, sample 23: 0.9473684210526315\n",
            "acc 0, sample 24: 1.0\n",
            "acc 1, sample 24: 0.9516129032258065\n",
            "acc 2, sample 24: 0.9629629629629629\n",
            "acc 3, sample 24: 0.9245283018867925\n",
            "acc 0, sample 25: 1.0\n",
            "acc 1, sample 25: 0.94\n",
            "acc 2, sample 25: 1.0\n",
            "acc 3, sample 25: 1.0\n",
            "acc 0, sample 26: 1.0\n",
            "acc 1, sample 26: 0.9487179487179487\n",
            "acc 2, sample 26: 0.9473684210526315\n",
            "acc 3, sample 26: 0.9473684210526315\n",
            "acc 0, sample 29: 0.975609756097561\n",
            "acc 1, sample 29: 0.9534883720930233\n",
            "acc 2, sample 29: 0.9333333333333333\n",
            "acc 3, sample 29: 0.8918918918918919\n",
            "acc 0, sample 30: 1.0\n",
            "acc 1, sample 30: 0.975\n",
            "acc 2, sample 30: 1.0\n",
            "acc 3, sample 30: 0.9354838709677419\n",
            "acc 0, sample 31: 1.0\n",
            "acc 1, sample 31: 1.0\n",
            "acc 2, sample 31: 1.0\n",
            "acc 3, sample 31: 1.0\n",
            "acc 0, sample 33: 1.0\n",
            "acc 1, sample 33: 0.975\n",
            "acc 2, sample 33: 1.0\n",
            "acc 3, sample 33: 0.9705882352941176\n",
            "acc 0, sample 34: 0.9753086419753086\n",
            "acc 1, sample 34: 0.918918918918919\n",
            "acc 2, sample 34: 0.9866666666666667\n",
            "acc 3, sample 34: 0.9846153846153847\n",
            "acc 0, sample 35: 0.9821428571428571\n",
            "acc 1, sample 35: 0.9183673469387755\n",
            "acc 2, sample 35: 0.9803921568627451\n",
            "acc 3, sample 35: 0.9555555555555556\n",
            "acc 0, sample 36: 0.9692307692307692\n",
            "acc 1, sample 36: 0.9482758620689655\n",
            "acc 2, sample 36: 0.9285714285714286\n",
            "acc 3, sample 36: 1.0\n",
            "acc 0, sample 37: 1.0\n",
            "acc 1, sample 37: 0.96\n",
            "acc 2, sample 37: 1.0\n",
            "acc 3, sample 37: 1.0\n",
            "acc 0, sample 38: 1.0\n",
            "acc 1, sample 38: 0.8490566037735849\n",
            "acc 2, sample 38: 0.9795918367346939\n",
            "acc 3, sample 38: 0.9302325581395349\n",
            "acc 0, sample 39: 1.0\n",
            "acc 1, sample 39: 0.8787878787878788\n",
            "acc 2, sample 39: 0.9242424242424242\n",
            "acc 3, sample 39: 1.0\n",
            "acc 0, sample 40: 0.9636363636363636\n",
            "acc 1, sample 40: 0.7678571428571429\n",
            "acc 2, sample 40: 0.92\n",
            "acc 3, sample 40: 0.9787234042553191\n",
            "acc 0, sample 41: 1.0\n",
            "acc 1, sample 41: 0.9146341463414634\n",
            "acc 2, sample 41: 0.9714285714285714\n",
            "acc 3, sample 41: 0.9859154929577465\n",
            "acc 0, sample 42: 1.0\n",
            "acc 1, sample 42: 1.0\n",
            "acc 2, sample 42: 1.0\n",
            "acc 3, sample 42: 0.9787234042553191\n",
            "acc 0, sample 43: 1.0\n",
            "acc 1, sample 43: 0.9487179487179487\n",
            "acc 2, sample 43: 1.0\n",
            "acc 3, sample 43: 0.9743589743589743\n",
            "acc 0, sample 46: 1.0\n",
            "acc 1, sample 46: 1.0\n",
            "acc 2, sample 46: 0.972972972972973\n",
            "acc 3, sample 46: 0.9354838709677419\n",
            "acc 0, sample 47: 1.0\n",
            "acc 1, sample 47: 0.9272727272727272\n",
            "acc 2, sample 47: 1.0\n",
            "acc 3, sample 47: 1.0\n",
            "acc 0, sample 51: 1.0\n",
            "acc 1, sample 51: 0.9811320754716981\n",
            "acc 2, sample 51: 0.9487179487179487\n",
            "acc 3, sample 51: 1.0\n",
            "acc 0, sample 52: 0.9393939393939394\n",
            "acc 1, sample 52: 0.8205128205128205\n",
            "acc 2, sample 52: 0.9722222222222222\n",
            "acc 3, sample 52: 0.967741935483871\n",
            "acc 0, sample 53: 1.0\n",
            "acc 1, sample 53: 0.9245283018867925\n",
            "acc 2, sample 53: 1.0\n",
            "acc 3, sample 53: 1.0\n",
            "acc 0, sample 54: 0.9487179487179487\n",
            "acc 1, sample 54: 0.8674698795180723\n",
            "acc 2, sample 54: 0.9583333333333334\n",
            "acc 3, sample 54: 0.9682539682539683\n",
            "acc 0, sample 55: 1.0\n",
            "acc 1, sample 55: 0.9736842105263158\n",
            "acc 2, sample 55: 1.0\n",
            "acc 3, sample 55: 0.9696969696969697\n",
            "total_predictions_dict dict_keys(['0', '1', '2', '3'])\n",
            "0.9925357291673065 0.93835598008982 0.9752703019057709 0.9706275393068314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chew fugues"
      ],
      "metadata": {
        "id": "uenr6Uavaic3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_chew_pr_fugue (file_name, sentences, nbr_voices):\n",
        "    path = \"AI-MA_project/bach_fugues/\"\n",
        "    fullname = os.path.join(path, \"wtc\"+ file_name +\".mid\")\n",
        "\n",
        "    part = partitura.load_score_midi(fullname)\n",
        "\n",
        "\n",
        "    ### apply chews method ### \n",
        "    chew_sep = partitura.musicanalysis.estimate_voices(part, monophonic_voices=True)\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(chew_sep==1)\n",
        "    pos_1 = np.where(chew_sep==2)\n",
        "    pos_2 = np.where(chew_sep==3)\n",
        "    if nbr_voices ==4:\n",
        "        pos_3 = np.where(chew_sep==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    \n",
        "    note_array_0 = partitura.utils.ensure_notearray(part)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part)[pos_2]\n",
        "    if nbr_voices==4:\n",
        "        note_array_3 = partitura.utils.ensure_notearray(part)[pos_3]\n",
        "\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    onset_beat_0 = note_array_0['onset_beat'][-1]\n",
        "    duration_beat_0 = note_array_0['duration_beat'][-1]\n",
        "    beat_0 = onset_beat_0 + duration_beat_0\n",
        "    \n",
        "    onset_beat_1 = note_array_1['onset_beat'][-1]\n",
        "    duration_beat_1 = note_array_1['duration_beat'][-1]\n",
        "    beat_1 = onset_beat_1 + duration_beat_1\n",
        "    \n",
        "    onset_beat_2 = note_array_2['onset_beat'][-1]\n",
        "    duration_beat_2 = note_array_2['duration_beat'][-1]\n",
        "    beat_2 = onset_beat_2 + duration_beat_2\n",
        "\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_0)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_1)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_2)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    if nbr_voices==4:\n",
        "        onset_beat_3 = note_array_3['onset_beat'][-1]\n",
        "        duration_beat_3 = note_array_3['duration_beat'][-1]\n",
        "        beat_3 = onset_beat_3 + duration_beat_3\n",
        "        pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_3)\n",
        "        pr_three = pr_three.toarray()\n",
        "    else:\n",
        "        pr_three = np.zeros(pr_two.shape)\n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0)) \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "vNsE3VVMa9Oo"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_chew_fugue( train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx != 26 and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                \n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "\n",
        "                if len(part) == 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################\n",
        "                    prediction = calculate_chew_pr_fugue(file_name,voices[:,:,:,-1],nbr_voices) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "C50fX7xjasD1"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True: \n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_chew_fugue(val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "yWz4I9b7asVR"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# McLeod fugues"
      ],
      "metadata": {
        "id": "xofnsEX4DAME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mcleod_pr (file_name, sentences,nbr_voices):\n",
        "    path = \"AI-MA_project/bach_fugues/\"\n",
        "    fullname = os.path.join(path, \"wtc\"+ file_name +\".mid\")\n",
        "    ### apply chews method ### \n",
        "    part_hmm = partitura.load_score_midi(fullname,part_voice_assign_mode=2)\n",
        "    voice_info = partitura.utils.note_array_from_part(part_hmm)[\"voice\"]\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(voice_info==1)\n",
        "    pos_1 = np.where(voice_info==2)\n",
        "    pos_2 = np.where(voice_info==3)\n",
        "    if nbr_voices ==4:\n",
        "        pos_3 = np.where(voice_info==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    note_array_0= partitura.utils.ensure_notearray(part_hmm)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part_hmm)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part_hmm)[pos_2]\n",
        "    if nbr_voices==4:\n",
        "        note_array_3 = partitura.utils.ensure_notearray(part_hmm)[pos_3]\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    onset_beat_0 = note_array_0['onset_beat'][-1]\n",
        "    duration_beat_0 = note_array_0['duration_beat'][-1]\n",
        "    beat_0 = onset_beat_0 + duration_beat_0\n",
        "    \n",
        "    onset_beat_1 = note_array_1['onset_beat'][-1]\n",
        "    duration_beat_1 = note_array_1['duration_beat'][-1]\n",
        "    beat_1 = onset_beat_1 + duration_beat_1\n",
        "    \n",
        "    onset_beat_2 = note_array_2['onset_beat'][-1]\n",
        "    duration_beat_2 = note_array_2['duration_beat'][-1]\n",
        "    beat_2 = onset_beat_2 + duration_beat_2\n",
        "\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_0)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_1)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_2)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    if nbr_voices==4:\n",
        "        onset_beat_3 = note_array_3['onset_beat'][-1]\n",
        "        duration_beat_3 = note_array_3['duration_beat'][-1]\n",
        "        beat_3 = onset_beat_3 + duration_beat_3\n",
        "        pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_3)\n",
        "        pr_three = pr_three.toarray()\n",
        "    else:\n",
        "        pr_three = np.zeros(pr_two.shape)\n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0)) \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "CU4iH-25aD5I"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mc_leod_fugues(train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            #if idx not in [16,17,18,27,28,32,44,45,48,49,50]: # and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "\n",
        "                if len(part) == 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "           \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "          \n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################                   \n",
        "                    prediction = calculate_mcleod_pr(file_name,voices[:,:,:,-1],nbr_voices) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "DgFfq9Q4Csei"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True: \n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_mc_leod_fugues(val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "5kSrFU8e-uol"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old training loop - matrix and non matrix format"
      ],
      "metadata": {
        "id": "4olpdwzyG8dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        #accuracy_v0_sum = 0\n",
        "        #accuracy_v1_sum = 0\n",
        "        #accuracy_v2_sum = 0\n",
        "        #accuracy_v3_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4: \n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                    v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                    mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_pred_flat:\", v_pred_flat.shape)\n",
        "                    \"\"\"\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4:                   \n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                    v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                    mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                    print(\"old mask\", mask_ori.shape)\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_ori_flat\", v_ori_flat.shape)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)   \n",
        "                    print(\"acc\",acc)                    \n",
        "                    accuracy_sum += acc \n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                    acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                    for i in range(len(prediction[0,:])):\n",
        "                      acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                      accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    \n",
        "                    \"\"\"\n",
        "                    pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "\n",
        "                    acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                    acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                    acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                    if nbr_voices == 4:\n",
        "                        acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                    \n",
        "                    # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    if nbr_voices == 4:\n",
        "                        accuracy_v3_sum += acc_v3 / len(lens)\n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "            train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "            train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "            train_accuracy_v3 = accuracy_v3_sum / 18   ## bc only 18 pieces with len 3\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "            history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "            history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "            #if nbr_voices == 4:\n",
        "            history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3)) \n",
        "            #else:\n",
        "            #    print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2)) \n",
        "            \"\"\"\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_v0_sum = 0\n",
        "                accuracy_v1_sum = 0\n",
        "                accuracy_v2_sum = 0\n",
        "                accuracy_v3_sum = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                        truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                        acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                        for i in range(len(prediction[0,:])):\n",
        "                          acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                          val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                        #print(\"val_accuracy_sum_list[3]\",val_accuracy_sum_list[3])\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(train_dataloader)\n",
        "                    #val_acc_list[3] = val_acc_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_acc_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "                #history[\"val_acc_new\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        # Predict the model's output on a batch\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                            \n",
        "                        # compute the accuracy \n",
        "                        acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                        acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                        acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                        if nbr_voices == 4:\n",
        "                            acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                            \n",
        "                            \n",
        "                        # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                        accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                        accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                        accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                        if nbr_voices == 4:\n",
        "                            accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                    val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                    val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                    val_accuracy_v3 = accuracy_v3_sum / 18  ##len(val_dataloader). - bc 18 pieces only with voice 3\n",
        "\n",
        "\n",
        "                    val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    val_acc_list[3] = val_accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    val_acc_list[4] = val_accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                    \n",
        "\n",
        "\n",
        "                history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "                history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "                history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "                #if nbr_voices == 4:\n",
        "                history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "                #else:\n",
        "                #    print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2))\n",
        "\n",
        "\n",
        "                history[\"val_acc_new\"].append(val_acc_list)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "\n",
        "                        \"\"\"\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                        if nbr_voices == 4: \n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                        v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                        mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)                      \n",
        "                        if nbr_voices == 4:                   \n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                        v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                        mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)\n",
        "                        accuracy_sum += acc \n",
        "                        \"\"\"\n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "nfDV8MKGHE3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "    \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "  \n",
        "\n",
        "    def __getitem__(self, idx):          \n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])    \n",
        "\n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "\n",
        "\n",
        "        return (v0, v1, v2, v3, v_all, length)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "x2lFUuw719EC",
        "outputId": "fcd32bf0-4605-4a38-d05a-cd6d71e4d033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass MusicDataset(Dataset):\\n\\n    def __init__(self, data_dir, transforms=None):\\n        self.transforms = transforms\\n        self.data_dir = data_dir\\n\\n        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\\n        self.labels = labels\\n        self.pr_dict = {}\\n        len_list = []\\n\\n        for iLabel in range(len(labels)):\\n            \\n            if iLabel == 4:   \\n                voice_files = []\\n                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \\n                for name in file_names:\\n                    with open(name ,\\'rb\\') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\\n                        loaded_obj = pickle.load(f)  \\n                        voice_files.append(loaded_obj) \\n                        len_list.append(len(loaded_obj.T))\\n                        \\n                self.pr_dict[self.labels[iLabel]] = voice_files \\n                self.pr_dict[\"length\"] = len_list\\n    \\n            else:\\n                voice_files = []\\n                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \\n                for name in file_names:\\n                    with open(name ,\\'rb\\') as f: \\n                        loaded_obj = pickle.load(f)     \\n                        voice_files.append(loaded_obj)\\n\\n                self.pr_dict[self.labels[iLabel]] = voice_files \\n\\n\\n    def __len__(self):\\n      \\n        return len(self.pr_dict[self.labels[0]])\\n  \\n\\n    def __getitem__(self, idx):          \\n\\n        out_list = []\\n        for key, value in self.pr_dict.items():\\n            out_list.append(self.pr_dict[key][idx])    \\n\\n        v0 = torch.tensor(out_list[0].T)\\n        v1 = torch.tensor(out_list[1].T)\\n        v2 = torch.tensor(out_list[2].T)\\n        v3 = torch.tensor(out_list[3].T)\\n        v_all = torch.tensor(out_list[4].T) \\n        length = self.pr_dict[\"length\"][idx]\\n\\n\\n        return (v0, v1, v2, v3, v_all, length)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    }
  ]
}