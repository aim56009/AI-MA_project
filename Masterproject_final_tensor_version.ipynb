{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject_final_tensor_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "SsyC2uB0KfaT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ea7f12-fb16-4c7a-8c0e-7b3366961588"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install partitura\n",
        "! pip install git+https://github.com/CPJKU/partitura.git@develop\n",
        "import partitura\n",
        "import statistics"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/CPJKU/partitura.git@develop\n",
            "  Cloning https://github.com/CPJKU/partitura.git (to revision develop) to /tmp/pip-req-build-sgw7x53_\n",
            "  Running command git clone -q https://github.com/CPJKU/partitura.git /tmp/pip-req-build-sgw7x53_\n",
            "  Running command git checkout -b develop --track origin/develop\n",
            "  Switched to a new branch 'develop'\n",
            "  Branch 'develop' set up to track remote branch 'develop' from 'origin'.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.4.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (4.2.6)\n",
            "Requirement already satisfied: lark-parser in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (0.12.0)\n",
            "Requirement already satisfied: xmlschema in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.11.2)\n",
            "Requirement already satisfied: mido in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.2.10)\n",
            "Requirement already satisfied: elementpath<3.0.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from xmlschema->partitura==0.4.0) (2.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path_to_musicxml = \"AI-MA_project/chorales_converted/chor001.xml\"\n",
        "#part = partitura.load_musicxml(path_to_musicxml)\n",
        "#partitura.save_score_midi(part,\"chor001_5.mid\",5)"
      ],
      "metadata": {
        "id": "j6dq9ptQGmHB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader - Set the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "fugues = False\n",
        "\n",
        "if fugues == True:\n",
        "    PATH_TO_DATA = \"AI-MA_project/bach_pr_fugues\"\n",
        "else:\n",
        "    PATH_TO_DATA = \"AI-MA_project/pianoroll_88\"\n",
        "\n",
        "batch_size = 1 \n",
        "workers = 0"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_new(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        self.name_list = [\"1f01\",\"1f02\",\"1f03\",\"1f04\",\"1f05\",\"1f06\",\"1f07\",\"1f08\",\"1f09\",\"1f10\",\"1f11\",\"1f12\",\"1f13\",\"1f14\",\"1f15\",\"1f16\",\"1f17\",\"1f18\",\"1f19\",\"1f20\",\"1f21\",\"1f22\",\"1f23\",\"1f24\",\"2f01\",\"2f02\",\"2f03\",\"2f04\",\"2f05\",\"2f06\",\"2f07\",\"2f08\",\"2f09\",\"2f10\",\"2f11\",\"2f12\",\"2f13\",\"2f14\",\"2f15\",\"2f16\",\"2f17\",\"2f18\",\"2f19\",\"2f20\",\"2f21\",\"2f22\",\"2f23\",\"2f24\"]\n",
        "        self.name_list_voice_3 =  ['1f01', '1f05', '1f12', '1f14', '1f16', '1f17', '1f18', '1f23', '1f24', '2f02', '2f05', '2f07', '2f08', '2f09', '2f16', '2f17',  '2f22', '2f23']\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "\n",
        "                        file_names_list.append(name[-8:-4])\n",
        "\n",
        "                    if \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[49:53] + \".pkl\" in sorted(glob.glob(os.path.join(PATH_TO_DATA, \"voice_3\", \"*.pkl\"))):\n",
        "                        nbr_voices_list.append(4)\n",
        "                    else:\n",
        "                        nbr_voices_list.append(3)\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "\n",
        "\n",
        "            if iLabel == 3:  \n",
        "                voice_files = []\n",
        "                file_names_3 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[2], \"*.pkl\")))   \n",
        "                \n",
        "                ###### loop over all filnames in voices_2 and if an element there is not present in voices_3: append \"missing\" to the voice_files of label=3 => important bc. self.pr_dict[voice_3] has then len 42 and otherwise it would only have len 18  .. these \"missing\" el are not considered later in the dataloader (if len=3 is a diff case of get_idx)\n",
        "                for name in file_names_2:\n",
        "                    if name[45:49] in self.name_list_voice_3:\n",
        "                      correct_name_3 = \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[45:49] + \".pkl\"\n",
        "                      with open(correct_name_3 ,'rb') as f:  \n",
        "                            loaded_obj = pickle.load(f)  \n",
        "                            voice_files.append(loaded_obj)\n",
        "                    else:\n",
        "                      voice_files.append(\"missing\")\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                \n",
        "\n",
        "                \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                          loaded_obj = pickle.load(f)     \n",
        "                          voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 4:\n",
        "            for key, value in self.pr_dict.items():\n",
        "              out_list.append(self.pr_dict[key][idx])\n",
        "                              \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.tensor(out_list[3].T)\n",
        "            v_all = torch.tensor(out_list[4].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 3:\n",
        "\n",
        "            for key, value in self.pr_dict.items():\n",
        "                if key != \"voice_3\":\n",
        "                  out_list.append(self.pr_dict[key][idx]) \n",
        "            \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.zeros(v2.shape)\n",
        "            v_all = torch.tensor(out_list[3].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "            \n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        "
      ],
      "metadata": {
        "id": "3FxK6qr1FqIl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_chor(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        file_names_list.append(name[-7:-4])      # e.g. name = AI-MA_project/pianoroll_88/voice_all/voice_all_001.pkl\n",
        "\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "              \n",
        "\n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])\n",
        "                            \n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "        file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "        voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "        \n",
        "        return (voices, length, 4, file_name)     # 4 bc nbr voices is always 4"
      ],
      "metadata": {
        "id": "3uQnok3VGngZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "if fugues == True:\n",
        "    dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "else:\n",
        "    dataset = MusicDataset_chor(PATH_TO_DATA)\n",
        "\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 1:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXsRYzQSUuQU",
        "outputId": "1efd71f1-3839-4549-b75d-dac737717c7f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "002 tensor([4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "pianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\n",
        "pianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\n",
        "pianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\n",
        "pianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\n",
        "pianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\n",
        "\n",
        "time_unit = \"beat\"\n",
        "time_div = 12\n",
        "piano_range = True\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fR2HaA_BqeHw",
        "outputId": "1915499e-e11b-4158-eeff-a52a0ce84752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\\npianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\\npianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\\npianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\\npianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\\n\\ntime_unit = \"beat\"\\ntime_div = 12\\npiano_range = True\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "fig, ax = plt.subplots(1, figsize=(20, 10))\n",
        "ax.imshow(pianoroll_all, origin=\"lower\", cmap='gray', interpolation='nearest', aspect='auto')\n",
        "ax.set_xlabel(f'Time ({time_unit}s/{time_div})')\n",
        "ax.set_ylabel('Piano key' if piano_range else 'MIDI pitch')\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "sRoyBJJVqX_u",
        "outputId": "6308eff8-c6b1-437d-bc59-86b37520a10c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfig, ax = plt.subplots(1, figsize=(20, 10))\\nax.imshow(pianoroll_all, origin=\"lower\", cmap=\\'gray\\', interpolation=\\'nearest\\', aspect=\\'auto\\')\\nax.set_xlabel(f\\'Time ({time_unit}s/{time_div})\\')\\nax.set_ylabel(\\'Piano key\\' if piano_range else \\'MIDI pitch\\')\\nplt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "  if i ==10:\n",
        "    all_voices, length, nbr_voices, _ = sample_batched\n",
        "    all_voices_pr = all_voices[0,:,:,-1].numpy()\n",
        "    \n",
        "    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\n",
        "    print(note_array.shape)\n",
        "    print(note_array[:10])\n",
        "    print(note_array.dtype.names)\n",
        "\n",
        "    #print(i,nbr_voices,all_voices.shape)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "b4QCaMEi3nw7",
        "outputId": "3f147e00-aa16-41c5-b3c3-65073371ac4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor i, sample_batched in enumerate(loader):\\n    all_voices, length, nbr_voices = sample_batched\\n    if nbr_voices ==3:\\n      print(i,nbr_voices,all_voices.shape)\\n    else:\\n      print(i,nbr_voices)\\n\\nfor i, sample_batched in enumerate(loader):\\n  if i ==10:\\n    all_voices, length, nbr_voices, _ = sample_batched\\n    all_voices_pr = all_voices[0,:,:,-1].numpy()\\n    \\n    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\\n    print(note_array.shape)\\n    print(note_array[:10])\\n    print(note_array.dtype.names)\\n\\n    #print(i,nbr_voices,all_voices.shape)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Music - Model\n"
      ],
      "metadata": {
        "id": "JNqxeacDwxNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UNET "
      ],
      "metadata": {
        "id": "QAIfIM69VHI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "XMdlm0_Vyyhc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_0 = []\n",
        "loss_1 = []\n",
        "loss_2 = []\n",
        "loss_3 = []\n",
        "loss_all = []\n",
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")                       # use weight parameters maybe take 1/88       \n",
        "\n",
        "    \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                      ### squeeze output here before returning                                       \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()    ### v3 is automatically tensor containing only 0 bc dataloader does this for pieces with 3 voices\n",
        "        \n",
        "        stack_pred = torch.stack([score_0, score_1, score_2, score_3], dim=2)\n",
        "        stack_gt = torch.stack([v0, v1, v2, v3], dim=2)\n",
        "\n",
        "        #print(\" loss between 3 pred/gt\", torch.nn.functional.l1_loss(score_3, v3))\n",
        "        #v_pred_argm = self.predict(voices[:,:,:,-1], sentences_len ) \n",
        "        #print(\"predict method\", v_pred_argm)\n",
        "        #print(\"shape v3\", v3.shape)\n",
        "\n",
        "        loss = self.loss(stack_pred, stack_gt)\n",
        "\n",
        "        \"\"\"\n",
        "        loss = self.loss(score_0, v0) +  self.loss(score_1, v1) +  self.loss(score_2, v2) +  self.loss(score_3, v3) #*1.5    \n",
        "        loss_0.append(self.loss(score_0, v0).cpu().detach().numpy())\n",
        "        loss_1.append(self.loss(score_1, v1).cpu().detach().numpy())\n",
        "        loss_2.append(self.loss(score_2, v2).cpu().detach().numpy())\n",
        "        loss_3.append(self.loss(score_3, v3).cpu().detach().numpy())\n",
        "        print(\"self.loss(score_0, v0)\",self.loss(score_0, v0).cpu().detach().numpy())\n",
        "        print(\"self.loss(score_1, v1)\",self.loss(score_1, v1).cpu().detach().numpy())\n",
        "        print(\"self.loss(score_2, v2)\",self.loss(score_2, v2).cpu().detach().numpy())\n",
        "        print(\"self.loss(score_3, v3)\",self.loss(score_3, v3).cpu().detach().numpy())\n",
        "        \"\"\"\n",
        "        loss_all.append(self.loss(stack_pred, stack_gt).cpu().detach().numpy())\n",
        "\n",
        "        print(\"loss\",loss) \n",
        "        \n",
        "        return loss   \n",
        "        \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "            \n",
        "\n",
        "        else:\n",
        "            sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "            prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            \n",
        "            mask_pred = np.squeeze(sentences)== 0\n",
        "            v_pred_argm[mask_pred] = -1\n",
        "\n",
        "            return v_pred_argm \n",
        "                       "
      ],
      "metadata": {
        "id": "CviiPTPOPW04"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"CNN\"\n",
        "lr = 0.0001  \n",
        "monophonic = True\n",
        "his = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "79cPe11WL6J0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "849b500b-8753-4dd3-d9ae-cf0a7a0c7742"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"CNN\"\\nlr = 0.0001  \\nmonophonic = True\\nhis = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Define Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                                   ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "                \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]             \n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "                    ### before\n",
        "                    #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                    #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                    #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                    #for i in range(len(prediction[0,:])):\n",
        "                    #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                    #  accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                    #prediction = model.predict(voices, lens, monophonic)                    #for voice vise masking\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)         #for mixed voice masking        \n",
        "\n",
        "\n",
        "                    ## ground truth in shape 1280x88 -> mixed voice\n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    truth = v_ori_argm       \n",
        "\n",
        "                    # outsource accurcy to further down -> just a placeholder right now\n",
        "                    v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            #train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            #train_acc_list[3] = accuracy_sum_list[3] / 18                        ## bc only 18 pieces with len 3\n",
        "            #train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "            #history[\"train_loss\"].append(train_loss)\n",
        "            #history[\"train_acc\"].append(train_acc_list)\n",
        "            #print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "            \n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "                        voices = voices.to(device).float()\n",
        "                        ### before\n",
        "                        #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                        #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                        #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                        #for i in range(len(prediction[0,:])):\n",
        "                        #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                        #  val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    #val_acc_list[3] = val_accuracy_sum_list[3] / 18                         ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_accuracy_sum_list[4] / 2                          ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                #history[\"val_acc\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        #prediction = model.predict(voices, lens, monophonic)                #for voice vise masking\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)     # for masking with mixed voice\n",
        "\n",
        "\n",
        "\n",
        "                        ## ground truth in shape 1280x88 -> mixed voice\n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        truth = v_ori_argm       \n",
        "\n",
        "                        # outsource accurcy to further down -> just a placeholder right now\n",
        "                        v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "                    \n",
        "                history[\"val_acc\"].append(val_accuracy)\n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    # save the model\n",
        "    #torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"RNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ge8pY70uHxF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f5f17d8f-c3e3-4ece-a84e-549f19397be9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"RNN\"\\nmonophonic = True\\nhis = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= [\"CNN\",\"RNN\"]\n",
        "monophonic_list = [True,False]\n",
        "\n",
        "for net in network_type:\n",
        "    for monophonic in monophonic_list: \n",
        "        print(\"network set to:\",net,\"monophnic:\",monophonic)\n",
        "        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Bs6-iNEBu8o",
        "outputId": "317002bb-e1bc-49cb-b44a-0dcc30c875ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= [\"CNN\",\"RNN\"]\\nmonophonic_list = [True,False]\\n\\nfor net in network_type:\\n    for monophonic in monophonic_list: \\n        print(\"network set to:\",net,\"monophnic:\",monophonic)\\n        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        if fugues == True:\n",
        "        ### uncomment for fugues ###\n",
        "            train_dataset = MusicDataset_new(PATH_TO_DATA) \n",
        "        ### uncomment for chorals ###\n",
        "        else:\n",
        "            train_dataset = MusicDataset_chor(PATH_TO_DATA) \n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        ### uncomment for fugues ###\n",
        "        if fugues == True:\n",
        "            dataset = MusicDataset_new(PATH_TO_DATA) \n",
        "        ### uncomment for chorals ###\n",
        "        else:\n",
        "            dataset = MusicDataset_chor(PATH_TO_DATA)\n",
        "        \n",
        "        \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicNetwork\n",
        "epochs = 10\n",
        "lr = 0.00001 # was 0.001\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Experiment"
      ],
      "metadata": {
        "id": "bdetlQP-LoRX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm",
        "outputId": "c2e4aab9-26d8-4148-baf8-542e2254f2fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataloader 314 val_dataloader 56\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Training on device: cuda\n",
            "monophonic set to: True\n",
            "loss tensor(4.3019, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.1012, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.2543, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.2593, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.9374, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.1901, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.9988, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.1677, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.0250, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.0934, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.0765, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.0375, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.9761, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.9759, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.8853, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.9504, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.9074, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.8943, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.8531, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.8180, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.8485, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.8131, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.6719, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.7041, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.6898, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.6625, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.5309, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.5885, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.6032, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.5060, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.5470, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.3836, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.4340, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.4526, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.4008, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.3696, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.3657, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.3342, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.3494, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.2229, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.0430, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.2184, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.1713, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.9801, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.1386, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.0187, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.1042, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.9714, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.9815, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.0310, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.9815, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.8823, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.9676, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.9679, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.8379, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.8906, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.8742, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.8487, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.7401, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.6461, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.3245, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.7402, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.6351, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.5907, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.6453, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.6419, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.5230, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.5116, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.5196, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.4371, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.3863, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.4447, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.3745, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.4036, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.3163, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2177, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.4012, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2601, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2854, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0777, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.1997, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2149, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.1750, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2287, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0835, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0899, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.1117, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0130, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0249, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0136, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0077, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0297, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.9590, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0056, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.9270, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0453, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.9214, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.8889, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.8287, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.8122, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7220, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7401, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7713, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7918, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6805, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7581, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7574, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7137, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6906, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6963, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4393, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7117, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5789, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6782, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6800, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4884, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6678, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5866, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3966, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2458, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5551, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3127, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4545, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2947, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3573, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4596, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4326, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5297, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4338, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4448, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3033, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1602, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3608, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3396, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3217, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3483, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3449, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2262, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3209, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1630, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2926, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2247, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2443, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1658, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2767, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1565, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1128, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2612, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0365, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0912, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0393, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1397, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1293, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0949, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0204, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0972, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0138, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0514, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8482, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0782, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9135, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9801, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9996, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8717, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8333, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9876, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9736, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9857, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9248, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0420, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9305, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9031, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6208, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8914, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8664, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7974, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7229, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8728, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8412, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6352, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8723, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7910, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7731, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8091, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8507, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7007, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7282, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7460, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5938, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7395, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6987, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6166, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7847, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6474, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7484, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8455, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6191, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7494, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6782, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6898, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6044, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6245, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6567, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6450, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6506, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5838, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6296, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6475, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7763, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5289, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6230, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6190, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5174, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5662, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5558, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4745, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5188, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6706, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5341, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5029, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5270, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5523, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5187, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4915, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5822, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5729, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6257, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5796, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5657, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4890, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4739, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3579, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4495, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5688, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5050, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5021, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3018, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4323, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4311, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4583, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4868, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3848, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4996, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3736, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3332, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4046, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5196, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4204, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4491, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3943, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3372, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5314, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4276, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3989, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5656, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4277, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4547, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3750, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4542, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3833, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3282, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4170, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4136, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3686, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3899, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4418, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4205, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3405, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3727, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3144, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2928, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4348, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4419, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3168, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3645, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3651, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3270, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3741, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3434, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3184, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2420, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3035, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2651, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4082, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2829, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3389, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3445, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3502, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3196, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2754, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3096, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2737, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3135, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2627, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2598, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3597, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3182, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3255, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2708, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2808, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3063, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4134, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2673, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3022, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2328, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2785, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2817, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2356, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2618, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2954, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2419, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2422, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2478, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3030, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "Train Loss: 1.472421881974123, Train Accuracy : 0.9880585225486216\n",
            " Validation Accuracy : 6.539035706801874\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "loss tensor(0.2530, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2118, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3095, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3198, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2569, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1957, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1988, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1988, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2368, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2332, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3151, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2275, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2659, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2714, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2149, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3048, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2203, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2168, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3809, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2466, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2265, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2506, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2067, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1700, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1909, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2224, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2122, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2010, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2798, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1890, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2724, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1725, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6651, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1883, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2013, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2084, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1990, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2198, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2214, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2480, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1812, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3769, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1891, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1585, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1802, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1778, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1776, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2322, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1486, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2079, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3935, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1560, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3173, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1855, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3270, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2029, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2268, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2413, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1605, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1518, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1580, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2221, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1650, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2412, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2800, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2613, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1815, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1652, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1706, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1816, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1844, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4499, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1934, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3093, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1405, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1333, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2232, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1435, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1957, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1291, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2341, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1467, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1272, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1563, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2127, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1686, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2953, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1453, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1398, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1799, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1965, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1676, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1699, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2374, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1464, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2644, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2119, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1202, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1391, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2379, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2405, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2472, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1686, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1560, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1028, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1762, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2955, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1373, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1504, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.5442, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1117, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1821, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1863, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1481, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1774, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1012, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1924, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1371, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0888, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1197, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3350, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1012, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1625, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0873, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1231, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1578, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1618, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1603, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1147, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1326, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1679, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1031, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1326, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1624, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3015, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1533, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2143, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1010, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1809, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1121, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1468, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1309, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1208, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1182, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3159, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1209, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1046, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2059, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1297, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1333, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1770, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1325, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1414, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1867, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1099, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1353, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1100, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2149, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0951, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2421, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0857, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1168, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1942, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1097, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0871, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1434, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3287, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1162, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1363, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2162, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2600, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1091, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0726, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1050, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1039, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0835, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0763, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1024, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1453, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0620, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1619, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1611, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0860, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0974, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1710, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1372, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1385, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1370, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1226, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0861, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1026, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0665, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1737, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0743, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1364, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3939, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0769, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2277, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1586, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1137, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0774, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1254, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1551, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1412, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1390, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1048, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0962, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1160, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3635, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0779, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1788, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0896, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0781, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0854, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1122, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0699, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0800, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.4435, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1080, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0750, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0989, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1031, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1350, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1184, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1781, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1956, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2466, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2977, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2056, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0963, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0904, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0585, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1438, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1951, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1387, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1253, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0898, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1378, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0775, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1092, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0938, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0812, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2207, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0696, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0630, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0811, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1603, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1198, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0934, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0863, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0638, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2748, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1131, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0806, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3176, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1272, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1715, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0926, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1383, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0981, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0751, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1599, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1488, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0839, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0952, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1699, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1371, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1020, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0929, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0640, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0648, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1986, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2091, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0662, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1556, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1556, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0879, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1313, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0888, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1056, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0495, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0830, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0605, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2463, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0793, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1135, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0788, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1556, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0791, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0600, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0832, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0635, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1116, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0849, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0673, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1606, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1271, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1128, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0829, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0719, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1065, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.3228, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0885, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1450, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1194, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1081, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0791, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0735, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0747, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0887, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0658, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0722, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0664, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1313, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "Train Loss: 0.16379563199581615, Train Accuracy : 0.9986513106918333\n",
            " Validation Accuracy : 6.598454521042768\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "loss tensor(0.1186, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0833, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1640, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1468, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1205, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0519, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0518, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0517, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0630, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0657, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1907, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0703, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0999, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1094, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0587, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1377, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0705, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0659, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2404, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1042, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0676, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0811, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0709, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0448, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0509, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0649, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0749, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0558, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1379, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0590, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1589, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0502, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.6249, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0530, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0628, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0649, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0582, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0948, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0955, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1265, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0746, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2599, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0603, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0545, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0643, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0631, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0567, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1294, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0410, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0824, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2708, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0496, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.1936, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0595, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.2076, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.0717, device='cuda:0', grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-a784cd00b7f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-ba75d9cec5c5>\u001b[0m in \u001b[0;36mstart_experiment\u001b[0;34m(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay, network_type, learn_all)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_dataloader\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"val_dataloader\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-335c349ae9a2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay, network_type, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonophonic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-05813fda84db>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, optimizer, train_dataloader, monophonic, epochs, val_dataloader, device, scheduler)\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;31m## ground truth in shape 1280x88 -> mixed voice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0msingle_voices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0mv_ori_argm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_voices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                     \u001b[0mmask_ori\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_voices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mv_ori_argm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_ori\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_0,'-o')\n",
        "plt.plot(loss_1,'-o')\n",
        "plt.plot(loss_2,'-o')\n",
        "plt.plot(loss_3,'-o')\n",
        "plt.xlabel('sample')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['loss 0','loss 1','loss 2','loss 3'])\n",
        "plt.title('loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPfmfthBjSZw",
        "outputId": "c7bbe166-5bfb-414d-9bbd-82208a3977d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy evalutaion F-scores"
      ],
      "metadata": {
        "id": "sJbWsH72N2Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. create folder with part object of all pieces \n",
        "2. load a piece from dataloader with true labels, the mixed piece and the part object \n",
        "3. create notearray from part object\n",
        "4. take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "\n",
        "Output: pianoroll\n",
        "\n",
        "1 note in notearray could be mulitple bins\n",
        "\n",
        "take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "note start at same time with different pitch -> different notes\n",
        "\n",
        "for each note array find corresponding matrix -> \n",
        "\n",
        "\n",
        "if note is only composed by 1 bin: save indx of vocie -> save it to note array\n",
        "\n",
        "if more than 1: look what are idx that compose this note -> majority note -> save it for the note array (if its 50/50 take it random -> count how often this happens) \n",
        "\n",
        "\n",
        "with idx : in note_array find which note corresponds to what voice"
      ],
      "metadata": {
        "id": "CFClch37N6nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader "
      ],
      "metadata": {
        "id": "afYHFVNMlMnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## makes cell output nothing\n",
        "%%capture  \n",
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch10.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4TAhTQcpmx8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dic with key:filename, val: part_obj  for fugues"
      ],
      "metadata": {
        "id": "5RVmMv6Q9CJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if PATH_TO_DATA == \"AI-MA_project/bach_pr_fugues\":\n",
        "    path_parts = \"AI-MA_project/bach_fugues\"\n",
        "    part_dic = {}\n",
        "\n",
        "    #### create a list with all filenames in the right order ####\n",
        "    file_names_part = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.mid'): continue\n",
        "        file_names_part.append(filename[3:7])\n",
        "    #print(file_names_part)\n",
        "\n",
        "    #### create a list with all part objects in the right order ####\n",
        "    part_list = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.mid'): continue\n",
        "        fullname = os.path.join(path_parts, filename)\n",
        "        part = partitura.load_score_midi(fullname)\n",
        "        part_list.append(part)\n",
        "    #print(part_list)\n",
        "\n",
        "    #### create a dict with keys:filenames , values: part object ####\n",
        "    for i in range(len(file_names_part)):\n",
        "        part_dic[file_names_part[i]] = part_list[i]\n",
        "    \n",
        "    print(part_dic.keys(),part_dic.values())"
      ],
      "metadata": {
        "id": "_XYM_KWu2qkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dic with key:filename, val: part_obj  for chorales"
      ],
      "metadata": {
        "id": "6D9oTp_lNQbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if PATH_TO_DATA == \"AI-MA_project/pianoroll_88\":\n",
        "    path_parts = \"AI-MA_project/chorales_converted\"\n",
        "    part_dic = {}\n",
        "\n",
        "    #### create a list with all filenames in the right order ####\n",
        "    file_names_part = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.xml'): continue\n",
        "        file_names_part.append(filename[4:7])\n",
        "    #print(file_names_part)\n",
        "\n",
        "    #### create a list with all part objects in the right order ####\n",
        "    part_list = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.xml'): continue\n",
        "        fullname = os.path.join(path_parts, filename)\n",
        "        part = partitura.load_musicxml(fullname)\n",
        "        part_list.append(part)\n",
        "    #print(part_list)\n",
        "\n",
        "    #### create a dict with keys:filenames , values: part object ####\n",
        "    for i in range(len(file_names_part)):\n",
        "        part_dic[file_names_part[i]] = part_list[i]\n",
        "    \n",
        "    print(\"part_dic.keys()\",part_dic.keys())\n",
        "    print(\"part_dic.values()\",part_dic.values())"
      ],
      "metadata": {
        "id": "_4q58c16NjbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate chorales"
      ],
      "metadata": {
        "id": "yphGmsr-NSV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate chorals"
      ],
      "metadata": {
        "id": "v4TJGKiUs086"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_one_choral(model, train_dataloader, part_dic,F1):\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match                                      \n",
        "            \n",
        "            #if idx > 40: # or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "                part_3 = part[3]\n",
        "\n",
        "                #note_array_0 = part_0.note_array\n",
        "                #note_array_1 = part_1.note_array\n",
        "                #note_array_2 = part_2.note_array\n",
        "                #note_array_3 = part_3.note_array                \n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                   \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]                   \n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        \n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "                        else:\n",
        "                            accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                        counting = 0\n",
        "                        ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                        for j in range(len(total_predictions_dict[i])):\n",
        "                            if total_predictions_dict[i][j][0] == gt:\n",
        "                                counting +=1  \n",
        "                        count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                    acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                    print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                    \n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "                    acc_score_dict[\"3\"].append(acc_3)\n",
        "                \n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    print(\"f1_v0 , sample {}:\".format(idx),f1_v0)\n",
        "                    print(\"f1_v1 , sample {}:\".format(idx),f1_v1)\n",
        "                    print(\"f1_v2 , sample {}:\".format(idx),f1_v2)\n",
        "                    \n",
        "                    if len(part)==4:\n",
        "                        pred_3 = accordance_dict[\"3\"]\n",
        "                        truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                        f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                        f_score_dict[\"3\"].append(f1_v3)\n",
        "                        print(\"f1_v3 , sample {}:\".format(idx),f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "UXr2DeiLSyLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_one_choral(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghAmXcXTTtD1",
        "outputId": "6cb25c44-c623-45b2-8780-65465230b667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0, sample 0: 0.967741935483871\n",
            "acc 1, sample 0: 0.967479674796748\n",
            "acc 2, sample 0: 0.9357798165137615\n",
            "acc 3, sample 0: 1.0\n",
            "acc 0, sample 1: 1.0\n",
            "acc 1, sample 1: 1.0\n",
            "acc 2, sample 1: 0.9833333333333333\n",
            "acc 3, sample 1: 1.0\n",
            "acc 0, sample 2: 1.0\n",
            "acc 1, sample 2: 1.0\n",
            "acc 2, sample 2: 0.9803921568627451\n",
            "acc 3, sample 2: 1.0\n",
            "acc 0, sample 3: 0.9791666666666666\n",
            "acc 1, sample 3: 0.9387755102040817\n",
            "acc 2, sample 3: 0.9565217391304348\n",
            "acc 3, sample 3: 1.0\n",
            "acc 0, sample 4: 1.0\n",
            "acc 1, sample 4: 0.9\n",
            "acc 2, sample 4: 0.8536585365853658\n",
            "acc 3, sample 4: 1.0\n",
            "acc 0, sample 5: 0.9666666666666667\n",
            "acc 1, sample 5: 0.9885057471264368\n",
            "acc 2, sample 5: 0.9425287356321839\n",
            "acc 3, sample 5: 0.9878048780487805\n",
            "acc 0, sample 6: 1.0\n",
            "acc 1, sample 6: 0.9166666666666666\n",
            "acc 2, sample 6: 0.9702970297029703\n",
            "acc 3, sample 6: 1.0\n",
            "acc 0, sample 7: 0.9824561403508771\n",
            "acc 1, sample 7: 0.8666666666666667\n",
            "acc 2, sample 7: 1.0\n",
            "acc 3, sample 7: 0.9782608695652174\n",
            "acc 0, sample 8: 1.0\n",
            "acc 1, sample 8: 1.0\n",
            "acc 2, sample 8: 0.9583333333333334\n",
            "acc 3, sample 8: 1.0\n",
            "acc 0, sample 9: 0.9523809523809523\n",
            "acc 1, sample 9: 0.925\n",
            "acc 2, sample 9: 0.9473684210526315\n",
            "acc 3, sample 9: 1.0\n",
            "acc 0, sample 10: 1.0\n",
            "acc 1, sample 10: 0.9811320754716981\n",
            "acc 2, sample 10: 0.9795918367346939\n",
            "acc 3, sample 10: 1.0\n",
            "acc 0, sample 11: 0.9873417721518988\n",
            "acc 1, sample 11: 1.0\n",
            "acc 2, sample 11: 0.9692307692307692\n",
            "acc 3, sample 11: 1.0\n",
            "acc 0, sample 12: 0.9444444444444444\n",
            "acc 1, sample 12: 0.9375\n",
            "acc 2, sample 12: 0.9777777777777777\n",
            "acc 3, sample 12: 1.0\n",
            "acc 0, sample 13: 1.0\n",
            "acc 1, sample 13: 0.9682539682539683\n",
            "acc 2, sample 13: 1.0\n",
            "acc 3, sample 13: 1.0\n",
            "acc 0, sample 14: 0.9726027397260274\n",
            "acc 1, sample 14: 0.9571428571428572\n",
            "acc 2, sample 14: 0.9193548387096774\n",
            "acc 3, sample 14: 1.0\n",
            "acc 0, sample 15: 1.0\n",
            "acc 1, sample 15: 0.9811320754716981\n",
            "acc 2, sample 15: 0.9795918367346939\n",
            "acc 3, sample 15: 0.975\n",
            "acc 0, sample 16: 0.9807692307692307\n",
            "acc 1, sample 16: 0.9433962264150944\n",
            "acc 2, sample 16: 1.0\n",
            "acc 3, sample 16: 1.0\n",
            "acc 0, sample 17: 1.0\n",
            "acc 1, sample 17: 0.9117647058823529\n",
            "acc 2, sample 17: 0.9473684210526315\n",
            "acc 3, sample 17: 0.967741935483871\n",
            "acc 0, sample 18: 1.0\n",
            "acc 1, sample 18: 0.9090909090909091\n",
            "acc 2, sample 18: 1.0\n",
            "acc 3, sample 18: 0.90625\n",
            "acc 0, sample 19: 1.0\n",
            "acc 1, sample 19: 0.9636363636363636\n",
            "acc 2, sample 19: 0.9803921568627451\n",
            "acc 3, sample 19: 1.0\n",
            "acc 0, sample 20: 1.0\n",
            "acc 1, sample 20: 0.9090909090909091\n",
            "acc 2, sample 20: 0.8431372549019608\n",
            "acc 3, sample 20: 1.0\n",
            "acc 0, sample 21: 1.0\n",
            "acc 1, sample 21: 0.9230769230769231\n",
            "acc 2, sample 21: 1.0\n",
            "acc 3, sample 21: 1.0\n",
            "acc 0, sample 22: 0.9736842105263158\n",
            "acc 1, sample 22: 0.9545454545454546\n",
            "acc 2, sample 22: 1.0\n",
            "acc 3, sample 22: 0.9791666666666666\n",
            "acc 0, sample 23: 0.9649122807017544\n",
            "acc 1, sample 23: 0.9787234042553191\n",
            "acc 2, sample 23: 0.9803921568627451\n",
            "acc 3, sample 23: 0.9736842105263158\n",
            "acc 0, sample 24: 1.0\n",
            "acc 1, sample 24: 0.9516129032258065\n",
            "acc 2, sample 24: 0.9629629629629629\n",
            "acc 3, sample 24: 0.9811320754716981\n",
            "acc 0, sample 25: 0.9433962264150944\n",
            "acc 1, sample 25: 1.0\n",
            "acc 2, sample 25: 1.0\n",
            "acc 3, sample 25: 1.0\n",
            "acc 0, sample 26: 1.0\n",
            "acc 1, sample 26: 0.9743589743589743\n",
            "acc 2, sample 26: 0.9736842105263158\n",
            "acc 3, sample 26: 0.9473684210526315\n",
            "acc 0, sample 27: 1.0\n",
            "acc 1, sample 27: 0.972972972972973\n",
            "acc 2, sample 27: 1.0\n",
            "acc 3, sample 27: 1.0\n",
            "acc 0, sample 28: 1.0\n",
            "acc 1, sample 28: 0.975609756097561\n",
            "acc 2, sample 28: 0.9523809523809523\n",
            "acc 3, sample 28: 1.0\n",
            "acc 0, sample 29: 0.975609756097561\n",
            "acc 1, sample 29: 0.9534883720930233\n",
            "acc 2, sample 29: 0.8888888888888888\n",
            "acc 3, sample 29: 0.972972972972973\n",
            "acc 0, sample 30: 1.0\n",
            "acc 1, sample 30: 0.975\n",
            "acc 2, sample 30: 1.0\n",
            "acc 3, sample 30: 1.0\n",
            "acc 0, sample 31: 1.0\n",
            "acc 1, sample 31: 1.0\n",
            "acc 2, sample 31: 1.0\n",
            "acc 3, sample 31: 1.0\n",
            "acc 0, sample 32: 0.9928571428571429\n",
            "acc 1, sample 32: 0.9622641509433962\n",
            "acc 2, sample 32: 0.9280575539568345\n",
            "acc 3, sample 32: 0.9568965517241379\n",
            "acc 0, sample 33: 0.9705882352941176\n",
            "acc 1, sample 33: 1.0\n",
            "acc 2, sample 33: 0.9761904761904762\n",
            "acc 3, sample 33: 1.0\n",
            "acc 0, sample 34: 0.9753086419753086\n",
            "acc 1, sample 34: 0.8918918918918919\n",
            "acc 2, sample 34: 1.0\n",
            "acc 3, sample 34: 0.9846153846153847\n",
            "acc 0, sample 35: 0.9821428571428571\n",
            "acc 1, sample 35: 0.8979591836734694\n",
            "acc 2, sample 35: 0.9607843137254902\n",
            "acc 3, sample 35: 1.0\n",
            "acc 0, sample 36: 0.9692307692307692\n",
            "acc 1, sample 36: 0.9827586206896551\n",
            "acc 2, sample 36: 0.9464285714285714\n",
            "acc 3, sample 36: 1.0\n",
            "acc 0, sample 37: 1.0\n",
            "acc 1, sample 37: 0.96\n",
            "acc 2, sample 37: 1.0\n",
            "acc 3, sample 37: 1.0\n",
            "acc 0, sample 38: 1.0\n",
            "acc 1, sample 38: 0.8679245283018868\n",
            "acc 2, sample 38: 0.9591836734693877\n",
            "acc 3, sample 38: 0.9767441860465116\n",
            "acc 0, sample 39: 0.9523809523809523\n",
            "acc 1, sample 39: 0.9696969696969697\n",
            "acc 2, sample 39: 1.0\n",
            "acc 3, sample 39: 1.0\n",
            "acc 0, sample 40: 0.9090909090909091\n",
            "acc 1, sample 40: 0.8214285714285714\n",
            "acc 2, sample 40: 0.92\n",
            "acc 3, sample 40: 0.9787234042553191\n",
            "acc 0, sample 41: 0.9871794871794872\n",
            "acc 1, sample 41: 0.9512195121951219\n",
            "acc 2, sample 41: 0.9714285714285714\n",
            "acc 3, sample 41: 1.0\n",
            "acc 0, sample 42: 1.0\n",
            "acc 1, sample 42: 1.0\n",
            "acc 2, sample 42: 1.0\n",
            "acc 3, sample 42: 0.9787234042553191\n",
            "acc 0, sample 43: 1.0\n",
            "acc 1, sample 43: 0.9487179487179487\n",
            "acc 2, sample 43: 1.0\n",
            "acc 3, sample 43: 0.9743589743589743\n",
            "acc 0, sample 44: 1.0\n",
            "acc 1, sample 44: 0.9253731343283582\n",
            "acc 2, sample 44: 0.95\n",
            "acc 3, sample 44: 1.0\n",
            "acc 0, sample 45: 1.0\n",
            "acc 1, sample 45: 1.0\n",
            "acc 2, sample 45: 0.8846153846153846\n",
            "acc 3, sample 45: 1.0\n",
            "acc 0, sample 46: 1.0\n",
            "acc 1, sample 46: 0.95\n",
            "acc 2, sample 46: 0.9459459459459459\n",
            "acc 3, sample 46: 1.0\n",
            "acc 0, sample 47: 0.9642857142857143\n",
            "acc 1, sample 47: 0.9636363636363636\n",
            "acc 2, sample 47: 1.0\n",
            "acc 3, sample 47: 1.0\n",
            "acc 0, sample 48: 0.9830508474576272\n",
            "acc 1, sample 48: 0.9166666666666666\n",
            "acc 2, sample 48: 0.9655172413793104\n",
            "acc 3, sample 48: 1.0\n",
            "acc 0, sample 49: 0.9782608695652174\n",
            "acc 1, sample 49: 0.9534883720930233\n",
            "acc 2, sample 49: 0.9714285714285714\n",
            "acc 3, sample 49: 1.0\n",
            "acc 0, sample 50: 1.0\n",
            "acc 1, sample 50: 0.9629629629629629\n",
            "acc 2, sample 50: 0.9433962264150944\n",
            "acc 3, sample 50: 1.0\n",
            "acc 0, sample 51: 1.0\n",
            "acc 1, sample 51: 0.9811320754716981\n",
            "acc 2, sample 51: 1.0\n",
            "acc 3, sample 51: 1.0\n",
            "acc 0, sample 52: 0.9696969696969697\n",
            "acc 1, sample 52: 0.8205128205128205\n",
            "acc 2, sample 52: 0.9166666666666666\n",
            "acc 3, sample 52: 1.0\n",
            "acc 0, sample 53: 0.9795918367346939\n",
            "acc 1, sample 53: 0.9433962264150944\n",
            "acc 2, sample 53: 1.0\n",
            "acc 3, sample 53: 1.0\n",
            "acc 0, sample 54: 0.9358974358974359\n",
            "acc 1, sample 54: 0.8674698795180723\n",
            "acc 2, sample 54: 0.9305555555555556\n",
            "acc 3, sample 54: 1.0\n",
            "acc 0, sample 55: 1.0\n",
            "acc 1, sample 55: 0.9736842105263158\n",
            "acc 2, sample 55: 1.0\n",
            "acc 3, sample 55: 1.0\n",
            "total_predictions_dict dict_keys(['0', '1', '2', '3'])\n",
            "0.9846559944851886 0.9488715572537995 0.9652351056782041 0.9914186416972107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    plt.plot(acc_score_dict[\"0\"],'-o')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend(['Accuracy0'])\n",
        "    plt.title('Accuracy vs Epochs')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "J92mdaQG0MXZ",
        "outputId": "cd015a47-a9a8-4af6-f506-c5012ea4004a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate on F1 meassure"
      ],
      "metadata": {
        "id": "zYMy2JARxEBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    acc_0 , acc_1, acc_2, acc_3 = evaluate_one_choral(model,val_dataloader,part_dic,F1=True)\n",
        "    acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "RWxVG3XAYTcC",
        "outputId": "ce5d25ad-1847-4c2b-93f7-eaea578b0d20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1_v0 , sample 0: 0.9666666666666666\n",
            "f1_v1 , sample 0: 0.9663865546218486\n",
            "f1_v2 , sample 0: 0.9313725490196079\n",
            "f1_v3 , sample 0: 1.0\n",
            "f1_v0 , sample 1: 1.0\n",
            "f1_v1 , sample 1: 1.0\n",
            "f1_v2 , sample 1: 0.983050847457627\n",
            "f1_v3 , sample 1: 1.0\n",
            "f1_v0 , sample 2: 1.0\n",
            "f1_v1 , sample 2: 1.0\n",
            "f1_v2 , sample 2: 0.98\n",
            "f1_v3 , sample 2: 1.0\n",
            "f1_v0 , sample 3: 0.9787234042553191\n",
            "f1_v1 , sample 3: 0.9347826086956522\n",
            "f1_v2 , sample 3: 0.9545454545454545\n",
            "f1_v3 , sample 3: 1.0\n",
            "f1_v0 , sample 4: 1.0\n",
            "f1_v1 , sample 4: 0.9189189189189189\n",
            "f1_v2 , sample 4: 0.9487179487179488\n",
            "f1_v3 , sample 4: 1.0\n",
            "f1_v0 , sample 5: 0.9655172413793104\n",
            "f1_v1 , sample 5: 0.9883720930232558\n",
            "f1_v2 , sample 5: 0.9390243902439025\n",
            "f1_v3 , sample 5: 0.9876543209876543\n",
            "f1_v0 , sample 6: 1.0\n",
            "f1_v1 , sample 6: 0.9306930693069307\n",
            "f1_v2 , sample 6: 0.9693877551020408\n",
            "f1_v3 , sample 6: 1.0\n",
            "f1_v0 , sample 7: 0.9821428571428572\n",
            "f1_v1 , sample 7: 0.8750000000000001\n",
            "f1_v2 , sample 7: 1.0\n",
            "f1_v3 , sample 7: 0.9777777777777777\n",
            "f1_v0 , sample 8: 1.0\n",
            "f1_v1 , sample 8: 1.0\n",
            "f1_v2 , sample 8: 0.9565217391304348\n",
            "f1_v3 , sample 8: 1.0\n",
            "f1_v0 , sample 9: 0.975609756097561\n",
            "f1_v1 , sample 9: 0.9333333333333333\n",
            "f1_v2 , sample 9: 0.972972972972973\n",
            "f1_v3 , sample 9: 1.0\n",
            "f1_v0 , sample 10: 1.0\n",
            "f1_v1 , sample 10: 0.9807692307692307\n",
            "f1_v2 , sample 10: 0.9791666666666666\n",
            "f1_v3 , sample 10: 1.0\n",
            "f1_v0 , sample 11: 0.9871794871794872\n",
            "f1_v1 , sample 11: 1.0\n",
            "f1_v2 , sample 11: 0.9682539682539683\n",
            "f1_v3 , sample 11: 1.0\n",
            "f1_v0 , sample 12: 0.9411764705882353\n",
            "f1_v1 , sample 12: 0.9333333333333333\n",
            "f1_v2 , sample 12: 0.9772727272727273\n",
            "f1_v3 , sample 12: 1.0\n",
            "f1_v0 , sample 13: 1.0\n",
            "f1_v1 , sample 13: 0.9672131147540983\n",
            "f1_v2 , sample 13: 1.0\n",
            "f1_v3 , sample 13: 1.0\n",
            "f1_v0 , sample 14: 0.971830985915493\n",
            "f1_v1 , sample 14: 0.955223880597015\n",
            "f1_v2 , sample 14: 0.9122807017543859\n",
            "f1_v3 , sample 14: 1.0\n",
            "f1_v0 , sample 15: 1.0\n",
            "f1_v1 , sample 15: 0.9807692307692307\n",
            "f1_v2 , sample 15: 0.9791666666666666\n",
            "f1_v3 , sample 15: 0.9743589743589743\n",
            "f1_v0 , sample 16: 0.9803921568627451\n",
            "f1_v1 , sample 16: 0.9400000000000001\n",
            "f1_v2 , sample 16: 1.0\n",
            "f1_v3 , sample 16: 1.0\n",
            "f1_v0 , sample 17: 1.0\n",
            "f1_v1 , sample 17: 0.9206349206349206\n",
            "f1_v2 , sample 17: 0.9444444444444444\n",
            "f1_v3 , sample 17: 0.9666666666666666\n",
            "f1_v0 , sample 18: 1.0\n",
            "f1_v1 , sample 18: 0.9\n",
            "f1_v2 , sample 18: 1.0\n",
            "f1_v3 , sample 18: 0.896551724137931\n",
            "f1_v0 , sample 19: 1.0\n",
            "f1_v1 , sample 19: 0.9622641509433962\n",
            "f1_v2 , sample 19: 0.98\n",
            "f1_v3 , sample 19: 1.0\n",
            "f1_v0 , sample 20: 1.0\n",
            "f1_v1 , sample 20: 0.9622641509433962\n",
            "f1_v2 , sample 20: 0.891304347826087\n",
            "f1_v3 , sample 20: 1.0\n",
            "f1_v0 , sample 21: 1.0\n",
            "f1_v1 , sample 21: 0.9494949494949495\n",
            "f1_v2 , sample 21: 1.0\n",
            "f1_v3 , sample 21: 1.0\n",
            "f1_v0 , sample 22: 0.9798657718120806\n",
            "f1_v1 , sample 22: 0.9523809523809523\n",
            "f1_v2 , sample 22: 1.0\n",
            "f1_v3 , sample 22: 0.9787234042553191\n",
            "f1_v0 , sample 23: 0.9636363636363636\n",
            "f1_v1 , sample 23: 0.9782608695652174\n",
            "f1_v2 , sample 23: 0.98\n",
            "f1_v3 , sample 23: 0.972972972972973\n",
            "f1_v0 , sample 24: 1.0\n",
            "f1_v1 , sample 24: 0.9491525423728813\n",
            "f1_v2 , sample 24: 0.9615384615384615\n",
            "f1_v3 , sample 24: 0.9807692307692307\n",
            "f1_v0 , sample 25: 0.9400000000000001\n",
            "f1_v1 , sample 25: 1.0\n",
            "f1_v2 , sample 25: 1.0\n",
            "f1_v3 , sample 25: 1.0\n",
            "f1_v0 , sample 26: 1.0\n",
            "f1_v1 , sample 26: 0.9736842105263158\n",
            "f1_v2 , sample 26: 0.972972972972973\n",
            "f1_v3 , sample 26: 0.958904109589041\n",
            "f1_v0 , sample 27: 1.0\n",
            "f1_v1 , sample 27: 0.9722222222222222\n",
            "f1_v2 , sample 27: 1.0\n",
            "f1_v3 , sample 27: 1.0\n",
            "f1_v0 , sample 28: 1.0\n",
            "f1_v1 , sample 28: 0.975\n",
            "f1_v2 , sample 28: 0.962962962962963\n",
            "f1_v3 , sample 28: 1.0\n",
            "f1_v0 , sample 29: 0.975\n",
            "f1_v1 , sample 29: 0.951219512195122\n",
            "f1_v2 , sample 29: 0.888888888888889\n",
            "f1_v3 , sample 29: 0.9722222222222222\n",
            "f1_v0 , sample 30: 1.0\n",
            "f1_v1 , sample 30: 0.9873417721518987\n",
            "f1_v2 , sample 30: 1.0\n",
            "f1_v3 , sample 30: 1.0\n",
            "f1_v0 , sample 31: 1.0\n",
            "f1_v1 , sample 31: 1.0\n",
            "f1_v2 , sample 31: 1.0\n",
            "f1_v3 , sample 31: 1.0\n",
            "f1_v0 , sample 32: 0.9928057553956835\n",
            "f1_v1 , sample 32: 0.9607843137254902\n",
            "f1_v2 , sample 32: 0.9389312977099237\n",
            "f1_v3 , sample 32: 0.9549549549549551\n",
            "f1_v0 , sample 33: 0.9696969696969697\n",
            "f1_v1 , sample 33: 1.0\n",
            "f1_v2 , sample 33: 0.975609756097561\n",
            "f1_v3 , sample 33: 1.0\n",
            "f1_v0 , sample 34: 0.9875\n",
            "f1_v1 , sample 34: 0.9197080291970803\n",
            "f1_v2 , sample 34: 1.0\n",
            "f1_v3 , sample 34: 0.9843749999999999\n",
            "f1_v0 , sample 35: 0.9818181818181818\n",
            "f1_v1 , sample 35: 0.9111111111111111\n",
            "f1_v2 , sample 35: 0.9591836734693878\n",
            "f1_v3 , sample 35: 1.0\n",
            "f1_v0 , sample 36: 0.9682539682539683\n",
            "f1_v1 , sample 36: 0.9824561403508771\n",
            "f1_v2 , sample 36: 0.9433962264150945\n",
            "f1_v3 , sample 36: 1.0\n",
            "f1_v0 , sample 37: 1.0\n",
            "f1_v1 , sample 37: 0.9583333333333334\n",
            "f1_v2 , sample 37: 1.0\n",
            "f1_v3 , sample 37: 1.0\n",
            "f1_v0 , sample 38: 1.0\n",
            "f1_v1 , sample 38: 0.8958333333333334\n",
            "f1_v2 , sample 38: 0.9574468085106383\n",
            "f1_v3 , sample 38: 0.9761904761904763\n",
            "f1_v0 , sample 39: 0.9586776859504132\n",
            "f1_v1 , sample 39: 0.96875\n",
            "f1_v2 , sample 39: 1.0\n",
            "f1_v3 , sample 39: 1.0\n",
            "f1_v0 , sample 40: 0.9215686274509803\n",
            "f1_v1 , sample 40: 0.88\n",
            "f1_v2 , sample 40: 0.9361702127659575\n",
            "f1_v3 , sample 40: 0.9782608695652174\n",
            "f1_v0 , sample 41: 0.9870129870129869\n",
            "f1_v1 , sample 41: 0.9554140127388536\n",
            "f1_v2 , sample 41: 0.9705882352941176\n",
            "f1_v3 , sample 41: 1.0\n",
            "f1_v0 , sample 42: 1.0\n",
            "f1_v1 , sample 42: 1.0\n",
            "f1_v2 , sample 42: 1.0\n",
            "f1_v3 , sample 42: 0.9782608695652174\n",
            "f1_v0 , sample 43: 1.0\n",
            "f1_v1 , sample 43: 0.945945945945946\n",
            "f1_v2 , sample 43: 1.0\n",
            "f1_v3 , sample 43: 0.9736842105263158\n",
            "f1_v0 , sample 44: 1.0\n",
            "f1_v1 , sample 44: 0.9193548387096774\n",
            "f1_v2 , sample 44: 0.9473684210526316\n",
            "f1_v3 , sample 44: 1.0\n",
            "f1_v0 , sample 45: 1.0\n",
            "f1_v1 , sample 45: 1.0\n",
            "f1_v2 , sample 45: 0.8817204301075269\n",
            "f1_v3 , sample 45: 1.0\n",
            "f1_v0 , sample 46: 1.0\n",
            "f1_v1 , sample 46: 0.9743589743589743\n",
            "f1_v2 , sample 46: 0.9428571428571428\n",
            "f1_v3 , sample 46: 1.0\n",
            "f1_v0 , sample 47: 0.962962962962963\n",
            "f1_v1 , sample 47: 0.9622641509433962\n",
            "f1_v2 , sample 47: 1.0\n",
            "f1_v3 , sample 47: 1.0\n",
            "f1_v0 , sample 48: 0.9827586206896551\n",
            "f1_v1 , sample 48: 0.9285714285714286\n",
            "f1_v2 , sample 48: 0.9642857142857143\n",
            "f1_v3 , sample 48: 1.0\n",
            "f1_v0 , sample 49: 0.9777777777777777\n",
            "f1_v1 , sample 49: 0.951219512195122\n",
            "f1_v2 , sample 49: 0.9705882352941176\n",
            "f1_v3 , sample 49: 1.0\n",
            "f1_v0 , sample 50: 1.0\n",
            "f1_v1 , sample 50: 0.9615384615384615\n",
            "f1_v2 , sample 50: 0.9607843137254902\n",
            "f1_v3 , sample 50: 1.0\n",
            "f1_v0 , sample 51: 1.0\n",
            "f1_v1 , sample 51: 0.9807692307692307\n",
            "f1_v2 , sample 51: 1.0\n",
            "f1_v3 , sample 51: 1.0\n",
            "f1_v0 , sample 52: 0.96875\n",
            "f1_v1 , sample 52: 0.8695652173913044\n",
            "f1_v2 , sample 52: 0.9253731343283582\n",
            "f1_v3 , sample 52: 1.0\n",
            "f1_v0 , sample 53: 0.9791666666666666\n",
            "f1_v1 , sample 53: 0.9400000000000001\n",
            "f1_v2 , sample 53: 1.0\n",
            "f1_v3 , sample 53: 1.0\n",
            "f1_v0 , sample 54: 0.9600000000000001\n",
            "f1_v1 , sample 54: 0.9078947368421052\n",
            "f1_v2 , sample 54: 0.9411764705882353\n",
            "f1_v3 , sample 54: 1.0\n",
            "f1_v0 , sample 55: 1.0\n",
            "f1_v1 , sample 55: 0.972972972972973\n",
            "f1_v2 , sample 55: 1.0\n",
            "f1_v3 , sample 55: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate fugues"
      ],
      "metadata": {
        "id": "TzTpXHznL02j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import statistics\n",
        "\n",
        "\n",
        "def evaluate_accuracy_for_all(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    note_counter_0 = 0\n",
        "    note_counter_1 = 0\n",
        "    note_counter_2 = 0\n",
        "    note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "                                      \n",
        "                print(\"nbr_voices:\",nbr_voices)\n",
        "            #if idx == 0 or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "\n",
        "                note_counter_0 += len(note_array_0)\n",
        "                note_counter_1 += len(note_array_1)\n",
        "                note_counter_2 += len(note_array_2)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "                if len(part)== 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = part_3.note_array\n",
        "                    note_counter_3 += len(note_array_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                   \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        \n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "                            \n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                      counting = 0\n",
        "                      ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                      for j in range(len(total_predictions_dict[i])):\n",
        "                          if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                      count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "\n",
        "                    if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    print(\"f1_v0 , sample {}:\".format(idx),f1_v0)\n",
        "                    print(\"f1_v1 , sample {}:\".format(idx),f1_v1)\n",
        "                    print(\"f1_v2 , sample {}:\".format(idx),f1_v2)\n",
        "                    if len(part)==4:\n",
        "                      pred_3 = accordance_dict[\"3\"]\n",
        "                      truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                      f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                      f_score_dict[\"3\"].append(f1_v3)\n",
        "                      print(\"f1_v3 , sample {}:\".format(idx),f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])\n",
        "        #return total_predictions_dict, total_truth_dict"
      ],
      "metadata": {
        "id": "0xbN5YU8nGT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, train_dataloader, part_dic,F1):\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader): \n",
        "        print(\"nbr_voices:\",nbr_voices)\n",
        "\n",
        "test(model,val_dataloader,part_dic,F1=False)"
      ],
      "metadata": {
        "id": "p0HZ9d5TO_Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True:\n",
        "    dict_pred , dict_gt, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "20MP5Gk5kc2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### only train for nbr_voices == 4"
      ],
      "metadata": {
        "id": "qO3rzvsK2Hb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss3 * 1.5 = (0.8989219661919758, 0.7166572856993837, 0.8185917288474246, 0.0)"
      ],
      "metadata": {
        "id": "6nwnRTADQvQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 epoch, no loss modifier:\n",
        "#ACC:(0.9165209182020722,\n",
        "# 0.7864434689151618,\n",
        "# 0.8130949796045199,\n",
        "# 0.003652274754166715)\n",
        "\n",
        "# 20 epoch, no loss modifier:\n",
        "#(0.7962210840410273, 0.8669639629052727, 0.751302181991106, 0.0)\n",
        "\n",
        "# 20 ep, loss3 *1,5\n",
        "#(0.8721136343927623,\n",
        "# 0.8319586824413445,\n",
        "# 0.7563218924966578,\n",
        "# 0.09029535181592076)"
      ],
      "metadata": {
        "id": "6mCYnJLnHfYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate fugues F1 score"
      ],
      "metadata": {
        "id": "52P6em3ANb1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True:\n",
        "    f1_v0, f1_v1, f1_v2, f1_v3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=True)\n",
        "    print(f1_v0, f1_v1, f1_v2, f1_v3)"
      ],
      "metadata": {
        "id": "FLLmyO6o5vW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### take 0 -> compare to truth of 0,1,2,3 -> overall voice\n",
        "\n",
        "count_list = []\n",
        "\n",
        "count_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "truth_dic = {'0': 0, '1': 1, '2': 2, '3': 3 }\n",
        "\n",
        "voice_entry_list = [\"0\", \"1\", \"2\", \"3\"]\n",
        "for voice_entry_one in voice_entry_list:\n",
        "    for voice_entry_two in voice_entry_list:\n",
        "        count_list = []\n",
        "        #print(\"voices:\",voice_entry_one,voice_entry_two)\n",
        "        for i in range(len(dict_pred[voice_entry_one])):\n",
        "            if dict_pred[voice_entry_one][i][0] == truth_dic[voice_entry_two]:      #dict_truth[voice_entry_two][i][0]:\n",
        "                count_list.append(1)\n",
        "            else:\n",
        "                count_list.append(0)\n",
        "        count_dict[voice_entry_one].append(count_list)\n",
        "\n",
        "dictionary_sum={}\n",
        "for i in voice_entry_list:\n",
        "    v0_match,v1_match,v2_match,v3_match = count_dict[i]\n",
        "    sum_v0 = np.sum(v0_match)\n",
        "    sum_v1 = np.sum(v1_match)\n",
        "    sum_v2 = np.sum(v2_match)\n",
        "    sum_v3 = np.sum(v3_match)\n",
        "    dictionary_sum[\"v0\"] = sum_v0\n",
        "    dictionary_sum[\"v1\"] = sum_v1\n",
        "    dictionary_sum[\"v2\"] = sum_v2\n",
        "    dictionary_sum[\"v3\"] = sum_v3\n",
        "\n",
        "    val_list = list(dictionary_sum.values())\n",
        "    \n",
        "    print(\"voice{} matches with\".format(i))\n",
        "    print(\"dict\",dictionary_sum)\n",
        "\n",
        "    max_sum = max(sum_v0,sum_v1,sum_v2,sum_v3)\n",
        "\n",
        "\n",
        "    print(\"max_sum\", val_list.index(max_sum) )\n",
        "\n",
        "    print(\"accuracy voice{}:\".format(i), max_sum/(sum_v0+sum_v1+sum_v2+sum_v3) )\n",
        "    print(\"________________\")\n",
        "    print(\" \")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BoQcV_i038DD",
        "outputId": "51322889-05f2-4c46-aa45-532f0721a003",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n### take 0 -> compare to truth of 0,1,2,3 -> overall voice\\n\\ncount_list = []\\n\\ncount_dict = {\\'0\\': [], \\'1\\': [], \\'2\\': [], \\'3\\': [] }\\ntruth_dic = {\\'0\\': 0, \\'1\\': 1, \\'2\\': 2, \\'3\\': 3 }\\n\\nvoice_entry_list = [\"0\", \"1\", \"2\", \"3\"]\\nfor voice_entry_one in voice_entry_list:\\n    for voice_entry_two in voice_entry_list:\\n        count_list = []\\n        #print(\"voices:\",voice_entry_one,voice_entry_two)\\n        for i in range(len(dict_pred[voice_entry_one])):\\n            if dict_pred[voice_entry_one][i][0] == truth_dic[voice_entry_two]:      #dict_truth[voice_entry_two][i][0]:\\n                count_list.append(1)\\n            else:\\n                count_list.append(0)\\n        count_dict[voice_entry_one].append(count_list)\\n\\ndictionary_sum={}\\nfor i in voice_entry_list:\\n    v0_match,v1_match,v2_match,v3_match = count_dict[i]\\n    sum_v0 = np.sum(v0_match)\\n    sum_v1 = np.sum(v1_match)\\n    sum_v2 = np.sum(v2_match)\\n    sum_v3 = np.sum(v3_match)\\n    dictionary_sum[\"v0\"] = sum_v0\\n    dictionary_sum[\"v1\"] = sum_v1\\n    dictionary_sum[\"v2\"] = sum_v2\\n    dictionary_sum[\"v3\"] = sum_v3\\n\\n    val_list = list(dictionary_sum.values())\\n    \\n    print(\"voice{} matches with\".format(i))\\n    print(\"dict\",dictionary_sum)\\n\\n    max_sum = max(sum_v0,sum_v1,sum_v2,sum_v3)\\n\\n\\n    print(\"max_sum\", val_list.index(max_sum) )\\n\\n    print(\"accuracy voice{}:\".format(i), max_sum/(sum_v0+sum_v1+sum_v2+sum_v3) )\\n    print(\"________________\")\\n    print(\" \")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR MONOPHONIC F1\n",
        "\n",
        "# start with GT\n",
        "# look at first note in pred-> save note label\n",
        "# look at second note in pred-> if same note as before : SUCESS if it is not: FAIL\n",
        " # DO This for all 4 voices\n",
        " ## in GT there is always the same voice following -> would always be an array of 1\n",
        "\n",
        "## POLYPHONIC \n",
        "\n",
        "# prbl after 1 note there can be multiple diff voices .. chords"
      ],
      "metadata": {
        "id": "-bR7gcej90qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you have the ground truth on the different parts that you get when you import your score. Each part correspond to a voice. So if your note array contains all notes of all voices, you have for each note in your note array a number that is the ground truth voice (that you take from the part) and a number that is the predicted voice (that you take from the maximum vote)."
      ],
      "metadata": {
        "id": "Z5q305YzvjMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start time, duration , pitch to separate \n",
        "\n",
        "use the onset_beat and duration_beat\n",
        "\n",
        "multiply them according to the values set when producing the pianorolls \n",
        "\n",
        "-> get the position in the pianoroll\n",
        "\n",
        "time_div = 12\n",
        "\n"
      ],
      "metadata": {
        "id": "EmvxtyaVKG27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "A94mchm4LV6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy_v0\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v1\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v2\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v3\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Accuracy0','Accuracy1','Accuracy2','Accuracy3'])\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TgJDHaxAgYN",
        "outputId": "382b6ebc-45a2-4bca-9c35-d7031e2bdcc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend('Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxMs8GEfMvPE",
        "outputId": "c75914d5-113a-4fde-f893-c5aa73d4fc01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"train_loss\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqMcJT5aFL01",
        "outputId": "05c2531c-3e79-42e2-90e8-0ea5ef2433d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chew chorals\n",
        "\n"
      ],
      "metadata": {
        "id": "fS4tzYkxr06Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_chew_pr (file_name, sentences):\n",
        "    path = \"AI-MA_project/chorales_converted/\"\n",
        "    fullname = os.path.join(path, \"chor\"+ file_name +\".xml\")\n",
        "    part = partitura.load_musicxml(fullname)\n",
        "        \n",
        "\n",
        "    ### apply chews method ### \n",
        "    chew_sep = partitura.musicanalysis.estimate_voices(part, monophonic_voices=True)\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_zero = np.where(chew_sep==1)\n",
        "    pos_one = np.where(chew_sep==2)\n",
        "    pos_two = np.where(chew_sep==3)\n",
        "    pos_three = np.where(chew_sep==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    part_zero = partitura.utils.ensure_notearray(part)[pos_zero]\n",
        "    part_one = partitura.utils.ensure_notearray(part)[pos_one]\n",
        "    part_two = partitura.utils.ensure_notearray(part)[pos_two]\n",
        "    part_three = partitura.utils.ensure_notearray(part)[pos_three]\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    pr_zero = partitura.utils.compute_pianoroll(part_zero, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(part_one, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(part_two, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    pr_three = partitura.utils.compute_pianoroll(part_three, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_three = pr_three.toarray()\n",
        "    \n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    #print(\"scores_comb.shape\",scores_comb.shape)\n",
        "    #print(\"sentences[:,None,:,:].shape\",sentences[:,None,:,:].shape)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "    \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "55-Dy39Cwg5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_chew(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    #note_counter_0 = 0\n",
        "    #note_counter_1 = 0\n",
        "    #note_counter_2 = 0\n",
        "    #note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx != 26 and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                #note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                #note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                #note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                #note_array_3 = part_3.note_array\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "                #note_counter_0 += len(note_array_0)\n",
        "                #note_counter_1 += len(note_array_1)\n",
        "                #note_counter_2 += len(note_array_2)\n",
        "                #note_counter_3 += len(note_array_3)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################            \n",
        "                    prediction = calculate_chew_pr(file_name,voices[:,:,:,-1]) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    #print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "NVvLm9pdryYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sampel 26&27 dont work "
      ],
      "metadata": {
        "id": "yD0w6nJQ82iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_chew(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "UD11ziChvL_-",
        "outputId": "6c86d405-ad96-4ecc-a146-a092d2605c1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmusicxml.py:302: UserWarning: Found repeat without start\n",
            "Starting point 0 is assumend\n",
            "  \"Starting point {} is assumend\".format(start_times[start_time_id]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0, sample 0: 1.0\n",
            "acc 1, sample 0: 0.9349593495934959\n",
            "acc 2, sample 0: 0.9724770642201835\n",
            "acc 3, sample 0: 0.9770114942528736\n",
            "acc 0, sample 1: 1.0\n",
            "acc 1, sample 1: 1.0\n",
            "acc 2, sample 1: 0.9833333333333333\n",
            "acc 3, sample 1: 1.0\n",
            "acc 0, sample 2: 0.9811320754716981\n",
            "acc 1, sample 2: 1.0\n",
            "acc 2, sample 2: 1.0\n",
            "acc 3, sample 2: 0.9767441860465116\n",
            "acc 0, sample 3: 0.9791666666666666\n",
            "acc 1, sample 3: 0.9183673469387755\n",
            "acc 2, sample 3: 1.0\n",
            "acc 3, sample 3: 0.9666666666666667\n",
            "acc 0, sample 4: 1.0\n",
            "acc 1, sample 4: 0.9\n",
            "acc 2, sample 4: 0.8536585365853658\n",
            "acc 3, sample 4: 1.0\n",
            "acc 0, sample 5: 1.0\n",
            "acc 1, sample 5: 0.9770114942528736\n",
            "acc 2, sample 5: 0.9655172413793104\n",
            "acc 3, sample 5: 0.926829268292683\n",
            "acc 0, sample 6: 1.0\n",
            "acc 1, sample 6: 0.9444444444444444\n",
            "acc 2, sample 6: 0.9702970297029703\n",
            "acc 3, sample 6: 0.9775280898876404\n",
            "acc 0, sample 7: 0.9298245614035088\n",
            "acc 1, sample 7: 0.9555555555555556\n",
            "acc 2, sample 7: 0.9607843137254902\n",
            "acc 3, sample 7: 0.9782608695652174\n",
            "acc 0, sample 8: 1.0\n",
            "acc 1, sample 8: 1.0\n",
            "acc 2, sample 8: 1.0\n",
            "acc 3, sample 8: 0.9565217391304348\n",
            "acc 0, sample 9: 0.9761904761904762\n",
            "acc 1, sample 9: 0.9\n",
            "acc 2, sample 9: 0.9473684210526315\n",
            "acc 3, sample 9: 1.0\n",
            "acc 0, sample 10: 1.0\n",
            "acc 1, sample 10: 0.9811320754716981\n",
            "acc 2, sample 10: 1.0\n",
            "acc 3, sample 10: 1.0\n",
            "acc 0, sample 11: 0.9873417721518988\n",
            "acc 1, sample 11: 0.9878048780487805\n",
            "acc 2, sample 11: 1.0\n",
            "acc 3, sample 11: 0.9615384615384616\n",
            "acc 0, sample 12: 1.0\n",
            "acc 1, sample 12: 0.875\n",
            "acc 2, sample 12: 1.0\n",
            "acc 3, sample 12: 1.0\n",
            "acc 0, sample 13: 1.0\n",
            "acc 1, sample 13: 0.9682539682539683\n",
            "acc 2, sample 13: 1.0\n",
            "acc 3, sample 13: 1.0\n",
            "acc 0, sample 14: 0.9863013698630136\n",
            "acc 1, sample 14: 0.9571428571428572\n",
            "acc 2, sample 14: 0.9516129032258065\n",
            "acc 3, sample 14: 0.9411764705882353\n",
            "acc 0, sample 15: 1.0\n",
            "acc 1, sample 15: 0.9811320754716981\n",
            "acc 2, sample 15: 1.0\n",
            "acc 3, sample 15: 0.975\n",
            "acc 0, sample 16: 0.9807692307692307\n",
            "acc 1, sample 16: 0.9433962264150944\n",
            "acc 2, sample 16: 0.9830508474576272\n",
            "acc 3, sample 16: 1.0\n",
            "acc 0, sample 17: 0.972972972972973\n",
            "acc 1, sample 17: 0.9411764705882353\n",
            "acc 2, sample 17: 0.9473684210526315\n",
            "acc 3, sample 17: 0.9354838709677419\n",
            "acc 0, sample 18: 1.0\n",
            "acc 1, sample 18: 0.9090909090909091\n",
            "acc 2, sample 18: 1.0\n",
            "acc 3, sample 18: 1.0\n",
            "acc 0, sample 19: 1.0\n",
            "acc 1, sample 19: 0.9818181818181818\n",
            "acc 2, sample 19: 0.9803921568627451\n",
            "acc 3, sample 19: 1.0\n",
            "acc 0, sample 20: 1.0\n",
            "acc 1, sample 20: 0.8909090909090909\n",
            "acc 2, sample 20: 0.8823529411764706\n",
            "acc 3, sample 20: 0.975\n",
            "acc 0, sample 21: 1.0\n",
            "acc 1, sample 21: 0.9423076923076923\n",
            "acc 2, sample 21: 0.9787234042553191\n",
            "acc 3, sample 21: 0.975609756097561\n",
            "acc 0, sample 22: 1.0\n",
            "acc 1, sample 22: 0.0\n",
            "acc 2, sample 22: 1.0\n",
            "acc 3, sample 22: 0.9791666666666666\n",
            "acc 0, sample 23: 0.9824561403508771\n",
            "acc 1, sample 23: 0.9574468085106383\n",
            "acc 2, sample 23: 1.0\n",
            "acc 3, sample 23: 0.9473684210526315\n",
            "acc 0, sample 24: 1.0\n",
            "acc 1, sample 24: 0.967741935483871\n",
            "acc 2, sample 24: 0.9629629629629629\n",
            "acc 3, sample 24: 0.9433962264150944\n",
            "acc 0, sample 25: 1.0\n",
            "acc 1, sample 25: 0.94\n",
            "acc 2, sample 25: 1.0\n",
            "acc 3, sample 25: 1.0\n",
            "acc 0, sample 28: 0.9777777777777777\n",
            "acc 1, sample 28: 0.926829268292683\n",
            "acc 2, sample 28: 0.9285714285714286\n",
            "acc 3, sample 28: 0.9722222222222222\n",
            "acc 0, sample 29: 0.9512195121951219\n",
            "acc 1, sample 29: 0.9534883720930233\n",
            "acc 2, sample 29: 0.9111111111111111\n",
            "acc 3, sample 29: 0.918918918918919\n",
            "acc 0, sample 30: 0.030303030303030304\n",
            "acc 1, sample 30: 0.0\n",
            "acc 2, sample 30: 0.9736842105263158\n",
            "acc 3, sample 30: 0.967741935483871\n",
            "acc 0, sample 31: 1.0\n",
            "acc 1, sample 31: 1.0\n",
            "acc 2, sample 31: 1.0\n",
            "acc 3, sample 31: 1.0\n",
            "acc 0, sample 32: 0.9857142857142858\n",
            "acc 1, sample 32: 0.9622641509433962\n",
            "acc 2, sample 32: 0.9640287769784173\n",
            "acc 3, sample 32: 0.9224137931034483\n",
            "acc 0, sample 33: 1.0\n",
            "acc 1, sample 33: 0.975\n",
            "acc 2, sample 33: 1.0\n",
            "acc 3, sample 33: 0.9705882352941176\n",
            "acc 0, sample 34: 0.9753086419753086\n",
            "acc 1, sample 34: 0.918918918918919\n",
            "acc 2, sample 34: 0.9866666666666667\n",
            "acc 3, sample 34: 0.9846153846153847\n",
            "acc 0, sample 35: 0.9642857142857143\n",
            "acc 1, sample 35: 0.9387755102040817\n",
            "acc 2, sample 35: 0.9607843137254902\n",
            "acc 3, sample 35: 0.9777777777777777\n",
            "acc 0, sample 36: 1.0\n",
            "acc 1, sample 36: 0.9482758620689655\n",
            "acc 2, sample 36: 0.9464285714285714\n",
            "acc 3, sample 36: 1.0\n",
            "acc 0, sample 37: 1.0\n",
            "acc 1, sample 37: 0.96\n",
            "acc 2, sample 37: 1.0\n",
            "acc 3, sample 37: 1.0\n",
            "acc 0, sample 38: 0.9803921568627451\n",
            "acc 1, sample 38: 0.8867924528301887\n",
            "acc 2, sample 38: 0.9795918367346939\n",
            "acc 3, sample 38: 0.9302325581395349\n",
            "acc 0, sample 39: 0.09523809523809523\n",
            "acc 1, sample 39: 0.8636363636363636\n",
            "acc 2, sample 39: 0.9242424242424242\n",
            "acc 3, sample 39: 1.0\n",
            "acc 0, sample 40: 0.9090909090909091\n",
            "acc 1, sample 40: 0.7857142857142857\n",
            "acc 2, sample 40: 0.92\n",
            "acc 3, sample 40: 0.9787234042553191\n",
            "acc 0, sample 41: 0.08974358974358974\n",
            "acc 1, sample 41: 0.0\n",
            "acc 2, sample 41: 0.9571428571428572\n",
            "acc 3, sample 41: 1.0\n",
            "acc 0, sample 42: 1.0\n",
            "acc 1, sample 42: 1.0\n",
            "acc 2, sample 42: 1.0\n",
            "acc 3, sample 42: 0.9787234042553191\n",
            "acc 0, sample 43: 0.9777777777777777\n",
            "acc 1, sample 43: 0.9743589743589743\n",
            "acc 2, sample 43: 1.0\n",
            "acc 3, sample 43: 0.9743589743589743\n",
            "acc 0, sample 44: 1.0\n",
            "acc 1, sample 44: 0.9253731343283582\n",
            "acc 2, sample 44: 0.9666666666666667\n",
            "acc 3, sample 44: 0.9827586206896551\n",
            "acc 0, sample 45: 1.0\n",
            "acc 1, sample 45: 1.0\n",
            "acc 2, sample 45: 0.9615384615384616\n",
            "acc 3, sample 45: 0.9130434782608695\n",
            "acc 0, sample 46: 1.0\n",
            "acc 1, sample 46: 1.0\n",
            "acc 2, sample 46: 1.0\n",
            "acc 3, sample 46: 0.9354838709677419\n",
            "acc 0, sample 47: 1.0\n",
            "acc 1, sample 47: 0.9454545454545454\n",
            "acc 2, sample 47: 1.0\n",
            "acc 3, sample 47: 1.0\n",
            "acc 0, sample 48: 0.9661016949152542\n",
            "acc 1, sample 48: 0.9333333333333333\n",
            "acc 2, sample 48: 0.9827586206896551\n",
            "acc 3, sample 48: 0.9803921568627451\n",
            "acc 0, sample 49: 1.0\n",
            "acc 1, sample 49: 0.9302325581395349\n",
            "acc 2, sample 49: 1.0\n",
            "acc 3, sample 49: 0.9705882352941176\n",
            "acc 0, sample 50: 1.0\n",
            "acc 1, sample 50: 0.9629629629629629\n",
            "acc 2, sample 50: 0.9622641509433962\n",
            "acc 3, sample 50: 0.975609756097561\n",
            "acc 0, sample 51: 1.0\n",
            "acc 1, sample 51: 0.9811320754716981\n",
            "acc 2, sample 51: 0.9743589743589743\n",
            "acc 3, sample 51: 1.0\n",
            "acc 0, sample 52: 0.9090909090909091\n",
            "acc 1, sample 52: 0.8205128205128205\n",
            "acc 2, sample 52: 0.9166666666666666\n",
            "acc 3, sample 52: 1.0\n",
            "acc 0, sample 53: 1.0\n",
            "acc 1, sample 53: 0.9245283018867925\n",
            "acc 2, sample 53: 1.0\n",
            "acc 3, sample 53: 1.0\n",
            "acc 0, sample 54: 0.9358974358974359\n",
            "acc 1, sample 54: 0.8554216867469879\n",
            "acc 2, sample 54: 0.9583333333333334\n",
            "acc 3, sample 54: 0.9682539682539683\n",
            "acc 0, sample 55: 1.0\n",
            "acc 1, sample 55: 0.9736842105263158\n",
            "acc 2, sample 55: 1.0\n",
            "acc 3, sample 55: 0.9696969696969697\n",
            "total_predictions_dict dict_keys(['0', '1', '2', '3'])\n",
            "0.9356314221612648 0.8907663175689277 0.9724951601540366 0.9752860342910543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# McLeod chorals"
      ],
      "metadata": {
        "id": "9ejZujau-TXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mcleod_pr (file_name, sentences):\n",
        "    path = \"AI-MA_project/hmm_sep/\"\n",
        "    fullname = os.path.join(path, \"chor\"+ file_name +\".mid\")\n",
        "        \n",
        "\n",
        "    ### apply chews method ### \n",
        "    part_hmm = partitura.load_score_midi(fullname,part_voice_assign_mode=2)\n",
        "    voice_info = partitura.utils.note_array_from_part(part_hmm)[\"voice\"]\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(voice_info==1)\n",
        "    pos_1 = np.where(voice_info==2)\n",
        "    pos_2 = np.where(voice_info==3)\n",
        "    pos_3 = np.where(voice_info==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    note_array_0= partitura.utils.ensure_notearray(part_hmm)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part_hmm)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part_hmm)[pos_2]\n",
        "    note_array_3 = partitura.utils.ensure_notearray(part_hmm)[pos_3]\n",
        "\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_three = pr_three.toarray()\n",
        "    \n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    #print(\"scores_comb.shape\",scores_comb.shape)\n",
        "    #print(\"sentences[:,None,:,:].shape\",sentences[:,None,:,:].shape)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "    \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "pQP8pq9a-S6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mc_leod(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx not in [16,17,18,27,28,32,44,45,48,49,50]: # and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                #note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                #note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                #note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                #note_array_3 = part_3.note_array\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################                   \n",
        "                    prediction = calculate_mcleod_pr(file_name,voices[:,:,:,-1]) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "2igkR5SI-Z1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "does not work for samples 16,17,18,27,28,32,44,45,48,49,50"
      ],
      "metadata": {
        "id": "g1JC5yeMBnnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_mc_leod(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "oxRr270dC_mj",
        "outputId": "ab650931-9a58-441a-c55e-3bd6cda5d9de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:419: UserWarning: pitch spelling\n",
            "  warnings.warn(\"pitch spelling\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: voice estimation\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: create_part\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add time sigs and measures\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: tie notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: find tuplets\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: done create_part\n",
            "  part_name=part_names.get(part_nr, None),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0, sample 0: 1.0\n",
            "acc 1, sample 0: 0.9512195121951219\n",
            "acc 2, sample 0: 0.963302752293578\n",
            "acc 3, sample 0: 0.9770114942528736\n",
            "acc 0, sample 1: 1.0\n",
            "acc 1, sample 1: 1.0\n",
            "acc 2, sample 1: 0.9833333333333333\n",
            "acc 3, sample 1: 1.0\n",
            "acc 0, sample 2: 1.0\n",
            "acc 1, sample 2: 0.9791666666666666\n",
            "acc 2, sample 2: 1.0\n",
            "acc 3, sample 2: 0.9767441860465116\n",
            "acc 0, sample 3: 1.0\n",
            "acc 1, sample 3: 0.9183673469387755\n",
            "acc 2, sample 3: 0.9782608695652174\n",
            "acc 3, sample 3: 0.9666666666666667\n",
            "acc 0, sample 4: 1.0\n",
            "acc 1, sample 4: 0.9\n",
            "acc 2, sample 4: 0.8536585365853658\n",
            "acc 3, sample 4: 1.0\n",
            "acc 0, sample 5: 0.9777777777777777\n",
            "acc 1, sample 5: 0.9655172413793104\n",
            "acc 2, sample 5: 0.9885057471264368\n",
            "acc 3, sample 5: 0.9024390243902439\n",
            "acc 0, sample 6: 1.0\n",
            "acc 1, sample 6: 0.9351851851851852\n",
            "acc 2, sample 6: 0.9801980198019802\n",
            "acc 3, sample 6: 0.9775280898876404\n",
            "acc 0, sample 7: 0.9824561403508771\n",
            "acc 1, sample 7: 0.8888888888888888\n",
            "acc 2, sample 7: 0.9803921568627451\n",
            "acc 3, sample 7: 0.9565217391304348\n",
            "acc 0, sample 8: 1.0\n",
            "acc 1, sample 8: 1.0\n",
            "acc 2, sample 8: 1.0\n",
            "acc 3, sample 8: 0.9565217391304348\n",
            "acc 0, sample 9: 0.9761904761904762\n",
            "acc 1, sample 9: 0.9\n",
            "acc 2, sample 9: 0.9473684210526315\n",
            "acc 3, sample 9: 1.0\n",
            "acc 0, sample 10: 1.0\n",
            "acc 1, sample 10: 0.9811320754716981\n",
            "acc 2, sample 10: 1.0\n",
            "acc 3, sample 10: 1.0\n",
            "acc 0, sample 11: 0.9873417721518988\n",
            "acc 1, sample 11: 0.9878048780487805\n",
            "acc 2, sample 11: 1.0\n",
            "acc 3, sample 11: 0.9615384615384616\n",
            "acc 0, sample 12: 1.0\n",
            "acc 1, sample 12: 0.875\n",
            "acc 2, sample 12: 1.0\n",
            "acc 3, sample 12: 1.0\n",
            "acc 0, sample 13: 1.0\n",
            "acc 1, sample 13: 0.9523809523809523\n",
            "acc 2, sample 13: 1.0\n",
            "acc 3, sample 13: 1.0\n",
            "acc 0, sample 14: 0.9863013698630136\n",
            "acc 1, sample 14: 0.9285714285714286\n",
            "acc 2, sample 14: 0.9838709677419355\n",
            "acc 3, sample 14: 0.9019607843137255\n",
            "acc 0, sample 15: 1.0\n",
            "acc 1, sample 15: 0.9811320754716981\n",
            "acc 2, sample 15: 1.0\n",
            "acc 3, sample 15: 0.95\n",
            "acc 0, sample 19: 1.0\n",
            "acc 1, sample 19: 0.9818181818181818\n",
            "acc 2, sample 19: 0.9803921568627451\n",
            "acc 3, sample 19: 1.0\n",
            "acc 0, sample 20: 1.0\n",
            "acc 1, sample 20: 0.9272727272727272\n",
            "acc 2, sample 20: 0.8823529411764706\n",
            "acc 3, sample 20: 0.95\n",
            "acc 0, sample 21: 1.0\n",
            "acc 1, sample 21: 0.9615384615384616\n",
            "acc 2, sample 21: 0.9787234042553191\n",
            "acc 3, sample 21: 0.975609756097561\n",
            "acc 0, sample 22: 1.0\n",
            "acc 1, sample 22: 0.9318181818181818\n",
            "acc 2, sample 22: 1.0\n",
            "acc 3, sample 22: 0.9791666666666666\n",
            "acc 0, sample 23: 1.0\n",
            "acc 1, sample 23: 0.9361702127659575\n",
            "acc 2, sample 23: 1.0\n",
            "acc 3, sample 23: 0.9473684210526315\n",
            "acc 0, sample 24: 1.0\n",
            "acc 1, sample 24: 0.9516129032258065\n",
            "acc 2, sample 24: 0.9629629629629629\n",
            "acc 3, sample 24: 0.9245283018867925\n",
            "acc 0, sample 25: 1.0\n",
            "acc 1, sample 25: 0.94\n",
            "acc 2, sample 25: 1.0\n",
            "acc 3, sample 25: 1.0\n",
            "acc 0, sample 26: 1.0\n",
            "acc 1, sample 26: 0.9487179487179487\n",
            "acc 2, sample 26: 0.9473684210526315\n",
            "acc 3, sample 26: 0.9473684210526315\n",
            "acc 0, sample 29: 0.975609756097561\n",
            "acc 1, sample 29: 0.9534883720930233\n",
            "acc 2, sample 29: 0.9333333333333333\n",
            "acc 3, sample 29: 0.8918918918918919\n",
            "acc 0, sample 30: 1.0\n",
            "acc 1, sample 30: 0.975\n",
            "acc 2, sample 30: 1.0\n",
            "acc 3, sample 30: 0.9354838709677419\n",
            "acc 0, sample 31: 1.0\n",
            "acc 1, sample 31: 1.0\n",
            "acc 2, sample 31: 1.0\n",
            "acc 3, sample 31: 1.0\n",
            "acc 0, sample 33: 1.0\n",
            "acc 1, sample 33: 0.975\n",
            "acc 2, sample 33: 1.0\n",
            "acc 3, sample 33: 0.9705882352941176\n",
            "acc 0, sample 34: 0.9753086419753086\n",
            "acc 1, sample 34: 0.918918918918919\n",
            "acc 2, sample 34: 0.9866666666666667\n",
            "acc 3, sample 34: 0.9846153846153847\n",
            "acc 0, sample 35: 0.9821428571428571\n",
            "acc 1, sample 35: 0.9183673469387755\n",
            "acc 2, sample 35: 0.9803921568627451\n",
            "acc 3, sample 35: 0.9555555555555556\n",
            "acc 0, sample 36: 0.9692307692307692\n",
            "acc 1, sample 36: 0.9482758620689655\n",
            "acc 2, sample 36: 0.9285714285714286\n",
            "acc 3, sample 36: 1.0\n",
            "acc 0, sample 37: 1.0\n",
            "acc 1, sample 37: 0.96\n",
            "acc 2, sample 37: 1.0\n",
            "acc 3, sample 37: 1.0\n",
            "acc 0, sample 38: 1.0\n",
            "acc 1, sample 38: 0.8490566037735849\n",
            "acc 2, sample 38: 0.9795918367346939\n",
            "acc 3, sample 38: 0.9302325581395349\n",
            "acc 0, sample 39: 1.0\n",
            "acc 1, sample 39: 0.8787878787878788\n",
            "acc 2, sample 39: 0.9242424242424242\n",
            "acc 3, sample 39: 1.0\n",
            "acc 0, sample 40: 0.9636363636363636\n",
            "acc 1, sample 40: 0.7678571428571429\n",
            "acc 2, sample 40: 0.92\n",
            "acc 3, sample 40: 0.9787234042553191\n",
            "acc 0, sample 41: 1.0\n",
            "acc 1, sample 41: 0.9146341463414634\n",
            "acc 2, sample 41: 0.9714285714285714\n",
            "acc 3, sample 41: 0.9859154929577465\n",
            "acc 0, sample 42: 1.0\n",
            "acc 1, sample 42: 1.0\n",
            "acc 2, sample 42: 1.0\n",
            "acc 3, sample 42: 0.9787234042553191\n",
            "acc 0, sample 43: 1.0\n",
            "acc 1, sample 43: 0.9487179487179487\n",
            "acc 2, sample 43: 1.0\n",
            "acc 3, sample 43: 0.9743589743589743\n",
            "acc 0, sample 46: 1.0\n",
            "acc 1, sample 46: 1.0\n",
            "acc 2, sample 46: 0.972972972972973\n",
            "acc 3, sample 46: 0.9354838709677419\n",
            "acc 0, sample 47: 1.0\n",
            "acc 1, sample 47: 0.9272727272727272\n",
            "acc 2, sample 47: 1.0\n",
            "acc 3, sample 47: 1.0\n",
            "acc 0, sample 51: 1.0\n",
            "acc 1, sample 51: 0.9811320754716981\n",
            "acc 2, sample 51: 0.9487179487179487\n",
            "acc 3, sample 51: 1.0\n",
            "acc 0, sample 52: 0.9393939393939394\n",
            "acc 1, sample 52: 0.8205128205128205\n",
            "acc 2, sample 52: 0.9722222222222222\n",
            "acc 3, sample 52: 0.967741935483871\n",
            "acc 0, sample 53: 1.0\n",
            "acc 1, sample 53: 0.9245283018867925\n",
            "acc 2, sample 53: 1.0\n",
            "acc 3, sample 53: 1.0\n",
            "acc 0, sample 54: 0.9487179487179487\n",
            "acc 1, sample 54: 0.8674698795180723\n",
            "acc 2, sample 54: 0.9583333333333334\n",
            "acc 3, sample 54: 0.9682539682539683\n",
            "acc 0, sample 55: 1.0\n",
            "acc 1, sample 55: 0.9736842105263158\n",
            "acc 2, sample 55: 1.0\n",
            "acc 3, sample 55: 0.9696969696969697\n",
            "total_predictions_dict dict_keys(['0', '1', '2', '3'])\n",
            "0.9925357291673065 0.93835598008982 0.9752703019057709 0.9706275393068314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chew fugues"
      ],
      "metadata": {
        "id": "uenr6Uavaic3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_chew_pr_fugue (file_name, sentences, nbr_voices):\n",
        "    path = \"AI-MA_project/bach_fugues/\"\n",
        "    fullname = os.path.join(path, \"wtc\"+ file_name +\".mid\")\n",
        "\n",
        "    part = partitura.load_score_midi(fullname)\n",
        "\n",
        "\n",
        "    ### apply chews method ### \n",
        "    chew_sep = partitura.musicanalysis.estimate_voices(part, monophonic_voices=True)\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(chew_sep==1)\n",
        "    pos_1 = np.where(chew_sep==2)\n",
        "    pos_2 = np.where(chew_sep==3)\n",
        "    if nbr_voices ==4:\n",
        "        pos_3 = np.where(chew_sep==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    \n",
        "    note_array_0 = partitura.utils.ensure_notearray(part)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part)[pos_2]\n",
        "    if nbr_voices==4:\n",
        "        note_array_3 = partitura.utils.ensure_notearray(part)[pos_3]\n",
        "\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    onset_beat_0 = note_array_0['onset_beat'][-1]\n",
        "    duration_beat_0 = note_array_0['duration_beat'][-1]\n",
        "    beat_0 = onset_beat_0 + duration_beat_0\n",
        "    \n",
        "    onset_beat_1 = note_array_1['onset_beat'][-1]\n",
        "    duration_beat_1 = note_array_1['duration_beat'][-1]\n",
        "    beat_1 = onset_beat_1 + duration_beat_1\n",
        "    \n",
        "    onset_beat_2 = note_array_2['onset_beat'][-1]\n",
        "    duration_beat_2 = note_array_2['duration_beat'][-1]\n",
        "    beat_2 = onset_beat_2 + duration_beat_2\n",
        "\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_0)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_1)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_2)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    if nbr_voices==4:\n",
        "        onset_beat_3 = note_array_3['onset_beat'][-1]\n",
        "        duration_beat_3 = note_array_3['duration_beat'][-1]\n",
        "        beat_3 = onset_beat_3 + duration_beat_3\n",
        "        pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_3)\n",
        "        pr_three = pr_three.toarray()\n",
        "    else:\n",
        "        pr_three = np.zeros(pr_two.shape)\n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0)) \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "vNsE3VVMa9Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_chew_fugue( train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx != 26 and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                \n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "\n",
        "                if len(part) == 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################\n",
        "                    prediction = calculate_chew_pr_fugue(file_name,voices[:,:,:,-1],nbr_voices) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "C50fX7xjasD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True: \n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_chew_fugue(val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "yWz4I9b7asVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# McLeod fugues"
      ],
      "metadata": {
        "id": "xofnsEX4DAME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mcleod_pr (file_name, sentences,nbr_voices):\n",
        "    path = \"AI-MA_project/bach_fugues/\"\n",
        "    fullname = os.path.join(path, \"wtc\"+ file_name +\".mid\")\n",
        "    ### apply chews method ### \n",
        "    part_hmm = partitura.load_score_midi(fullname,part_voice_assign_mode=2)\n",
        "    voice_info = partitura.utils.note_array_from_part(part_hmm)[\"voice\"]\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(voice_info==1)\n",
        "    pos_1 = np.where(voice_info==2)\n",
        "    pos_2 = np.where(voice_info==3)\n",
        "    if nbr_voices ==4:\n",
        "        pos_3 = np.where(voice_info==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    note_array_0= partitura.utils.ensure_notearray(part_hmm)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part_hmm)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part_hmm)[pos_2]\n",
        "    if nbr_voices==4:\n",
        "        note_array_3 = partitura.utils.ensure_notearray(part_hmm)[pos_3]\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    onset_beat_0 = note_array_0['onset_beat'][-1]\n",
        "    duration_beat_0 = note_array_0['duration_beat'][-1]\n",
        "    beat_0 = onset_beat_0 + duration_beat_0\n",
        "    \n",
        "    onset_beat_1 = note_array_1['onset_beat'][-1]\n",
        "    duration_beat_1 = note_array_1['duration_beat'][-1]\n",
        "    beat_1 = onset_beat_1 + duration_beat_1\n",
        "    \n",
        "    onset_beat_2 = note_array_2['onset_beat'][-1]\n",
        "    duration_beat_2 = note_array_2['duration_beat'][-1]\n",
        "    beat_2 = onset_beat_2 + duration_beat_2\n",
        "\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_0)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_1)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_2)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    if nbr_voices==4:\n",
        "        onset_beat_3 = note_array_3['onset_beat'][-1]\n",
        "        duration_beat_3 = note_array_3['duration_beat'][-1]\n",
        "        beat_3 = onset_beat_3 + duration_beat_3\n",
        "        pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_3)\n",
        "        pr_three = pr_three.toarray()\n",
        "    else:\n",
        "        pr_three = np.zeros(pr_two.shape)\n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0)) \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "CU4iH-25aD5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mc_leod_fugues(train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            #if idx not in [16,17,18,27,28,32,44,45,48,49,50]: # and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "\n",
        "                if len(part) == 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "           \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "          \n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################                   \n",
        "                    prediction = calculate_mcleod_pr(file_name,voices[:,:,:,-1],nbr_voices) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "DgFfq9Q4Csei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True: \n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_mc_leod_fugues(val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "5kSrFU8e-uol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old training loop - matrix and non matrix format"
      ],
      "metadata": {
        "id": "4olpdwzyG8dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        #accuracy_v0_sum = 0\n",
        "        #accuracy_v1_sum = 0\n",
        "        #accuracy_v2_sum = 0\n",
        "        #accuracy_v3_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4: \n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                    v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                    mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_pred_flat:\", v_pred_flat.shape)\n",
        "                    \"\"\"\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4:                   \n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                    v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                    mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                    print(\"old mask\", mask_ori.shape)\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_ori_flat\", v_ori_flat.shape)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)   \n",
        "                    print(\"acc\",acc)                    \n",
        "                    accuracy_sum += acc \n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                    acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                    for i in range(len(prediction[0,:])):\n",
        "                      acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                      accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    \n",
        "                    \"\"\"\n",
        "                    pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "\n",
        "                    acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                    acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                    acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                    if nbr_voices == 4:\n",
        "                        acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                    \n",
        "                    # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    if nbr_voices == 4:\n",
        "                        accuracy_v3_sum += acc_v3 / len(lens)\n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "            train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "            train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "            train_accuracy_v3 = accuracy_v3_sum / 18   ## bc only 18 pieces with len 3\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "            history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "            history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "            #if nbr_voices == 4:\n",
        "            history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3)) \n",
        "            #else:\n",
        "            #    print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2)) \n",
        "            \"\"\"\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_v0_sum = 0\n",
        "                accuracy_v1_sum = 0\n",
        "                accuracy_v2_sum = 0\n",
        "                accuracy_v3_sum = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                        truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                        acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                        for i in range(len(prediction[0,:])):\n",
        "                          acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                          val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                        #print(\"val_accuracy_sum_list[3]\",val_accuracy_sum_list[3])\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(train_dataloader)\n",
        "                    #val_acc_list[3] = val_acc_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_acc_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "                #history[\"val_acc_new\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        # Predict the model's output on a batch\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                            \n",
        "                        # compute the accuracy \n",
        "                        acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                        acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                        acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                        if nbr_voices == 4:\n",
        "                            acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                            \n",
        "                            \n",
        "                        # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                        accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                        accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                        accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                        if nbr_voices == 4:\n",
        "                            accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                    val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                    val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                    val_accuracy_v3 = accuracy_v3_sum / 18  ##len(val_dataloader). - bc 18 pieces only with voice 3\n",
        "\n",
        "\n",
        "                    val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    val_acc_list[3] = val_accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    val_acc_list[4] = val_accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                    \n",
        "\n",
        "\n",
        "                history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "                history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "                history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "                #if nbr_voices == 4:\n",
        "                history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "                #else:\n",
        "                #    print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2))\n",
        "\n",
        "\n",
        "                history[\"val_acc_new\"].append(val_acc_list)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "\n",
        "                        \"\"\"\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                        if nbr_voices == 4: \n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                        v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                        mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)                      \n",
        "                        if nbr_voices == 4:                   \n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                        v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                        mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)\n",
        "                        accuracy_sum += acc \n",
        "                        \"\"\"\n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "nfDV8MKGHE3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "    \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "  \n",
        "\n",
        "    def __getitem__(self, idx):          \n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])    \n",
        "\n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "\n",
        "\n",
        "        return (v0, v1, v2, v3, v_all, length)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "x2lFUuw719EC",
        "outputId": "fcd32bf0-4605-4a38-d05a-cd6d71e4d033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass MusicDataset(Dataset):\\n\\n    def __init__(self, data_dir, transforms=None):\\n        self.transforms = transforms\\n        self.data_dir = data_dir\\n\\n        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\\n        self.labels = labels\\n        self.pr_dict = {}\\n        len_list = []\\n\\n        for iLabel in range(len(labels)):\\n            \\n            if iLabel == 4:   \\n                voice_files = []\\n                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \\n                for name in file_names:\\n                    with open(name ,\\'rb\\') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\\n                        loaded_obj = pickle.load(f)  \\n                        voice_files.append(loaded_obj) \\n                        len_list.append(len(loaded_obj.T))\\n                        \\n                self.pr_dict[self.labels[iLabel]] = voice_files \\n                self.pr_dict[\"length\"] = len_list\\n    \\n            else:\\n                voice_files = []\\n                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \\n                for name in file_names:\\n                    with open(name ,\\'rb\\') as f: \\n                        loaded_obj = pickle.load(f)     \\n                        voice_files.append(loaded_obj)\\n\\n                self.pr_dict[self.labels[iLabel]] = voice_files \\n\\n\\n    def __len__(self):\\n      \\n        return len(self.pr_dict[self.labels[0]])\\n  \\n\\n    def __getitem__(self, idx):          \\n\\n        out_list = []\\n        for key, value in self.pr_dict.items():\\n            out_list.append(self.pr_dict[key][idx])    \\n\\n        v0 = torch.tensor(out_list[0].T)\\n        v1 = torch.tensor(out_list[1].T)\\n        v2 = torch.tensor(out_list[2].T)\\n        v3 = torch.tensor(out_list[3].T)\\n        v_all = torch.tensor(out_list[4].T) \\n        length = self.pr_dict[\"length\"][idx]\\n\\n\\n        return (v0, v1, v2, v3, v_all, length)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    }
  ]
}