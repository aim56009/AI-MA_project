{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject_final_tensor_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "SsyC2uB0KfaT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28acce1a-22bc-4dd3-dd7d-94a7ed616f98"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install partitura\n",
        "import partitura"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting partitura\n",
            "  Downloading partitura-0.4.0-py3-none-any.whl (218 kB)\n",
            "\u001b[K     |████████████████████████████████| 218 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from partitura) (4.2.6)\n",
            "Collecting mido\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 9.3 MB/s \n",
            "\u001b[?25hCollecting xmlschema\n",
            "  Downloading xmlschema-1.11.1-py3-none-any.whl (334 kB)\n",
            "\u001b[K     |████████████████████████████████| 334 kB 20.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.4.1)\n",
            "Collecting lark-parser\n",
            "  Downloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 56.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.21.6)\n",
            "Collecting elementpath<3.0.0,>=2.5.0\n",
            "  Downloading elementpath-2.5.3-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 60.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: elementpath, xmlschema, mido, lark-parser, partitura\n",
            "Successfully installed elementpath-2.5.3 lark-parser-0.12.0 mido-1.2.10 partitura-0.4.0 xmlschema-1.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5409d26-c2f0-461b-f1f1-1937e2c9f781"
      },
      "source": [
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI-MA_project'...\n",
            "remote: Enumerating objects: 5473, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
            "remote: Total 5473 (delta 36), reused 59 (delta 5), pack-reused 5364\u001b[K\n",
            "Receiving objects: 100% (5473/5473), 6.27 MiB | 19.52 MiB/s, done.\n",
            "Resolving deltas: 100% (4635/4635), done.\n",
            "Checking out files: 100% (5329/5329), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "batch_size = 1 \n",
        "PATH_TO_DATA = \"AI-MA_project/bach_pr_fugues\"\n",
        "#PATH_TO_DATA = \"AI-MA_project/pianoroll_88\"\n",
        "workers = 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1-u8zYd-gyo"
      },
      "source": [
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "    \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "  \n",
        "\n",
        "    def __getitem__(self, idx):          \n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])    \n",
        "\n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "\n",
        "\n",
        "        return (v0, v1, v2, v3, v_all, length)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_new(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        self.name_list = [\"1f01\",\"1f02\",\"1f03\",\"1f04\",\"1f05\",\"1f06\",\"1f07\",\"1f08\",\"1f09\",\"1f10\",\"1f11\",\"1f12\",\"1f13\",\"1f14\",\"1f15\",\"1f16\",\"1f17\",\"1f18\",\"1f19\",\"1f20\",\"1f21\",\"1f22\",\"1f23\",\"1f24\",\"2f01\",\"2f02\",\"2f03\",\"2f04\",\"2f05\",\"2f06\",\"2f07\",\"2f08\",\"2f09\",\"2f10\",\"2f11\",\"2f12\",\"2f13\",\"2f14\",\"2f15\",\"2f16\",\"2f17\",\"2f18\",\"2f19\",\"2f20\",\"2f21\",\"2f22\",\"2f23\",\"2f24\"]\n",
        "        self.name_list_voice_3 =  ['1f01', '1f05', '1f12', '1f14', '1f16', '1f17', '1f18', '1f23', '1f24', '2f02', '2f05', '2f07', '2f08', '2f09', '2f16', '2f17',  '2f22', '2f23']\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "\n",
        "                        file_names_list.append(name[-8:-4])\n",
        "\n",
        "                    if \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[49:53] + \".pkl\" in sorted(glob.glob(os.path.join(PATH_TO_DATA, \"voice_3\", \"*.pkl\"))):\n",
        "                        nbr_voices_list.append(4)\n",
        "                    else:\n",
        "                        nbr_voices_list.append(3)\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "\n",
        "\n",
        "            if iLabel == 3:  \n",
        "                voice_files = []\n",
        "                file_names_3 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[2], \"*.pkl\")))   \n",
        "                \n",
        "                ###### loop over all filnames in voices_2 and if an element there is not present in voices_3: append \"missing\" to the voice_files of label=3 => important bc. self.pr_dict[voice_3] has then len 42 and otherwise it would only have len 18  .. these \"missing\" el are not considered later in the dataloader (if len=3 is a diff case of get_idx)\n",
        "                for name in file_names_2:\n",
        "                    if name[45:49] in self.name_list_voice_3:\n",
        "                      correct_name_3 = \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[45:49] + \".pkl\"\n",
        "                      with open(correct_name_3 ,'rb') as f:  \n",
        "                            loaded_obj = pickle.load(f)  \n",
        "                            voice_files.append(loaded_obj)\n",
        "                    else:\n",
        "                      voice_files.append(\"missing\")\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                \n",
        "\n",
        "                \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                          loaded_obj = pickle.load(f)     \n",
        "                          voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 4:\n",
        "            for key, value in self.pr_dict.items():\n",
        "              out_list.append(self.pr_dict[key][idx])\n",
        "                              \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.tensor(out_list[3].T)\n",
        "            v_all = torch.tensor(out_list[4].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 3:\n",
        "\n",
        "            for key, value in self.pr_dict.items():\n",
        "                if key != \"voice_3\":\n",
        "                  out_list.append(self.pr_dict[key][idx]) \n",
        "            \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.zeros(v2.shape)\n",
        "            v_all = torch.tensor(out_list[3].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "            \n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        "
      ],
      "metadata": {
        "id": "3FxK6qr1FqIl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "#dataset = MusicDataset(PATH_TO_DATA)\n",
        "dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices, file_name = sample_batched\n",
        "    print(file_name[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXsRYzQSUuQU",
        "outputId": "9d5a2adf-b058-4c97-bbf5-4feda4d55c4d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1f01\n",
            "1f03\n",
            "1f05\n",
            "1f06\n",
            "1f07\n",
            "1f08\n",
            "1f09\n",
            "1f11\n",
            "1f12\n",
            "1f13\n",
            "1f14\n",
            "1f16\n",
            "1f17\n",
            "1f18\n",
            "1f19\n",
            "1f21\n",
            "1f23\n",
            "1f24\n",
            "2f01\n",
            "2f02\n",
            "2f03\n",
            "2f04\n",
            "2f05\n",
            "2f06\n",
            "2f07\n",
            "2f08\n",
            "2f09\n",
            "2f11\n",
            "2f12\n",
            "2f13\n",
            "2f14\n",
            "2f15\n",
            "2f16\n",
            "2f17\n",
            "2f18\n",
            "2f19\n",
            "2f20\n",
            "2f21\n",
            "2f22\n",
            "2f23\n",
            "2f24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "  if i ==10:\n",
        "    all_voices, length, nbr_voices, _ = sample_batched\n",
        "    all_voices_pr = all_voices[0,:,:,-1].numpy()\n",
        "    \n",
        "    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\n",
        "    print(note_array.shape)\n",
        "    print(note_array[:10])\n",
        "    print(note_array.dtype.names)\n",
        "\n",
        "    #print(i,nbr_voices,all_voices.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4QCaMEi3nw7",
        "outputId": "0ae9c5f0-9428-4206-9633-79190d65c082"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2868,)\n",
            "[(0, 2.75, 0.08333334, 1) (1, 2.75, 0.08333334, 1)\n",
            " (2, 2.75, 0.08333334, 1) (3, 2.75, 0.08333334, 1)\n",
            " (4, 2.75, 0.08333334, 1) (5, 2.75, 0.08333334, 1)\n",
            " (6, 2.75, 0.08333334, 1) (7, 2.75, 0.08333334, 1)\n",
            " (8, 2.75, 0.08333334, 1) (9, 2.75, 0.08333334, 1)]\n",
            "('pitch', 'onset_beat', 'duration_beat', 'velocity')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Music - Model\n"
      ],
      "metadata": {
        "id": "JNqxeacDwxNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UNET "
      ],
      "metadata": {
        "id": "QAIfIM69VHI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "XMdlm0_Vyyhc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "          \n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                                                            \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()\n",
        "\n",
        "        if nbr_voices==4:\n",
        "            loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) + self.loss(score_3, v3)           \n",
        "        else:\n",
        "            loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) \n",
        "        \n",
        "        return loss\n",
        "        \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "            \n",
        "\n",
        "        else:\n",
        "            # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
        "            #print(\"predictmethod\",scores_comb.shape)\n",
        "            predicted = scores_comb.argmax(dim=3)\n",
        "            return np.squeeze(predicted.cpu().numpy())"
      ],
      "metadata": {
        "id": "CviiPTPOPW04"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"RNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(2, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "79cPe11WL6J0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1b3de5b6-af24-4821-ada1-2598a77cf40e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"RNN\"\\nmonophonic = True\\nhis = start_experiment(2, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Define Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                                   ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                    acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                    for i in range(len(prediction[0,:])):\n",
        "                      acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                      accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    \"\"\"\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                    comp_list = [0,0,0,0]\n",
        "                    idx_list = [-1,-1,-1,-1]\n",
        "                    voices_v = [0,0,0,0]\n",
        "\n",
        "                    for i in range(4):\n",
        "                      for j in range(4):\n",
        "                        intermed = sum(prediction[:,i] == truth[:,j])\n",
        "                        if intermed > comp_list[i] :\n",
        "                          comp_list[i] = intermed\n",
        "                          idx_list[i] = j\n",
        "                          voices_v[i] = intermed / len(prediction[:,i])\n",
        "                    avc = 100 * sum(voices_v) / 4\n",
        "                    \"\"\"\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                        ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                        truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                        acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                        for i in range(len(prediction[0,:])):\n",
        "                          acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                          val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    val_acc_list[3] = val_accuracy_sum_list[3] / 18                         ## bc only 18 pieces with len 3\n",
        "                    val_acc_list[4] = val_accuracy_sum_list[4] / 2                          ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "                history[\"val_acc\"].append(val_acc_list)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "    # save the model\n",
        "    #torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#network_type= \"RNN\"\n",
        "#monophonic = True\n",
        "#his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "metadata": {
        "id": "ge8pY70uHxF9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= [\"CNN\",\"RNN\"]\n",
        "monophonic_list = [True,False]\n",
        "\n",
        "for net in network_type:\n",
        "    for monophonic in monophonic_list: \n",
        "        print(\"network set to:\",net,\"monophnic:\",monophonic)\n",
        "        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Bs6-iNEBu8o",
        "outputId": "6a48d79c-bfda-4abe-8db6-311de9dad6be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= [\"CNN\",\"RNN\"]\\nmonophonic_list = [True,False]\\n\\nfor net in network_type:\\n    for monophonic in monophonic_list: \\n        print(\"network set to:\",net,\"monophnic:\",monophonic)\\n        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        path_train, path_validation = sklearn.model_selection.train_test_split(PATH_TO_DATA, test_size=0.15, random_state=10,)\n",
        "\n",
        "        print(\"Train and validation lenghts: \", len(path_train), len(path_validation))\n",
        "        train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "        validation_dataset = MusicDataset_new(path_validation) #MusicDataset(path_validation)\n",
        "        \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicNetwork\n",
        "epochs = 1\n",
        "lr = 0.001  \n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train + valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Experiment"
      ],
      "metadata": {
        "id": "bdetlQP-LoRX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm",
        "outputId": "8915af4e-f72e-42fa-f73a-6bda9617a8e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and validation lenghts:  23 5\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Training on device: cuda\n",
            "monophonic set to: True\n",
            "Train Loss: 7.4724444994112345, Train Accuracy_0 : 0.6896227107607982, Train Accuracy_1 : 0.4928260836621782,Train Accuracy_2 : 0.516853200943281, Train Accuracy_3 : 0.3135515500503894, Train Accuracy_4 : 0.0\n",
            " Validation Accuracy_0 : 0.8145968864423251, Validation Accuracy_1 : 0.6551530754644497, Validation Accuracy_2 : 0.5708389227363013, Validation Accuracy_3 : 0.5414883941237125, Validation Accuracy_4 : 0.0\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy evalutaion F-scores"
      ],
      "metadata": {
        "id": "sJbWsH72N2Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. create folder with part object of all pieces \n",
        "2. load a piece from dataloader with true labels, the mixed piece and the part object \n",
        "3. create notearray from part object\n",
        "4. take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "\n",
        "Output: pianoroll\n",
        "\n",
        "1 note in notearray could be mulitple bins\n",
        "\n",
        "take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "note start at same time with different pitch -> different notes\n",
        "\n",
        "for each note array find corresponding matrix -> \n",
        "\n",
        "\n",
        "if note is only composed by 1 bin: save indx of vocie -> save it to note array\n",
        "\n",
        "if more than 1: look what are idx that compose this note -> majority note -> save it for the note array (if its 50/50 take it random -> count how often this happens) \n",
        "\n",
        "\n",
        "with idx : in note_array find which note corresponds to what voice"
      ],
      "metadata": {
        "id": "CFClch37N6nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "metadata": {
        "id": "afYHFVNMlMnJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch1.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4TAhTQcpmx8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd26acf3-aafb-4d6c-ed2c-5a1da121e1d3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MusicNetwork(\n",
              "  (rnn): GRU(88, 300, num_layers=2, batch_first=True)\n",
              "  (cnn): UNET(\n",
              "    (double_conv_downs): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up_trans): ModuleList(\n",
              "      (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (double_conv_ups): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (max_pool_2x2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (final_conv): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (top_layer_voice_0): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_1): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_2): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_3): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (loss): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dic with key:filename, val: part_obj"
      ],
      "metadata": {
        "id": "5RVmMv6Q9CJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_parts = \"AI-MA_project/bach_fugues\"\n",
        "part_dic = {}\n",
        "\n",
        "#### create a list with all filenames in the right order ####\n",
        "file_names_part = []\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    file_names_part.append(filename[3:7])\n",
        "#print(file_names_part)\n",
        "\n",
        "#### create a list with all part objects in the right order ####\n",
        "part_list = []\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    fullname = os.path.join(path_parts, filename)\n",
        "    part = partitura.load_score_midi(fullname)\n",
        "    part_list.append(part)\n",
        "#print(part_list)\n",
        "\n",
        "#### create a dict with keys:filenames , values: part object ####\n",
        "for i in range(len(file_names_part)):\n",
        "      part_dic[file_names_part[i]] = part_list[i]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XYM_KWu2qkX",
        "outputId": "4d2ae9d5-3a83-46bc-c0c4-ef2379760eab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=76 velocity=64 time=30\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=78 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=119\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=419\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=72 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=43 velocity=64 time=360\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=299\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=60 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=77 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=75 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=80 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=73 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=68 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=71 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=59 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=56 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=58 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=61 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=60\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=55 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=389\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=2 note=49 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=67 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=66 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=359\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "part_dic.keys(),part_dic.values()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt3uHTJY9Ojj",
        "outputId": "517b665b-31a3-4304-b99a-746010f4529d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dict_keys(['1f01', '1f02', '1f03', '1f04', '1f05', '1f06', '1f07', '1f08', '1f09', '1f10', '1f11', '1f12', '1f13', '1f14', '1f15', '1f16', '1f17', '1f18', '1f19', '1f20', '1f21', '1f22', '1f23', '1f24', '2f01', '2f02', '2f03', '2f04', '2f05', '2f06', '2f07', '2f08', '2f09', '2f10', '2f11', '2f12', '2f13', '2f14', '2f15', '2f16', '2f17', '2f18', '2f19', '2f20', '2f21', '2f22', '2f23', '2f24']),\n",
              " dict_values([[<partitura.score.Part object at 0x7fa0b1c27110>, <partitura.score.Part object at 0x7fa0b1c3e210>, <partitura.score.Part object at 0x7fa0b1ab5250>, <partitura.score.Part object at 0x7fa0b19ebb50>], [<partitura.score.Part object at 0x7fa0b18cea50>, <partitura.score.Part object at 0x7fa0b1844510>, <partitura.score.Part object at 0x7fa0b1783c50>], [<partitura.score.Part object at 0x7fa0b1b5cd10>, <partitura.score.Part object at 0x7fa0b15680d0>, <partitura.score.Part object at 0x7fa0b127d4d0>], [<partitura.score.Part object at 0x7fa0b0d9e150>, <partitura.score.Part object at 0x7fa0b0dd0b10>, <partitura.score.Part object at 0x7fa0b0cbba10>, <partitura.score.Part object at 0x7fa0b0b822d0>, <partitura.score.Part object at 0x7fa0b0a97e90>], [<partitura.score.Part object at 0x7fa0b8fdf750>, <partitura.score.Part object at 0x7fa0b08cc2d0>, <partitura.score.Part object at 0x7fa0b1621290>, <partitura.score.Part object at 0x7fa0b080bc50>], [<partitura.score.Part object at 0x7fa0b05e4910>, <partitura.score.Part object at 0x7fa0b0560a10>, <partitura.score.Part object at 0x7fa0b0476910>], [<partitura.score.Part object at 0x7fa0b01d7150>, <partitura.score.Part object at 0x7fa0b0269390>, <partitura.score.Part object at 0x7fa0b00da090>], [<partitura.score.Part object at 0x7fa099e4ec90>, <partitura.score.Part object at 0x7fa099e15890>, <partitura.score.Part object at 0x7fa099ba2e50>], [<partitura.score.Part object at 0x7fa0b0efc090>, <partitura.score.Part object at 0x7fa0998b56d0>, <partitura.score.Part object at 0x7fa09978bcd0>], [<partitura.score.Part object at 0x7fa0995fcb90>, <partitura.score.Part object at 0x7fa09963bad0>], [<partitura.score.Part object at 0x7fa099247d10>, <partitura.score.Part object at 0x7fa099195810>, <partitura.score.Part object at 0x7fa099093c50>], [<partitura.score.Part object at 0x7fa099e42310>, <partitura.score.Part object at 0x7fa098df7d50>, <partitura.score.Part object at 0x7fa098bd0050>, <partitura.score.Part object at 0x7fa098a53610>], [<partitura.score.Part object at 0x7fa0992414d0>, <partitura.score.Part object at 0x7fa09877e090>, <partitura.score.Part object at 0x7fa09865a1d0>], [<partitura.score.Part object at 0x7fa0b8f679d0>, <partitura.score.Part object at 0x7fa09831e050>, <partitura.score.Part object at 0x7fa0b0efcfd0>, <partitura.score.Part object at 0x7fa0982754d0>], [<partitura.score.Part object at 0x7f9df7f43b50>, <partitura.score.Part object at 0x7fa0992f8090>, <partitura.score.Part object at 0x7fa098069850>], [<partitura.score.Part object at 0x7f9df7b63050>, <partitura.score.Part object at 0x7f9df7a12090>, <partitura.score.Part object at 0x7f9df79ef250>, <partitura.score.Part object at 0x7f9df79076d0>], [<partitura.score.Part object at 0x7f9df774e250>, <partitura.score.Part object at 0x7f9df76c3b90>, <partitura.score.Part object at 0x7f9df76420d0>, <partitura.score.Part object at 0x7f9df75a1910>], [<partitura.score.Part object at 0x7fa098130310>, <partitura.score.Part object at 0x7f9df742ec90>, <partitura.score.Part object at 0x7f9df71c84d0>, <partitura.score.Part object at 0x7f9df70d3050>], [<partitura.score.Part object at 0x7f9df6ff6b10>, <partitura.score.Part object at 0x7f9df6f0d090>, <partitura.score.Part object at 0x7f9df6cc7490>], [<partitura.score.Part object at 0x7f9df6fe6e10>, <partitura.score.Part object at 0x7f9df668a090>, <partitura.score.Part object at 0x7f9df6407110>, <partitura.score.Part object at 0x7f9df62295d0>], [<partitura.score.Part object at 0x7fa0980cd810>, <partitura.score.Part object at 0x7f9df5f1f510>, <partitura.score.Part object at 0x7f9df5da4210>], [<partitura.score.Part object at 0x7fa099244350>, <partitura.score.Part object at 0x7f9df5adb050>, <partitura.score.Part object at 0x7f9df7f94910>, <partitura.score.Part object at 0x7f9df5a09390>, <partitura.score.Part object at 0x7f9df59bd5d0>], [<partitura.score.Part object at 0x7fa09845cdd0>, <partitura.score.Part object at 0x7f9df56e7050>, <partitura.score.Part object at 0x7f9df563c050>, <partitura.score.Part object at 0x7f9df5523250>], [<partitura.score.Part object at 0x7f9df68dab50>, <partitura.score.Part object at 0x7f9df6984090>, <partitura.score.Part object at 0x7f9df51fe490>, <partitura.score.Part object at 0x7f9df5029050>], [<partitura.score.Part object at 0x7f9df4d4bc50>, <partitura.score.Part object at 0x7f9df53fc1d0>, <partitura.score.Part object at 0x7f9df4bff5d0>], [<partitura.score.Part object at 0x7f9df5809e10>, <partitura.score.Part object at 0x7fa0980cdd10>, <partitura.score.Part object at 0x7f9df484a910>, <partitura.score.Part object at 0x7f9df4780e10>], [<partitura.score.Part object at 0x7f9df45bc850>, <partitura.score.Part object at 0x7f9df4618050>, <partitura.score.Part object at 0x7f9df44d7050>], [<partitura.score.Part object at 0x7f9df42787d0>, <partitura.score.Part object at 0x7f9df42a8210>, <partitura.score.Part object at 0x7f9df3ff3bd0>], [<partitura.score.Part object at 0x7f9df43b7990>, <partitura.score.Part object at 0x7f9df3c45cd0>, <partitura.score.Part object at 0x7f9df3bdf850>, <partitura.score.Part object at 0x7f9df3ae14d0>], [<partitura.score.Part object at 0x7f9df499bd90>, <partitura.score.Part object at 0x7f9df37cf790>, <partitura.score.Part object at 0x7f9df5273690>], [<partitura.score.Part object at 0x7f9df4722bd0>, <partitura.score.Part object at 0x7f9df3548490>, <partitura.score.Part object at 0x7f9df7832550>, <partitura.score.Part object at 0x7f9df34b0890>], [<partitura.score.Part object at 0x7f9df324fd10>, <partitura.score.Part object at 0x7f9df32aae50>, <partitura.score.Part object at 0x7f9df32ce790>, <partitura.score.Part object at 0x7f9df2fc4050>], [<partitura.score.Part object at 0x7f9df368e350>, <partitura.score.Part object at 0x7f9df2db0fd0>, <partitura.score.Part object at 0x7f9df2d7ead0>, <partitura.score.Part object at 0x7f9df2c4a090>], [<partitura.score.Part object at 0x7f9df277e1d0>, <partitura.score.Part object at 0x7f9df29deb90>, <partitura.score.Part object at 0x7f9df256a510>], [<partitura.score.Part object at 0x7f9df2bbe810>, <partitura.score.Part object at 0x7f9df2797950>, <partitura.score.Part object at 0x7f9df1f30050>], [<partitura.score.Part object at 0x7f9df1cb9910>, <partitura.score.Part object at 0x7f9df1c98d50>, <partitura.score.Part object at 0x7f9df1a7c050>], [<partitura.score.Part object at 0x7f9df1cb1f10>, <partitura.score.Part object at 0x7f9df1871790>, <partitura.score.Part object at 0x7f9df215a2d0>], [<partitura.score.Part object at 0x7f9df15f9390>, <partitura.score.Part object at 0x7f9df1314090>, <partitura.score.Part object at 0x7f9df11766d0>], [<partitura.score.Part object at 0x7f9df7b73b10>, <partitura.score.Part object at 0x7f9df22b42d0>, <partitura.score.Part object at 0x7f9df0ccdd50>], [<partitura.score.Part object at 0x7f9df0a770d0>, <partitura.score.Part object at 0x7f9df0a70cd0>, <partitura.score.Part object at 0x7f9df06e5610>, <partitura.score.Part object at 0x7f9df053d9d0>], [<partitura.score.Part object at 0x7f9df02345d0>, <partitura.score.Part object at 0x7f9df013c910>, <partitura.score.Part object at 0x7f9df00530d0>, <partitura.score.Part object at 0x7f9deff14910>], [<partitura.score.Part object at 0x7f9defbc4c10>, <partitura.score.Part object at 0x7f9def9f00d0>, <partitura.score.Part object at 0x7f9def874790>], [<partitura.score.Part object at 0x7f9defc3a850>, <partitura.score.Part object at 0x7f9def506d10>, <partitura.score.Part object at 0x7f9def40d610>], [<partitura.score.Part object at 0x7f9defc2db50>, <partitura.score.Part object at 0x7f9def1fb550>, <partitura.score.Part object at 0x7f9def121750>], [<partitura.score.Part object at 0x7f9df159d7d0>, <partitura.score.Part object at 0x7f9def06d0d0>, <partitura.score.Part object at 0x7f9deeece050>], [<partitura.score.Part object at 0x7f9deec29050>, <partitura.score.Part object at 0x7f9df0f15fd0>, <partitura.score.Part object at 0x7f9dee8ae550>, <partitura.score.Part object at 0x7f9dee688350>], [<partitura.score.Part object at 0x7f9dee38d810>, <partitura.score.Part object at 0x7f9dee35ee50>, <partitura.score.Part object at 0x7f9deecdf050>, <partitura.score.Part object at 0x7f9dee051150>], [<partitura.score.Part object at 0x7f9dede370d0>, <partitura.score.Part object at 0x7f9dede1c8d0>, <partitura.score.Part object at 0x7f9dedc732d0>]]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "geht gerade nur für monophonic True"
      ],
      "metadata": {
        "id": "v4TJGKiUs086"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "asGAiQzdHCeV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_sum_list = [0 for i in range(5)]\n",
        "\n",
        "def evaluate_accuracy(model, train_dataloader, part_dic):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "            if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "              print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "            \n",
        "            # load correct part object\n",
        "            file_name = file_name[0]\n",
        "\n",
        "            part = part_dic[file_name]\n",
        "\n",
        "\n",
        "            part_zero = part[0]\n",
        "            part_one = part[1]\n",
        "            part_two = part[2]\n",
        "\n",
        "            if len(part) ==4:\n",
        "              part_three = part[3]\n",
        "              note_array_3 = part_three.note_array\n",
        "\n",
        "            note_array_0 = part_zero.note_array\n",
        "            note_array_1 = part_one.note_array\n",
        "            note_array_2 = part_two.note_array\n",
        "            \n",
        "            \n",
        "            print(\"note_array shapes for filename:\",file_name)\n",
        "            print(note_array_0.shape)\n",
        "            print(note_array_1.shape)\n",
        "            print(note_array_2.shape)\n",
        "            print(note_array_3.shape)\n",
        "\n",
        "            print(note_array_0[:5])\n",
        "            print(note_array_0.dtype.names)\n",
        "            \n",
        "            print(note_array_1[:5])\n",
        "            print(note_array_1.dtype.names)\n",
        "\n",
        "            print(note_array_2[:5])\n",
        "            print(note_array_2.dtype.names)\n",
        "\n",
        "            print(note_array_3[:5])\n",
        "            print(note_array_3.dtype.names)\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "            # do model prediction\n",
        "            model.eval()\n",
        "            voices = voices.to(device).float()\n",
        "            monophonic=True\n",
        "            with torch.no_grad():\n",
        "\n",
        "                prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "\n",
        "                \n",
        "                acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                \n",
        "                \n",
        "                for i in range(len(prediction[0,:])):\n",
        "                  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                  accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "                \n",
        "                #print(accuracy_sum_list)\n",
        "            #print(file_name)\n",
        "\n",
        "    \n",
        "    train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "    train_acc_list[3] = accuracy_sum_list[3] / 18                               ## bc only 18 pieces with len 3\n",
        "    train_acc_list[4] = accuracy_sum_list[4] / 2                                ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "    return train_acc_list"
      ],
      "metadata": {
        "id": "EH3o-VHpN4op"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate_accuracy(model,train_dataloader,part_dic)"
      ],
      "metadata": {
        "id": "oG0MpEImjGzV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_sum_list = [0 for i in range(5)]\n",
        "\n",
        "def evaluate_accuracy_for_one(model, train_dataloader, part_dic):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "            if idx == 0:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "\n",
        "                part = part_dic[file_name]\n",
        "\n",
        "                part_zero = part[0]\n",
        "                note_array_0 = part_zero.note_array\n",
        "                \n",
        "                #print(note_array_0[145:147])\n",
        "                #print(note_array_0.dtype.names)\n",
        "\n",
        "                #print(\"position \", np.where(note_array_0[\"onset_beat\"] == 70))\n",
        "\n",
        "                onset_beat = note_array_0[\"onset_beat\"]\n",
        "                duration_beat = note_array_0[\"duration_beat\"]\n",
        "\n",
        "                note_idx_start = []\n",
        "                note_idx_end = []\n",
        "\n",
        "                for i in range(len(onset_beat)):\n",
        "                    onset = onset_beat[i]\n",
        "                    duration = duration_beat[i]\n",
        "\n",
        "                    note_range = 12 * (onset+duration)\n",
        "                    onset_pr = 12 * onset\n",
        "                    note_idx_start.append(onset_pr)\n",
        "                    note_idx_end.append(note_range)\n",
        "                \n",
        "\n",
        "                #print(\"note idx start:\",note_idx_start)\n",
        "                #modulo_list = [num % 1 == 0 for num in note_idx_start]\n",
        "                #print(\"modulo\", modulo_list)\n",
        "                #print(\"where\",np.where( modulo_list==False  ) )\n",
        "\n",
        "\n",
        "                ### round every entry up to next integer for the starting idx ###\n",
        "                note_idx_start = [math.ceil(num) for num in note_idx_start]\n",
        "                ### round every entry down to next integer for the ending idx###\n",
        "                note_idx_end = [math.floor(num) for num in note_idx_end]\n",
        "\n",
        "\n",
        "                print(\"note idx start:\",note_idx_start)\n",
        "                print(\"note idx end:\",note_idx_end)\n",
        "\n",
        "                               \n",
        "                # do model prediction\n",
        "                model.eval()\n",
        "                voices = voices.to(device).float()\n",
        "                monophonic=True\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "\n",
        "                #print(\"prediction 0:\", prediction[0:72,0])\n",
        "                #print(\"truth:\", truth[0:72,0])\n",
        "\n",
        "                \n",
        "\n",
        "                for i, j in zip(note_idx_start, note_idx_end):\n",
        "                    pred_list = prediction[i:j,0]\n",
        "                    \n",
        "                    result = all(elem == pred_list[0] for elem in pred_list)\n",
        "                    #print(\"truth:\",truth[i:j,0])\n",
        "                    \n",
        "                    result_test = all(elem == truth[i:j,0][0] for elem in truth[i:j,0])\n",
        "                    if result_test == False:\n",
        "                        print(\"truth result\", truth[i:j,0], i,j)\n",
        "                        #print(\"previous result\", truth[855:856,0])\n",
        "                        #print(\"test result\", truth[857:858,0])\n",
        "\n",
        "                    \n",
        "                    if result == False:\n",
        "                        print(\"truth:\",truth[i:j,0])\n",
        "                        #print(\"pred_false:\",pred_list)\n",
        "                        major, major_idx = torch.mode(pred_list,0)\n",
        "                        major = major.numpy().tolist()\n",
        "                        pred_list = [major for i in pred_list]\n",
        "                        print(\"vote pred list:\", pred_list )"
      ],
      "metadata": {
        "id": "KG7ZAlNZGUw7"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_accuracy_for_one(model,train_dataloader,part_dic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc1KjJs3G4Wi",
        "outputId": "fced622e-cd77-4877-b917-4d162a28657a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "note idx start: [72, 78, 84, 90, 99, 101, 102, 108, 114, 120, 129, 132, 135, 138, 141, 144, 147, 150, 153, 156, 159, 162, 165, 168, 171, 174, 180, 186, 192, 198, 201, 204, 207, 210, 219, 222, 225, 228, 231, 234, 237, 240, 243, 246, 270, 288, 294, 300, 306, 315, 317, 318, 324, 330, 336, 345, 348, 351, 354, 360, 369, 372, 375, 378, 402, 411, 414, 438, 450, 453, 456, 459, 462, 465, 468, 471, 474, 477, 480, 483, 486, 501, 504, 507, 510, 516, 522, 528, 531, 534, 537, 540, 543, 546, 549, 552, 552, 555, 555, 558, 564, 567, 570, 576, 582, 588, 600, 603, 606, 615, 618, 696, 702, 708, 714, 723, 725, 726, 732, 738, 744, 750, 759, 761, 762, 768, 774, 780, 789, 792, 795, 798, 804, 810, 816, 822, 828, 831, 834, 837, 840, 843, 846, 849, 852, 855, 857, 858, 861, 864, 867, 870, 879, 882, 948, 954, 960, 966, 975, 977, 978, 984, 990, 996, 1005, 1008, 1011, 1014, 1017, 1020, 1023, 1026, 1029, 1032, 1035, 1038, 1041, 1044, 1047, 1050, 1062, 1068, 1074, 1080, 1092, 1098, 1110, 1116, 1122, 1128, 1140, 1146, 1152, 1158, 1173, 1176, 1179, 1182, 1185, 1188, 1191, 1194, 1221, 1223, 1224, 1227, 1230, 1233, 1236, 1244, 1245, 1247, 1248, 1251, 1254, 1263, 1266]\n",
            "note idx end: [78, 84, 90, 99, 100, 102, 108, 114, 120, 129, 132, 135, 138, 141, 144, 147, 150, 153, 156, 159, 162, 165, 168, 171, 174, 180, 186, 192, 198, 201, 204, 207, 210, 219, 222, 225, 228, 231, 234, 237, 240, 243, 246, 270, 282, 294, 300, 306, 315, 316, 318, 324, 330, 336, 345, 348, 351, 354, 360, 369, 372, 375, 378, 402, 408, 414, 438, 450, 453, 456, 459, 462, 465, 468, 471, 474, 477, 480, 483, 486, 492, 504, 507, 510, 516, 522, 528, 531, 534, 537, 540, 543, 546, 549, 552, 555, 555, 558, 558, 564, 567, 570, 576, 582, 588, 600, 603, 606, 615, 618, 630, 702, 708, 714, 723, 724, 726, 732, 738, 744, 750, 759, 760, 762, 768, 774, 780, 789, 792, 795, 798, 804, 810, 816, 822, 828, 831, 834, 837, 840, 843, 846, 849, 852, 855, 856, 858, 861, 864, 867, 870, 879, 882, 888, 954, 960, 966, 975, 976, 978, 984, 990, 996, 1005, 1008, 1011, 1014, 1017, 1020, 1023, 1026, 1029, 1032, 1035, 1038, 1041, 1044, 1047, 1050, 1062, 1068, 1074, 1080, 1092, 1098, 1110, 1116, 1122, 1128, 1140, 1146, 1152, 1158, 1173, 1176, 1179, 1182, 1185, 1188, 1191, 1194, 1218, 1222, 1224, 1227, 1230, 1233, 1236, 1243, 1245, 1246, 1248, 1251, 1254, 1263, 1266, 1290]\n",
            "truth: tensor([48, 48, 48, 48, 48, 48])\n",
            "vote pred list: [48, 48, 48, 48, 48, 48]\n",
            "truth: tensor([46, 46, 46])\n",
            "vote pred list: [46, 46, 46]\n",
            "truth: tensor([45, 45, 45])\n",
            "vote pred list: [45, 45, 45]\n",
            "truth: tensor([53, 53, 53, 53, 53, 53])\n",
            "vote pred list: [53, 53, 53, 53, 53, 53]\n",
            "truth: tensor([53, 53, 53, 53, 53, 53])\n",
            "vote pred list: [53, 53, 53, 53, 53, 53]\n",
            "truth: tensor([56, 56, 56])\n",
            "vote pred list: [56, 56, 56]\n",
            "truth: tensor([48, 48, 48, 48, 48, 48])\n",
            "vote pred list: [48, 48, 48, 48, 48, 48]\n",
            "truth: tensor([55, 55, 55])\n",
            "vote pred list: [55, 55, 55]\n",
            "truth: tensor([46, 46, 46, 46, 46, 46])\n",
            "vote pred list: [46, 46, 46, 46, 46, 46]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "71.25*12, 12*(71.25+0.125)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZC1DIEqsAtR",
        "outputId": "03e311f9-dd6f-4338-e810-5c3da9c36d0f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(855.0, 856.5)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "71.375 *12 , 12*(71.375 +0.125 )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKAqP1X3tnUG",
        "outputId": "81e79a8e-6c90-484d-ab3e-1b1d55974f30"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(856.5, 858.0)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "856/12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMEZFeluhxDc",
        "outputId": "c25b22d1-5922-4e03-b80d-73316400dda6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71.33333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier"
      ],
      "metadata": {
        "id": "XVob3X4-TU2K"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "start time, duration , pitch to separate \n",
        "\n",
        "use the onset_beat and duration_beat\n",
        "\n",
        "multiply them according to the values set when producing the pianorolls \n",
        "\n",
        "-> get the position in the pianoroll\n",
        "\n",
        "time_div = 12\n",
        "\n"
      ],
      "metadata": {
        "id": "EmvxtyaVKG27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "A94mchm4LV6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy_v0\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v1\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v2\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v3\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Accuracy0','Accuracy1','Accuracy2','Accuracy3'])\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TgJDHaxAgYN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a3efe3db-fa30-486b-ddcc-b7e50bee4208"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend('Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxMs8GEfMvPE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7b7ba3c6-3aaf-46b6-8cad-0c6bb6a79add"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"train_loss\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqMcJT5aFL01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "150b0fa4-58f7-48be-c5d4-5b235e3d9b93"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZwElEQVR4nO3de5RV9X338fcH5CaggzCaAFE0NqhUhKejxpLnQUMJEgNRYyNEjUmapfRpayiJS3y8NJdmRUIvVtOU0nRFqxETjZgabfFSb2mamBGJaHSiSFAGxREdkJtB+D5/nI3ZHM6Mw8zsc5j5fV5r7cU5v/07+3x/sDifs397n70VEZiZWbr61LoAMzOrLQeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmvYSkz0j6Sa3rsJ7HQWD7LUm/kfRHta6jMySdKmmXpM1lyym1rs2s3AG1LsCsF1sXEaNrXYTZu/EegfU4kgZIulbSumy5VtKAbN0IST+W1CrpdUmPSuqTrbtMUrOkNyU1SZpSYdsnS3pFUt9c21mSnswenySpUdImSesl/V0nx/CQpG9Ieizb1o8kHZJbP1PS09k4HpJ0bG7d+yTdIalF0gZJ3yrb9t9IekPSaknTc+2fkfRCNv7Vks7rTO3W+zgIrCe6AvggMAE4ATgJuDJb90VgLVAPHAb8PyAkjQX+HDgxIoYC04DflG84In4ObAE+nGv+FHBL9vgfgH+IiIOA9wM/6MI4Pg18Dngv8DZwHYCkDwBLgLnZOO4B7pLUPwuoHwNrgDHAKODW3DZPBpqAEcA3gX9VyeBs+9Oz8f8hsKILtVsv4iCwnug84KsR8WpEtABfAS7I1u2g9MF6RETsiIhHo3RBrZ3AAOA4Sf0i4jcRsaqN7S8BZgNIGgp8NGvbvf2jJY2IiM0R8bN26hyZfaPPL4Nz62+KiKciYgtwFfDJ7IP+XODuiLgvInYAfwMMovThfRIwErg0IrZExPaIyB8gXhMR/xIRO4Ebs7+Lw7J1u4DflzQoIl6OiKfbqd0S4iCwnmgkpW/Eu63J2gAWAs8D92bTIPMBIuJ5St+wvwy8KulWSSOp7Bbg7Gy66WxgeUTsfr8/AT4APCvpF5I+1k6d6yKirmzZklv/UtkY+lH6Jr/H+CJiV9Z3FPA+Sh/2b7fxnq/kXrc1ezgke99zgTnAy5LulnRMO7VbQhwE1hOtA47IPT88ayMi3oyIL0bEUcBMYN7uYwERcUtEfCh7bQALKm08In5F6YN4OntOCxERz0XEbODQ7PW3l33L3xfvKxvDDuC18vFJUta3mVIgHC5pn0/0iIhlETGV0l7Cs8C/dLJu62UcBLa/6ydpYG45gNI0zZWS6iWNAK4GbgaQ9DFJR2cfnhspTQntkjRW0oezb/nbgW2UpkracgvwBeD/ALftbpR0vqT67Ft6a9bc3nbac76k4yQdCHwVuD2b0vkBcIakKZL6UTru8RbwU+Ax4GXgGkmDs7+TSe/2RpIOk/TxLLTeAjZ3oW7rZRwEtr+7h9KH9u7ly8BfA43Ak8BKYHnWBvB7wP2UPuj+B/h2RDxI6fjANZS+cb9C6Rv95e287xJgMvBfEfFarv104GlJmykdOJ4VEdva2MbICr8j+ERu/U3ADVk9A4FLACKiCTgfuD6rdwYwIyJ+mwXFDOBo4EVKB8bPbWccu/UB5lHa23g9G9ufduB1lgD5xjRm1SfpIeDmiPhOrWsx8x6BmVniHARmZonz1JCZWeK8R2Bmlrged9G5ESNGxJgxY2pdhplZj/L444+/FhH1ldb1uCAYM2YMjY2NtS7DzKxHkbSmrXWeGjIzS5yDwMwscQ4CM7PE9bhjBGZmnbVjxw7Wrl3L9u3ba11KYQYOHMjo0aPp169fh1/jIDCzZKxdu5ahQ4cyZswYStcl7F0igg0bNrB27VqOPPLIDr/OU0Nmlozt27czfPjwXhkCAJIYPnz4Pu/xOAjMLCm9NQR268z4HARmZolzEJiZVdGQIUNqXcJefLDYzKwNdz7RzMJlTaxr3cbIukFcOm0sZ04cVeuyup33CMzMKrjziWYuv2Mlza3bCKC5dRuX37GSO59o7vb3WrFiBR/84AcZP348Z511Fm+88QYA1113Hccddxzjx49n1qxZADz88MNMmDCBCRMmMHHiRN58880uv3+Puwx1Q0ND+FpDZtYZzzzzDMceeywAX7nraX61blObfZ94sZXf7tz7ts79+/Zh4uF1FV9z3MiD+KsZ49qtYciQIWzevHmPtvHjx3P99dczefJkrr76ajZt2sS1117LyJEjWb16NQMGDKC1tZW6ujpmzJjB/PnzmTRpEps3b2bgwIEccMCekzv5ce4m6fGIaKhUk/cIzMwqqBQC7bV31saNG2ltbWXy5MkAXHjhhTzyyCNAKSDOO+88br755nc+7CdNmsS8efO47rrraG1t3SsEOsPHCMwsSe/2zX3SNf9Fc+u2vdpH1Q3i+xefUlRZe7j77rt55JFHuOuuu/j617/OypUrmT9/PmeccQb33HMPkyZNYtmyZRxzzDFdeh/vEZiZVXDptLEM6td3j7ZB/fpy6bSx3fo+Bx98MMOGDePRRx8F4KabbmLy5Mns2rWLl156idNOO40FCxawceNGNm/ezKpVqzj++OO57LLLOPHEE3n22We7XIP3CMzMKth9dlB3nzW0detWRo8e/c7zefPmceONNzJnzhy2bt3KUUcdxXe/+1127tzJ+eefz8aNG4kILrnkEurq6rjqqqt48MEH6dOnD+PGjWP69Oldqgd8sNjMElLpIGpvtN8cLJY0VtKK3LJJ0tyyPgdLukvSLyU9LemzRdVjZmaVFTY1FBFNwAQASX2BZmBpWbc/A34VETMk1QNNkr4XEb8tqi4zM9tTtQ4WTwFWRUT5PTMDGKrSVZKGAK8Db1epJjNLUE+bDt9XnRlftYJgFrCkQvu3gGOBdcBK4AsRsddJupIuktQoqbGlpaXYSs2s1xo4cCAbNmzotWGw+34EAwcO3KfXFX6wWFJ/Sh/04yJifdm6c4BJwDzg/cB9wAkR0ebP/Xyw2Mw6K+U7lLV3sLgap49OB5aXh0Dms8A1UUqj5yWtBo4BHqtCXWaWmH79+u3TnbtSUY2podlUnhYCeJHS8QMkHQaMBV6oQk1mZpYpdI9A0mBgKnBxrm0OQEQsAr4G3CBpJSDgsoh4rciazMxsT4UGQURsAYaXtS3KPV4HfKTIGszMrH2+1pCZWeIcBGZmiXMQmJklzkFgZpY4B4GZWeIcBGZmiXMQmJklzkFgZpY4B4GZWeIcBGZmiXMQmJklzkFgZpY4B4GZWeIcBGZmiXMQmJklzkFgZpY4B4GZWeIcBGZmiSssCCSNlbQit2ySNLesz6W59U9J2inpkKJqMjOzvRV2z+KIaAImAEjqCzQDS8v6LAQWZn1mAH8ZEa8XVZOZme2tWlNDU4BVEbGmnT6zgSVVqsfMzDLVCoJZtPMhL+lA4HTgh22sv0hSo6TGlpaWgko0M0tT4UEgqT8wE7itnW4zgP9ua1ooIhZHRENENNTX1xdRpplZsqqxRzAdWB4R69vp0+4eg5mZFacaQdDu3L+kg4HJwI+qUIuZmZUpNAgkDQamAnfk2uZImpPrdhZwb0RsKbIWMzOrrLDTRwGyD/fhZW2Lyp7fANxQZB1mZtY2/7LYzCxxDgIzs8Q5CMzMEucgMDNLnIPAzCxxDgIzs8Q5CMzMEucgMDNLnIPAzCxxDgIzs8Q5CMzMEucgMDNLnIPAzCxxDgIzs8Q5CMzMEucgMDNLnIPAzCxxhQWBpLGSVuSWTZLmVuh3arb+aUkPF1WPmZlVVtitKiOiCZgAIKkv0AwszfeRVAd8Gzg9Il6UdGhR9ZiZWWXVmhqaAqyKiDVl7Z8C7oiIFwEi4tUq1WNmZplqBcEsYEmF9g8AwyQ9JOlxSZ+u9GJJF0lqlNTY0tJSaKFmZqkpPAgk9QdmArdVWH0A8AfAGcA04CpJHyjvFBGLI6IhIhrq6+sLrdfMLDWFHSPImQ4sj4j1FdatBTZExBZgi6RHgBOAX1ehLjMzozpTQ7OpPC0E8CPgQ5IOkHQgcDLwTBVqMjOzTKF7BJIGA1OBi3NtcwAiYlFEPCPpP4EngV3AdyLiqSJrMjOzPRUaBNmUz/CytkVlzxcCC4usw8zM2uZfFpuZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWuMKCQNJYSStyyyZJc8v6nCppY67P1UXVY2ZmlRV2z+KIaAImAEjqCzQDSyt0fTQiPlZUHWZm1r5qTQ1NAVZFxJoqvZ+ZmXVQtYJgFrCkjXWnSPqlpP+QNK5SB0kXSWqU1NjS0lJclWZmCepQEEj6gqSDVPKvkpZL+kgHX9sfmAncVmH1cuCIiDgBuB64s9I2ImJxRDREREN9fX1H3tbMzDqoo3sEn4uITcBHgGHABcA1HXztdGB5RKwvXxERmyJic/b4HqCfpBEd3K6ZmXWDjgaBsj8/CtwUEU/n2t7NbNqYFpL0HknKHp+U1bOhg9s1M7Nu0NGzhh6XdC9wJHC5pKHArnd7kaTBwFTg4lzbHICIWAScA/yppLeBbcCsiIh9G4JZ7d35RDMLlzWxrnUbI+sGcem0sZw5cVStyzLrEHXkc1dSH0qngr4QEa2SDgFGR8STRRdYrqGhIRobG6v9tmZtuvOJZi6/YyXbdux8p21Qv7584+zjHQa235D0eEQ0VFrX0amhU4CmLATOB64ENnZXgWY92cJlTXuEAMC2HTtZuKypRhWZ7ZuOBsE/AVslnQB8EVgF/FthVZn1IOtat+1Tu9n+pqNB8HY2d/9x4FsR8Y/A0OLKMus5RtYN2qd2s/1NR4PgTUmXUzpt9O7smEG/4soy6zkunTaWQf367tE2qF9fLp02tkYVme2bjgbBucBblH5P8AowGlhYWFVmPciZE0fxjbOPZ1TdIASMqhvkA8XWo3TorCEASYcBJ2ZPH4uIVwurqh0+a8jMbN91+awhSZ8EHgP+GPgk8HNJ53RfiWZmVisd/UHZFcCJu/cCJNUD9wO3F1WYmZlVR0ePEfQpmwrasA+vNTOz/VhH9wj+U9IyfnfNoHOBe4opyczMqqlDQRARl0r6BDApa1ocEZXuNmZmZj1Mh29VGRE/BH5YYC1mZlYD7QaBpDeBSueXCoiIOKiQqszMrGraDYKI8GUkzMx6OZ/5Y2aWOAeBmVniHARmZokrLAgkjZW0IrdskjS3jb4nSnrbl60wM6u+Dp8+uq8ioonS7S2R1BdoBvb67UG2bgFwb1G1mJlZ26o1NTQFWBURayqs+wtKv0+oydVMzcxSV60gmMXvLk/xDkmjgLMo3QqzTZIuktQoqbGlpaWgEs3M0lR4EEjqD8wEbquw+lrgsojY1d42ImJxRDREREN9fX0RZZqZJauwYwQ504HlEbG+wroG4FZJACOAj0p6OyLurEJdZmZGdYJgNhWmhQAi4sjdjyXdAPzYIWBmVl2FTg1JGgxMBe7Itc2RNKfI9zUzs44rdI8gIrYAw8vaFrXR9zNF1mJmZpX5l8VmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYGaWOAeBmVniHARmZolzEJiZJa6wIJA0VtKK3LJJ0tyyPh+X9GS2vlHSh4qqx8zMKivsnsUR0QRMAJDUF2gGlpZ1ewD494gISeOBHwDHFFWTmZntrdCb1+dMAVZFxJp8Y0Rszj0dDESV6jEzs0y1jhHMApZUWiHpLEnPAncDn2ujz0XZ1FFjS0tLgWWamaWn8CCQ1B+YCdxWaX1ELI2IY4Azga+10WdxRDREREN9fX1xxZqZJagaewTTgeURsb69ThHxCHCUpBFVqMnMzDLVCILZtD0tdLQkZY//FzAA2FCFmszMLFPowWJJg4GpwMW5tjkAEbEI+ATwaUk7gG3AuRHhA8ZmZlVUaBBExBZgeFnbotzjBcCCImswM7P2+ZfFZmaJcxCYmSXOQWBmljgHgZlZ4hwEZmaJcxCYmSXOQWBmljgHgZlZ4hwEZmaJcxCYmSXOQWBmljgHgZlZ4hwEZmaJcxCYmSXOQWBmljgHgZlZ4hwEZmaJKywIJI2VtCK3bJI0t6zPeZKelLRS0k8lnVBUPWZmVllht6qMiCZgAoCkvkAzsLSs22pgckS8IWk6sBg4uaiazMxsb4XeszhnCrAqItbkGyPip7mnPwNGV6keMzPLVOsYwSxgybv0+RPgPyqtkHSRpEZJjS0tLd1enJlZygoPAkn9gZnAbe30OY1SEFxWaX1ELI6IhohoqK+vL6ZQM7NEVWNqaDqwPCLWV1opaTzwHWB6RGyoQj1mZpZTjamh2bQxLSTpcOAO4IKI+HUVajEzszKF7hFIGgxMBS7Otc0BiIhFwNXAcODbkgDejoiGImsyM7M9FRoEEbGF0gd9vm1R7vHngc8XWYOZmbXPvyw2M0ucg8DMLHEOAjOzxDkIzMwS5yAwM0ucg8DMLHEOAjOzxDkIzMwS5yAwM0ucg8DMLHEOAjOzxDkIzMwS5yAwM0ucg8DMLHEOAjOzxDkIzMwS5yAwM0tcYUEgaaykFbllk6S5ZX2OkfQ/kt6S9KWiajEzs7YVdqvKiGgCJgBI6gs0A0vLur0OXAKcWVQdZmbWvmpNDU0BVkXEmnxjRLwaEb8AdlSpDjMzK1OtIJgFLKnSe5mZ2T4oPAgk9QdmArd1YRsXSWqU1NjS0tJ9xZmZWVX2CKYDyyNifWc3EBGLI6IhIhrq6+u7sTQzM6tGEMzG00JmZvutws4aApA0GJgKXJxrmwMQEYskvQdoBA4CdmWnlx4XEZuKrMvMzH6n0CCIiC3A8LK2RbnHrwCji6zBzMza518Wm5klzkFgZpY4B4GZWeIcBGZmiXMQmJklThFR6xr2iaQWYM27dtz/jABeq3URVeYx936pjRd67piPiIiKv8jtcUHQU0lqjIiGWtdRTR5z75faeKF3jtlTQ2ZmiXMQmJklzkFQPYtrXUANeMy9X2rjhV44Zh8jMDNLnPcIzMwS5yAwM0ucg6AbSTpE0n2Snsv+HNZGvwuzPs9JurDC+n+X9FTxFXddV8Ys6UBJd0t6VtLTkq6pbvUdJ+l0SU2Snpc0v8L6AZK+n63/uaQxuXWXZ+1NkqZVs+6u6OyYJU2V9LikldmfH6527Z3VlX/nbP3hkjZL+lK1au4WEeGlmxbgm8D87PF8YEGFPocAL2R/DsseD8utPxu4BXiq1uMpeszAgcBpWZ/+wKPA9FqPqUL9fYFVwFFZnb+kdN+MfJ//CyzKHs8Cvp89Pi7rPwA4MttO31qPqeAxTwRGZo9/H2iu9XiKHnNu/e2Ubsv7pVqPZ18W7xF0r48DN2aPbwTOrNBnGnBfRLweEW8A9wGnA0gaAswD/roKtXaXTo85IrZGxIMAEfFbYDn75/0pTgKej4gXsjpvpTTuvPzfw+3AFEnK2m+NiLciYjXwfLa9/V2nxxwRT0TEuqz9aWCQpAFVqbpruvLvjKQzgdWUxtyjOAi612ER8XL2+BXgsAp9RgEv5Z6vzdoAvgb8LbC1sAq7X1fHDICkOmAG8EARRXbRu9af7xMRbwMbKd2UqSOv3R91Zcx5n6B0z/K3CqqzO3V6zNmXuMuAr1Shzm5X6B3KeiNJ9wPvqbDqivyTiAhJHT43V9IE4P0R8Zfl8461VtSYc9s/gNJ9ra+LiBc6V6XtbySNAxYAH6l1LVXwZeDvI2JztoPQozgI9lFE/FFb6yStl/TeiHhZ0nuBVyt0awZOzT0fDTwEnAI0SPoNpX+XQyU9FBGnUmMFjnm3xcBzEXFtN5RbhGbgfbnno7O2Sn3WZsF2MLChg6/dH3VlzEgaDSwFPh0Rq4ovt1t0ZcwnA+dI+iZQR+ke7Nsj4lvFl90Nan2QojctwEL2PHD6zQp9DqE0jzgsW1YDh5T1GUPPOVjcpTFTOh7yQ6BPrcfSzhgPoHSA+0h+dxBxXFmfP2PPg4g/yB6PY8+DxS/QMw4Wd2XMdVn/s2s9jmqNuazPl+lhB4trXkBvWijNjz4APAfcn/uwawC+k+v3OUoHDZ8HPlthOz0pCDo9ZkrfuAJ4BliRLZ+v9ZjaGOdHgV9TOqvkiqztq8DM7PFASmeLPA88BhyVe+0V2eua2A/PiuruMQNXAlty/6YrgENrPZ6i/51z2+hxQeBLTJiZJc5nDZmZJc5BYGaWOAeBmVniHARmZolzEJiZJc5BYFZFkk6V9ONa12GW5yAwM0ucg8CsAknnS3pM0gpJ/yypb3ad+b/P7p3wgKT6rO8EST+T9KSkpbvvySDpaEn3S/qlpOWS3p9tfoik27P7MHxv99UrzWrFQWBWRtKxwLnApIiYAOwEzgMGA40RMQ54GPir7CX/BlwWEeOBlbn27wH/GBEnAH8I7L5K60RgLqV7FRwFTCp8UGbt8EXnzPY2BfgD4BfZl/VBlC6mtwv4ftbnZuAOSQcDdRHxcNZ+I3CbpKHAqIhYChAR2wGy7T0WEWuz5ysoXVLkJ8UPy6wyB4HZ3gTcGBGX79EoXVXWr7PXZ8lfm38n/n9oNeapIbO9PUDpksKHwjv3ZT6C0v+Xc7I+nwJ+EhEbgTck/e+s/QLg4Yh4k9Klis/MtjFA0oFVHYVZB/mbiFmZiPiVpCuBeyX1AXZQuvzwFuCkbN2rlI4jAFwILMo+6F8APpu1XwD8s6SvZtv44yoOw6zDfPVRsw6StDkihtS6DrPu5qkhM7PEeY/AzCxx3iMwM0ucg8DMLHEOAjOzxDkIzMwS5yAwM0vc/weLXga4vlDzEQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old training loop - matrix and non matrix format"
      ],
      "metadata": {
        "id": "4olpdwzyG8dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        #accuracy_v0_sum = 0\n",
        "        #accuracy_v1_sum = 0\n",
        "        #accuracy_v2_sum = 0\n",
        "        #accuracy_v3_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4: \n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                    v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                    mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_pred_flat:\", v_pred_flat.shape)\n",
        "                    \"\"\"\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4:                   \n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                    v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                    mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                    print(\"old mask\", mask_ori.shape)\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_ori_flat\", v_ori_flat.shape)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)   \n",
        "                    print(\"acc\",acc)                    \n",
        "                    accuracy_sum += acc \n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                    acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                    for i in range(len(prediction[0,:])):\n",
        "                      acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                      accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    \n",
        "                    \"\"\"\n",
        "                    pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "\n",
        "                    acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                    acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                    acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                    if nbr_voices == 4:\n",
        "                        acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                    \n",
        "                    # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    if nbr_voices == 4:\n",
        "                        accuracy_v3_sum += acc_v3 / len(lens)\n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "            train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "            train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "            train_accuracy_v3 = accuracy_v3_sum / 18   ## bc only 18 pieces with len 3\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "            history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "            history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "            #if nbr_voices == 4:\n",
        "            history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3)) \n",
        "            #else:\n",
        "            #    print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2)) \n",
        "            \"\"\"\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_v0_sum = 0\n",
        "                accuracy_v1_sum = 0\n",
        "                accuracy_v2_sum = 0\n",
        "                accuracy_v3_sum = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                        truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                        acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                        for i in range(len(prediction[0,:])):\n",
        "                          acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                          val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                        #print(\"val_accuracy_sum_list[3]\",val_accuracy_sum_list[3])\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(train_dataloader)\n",
        "                    #val_acc_list[3] = val_acc_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_acc_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "                #history[\"val_acc_new\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        # Predict the model's output on a batch\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                            \n",
        "                        # compute the accuracy \n",
        "                        acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                        acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                        acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                        if nbr_voices == 4:\n",
        "                            acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                            \n",
        "                            \n",
        "                        # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                        accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                        accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                        accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                        if nbr_voices == 4:\n",
        "                            accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                    val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                    val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                    val_accuracy_v3 = accuracy_v3_sum / 18  ##len(val_dataloader). - bc 18 pieces only with voice 3\n",
        "\n",
        "\n",
        "                    val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    val_acc_list[3] = val_accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    val_acc_list[4] = val_accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                    \n",
        "\n",
        "\n",
        "                history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "                history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "                history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "                #if nbr_voices == 4:\n",
        "                history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "                #else:\n",
        "                #    print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2))\n",
        "\n",
        "\n",
        "                history[\"val_acc_new\"].append(val_acc_list)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "\n",
        "                        \"\"\"\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                        if nbr_voices == 4: \n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                        v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                        mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)                      \n",
        "                        if nbr_voices == 4:                   \n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                        v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                        mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)\n",
        "                        accuracy_sum += acc \n",
        "                        \"\"\"\n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "nfDV8MKGHE3J"
      }
    }
  ]
}