{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject_final_tensor_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "SsyC2uB0KfaT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f153da-a46b-45a2-80e7-41d9b63c744d"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install partitura\n",
        "import partitura\n",
        "import statistics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: partitura in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.4.1)\n",
            "Requirement already satisfied: mido in /usr/local/lib/python3.7/dist-packages (from partitura) (1.2.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.21.6)\n",
            "Requirement already satisfied: xmlschema in /usr/local/lib/python3.7/dist-packages (from partitura) (1.11.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from partitura) (4.2.6)\n",
            "Requirement already satisfied: lark-parser in /usr/local/lib/python3.7/dist-packages (from partitura) (0.12.0)\n",
            "Requirement already satisfied: elementpath<3.0.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from xmlschema->partitura) (2.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader - Set the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "fugues = True\n",
        "\n",
        "if fugues == True:\n",
        "    PATH_TO_DATA = \"AI-MA_project/bach_pr_fugues\"\n",
        "else:\n",
        "    PATH_TO_DATA = \"AI-MA_project/pianoroll_88\"\n",
        "\n",
        "batch_size = 1 \n",
        "workers = 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_new(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        self.name_list = [\"1f01\",\"1f02\",\"1f03\",\"1f04\",\"1f05\",\"1f06\",\"1f07\",\"1f08\",\"1f09\",\"1f10\",\"1f11\",\"1f12\",\"1f13\",\"1f14\",\"1f15\",\"1f16\",\"1f17\",\"1f18\",\"1f19\",\"1f20\",\"1f21\",\"1f22\",\"1f23\",\"1f24\",\"2f01\",\"2f02\",\"2f03\",\"2f04\",\"2f05\",\"2f06\",\"2f07\",\"2f08\",\"2f09\",\"2f10\",\"2f11\",\"2f12\",\"2f13\",\"2f14\",\"2f15\",\"2f16\",\"2f17\",\"2f18\",\"2f19\",\"2f20\",\"2f21\",\"2f22\",\"2f23\",\"2f24\"]\n",
        "        self.name_list_voice_3 =  ['1f01', '1f05', '1f12', '1f14', '1f16', '1f17', '1f18', '1f23', '1f24', '2f02', '2f05', '2f07', '2f08', '2f09', '2f16', '2f17',  '2f22', '2f23']\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einfÃ¼hren damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "\n",
        "                        file_names_list.append(name[-8:-4])\n",
        "\n",
        "                    if \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[49:53] + \".pkl\" in sorted(glob.glob(os.path.join(PATH_TO_DATA, \"voice_3\", \"*.pkl\"))):\n",
        "                        nbr_voices_list.append(4)\n",
        "                    else:\n",
        "                        nbr_voices_list.append(3)\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "\n",
        "\n",
        "            if iLabel == 3:  \n",
        "                voice_files = []\n",
        "                file_names_3 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[2], \"*.pkl\")))   \n",
        "                \n",
        "                ###### loop over all filnames in voices_2 and if an element there is not present in voices_3: append \"missing\" to the voice_files of label=3 => important bc. self.pr_dict[voice_3] has then len 42 and otherwise it would only have len 18  .. these \"missing\" el are not considered later in the dataloader (if len=3 is a diff case of get_idx)\n",
        "                for name in file_names_2:\n",
        "                    if name[45:49] in self.name_list_voice_3:\n",
        "                      correct_name_3 = \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[45:49] + \".pkl\"\n",
        "                      with open(correct_name_3 ,'rb') as f:  \n",
        "                            loaded_obj = pickle.load(f)  \n",
        "                            voice_files.append(loaded_obj)\n",
        "                    else:\n",
        "                      voice_files.append(\"missing\")\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                \n",
        "\n",
        "                \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                          loaded_obj = pickle.load(f)     \n",
        "                          voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 4:\n",
        "            for key, value in self.pr_dict.items():\n",
        "              out_list.append(self.pr_dict[key][idx])\n",
        "                              \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.tensor(out_list[3].T)\n",
        "            v_all = torch.tensor(out_list[4].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 3:\n",
        "\n",
        "            for key, value in self.pr_dict.items():\n",
        "                if key != \"voice_3\":\n",
        "                  out_list.append(self.pr_dict[key][idx]) \n",
        "            \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.zeros(v2.shape)\n",
        "            v_all = torch.tensor(out_list[3].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "            \n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        "
      ],
      "metadata": {
        "id": "3FxK6qr1FqIl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_chor(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einfÃ¼hren damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        file_names_list.append(name[-7:-4])      # e.g. name = AI-MA_project/pianoroll_88/voice_all/voice_all_001.pkl\n",
        "\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "              \n",
        "\n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])\n",
        "                            \n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "        file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "        voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "        \n",
        "        return (voices, length, 4, file_name)     # 4 bc nbr voices is always 4"
      ],
      "metadata": {
        "id": "3uQnok3VGngZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "if fugues == True:\n",
        "    dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "else:\n",
        "    dataset = MusicDataset_chor(PATH_TO_DATA)\n",
        "\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 1:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXsRYzQSUuQU",
        "outputId": "471202a0-bf09-417b-c247-c72a794c4ddb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1f03 tensor([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\n",
        "pianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\n",
        "pianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\n",
        "pianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\n",
        "pianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\n",
        "\n",
        "time_unit = \"beat\"\n",
        "time_div = 12\n",
        "piano_range = True"
      ],
      "metadata": {
        "id": "fR2HaA_BqeHw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, figsize=(20, 10))\n",
        "ax.imshow(pianoroll_all, origin=\"lower\", cmap='gray', interpolation='nearest', aspect='auto')\n",
        "ax.set_xlabel(f'Time ({time_unit}s/{time_div})')\n",
        "ax.set_ylabel('Piano key' if piano_range else 'MIDI pitch')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "sRoyBJJVqX_u",
        "outputId": "72e94280-08d8-49a6-cc5b-bea3aeb40cff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJgAAAJNCAYAAAB9d88WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdf7R+WV0f9vfnO6BBtAE0ZY1ACprRFKuOMkWsxoWkIGAE86OItjohdJGuSISsdlW0XdXocklao9HEmE4ERcuPUH/EicslQYLR1VaBAQoMhDBBkJkMTCwKWlIiuvvHPYM3w9y59zzfe86zzz6v11p3fe9znnvvd5+zzzn3+bzv3vup1loAAAAA4FBXjt0AAAAAALZNwAQAAADAVREwAQAAAHBVBEwAAAAAXBUBEwAAAABXRcAEAAAAwFW537EbsISqasduA2zBYx7zmNnfc8sttyzQEoB5Rrp/9bovvbZrDXvedwA4x2+11v7EvT1RrY2XxQiY4GIOuf6raoGWAMwz0v2r133ptV1r2PO+A8A5bmmt3XBvT5giBwAAAMBVETABAAAAcFUETAAAAABclSEX+QYuxnoRy5u7joc+gYtZ61pZ4xo+5HvcW+ZxvABgeQImYJfWWsBVkQJs1UgLibsXA8DyBEzAEPx1GvZl78HEKOFPr8fXu8gBwHzWYAIAAADgqhjBBAzBX46Brep1tMye76t73ncAOJSACQA2qtdgYg2j7EdikW8AYAwCJgDYqFHW4WE+x3gMgkIARiJgAliQ4mGfRgplem0XyxvpPO6V4wXASARMAHAOQSF7ZIQcADCHgAmAzdr7W9XD1vV6bQm+AGA+ARPAgowAANge98hxGIEKsB4BE8DG9fpiWPAFsB+9Bjl+rwCsR8AEwCLWeFE/UuEgkIP96TWUOUTPbQNgHQImAOhAr8WZ4Is9sr4bAMwnYAKADvQa5CiA2aNez/te7xMAkAiYAKALhxSBik3YtpGmyAHAlaV+cFU9oqpeW1Vvr6pbq+p50/bvrKo7qurN08dTT33Pt1XVbVX1zqr6qlPbnzxtu62qXrBUmwFgS6pq9kevWmuzPmCOXs+vUa5fAEiSWuqXaFVdm+Ta1tobq+rTktyS5GuTPCPJ77XWvu8eX//oJC9P8tgkn5nkl5J8zvT0v0zyxCS3J3l9kq9vrb39Pv5vrzwBYEOM5OiPEXIAwL24pbV2w709sdgUudbanUnunD7/3ap6R5KH3ce3PD3JK1prH03yG1V1W07CpiS5rbX27iSpqldMX3tmwAQAXA6LHfdpjUBOnwAAcyw2Re60qnpkki9K8uvTpudW1Vuq6sVV9eBp28OSvO/Ut90+bTtrOwCZP/XD9KL59nx8e52Gt/fzvsc+AQD2bfGAqao+NclPJ3l+a+3DSX4kyWcnuT4nI5z+9iX9P8+pqjdU1Rsu4+cBXIY1CuBeA4CRzD2+ew8/1uC8Zw7XIwAsb9F3kauq++ckXHppa+1nkqS19oFTz//DJD8/PbwjySNOffvDp225j+0f11q7KclN08/1ygCAS2N9oP3S92PQLwCwvMUCpjr5Tf6iJO9orX3/qe3XTuszJcmfT/K26fObk7ysqr4/J4t8X5fkdUkqyXVV9aicBEvPTPINS7Ub4DLtuagZaYHgXts10jHuleO1T64tAJhvyRFMX5bkG5O8tarePG379iRfX1XXJ2lJ3pPkryZJa+3WqnplThbv/liSb26t/UGSVNVzk7wqyTVJXtxau3XBdgPAv6fXUSwKWgAAelEjzjM3RQ7g+IwAWJ5jPAb9CABsyC2ttRvu7YlF12ACYAwK4HnWOl57PsYj0Y/71evoSAA4hIAJAC7ZWkWg4G+/BBNj0C8AjETABMC5DimChB/LW+N46cflOcYAwAgETAAAR7RGgCuQAgCWJmACYBFG18ByXF8AQG8ETABsVq9rHSmyGcGez2PhGgDMJ2ACAM4kxGOPRjq/hGUArEXABADnmFtsKegYgfN4DPoEgLUImACAM60VMiiC51ljxJc+AQDmEDABAGfqNWTY++iaUfZl7/0IACMRMAGwiD0XjqPsBwAAXJSACYBFjBSyWIC6P47x8tYIifUjAIxDwATAZlkfiBH0OtrPeb884TUAIxEwAbAIox+AXvQa5LiHATASARPsWK9/NWcMzhWgF73ej3oNvgDgEAIm2DEvVLkoYSQsp9drxXUPAMwhYALYOFPRYH/WGPniugcA5hAwAWzcGkWgkQzsVa9TmFxfY9CPAIxEwATAuXotggRf7FWvwRcAsF8CJgDO1WuQo2gGAIA+CJgAOJcgZ55eA7lDjLQvhxhpXwAAliRgAoBz7Hk60kj7AgDAcgRMAHSj1yBn7v8z0qifkfZlJI4xANAbARMA3dhzkAMAAFsmYAIWJQBgSc6V5Y10jN2PAACWI2ACFqU4Y45ep8jN1Wu76JPgCwAYgYAJgHOtVQArmgEAYJsETACca63gZ5QRTPTJ+QIAsBwBEwDd6DUAEHyxR6buAQBzCJgA6EavQY6iGQAA7puACYBuCHLmGWmESa/h4hpG2hcAYL8ETABwySyKPt9I+wIAsEcCJgA2q9cRPL0uip4IcuYysgoA4GIETMAQ9lwE7pl+BACAPlw5dgMAAAAA2DYjmIAhGMmyT3ufIjbSvjCPUZsAQG8ETAAsYo3wR9EMAAB9EDABsIg1wp+9j2Biec4XAICLETABsFm9vlubUAIuRkgMAOMQMAGwWWsVpwpaAAC4bwImADZL8LM8I0yW5xgDACMQMAHARllIfQyHHGPTNgGA3giYAOAcvRbzew4N9j7qZ6R9AQDGIGACdmnvxSnz6Pv+jNQn7kcAwAgETMAuKc5YksAAAIC9ETABwCVbKywSZLF1a6w/dej/AwDMI2ACgEu2VgGsaGaP1jjvhVgAMJ+ACYDN6rUIVGgygl6vrzWMsh8AsCYBE8CCen33sVE4XrAc19fy/I4AYCQCJoAF7bkY2PPoBwAA2BsBEwCLEBYxx55Hcqyx0PVIxwsA6JOACYDNMkpqHHvuF+fx8noN5PQjACMRMAGwWYqz/RoplOm1XXs+xiPtOwCsRcAEAOfodfTDnu39GK9xTu75GK+17+4tAIxEwAQAsDFrBA1G8QAAcwiYAOAciub96nWEiRFMAEBvBEwAcMmM/BhHr/1iBBMA0BsBEwCco9dRLLAkIdbyHGMARiJgAoBz9FpsKRwBAOjFlWM3AAAAAIBtM4IJADZqz6ORjN4CAOiLgAkA2JyRwqI9h2W97see+wQADiVgAoCNUgSPQZ8sb88L9R+yL+4tABzCGkwAAAAAXBUjmABgo7zFOVzM3HPSeQ8A8wmYAGCj1iiCFc2MYM9T5ABgLQImANgoRTBLGmkUT6/tAoCRCJgAYKNGCgCYZ40ROc6V5fU6skrfA3AIARMAwMYIAOYR5ADA8ryLHAAAAABXxQgmADZr71PERtoXluVa8S5yALA0ARMAm6WgW55Cewz6BABYmoAJADZqjfBHMLFfew4X19qPXteGAoBDCJgAYKPWKDbXChkU2v3Z8zFe67zf8zEGYDwCJgAAOMUIJgCYT8AEABydwnmfep2GZwQTAMwnYAIANqfXYIJ5eu2TXtt1CNcKAGsRMAEAZzJVCACAixAwAQCcQfAFAHAxV47dAAAAAAC2zQgmAIAz7HlEkrV7AIA5BEwAAHyCkcKiPU917HVfBJgA4xEwAQBHN7dwXKs43XMwMRL9Ms8a15c+ARiPNZgAAAAAuCpGMAEAnGGNURamCtEb5z0AhxAwAQAckaKZJfUa5DjvAcYjYAIAzqQ4ZY96Pe8P0Wu7ABiPgAkANspCvHAxFmsHgOUJmADgHL0Wp3sugkcaYcLy9D0ALM+7yAEAAABwVYxgAoBzzB39YHTN8hwv5uh1FCIAjETABABwBsHEGPQLACxPwAQAl2ytYtZIqeU5Xvvk2gKA+QRMAHDJ1ipOFbRjEGb0x/EFgPkETABwyUYqToUfy3O89ssUTABGsti7yFXVI6rqtVX19qq6taqeN21/SFW9uqreNf374Gl7VdUPVdVtVfWWqvriUz/rxunr31VVNy7VZgC4DK212R+9qqrZHwAA7E8t9aK2qq5Ncm1r7Y1V9WlJbknytUn+cpIPttZeWFUvSPLg1tq3VtVTk/z1JE9N8iVJfrC19iVV9ZAkb0hyQ5I2/ZzHtNZ++z7+735fqQPAhhjB1CcjXwCAI7mltXbDvT2x2Aim1tqdrbU3Tp//bpJ3JHlYkqcnecn0ZS/JSeiUaftPtBO/luRBU0j1VUle3Vr74BQqvTrJk5dqNwDAmg4Z8WZUGQDQm1XWYKqqRyb5oiS/nuShrbU7p6fen+Sh0+cPS/K+U992+7TtrO0AAJvXawBk9BoAMMfiAVNVfWqSn07y/Nbah0+/8GittcuazlZVz0nynMv4WQCwBWsEAAKD/dpz3wvXAGC+xabIJUlV3T8n4dJLW2s/M23+wDT17e51mu6att+R5BGnvv3h07aztv97Wms3tdZuOGsuIACMZo0FuEdasBwAgOUs+S5yleRFSd7RWvv+U0/dnOTud4K7McnPndr+TdO7yT0uyYemqXSvSvKkqnrw9I5zT5q2AQAL6/Vd5ARfLKnX8/4QrhUA1rLkFLkvS/KNSd5aVW+etn17khcmeWVVPTvJe5M8Y3ruF3LyDnK3JflIkmclSWvtg1X13UleP33dd7XWPrhguwGAlXlXtP6YJjYGfQLAWmrEv1Jc1rpOAAA9WiOQEzABAPfilrOWJlrlXeQAAC7T3sOPkfYFABiDgAkA2Jw113qaQ/ADAOyVgAkA4Ay9BkZrBF+97vshBIUAsDwBEwDAGXoNJgQg8zhe+9XrNQwwIgETAADwcSOtcdZruwBGJGACAI6u11EGilOW5LwHYCQCJgDgTGuNZFDQ0hPnPQDMJ2ACAM7k3dr2a6RpUnONsh8AsCYBEwBwJiM5xrDnsAgAWIeACQA4U68hg8Bknj3ve8+M3ANgJAImAGBzFNrzCOQAgKUJmAAAGFqvI4WEeACMRMAEADC4XoOMXtf4MuILAOYTMAEAmyMAGEOvfdJruw7hWgFgLVeO3QAAAAAAtk3ABAAAAMBVMUUOADi6XhdhhiWtcd67VgBYi4AJADg6izDv0977caR9AQBT5AAAAAC4KkYwAQCbs9bID1P3luV47dfeR68BjEjABABwhj0XtAIAAGAOARMAAJwiXFue4wUwHgETAACfYKQAoNepjr22CwAOIWACADbHCBPm6LXv12iXawWAtQiYAIDNMcIELuaQc1IoBcAhrhy7AQAAAABsmxFMAMDmrDXCwqgMts5oJADWImACADbHFDnm2HM/jrQvAPTNFDkAAAAArooRTAAAZ/AuX2NwvPrjvAcYj4AJAOCIei2aBQAAwBwCJgAAOAJrQwEwEgETAACfYM8BwEjvUmgkGgBrETABAJxhzyNM9mykfjxkX4RSABxCwAQAcAYjTNgj5z0Ah7hy7AYAAAAAsG1GMAEAnGGNKXJGZQAAIxAwAcBGmWKyPMcLAOBiBEwAsFHCj+VZ5BsA4GIETADA5oz0NvIAACMQMAEsyOgHWIZrhSWZfro8xwtgPAImgAV5Ac3WKbTZo7XOYX+EAGAkAiYA2Kg1wh8FLSyn1+tLsAzAIQRMAAAwKGERAGsRMAEsyPQHluR8gW1b43eE+wQAaxEwASzIC/tl+cs8cBG9hv3uRwCMRMAEwGZZiBe2ba2Q2DUJAMsTMAGwiJFGF/XaLsYw0rUy1yj7AQAkV47dAAAAAAC2zQgmgAXteWrVSPsCSzrkWtnzvQUA6JOACQBgYwRG8wjkAGB5AiYAAIbWa2Ak+AJgJAImgAUpBvZpz4s2AwCwTwImALhka4VFgqwx6Mf90o8AjETABAAbpTgdg0W+AYARCJgAFqQIBAAA9kDABLAggdGyTC0ahzB2nr3vPwDQHwETAHB0ApN5BHIAQG+uHLsBAAAAAGybEUwALGKN6WtGZbBXa5z7pqACAHMImAAA+AR7DouEawAwn4AJAABOGSksEpYBsBYBEwCLUKAsT+EIAEAvBEwAsFHW4dkv7yLHRel7ANYiYAIAzqQ47ZNwEQDojYAJYEFGGQAAAHsgYAJYkMBoDILC/hhdszzHCwCYQ8AEAGyO8GN5QjwAYI4rx24AAAAAANtmBBMAixhp9EOv7YIlWUgcAJhDwAQAwFH0GhYJvgBgPgETAACcsvewSMAGwCEETAAsQrExBoUmAAAXIWACAM50SFgklBqDftwv/QjAIQRMAMClUpyOwSLfAMAcV47dAAAAAAC2zQgmAIAj2vMonlH2AwAQMAEAGzRSKNNruwAA5hAwwY6NVKAB++JetDy/IwCAOQRMAABH1GuQIywCAOYQMMGOjVQ8zC3QRtp3YNt6vR/1GnwBAH0SMAFDUNQsS6EJ+7PGNezeAgDjEDABAPAJ1gh/hEUAMA4BEwDnUgTOY1QGI3BOLs/0bgBGImACWJDiYZ/0I/Sl13uxewUAIxEwASxolOLBiBxgy3q9H/UafAHAIQRMAABwBBZSB2AkAiYAADiCNUYwCYsAWIuACYBzKVDmMWIALmbv18pI+wIAAiZgUXsvHtgn5zBLG2Xtnl7bBQDMJ2ACFtVr8SD4ArZslPuRezEAjEPABOySAqVPo4zKAC5mrWvYvQUAlidgAgDgKNYawSQwAoDlCZgA6IYicB7Ti9g65+Py3CcAWIuACWDjFA/7pR+hH73ei90nAFjLlaV+cFW9uKruqqq3ndr2nVV1R1W9efp46qnnvq2qbquqd1bVV53a/uRp221V9YKl2guwVVU1+6NXrbVZHwC96PVePPe+6t4KwKGWHMH040n+XpKfuMf2H2itfd/pDVX16CTPTPJ5ST4zyS9V1edMT/9wkicmuT3J66vq5tba2xdsNwAADOGQIKvX0VgA9G2xgKm19itV9cgLfvnTk7yitfbRJL9RVbcleez03G2ttXcnSVW9YvpaARMAAJxDWATAWhabIncfnltVb5mm0D142vawJO879TW3T9vO2g7AgHqcXtIzU1/gYvZ8nfQ6dQ+A8awdMP1Iks9Ocn2SO5P87cv6wVX1nKp6Q1W94bJ+JrAdey4e2C+FI0saKcB0nQDA8lZ9F7nW2gfu/ryq/mGSn58e3pHkEae+9OHTttzH9nv+7JuS3DT97H5f4QCL6LUgmFtw9bofwP6MdD9yLwaA5a0aMFXVta21O6eHfz7J3e8wd3OSl1XV9+dkke/rkrwuSSW5rqoelZNg6ZlJvmHNNgNcDUXKsqwtAlzE3OvevQUA5lssYKqqlyd5fJLPqKrbk3xHksdX1fVJWpL3JPmrSdJau7WqXpmTxbs/luSbW2t/MP2c5yZ5VZJrkry4tXbrUm0GAGA8RjABwPKq5/nyhzJFDgAuh5EcsD+uewDuwy2ttRvu7YlVp8gBsE2Kjf3Sj7CcXkdWue4BOISACYBz9VpsCL6ALev1fuTeCsAhBEwAAMDHrREWCbEAxiNgAgAAPm6N8EdYBDAeARMAm6VAWZ5RBuzR3s/7kfYFgPUImIBd2nvxABflvGdJvd6LnfcAMJ+ACdilkYqHXgs0gPP0ei9yXwWA+QRMABunqFler28lDixjrWvYvQWAkQiYAOAcvRZ1RlnAtnm3NgBGImACgI3qtQhU0EI/er223CcAxiNgAmARiof90o/Qj17vxe4TAOM5N2Cqqs9vrb11jcYAMI6RigfrpABb1ev9qNfgC4DDXWQE09+vqk9O8uNJXtpa+9CyTQIYh2BiDPoF4HJZSB1gPOcGTK21P1NV1yX5K0luqarXJfmx1tqrF28dwMb1+kLVC24ALlOvv1f8/gJYT130l0FVXZPka5P8UJIPJ6kk395a+5nlmneYqpo/5haAzTHFAmCb3L8BNuuW1toN9/bERdZg+oIkz0ry1UleneRrWmtvrKrPTPJ/JekuYALgvvX6l+a5em0XjEAAwJKcKwDjucgaTH83yY/mZLTSv717Y2vtX1fV/7hYywBYzBov7BWnsG3uE8wxyh8uADjchabIVdUDkvzJ1to7l2/S1TNFDoDLpHCCZfQaMPXaLgDowFVNkfuaJN+X5JOSPKqqrk/yXa21p11uGwGgT70Wjopgtu6Q83GN8951sjz3L4DxXGSK3HcmeWySX06S1tqbq+pRC7YJALgAIzkAAOjFRQKm32+tfegeLxZNQQOAIzOSA7Ztz9NvR9oXAE5cJGC6taq+Ick1VXVdkm9J8n8u2ywA2BcjheBiRhq5N/d73CcA6NmVC3zNX0/yeUk+muRlST6c5HlLNgoA9qaqZn+sobU2+wO2rtfrEQB6du67yFXVs1trL7rHthe21l6waMuugneRA4CxGckxj+M1j+MFAGc6/F3kkvzFqvr/WmsvTZKq+ntJHnCZrQMAmGPNEVxzCBnm6TXI0Y8AMN+FAqYkN1fVHyZ5cpLfaa09e9lmAQAcnzVylrXnfQeA0Zw5Ra6qHnLq4acl+cdJ/o8k/1OStNY+uHjrDmSKHACMTZAzBv0IAJtz5hS5+wqYfiNJS1Kn/r1ba6191mW38rIImADgeIQGXJRzBQA2Z/4aTK21Ry3XHgBgCwQALKnXc8V5DwDzXWQNJgBgYb0WtIpm6Eev9wkASARMANCFXotABS30w7UFQM8ETABwjj2/Vf1I+wJ7JCQGYC0XCpiq6mlJvmJ6+M9ba/9kuSYBwMWsVTgptoCtcv8CYC3nBkxV9b1JHpvkpdOmb6mqL22tffuiLQOAc/RaOBkxsDzHeHl7Psaj7AcArKnOe/FQVW9Jcn1r7Q+nx9ckeVNr7QtWaN9Bqmr+KyIAOMOep8j1as/hx1ocYwDgXtzSWrvh3p646BpMD0rywenzP34pTQJgaCMVp722izH0eq047wGAOS4SMH1vkjdV1WuTVE7WYnrBoq0CGMSeR76MtC8sr9drZY12uVYAgBGcO0UuSarq2iT/6fTwda219y/aqqtkihzAPvQ68gPmcB4DABty1VPkriT5renrP6eqPqe19iuX1ToAxrNG0azI3q+RQple2zXSMQYAlneRd5H7W0m+LsmtSf5w2tySCJgAAK5Sr0GOsAgAmOMiI5i+NsnnttY+unRjABjHGsXpWoV5r+sD7dlIx7jXfek1+AIA+nSRgOndSe6fRMAEwC4pmver13DR4uMAQG8uEjB9JMmbq+o1ORUytda+ZbFWAcCGGfkxjl77ZaQRggDAGC4SMN08fQBAV0YaLQK9EWIBAHOcGzC11l6yRkMAYBSKZgAA9uYi7yJ3XZLvTfLoJH/s7u2ttc9asF0AsFm9jqxKBFkAACzjIlPkfizJdyT5gSRfmeRZSa4s2SgA4Hx7DouEawAAfanzXqBV1S2ttcdU1Vtba59/etsqLTxAVc1/1QkAZ7AGE0sSli3PNQwAl+aW1toN9/bERUYwfbSqriR5V1U9N8kdST71MlsHAD2bW2yuFRgIJsagTwCAEVwkYHpekk9J8i1JvjvJE5LcuGSjAIDzCSbYurVCUtcKACzv3ClyW2SKHADHNNLIopH2BS7KeQ8AZ5o/Ra6q/k5r7flV9U+SfMJv2dba0y6xgQDATGsUwYpmRmANJgBY3n1NkfvJ6d/vW6MhAMA8imC2zhQ5ABjHfQVMt1bV85P8qSRvTfKi1trH1mkWAJyv12ksill641oBAJZ2XwHTS5L8fpJfTfKUJI/OyYLfAABcElMdAYAR3FfA9OjW2ucnSVW9KMnr1mkSAL0x+oG9WmPtnl7P416vewCgT/cVMP3+3Z+01j7mBQNAn4x+gOXs+dxfY9+FWOOwkDoA9xUwfWFVfXj6vJI8YHpcSVpr7T9YvHUAnEsRCJzHNczSnC8AnBkwtdauWbMhAPSr18JB0QwXc8h5b3QkS3L/BhjPfY1gAoAk/RYCig0AAOiDgAmAc/Ua5PQafMEIRpp+a32g/jjGAOMRMAGwCNNrgF64V+yXcBFgPQImABYx0ugHYF/cW8ahXwDWI2ACYLPWKhz8BRyW4RoGgHEImAAA4AgEXwCM5MqxGwAAAADAthnBBAAAR7DnEUnWuQIYj4AJAM5hwXKAyzXS/cv9G+CEgAkAgKH1Wsxbg2kM+gXghDWYAAAAALgqRjABQAcO+Qu4aRmwbabfAjASARMAnKPXaSyKQFjGSKFMr+0CYDwCJgA2a60iUIHWn5ECgF7t+RiPsh8AsCYBEwCLWKM4VQSOo9dRYr22aw0j7QsAsDwBEwBs1EgjTLRrnj0HXwBAn7yLHAAAAABXxQgmgI3rdRSLEROwHNfXPEZ8AcDyBEwAC1qjqFEIAdy3Xu+Tgi8ARiJgAliQYoAljXR+KbT70+voyJE4XgCMRMAEABzd3EJb+AEA0BcBEwBwdEYw9eeQY6wfAWC/BEwAwNEJGubpNcjRj/P02o8AcAgBEwDAER0y3Y8xCIwAGImACQDYhZHWbeq1XQDAfgmYAOCSjRRkjKTXY9xru5zHAMAcAiYANqvXAliRDQDA3giYANistYIcC/GyR87jeXoNvAFgLQImALrRa5CjCATO0+t9QvAFwFoETAAAHIXwY3mOFwBrETAB0I25hZDiFAAA+iBgAqAbvU6RA5ZxyDUsWAaAPgmYAOiGInAehTZ75Byex30CgLUImADgkq1V0CkCgfO4TwCwFgETAJyj16l7RiYA53GfAGAtV5b6wVX14qq6q6redmrbQ6rq1VX1runfB0/bq6p+qKpuq6q3VNUXn/qeG6evf1dV3bhUewFga6pq9sdcrbXZHwAA7M9iAVOSH0/y5Htse0GS17TWrkvymulxkjwlyXXTx3OS/EhyEkgl+Y4kX5LksUm+4+5QCgD2bo3wZ40QCwCA7Vtsilxr7Veq6pH32Pz0JI+fPn9Jkl9O8q3T9p9oJ698f62qHlRV105f++rW2geTpKpenZPQ6uVLtRuAT7T3KRa97ssa7dp730NPXI8A9GztNZge2lq7c/r8/UkeOn3+sCTvO/V1t0/bztoOAEPptXBUnAIAcBFHW+S7tdaq6tIWaqiq5+Rkeh0AXKo1FvkW5ADncZ/oUwRLC54AABqqSURBVK9/IABY29oB0weq6trW2p3TFLi7pu13JHnEqa97+LTtjvzRlLq7t//yvf3g1tpNSW5KkssMrgCg10JAUQOcx31ieY4XwIklF/m+Nzcnufud4G5M8nOntn/T9G5yj0vyoWkq3auSPKmqHjwt7v2kaRsAAOzKIQv7W6gfgLUsNoKpql6ek9FHn1FVt+fk3eBemOSVVfXsJO9N8ozpy38hyVOT3JbkI0melSSttQ9W1Xcnef30dd9194LfAKzn0Le3X+P/2TOLfMO+rHVtue4BOEQd8gukd6bIAb1YY+0elqcfYdtcwwBwaW5prd1wb08cbZFvgD0wwqQ/jhf0Y63r0TUMAMsTMAFD2PNfp0falzWsMd1PnzDHnkPPUfYDABAwAQAcVa8hy56DLwBgPgETMARFzbJGKjRH2he4KOc9ALA0ARPABe25QBtlPxJT5NinXs/JPd9XAWA0AiaAC1LUzKNwBM6z1jUvJAaA5QmYABa056JmpH0BluFd5ABgHAImAACOwggmABiHgAlgQWsUKaaiLc/x4qJ6vR57bddaRtoXAOiVgAkAOrD3AIBlOVcAgKUJmAA2zuiHMTheY1jjXQoP/X8AAJYkYALYuDWKU8VsnwQTYxASAwAjEDABbJziFAAAODYBE8DGGcG0X/oFAIBeCJgANq7XkMGopzHoRwAALkLABAAbZfQaF2XxcQBgaQImANiokYr5uWHGSPsOADCCK8duAAAAAADbZgQTAHB0ex6RZCoaADACARMAixipADZ9iyU5XwCAEQiYANistUZ+CACWJ8TrzxrH2OgtABiHNZgAAAAAuCpGMAFwrl5HGRjJAAAAfRAwAXCukYKcXsMyAADYMgETAOcaKZTptV17p18AALZNwASwcWuEP4p/2J8931tGCtUBYC0CJoAL6rXgUNQAXK5e76u9/h4CgETABHBhe36RrqiBbXMNj6HnPpl7jvW8LwAcRsAEwLlGKgQU2uzR3s9h4QcALE/ABMC5Rgplem0XLGmka/gQI+0LAPRKwATAuUYqzvZeaAMswX0SAAETALtySBFkeg17JIwFAOYQMAHAOdYomhXz8zhe8/S67/oRAMYhYAKADvRaNAsAAAC4CAETAHSg1yCn17Co13YBAOyVgAkAzrHGGkwCk3l6DeSYR58AwDgETAAXpKDdL/0Iy3BfBYBxCJgALkhRM4/CkSU5V5a3xjWsH/fL7wiA8QiYAFjESIXAGlPkoDd7Po+FHwAwn4AJgEWMVKD12i5Y0kjX8Fyj7AcArEnABMAiRirQjGACuFzukwDjETABwDnWKIT2PFqE/XLeA8A4BEwA0IFDimbF+fL2PHptjX1Z63jtuR8BYC0CJgDYqJGKYAEAAMC2CZgAYKNGGsGkXQAA2yZgAoAOjBQWjcTIqjHoFwBYnoAJYEGKUy5K38Ny3IsBYHkCJoAFKVKWZdQPS3O+LGuta1g/7pdwEWA9AiYANmukQkBYxh7t/RwWfgAwEgETAJs1UijTa7tgSSNdw4cYaV8AQMAEwGaNVJztvdAGWIL7JMB6BEwA0IFDiiChFFyMqWgAsDwBEwBs1BpFsBBreXs+xmvtx9z/Z899AgCHEjABAGfqtWgWAAAA9EXABACcqdcgZ6SwaKR9AQD2S8AEALAx1hRaluMFAPMJmAA2rtcRJozBubI81zAAMAIBE8DGKTSZo9eRL722aw0j7QsAsF8CJgDg6HoNWfYcfAEAzCFgAuBcpvCMQ78AALAEARMA5xJK7Fev4eJa7XLuAwBcjIAJgEX0GkwwT699Ym0oAIC+CJgAWIRCmzl6DXJ6PY97PV4AwH4JmAAAztBrkCMwAgB6I2ACYLNMwxtHr/3Sa7t6Db4AgP26cuwGAAAAALBtRjABsFkWegYAgD4ImADgHHsOjExDXJ5jDACMQMAEAHBEh4RFRtUBAL0RMAEAZ+o1mNj7qJ9R9mXv/QgAIxEwAbBZitP92ns/jjKCqdd2AQDzCZgA2CyLfLNXa5xjAlwAYA4BEwCbtVYBrGgGAID7JmACANiYNUbVCVYBgDkETABslgKYpfU6PdK5DwD0RsAELMoaHizJ+QUAAH0QMAEAnMFi2gAAFyNgAhalCFreSMVpr9ORYEm9nscj3VsAgOUJmAAA+ARGbwEAcwiYADZupGJrpH0ZhQCAJY10rrhWANg7ARMAbNQaBe0hBbBCmz3q9Rx2PQKwFgETAFyytQo6U5iA8wiJAViLgAkAzrHnxccVp3AxI533vbYLgL4JmADgHIot4Dy93idGCr4A6JuACQAABmUUIgBrETAB0I1ep6L12q5e7X3/YetGuobdvwHWI2ACgHMoOAC2ae792+gtgMMJmADoRq8v0v0FHOBy9RrkuH8DHE7ABAAAfNwa4Y8gB2A8AiYAAODj1gh/eh3BBMDhBEwAcI5ei5peC7Re2wVLct7Ps+d9BxiVgAkAuFQKR/bokPPe+m4AjETABAAbNVKxqdDuz0gjcno9v3o9XgBwCAETAHB0Cu19WivEcn4BwPIETAAAHMVawU+vI5gAYCRHCZiq6j1JfjfJHyT5WGvthqp6SJJ/lOSRSd6T5Bmttd+uk9/wP5jkqUk+kuQvt9beeIx2A0BPRprC1Ks9BxMj7QsAsLwrR/y/v7K1dn1r7Ybp8QuSvKa1dl2S10yPk+QpSa6bPp6T5EdWbykAsKjW2qyPtVTVrA/mmdvvh/a9fgSA5fU0Re7pSR4/ff6SJL+c5Fun7T/RTl5R/FpVPaiqrm2t3XmUVgLQjb2P4FljX6yRw5L0+zj2PNoPgBPHCphakn9aVS3J/9pauynJQ0+FRu9P8tDp84cled+p77192iZgAoCFWSOHJe09JB6JfgHgWAHTl7fW7qiq/zDJq6vqX5x+srXWpvDpwqrqOTmZQgcAbMzc4lQwAQDQl6METK21O6Z/76qqn03y2CQfuHvqW1Vdm+Su6cvvSPKIU9/+8GnbPX/mTUluSpK54RQA27T3wGCkkMUIpn3Sj/s10v0LgBOrL/JdVQ+sqk+7+/MkT0rytiQ3J7lx+rIbk/zc9PnNSb6pTjwuyYesvwQAY5m7CPNai0MzBudKf+Ze88IlgP4dYwTTQ5P87PRL4n5JXtZa+8Wqen2SV1bVs5O8N8kzpq//hSRPTXJbko8kedb6TQaAZflr/jwj7bu+X57jtV9GRwKsp0b8K40pcgBsjZChT4rTfXI9AsCZbmmt3XBvTxxrkW8AgO6tERoIMwCAEQiYAKADhwQGggkAAHohYAKAjTK6Bpax1jlsCiYAIxEwAQBn2nuhvUa7hAb7tee+F14DjEfABABwhl4L2l4DObiokc5JYRnACQETAMDGmB4JAPRGwAQAHN3cYGKk8KPXfen1eK2h1z6hT/oe4ISACQDgiIxG6s9Ia3wBwFoETACwUXsODUbZj7Uccrz2fH6tpdfjpe8BOISACQA2aqSCzkiO/jjGy+v1vNf3ABxCwAQAAEew5yDHKCmA8QiYAGCjRirQem0XY+h1pNBIHGMABEwAC/KCmyU5X4Be9Ho/8nsYYD0CJoAFeaG6rJFG8MAerXUNu+4BYHkCJgA2y1uJw7b1eq0IrwFgPgETAJtl9ANsW69BjmseAOYTMAGwWYrA5fUaAMCSnPfj0C8A6xEwAUAHei1oFWcAAFyEgAkAOtBrkNNr8DWSPR/jUfYDABAwAQD3wULqyxtpXwCA/RIwAQBnspA6AAAXIWACADZnz9PK1rLnYzzKfgDAmgRMABu35yKQ5TlXAAC4CAETwMYJAJa35/WBeuUYAwD0RcAEAOdYI8wwEg2WIyQGgOUJmADgHGsUpwpaAAC2TMAEAOcQ/izPCJP+OMbLc94DMBIBEwBs1EjT6nptFwAAFyNgAoCNGimUMZJjWSOFkYcYaV8AoFcCJgDg6OYGAHsPTOba874DAOsQMAEAR2cEEwDAtgmYAADOIPha1t5Hoo20LwAgYALgXHsvAlme8wUAYNsETACcS/G/PCFenxxjAICLETABQAcOCTKEUrAM1xYAzCdgAoAOKGgBANgyARMAdEBYNI9Abgz6ZHmuFQDWImACAM7Ua3GqAAYA6IuACQA4U69BTq/B10j2fIxH2Q8AWJOACQDYnLUCgLkhy0jBxEj7AgAsT8AEABxdr0HOGv9PryOFem0X8+gTANYiYAIAjm5uESz8AADoi4AJgEUIAFiSc2V5jvHyeh25BwCHEDABsAiFEHP0Wmiv0S7XCgAwAgETAJtllNQ4eu0Xax2NodcAUz8CMBIBEwCbpTjbr5FCmV7btedjPNK+A8BaBEwAAEfUa5ghMAEA5hAwAcA5ep1es2cjHeNe96XX4GsNI63xBQBrETABwDkUdfu15wDA+lMAwBwCJgC4ZIrmcey5X9Y4j/d8fAFgNAImADjHnkexsF9GMC3PMQZgJAImADiHd6ACAID7JmACgEvW6wLBiSALAIBlCJgAYKP2HBYJ15ZnDSYAYA4BEwCwOSMFE72GZXs+xiPtOwCsRcAEABvVazDBPPqEJR1yfrm3AHAIARMAXLK1ijMFHVvnWgGAcQiYAOCSjVTMGsnAkno9V5z3ADCfgAkANsoizCxpzyHLKPsBAGsSMAHARimCWdIa59eeQywAGI2ACQBgcL0GOXsPi3p9d7u99wsAhxEwAQAMzjuJAQBLEzABwEYJAFjSSOeKkUIAsDwBEwBslOKUi+o1jFyrXa4VAFiegAkAOFOvwQTz9Nona7Wr1xFMADASARMAcCaF9n6NFC722i7BFwAjETABADC0XoMcgREAIxEwAQBH12sAsGcjHeNe98V5D8BIBEwAwNEpnPdppGl4hxhpIXUAEDABAJujaB7D3vtkjRFMez/GAKxHwAQAHJ2pQuyREUwAjETABAAc3dyCVtEMANAXARMAsDlrhUVGVgEAXIyACQDgDHsOjIwSAwDmEDABAPAJRgqL9jwS7ZB9ES4CcAgBEwDAGfYcTIxEv8yzRiilTwDGI2ACADZnrREWimC2zrUCwFoETADA5oxUzJqOxJJ6PVec9wDjETABAJszUnHaa7sYQ6/XivMeYDwCJgBgcxSnjGCNdYtcKwCsRcAEAMDQel2AWvgDwEgETAAADG1ukNPrtDIA6JmACQDYHAEAS3KuAMB8AiYAYHMEAMzR6xQ5ABiJgAkA4AyCiTHoFwBYnoAJAOAMgol9MgUTAOa7cuwGAAAAALBtRjABAByR0TL9cXwBYD4BEwDAEQkz9ssaXwCMRMAEALAxgokx6BcARiJgAgA4IlPkAIARCJgAAI7okLDICCYAoDcCJgCAjVkjMDKyCgCYQ8AEAMAn2HNYJFwDgPkETAAAcMpIYZGwDIC1XDl2Ay6qqp5cVe+sqtuq6gXHbg8AwMhaa7M/6E9Vzf4AgENsYgRTVV2T5IeTPDHJ7UleX1U3t9beftyWAQCsb41FvgUNAMAcmwiYkjw2yW2ttXcnSVW9IsnTkwiYAIDdGSX8MX0LAMaxlYDpYUned+rx7Um+5EhtAQAY3hrhj7AIAMaxlYDpXFX1nCTPmR7+XpJ3nvGln5Hkt1ZpFL3R9/uk3/dL3++Xvr8EGw1/9P1+6fv90vf7pN+P5z8664mtBEx3JHnEqccPn7Z9XGvtpiQ3nfeDquoNrbUbLrd5bIG+3yf9vl/6fr/0/X7p+/3S9/ul7/dJv/dpK+8i9/ok11XVo6rqk5I8M8nNR24TAAAAANnICKbW2seq6rlJXpXkmiQvbq3deuRmAQAAAJCNBExJ0lr7hSS/cAk/6txpdAxL3++Tft8vfb9f+n6/9P1+6fv90vf7pN87VIe8QwgAAAAA3G0razABAAAA0KndBExV9eSqemdV3VZVLzh2e1hOVb24qu6qqred2vaQqnp1Vb1r+vfBx2wjy6iqR1TVa6vq7VV1a1U9b9qu/wdXVX+sql5XVf/31Pd/c9r+qKr69ene/4+mN4pgMFV1TVW9qap+fnqs33egqt5TVW+tqjdX1Rumbe73O1BVD6qqn6qqf1FV76iqL9X346uqz52u97s/PlxVz9f3+1BVf2N6jfe2qnr59NrP7/vO7CJgqqprkvxwkqckeXSSr6+qRx+3VSzox5M8+R7bXpDkNa2165K8ZnrMeD6W5L9trT06yeOSfPN0rev/8X00yRNaa1+Y5PokT66qxyX5W0l+oLX2p5L8dpJnH7GNLOd5Sd5x6rF+34+vbK1df+qtqt3v9+EHk/xia+1PJ/nCnFz/+n5wrbV3Ttf79Ukek+QjSX42+n54VfWwJN+S5IbW2n+Skzf+emb8vu/OLgKmJI9Ncltr7d2ttX+X5BVJnn7kNrGQ1tqvJPngPTY/PclLps9fkuRrV20Uq2it3dlae+P0+e/m5AXnw6L/h9dO/N708P7TR0vyhCQ/NW3X9wOqqocn+eokPzo9ruj3PXO/H1xV/fEkX5HkRUnSWvt3rbXfib7fmz+b5F+11t4bfb8X90vygKq6X5JPSXJn/L7vzl4Cpocled+px7dP29iPh7bW7pw+f3+Shx6zMSyvqh6Z5IuS/Hr0/y5M06TenOSuJK9O8q+S/E5r7WPTl7j3j+nvJPnvk/zh9PjTo9/3oiX5p1V1S1U9Z9rmfj++RyX5N0l+bJoa+6NV9cDo+715ZpKXT5/r+8G11u5I8n1JfjMnwdKHktwSv++7s5eACT6unbx1ordPHFhVfWqSn07y/Nbah08/p//H1Vr7g2nY/MNzMnL1Tx+5SSysqv5ckrtaa7ccuy0cxZe31r44J0sgfHNVfcXpJ93vh3W/JF+c5Edaa1+U5P/NPaZE6fuxTevsPC3J/37P5/T9mKZ1tZ6ek4D5M5M8MJ+4JAod2EvAdEeSR5x6/PBpG/vxgaq6Nkmmf+86cntYSFXdPyfh0ktbaz8zbdb/OzJNlXhtki9N8qBpKHXi3j+iL0vytKp6T06mvz8hJ2uz6PcdmP6indbaXTlZh+Wxcb/fg9uT3N5a+/Xp8U/lJHDS9/vxlCRvbK19YHqs78f3nyf5jdbav2mt/X6Sn8nJawC/7zuzl4Dp9Umum1aZ/6ScDKm8+chtYl03J7lx+vzGJD93xLawkGntlRcleUdr7ftPPaX/B1dVf6KqHjR9/oAkT8zJGlyvTfKXpi/T94NprX1ba+3hrbVH5uR3+z9rrf2X0e/Dq6oHVtWn3f15kicleVvc74fXWnt/kvdV1edOm/5skrdH3+/J1+ePpscl+n4PfjPJ46rqU6bX+3df937fd6ZORhGOr6qempN1Gq5J8uLW2vccuUkspKpenuTxST4jyQeSfEeSf5zklUn+ZJL3JnlGa+2eC4GzcVX15Ul+Nclb80frsXx7TtZh0v8Dq6ovyMnijtfk5I8nr2ytfVdVfVZORrY8JMmbkvxXrbWPHq+lLKWqHp/kv2ut/Tn9Pr6pj392eni/JC9rrX1PVX163O+HV1XX52Rh/09K8u4kz8p074++H9oUKP9mks9qrX1o2ua634Gq+ptJvi4n7xr9piT/dU7WXPL7viO7CZgAAAAAWMZepsgBAAAAsBABEwAAAABXRcAEAAAAwFURMAEAAABwVQRMAAAAAFwVARMAMLyq+vSqevP08f6qumP6/Peq6u8v9H8+v6q+afr8l6vqhkv4mY+sqm+44Nf+g6r6sqr6L6rq1qr6w9NtqKonVtUtVfXW6d8nnHrul6rqwVfbXgBgPwRMAMDwWmv/T2vt+tba9Un+QZIfmB5/amvtr132/1dV90vyV5K87JJ/9COTXChgSvK4JL+W5G1J/kKSX7nH87+V5Gtaa5+f5MYkP3nquZ9McunHBQAYl4AJANitqnp8Vf389Pl3VtVLqupXq+q9VfUXqup/nkb4/GJV3X/6usdU1T+fRv28qqquvZcf/YQkb2ytfezUtm+cRk29raoeO/2sB1bVi6vqdVX1pqp6+rT9kVM73jh9/GfTz3hhkj8z/Zy/UVWfN33vm6vqLVV13fT9/3GSf9la+4PW2jtaa++8ZwNba29qrf3r6eGtSR5QVZ88Pb45yddfzbEFAPZFwAQA8Ec+Oyfh0NOS/G9JXjuN8Pm3Sb56Cpn+bpK/1Fp7TJIXJ/mee/k5X5bklnts+5RpBNVfm74vSf6HJP+stfbYJF+Z5H+pqgcmuSvJE1trX5zk65L80PT1L0jyq9Poqx9I8t8k+cHp596Q5Pbp656S5Bdn7PdfzEkg9tEkaa39dpJPrqpPn/EzAPj/27ufF62qOI7j7w8quNHcKCrhQiIFw6wWMiIucxEIkdgqJVq40UUQIgQiuHKZiH+AC9s5GKiIhCBEtcnxFyJitAhFwYVIJA3O18U9MsNlhhm5pqLv1+75nvOce+7u4cP3nEd6i81/1RuQJEl6jZytqvEkV4F5TIY0V+mOp60BPgDOJ6HNuTvNOiuAG73ajwBVdTHJ4iRLgE+BbUm+a3MWAquAO8DRJBuAJ8D7M+z3V+D7JO8CJ6vqVqtvBb6eywsnWQccbnuZ6j6wEngwl3UkSdLbzYBJkiRp0rMOnokk41VVrT5B97spwPWqGpllnX/pwqKpaprPAb7oH2FLchC4B3xI13H+eLqHVNWJJL8DnwFnkuymu3dpyZTjbzNqwdQosLOqbveGF7b3kCRJmpVH5CRJkubuJrA0yQhAkgWtA6jvBvBer/Zl+85m4GFVPQTOAXvT2qGSfNTmvgPcraoJ4Cu6TimAR8CiZwsmWQ38WVVHgFPAerqjdhdme5HWQXUa2F9Vv/TGAiwH/pptHUmSJDBgkiRJmrOq+g/YDhxOchkYAzZNM/UssKVXe5zkEt2/2H3TaoeABcCVJNfbZ4BjwK72jLXAP61+BXiS5HKSb4EdwLUkY3RH947Tu38pyedJ/gZGgNNJzrWhPXQh2IF2SfhYkmVt7BPgt94l5ZIkSTPKZOe3JEmSXpQko8C+Kfcivazn/gFsrKrxAWv8APxUVT+/uJ1JkqQ3mR1MkiRJ/4/9dJd9v1RV9fGQcKm5ZrgkSZKehx1MkiRJkiRJGsQOJkmSJEmSJA1iwCRJkiRJkqRBDJgkSZIkSZI0iAGTJEmSJEmSBjFgkiRJkiRJ0iAGTJIkSZIkSRrkKd8CGp0pbaSlAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "  if i ==10:\n",
        "    all_voices, length, nbr_voices, _ = sample_batched\n",
        "    all_voices_pr = all_voices[0,:,:,-1].numpy()\n",
        "    \n",
        "    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\n",
        "    print(note_array.shape)\n",
        "    print(note_array[:10])\n",
        "    print(note_array.dtype.names)\n",
        "\n",
        "    #print(i,nbr_voices,all_voices.shape)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "b4QCaMEi3nw7",
        "outputId": "9fbb103d-025e-4afe-d698-de65fd5a6908"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor i, sample_batched in enumerate(loader):\\n    all_voices, length, nbr_voices = sample_batched\\n    if nbr_voices ==3:\\n      print(i,nbr_voices,all_voices.shape)\\n    else:\\n      print(i,nbr_voices)\\n\\nfor i, sample_batched in enumerate(loader):\\n  if i ==10:\\n    all_voices, length, nbr_voices, _ = sample_batched\\n    all_voices_pr = all_voices[0,:,:,-1].numpy()\\n    \\n    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\\n    print(note_array.shape)\\n    print(note_array[:10])\\n    print(note_array.dtype.names)\\n\\n    #print(i,nbr_voices,all_voices.shape)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Music - Model\n"
      ],
      "metadata": {
        "id": "JNqxeacDwxNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UNET "
      ],
      "metadata": {
        "id": "QAIfIM69VHI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "XMdlm0_Vyyhc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_0 = []\n",
        "loss_1 = []\n",
        "loss_2 = []\n",
        "loss_3 = []\n",
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")                       # use weight parameters maybe take 1/88       \n",
        "\n",
        "    \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                      ### squeeze output here before returning                                       \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()\n",
        "\n",
        "        #print(\"nbr_voices\",nbr_voices)\n",
        "\n",
        "\n",
        "        if nbr_voices==4:\n",
        "            loss = self.loss(score_0, v0) +  self.loss(score_1, v1) +  self.loss(score_2, v2) +  self.loss(score_3, v3) #*1.5\n",
        "            \n",
        "            loss_0.append(self.loss(score_0, v0).cpu().detach().numpy())\n",
        "            loss_1.append(self.loss(score_1, v1).cpu().detach().numpy())\n",
        "            loss_2.append(self.loss(score_2, v2).cpu().detach().numpy())\n",
        "            loss_3.append(self.loss(score_3, v3).cpu().detach().numpy())\n",
        "            print(\"self.loss(score_0, v0)\",self.loss(score_0, v0).cpu().detach().numpy())\n",
        "            print(\"self.loss(score_1, v1)\",self.loss(score_1, v1).cpu().detach().numpy())\n",
        "            print(\"self.loss(score_2, v2)\",self.loss(score_2, v2).cpu().detach().numpy())\n",
        "            print(\"self.loss(score_3, v3)\",self.loss(score_3, v3).cpu().detach().numpy())\n",
        "            print(\"loss\",loss)      \n",
        "        \n",
        "        else:\n",
        "            loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) \n",
        "\n",
        "            loss_0.append(self.loss(score_0, v0).cpu().detach().numpy())\n",
        "            loss_1.append(self.loss(score_1, v1).cpu().detach().numpy())\n",
        "            loss_2.append(self.loss(score_2, v2).cpu().detach().numpy())\n",
        "            print(\"self.loss(score_0, v0)\",self.loss(score_0, v0).cpu().detach().numpy())\n",
        "            print(\"self.loss(score_1, v1)\",self.loss(score_1, v1).cpu().detach().numpy())\n",
        "            print(\"self.loss(score_2, v2)\",self.loss(score_2, v2).cpu().detach().numpy())\n",
        "            print(\"loss\",loss)\n",
        "        \n",
        "        return loss   #change also to matrix version\n",
        "        \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "            \n",
        "\n",
        "        else:\n",
        "            # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
        "            #predicted = scores_comb.argmax(dim=3)\n",
        "            #return np.squeeze(predicted.cpu().numpy())\n",
        "\n",
        "            sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "            #print(\"scores_comb\",scores_comb.shape)\n",
        "            #print(\"sentences[:,None,:,:]\",sentences[:,None,:,:].shape)\n",
        "            prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            \n",
        "            mask_pred = np.squeeze(sentences)== 0\n",
        "            v_pred_argm[mask_pred] = -1\n",
        "\n",
        "            return v_pred_argm \n",
        "                       "
      ],
      "metadata": {
        "id": "CviiPTPOPW04"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"CNN\"\n",
        "lr = 0.0001  \n",
        "monophonic = True\n",
        "his = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "79cPe11WL6J0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3a429ab-bfdf-48c7-ab5a-c8f8c379b1ae"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"CNN\"\\nlr = 0.0001  \\nmonophonic = True\\nhis = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Define Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                                   ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "                \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]             \n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "                    ### before\n",
        "                    #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                    #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                    #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                    #for i in range(len(prediction[0,:])):\n",
        "                    #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                    #  accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                    #prediction = model.predict(voices, lens, monophonic)                    #for voice vise masking\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)         #for mixed voice masking        \n",
        "\n",
        "\n",
        "                    ## ground truth in shape 1280x88 -> mixed voice\n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    truth = v_ori_argm       \n",
        "\n",
        "                    # outsource accurcy to further down -> just a placeholder right now\n",
        "                    v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            #train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            #train_acc_list[3] = accuracy_sum_list[3] / 18                        ## bc only 18 pieces with len 3\n",
        "            #train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "            #history[\"train_loss\"].append(train_loss)\n",
        "            #history[\"train_acc\"].append(train_acc_list)\n",
        "            #print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "            \n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "                        voices = voices.to(device).float()\n",
        "                        ### before\n",
        "                        #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                        #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                        #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                        #for i in range(len(prediction[0,:])):\n",
        "                        #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                        #  val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    #val_acc_list[3] = val_accuracy_sum_list[3] / 18                         ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_accuracy_sum_list[4] / 2                          ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                #history[\"val_acc\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        #prediction = model.predict(voices, lens, monophonic)                #for voice vise masking\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)     # for masking with mixed voice\n",
        "\n",
        "\n",
        "\n",
        "                        ## ground truth in shape 1280x88 -> mixed voice\n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        truth = v_ori_argm       \n",
        "\n",
        "                        # outsource accurcy to further down -> just a placeholder right now\n",
        "                        v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "                    \n",
        "                history[\"val_acc\"].append(val_accuracy)\n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    # save the model\n",
        "    #torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"RNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ge8pY70uHxF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30426481-eb7f-4bd5-e06a-0ba935517c3c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"RNN\"\\nmonophonic = True\\nhis = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= [\"CNN\",\"RNN\"]\n",
        "monophonic_list = [True,False]\n",
        "\n",
        "for net in network_type:\n",
        "    for monophonic in monophonic_list: \n",
        "        print(\"network set to:\",net,\"monophnic:\",monophonic)\n",
        "        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Bs6-iNEBu8o",
        "outputId": "d78aa49f-4aad-4ad5-96cb-53c7cafdcbac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= [\"CNN\",\"RNN\"]\\nmonophonic_list = [True,False]\\n\\nfor net in network_type:\\n    for monophonic in monophonic_list: \\n        print(\"network set to:\",net,\"monophnic:\",monophonic)\\n        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        if fugues == True:\n",
        "        ### uncomment for fugues ###\n",
        "            train_dataset = MusicDataset_new(PATH_TO_DATA) \n",
        "        ### uncomment for chorals ###\n",
        "        else:\n",
        "            train_dataset = MusicDataset_chor(PATH_TO_DATA) \n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        ### uncomment for fugues ###\n",
        "        if fugues == True:\n",
        "            dataset = MusicDataset_new(PATH_TO_DATA) \n",
        "        ### uncomment for chorals ###\n",
        "        else:\n",
        "            dataset = MusicDataset_chor(PATH_TO_DATA)\n",
        "        \n",
        "        \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicNetwork\n",
        "epochs = 10\n",
        "lr = 0.00001 # was 0.001\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Experiment"
      ],
      "metadata": {
        "id": "bdetlQP-LoRX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm"
      },
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_0,'-o')\n",
        "plt.plot(loss_1,'-o')\n",
        "plt.plot(loss_2,'-o')\n",
        "plt.plot(loss_3,'-o')\n",
        "plt.xlabel('sample')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['loss 0','loss 1','loss 2','loss 3'])\n",
        "plt.title('loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "UPfmfthBjSZw",
        "outputId": "7713751e-f9a8-4947-e1b6-6cb107ce9026"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy evalutaion F-scores"
      ],
      "metadata": {
        "id": "sJbWsH72N2Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. create folder with part object of all pieces \n",
        "2. load a piece from dataloader with true labels, the mixed piece and the part object \n",
        "3. create notearray from part object\n",
        "4. take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "\n",
        "Output: pianoroll\n",
        "\n",
        "1 note in notearray could be mulitple bins\n",
        "\n",
        "take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "note start at same time with different pitch -> different notes\n",
        "\n",
        "for each note array find corresponding matrix -> \n",
        "\n",
        "\n",
        "if note is only composed by 1 bin: save indx of vocie -> save it to note array\n",
        "\n",
        "if more than 1: look what are idx that compose this note -> majority note -> save it for the note array (if its 50/50 take it random -> count how often this happens) \n",
        "\n",
        "\n",
        "with idx : in note_array find which note corresponds to what voice"
      ],
      "metadata": {
        "id": "CFClch37N6nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader "
      ],
      "metadata": {
        "id": "afYHFVNMlMnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da23d5e-fbcf-49cb-8e90-4808298d209c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fcd6cbb14d0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## makes cell output nothing\n",
        "%%capture  \n",
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch10.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4TAhTQcpmx8m"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dic with key:filename, val: part_obj  for fugues"
      ],
      "metadata": {
        "id": "5RVmMv6Q9CJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if PATH_TO_DATA == \"AI-MA_project/bach_pr_fugues\":\n",
        "    path_parts = \"AI-MA_project/bach_fugues\"\n",
        "    part_dic = {}\n",
        "\n",
        "    #### create a list with all filenames in the right order ####\n",
        "    file_names_part = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.mid'): continue\n",
        "        file_names_part.append(filename[3:7])\n",
        "    #print(file_names_part)\n",
        "\n",
        "    #### create a list with all part objects in the right order ####\n",
        "    part_list = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.mid'): continue\n",
        "        fullname = os.path.join(path_parts, filename)\n",
        "        part = partitura.load_score_midi(fullname)\n",
        "        part_list.append(part)\n",
        "    #print(part_list)\n",
        "\n",
        "    #### create a dict with keys:filenames , values: part object ####\n",
        "    for i in range(len(file_names_part)):\n",
        "        part_dic[file_names_part[i]] = part_list[i]\n",
        "    \n",
        "    print(part_dic.keys(),part_dic.values())"
      ],
      "metadata": {
        "id": "_XYM_KWu2qkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6175415f-6b9b-4b50-a32d-837ffb9d4f8f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=76 velocity=64 time=30\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=78 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=119\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=419\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=72 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=43 velocity=64 time=360\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=299\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=60 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=77 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=75 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=80 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=73 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=68 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=71 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=59 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=56 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=58 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=61 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=60\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=55 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=389\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=2 note=49 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=67 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=66 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=359\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['1f01', '1f02', '1f03', '1f04', '1f05', '1f06', '1f07', '1f08', '1f09', '1f10', '1f11', '1f12', '1f13', '1f14', '1f15', '1f16', '1f17', '1f18', '1f19', '1f20', '1f21', '1f22', '1f23', '1f24', '2f01', '2f02', '2f03', '2f04', '2f05', '2f06', '2f07', '2f08', '2f09', '2f10', '2f11', '2f12', '2f13', '2f14', '2f15', '2f16', '2f17', '2f18', '2f19', '2f20', '2f21', '2f22', '2f23', '2f24']) dict_values([[<partitura.score.Part object at 0x7fcc9623a6d0>, <partitura.score.Part object at 0x7fcc96ba88d0>, <partitura.score.Part object at 0x7fcc960ecb50>, <partitura.score.Part object at 0x7fcc96041410>], [<partitura.score.Part object at 0x7fcc96323150>, <partitura.score.Part object at 0x7fcc90e075d0>, <partitura.score.Part object at 0x7fcc90d5b350>], [<partitura.score.Part object at 0x7fcc90a438d0>, <partitura.score.Part object at 0x7fcc96bd1990>, <partitura.score.Part object at 0x7fcc908a8050>], [<partitura.score.Part object at 0x7fcc90629bd0>, <partitura.score.Part object at 0x7fcc90381390>, <partitura.score.Part object at 0x7fcc9027b090>, <partitura.score.Part object at 0x7fcc9013a710>, <partitura.score.Part object at 0x7fcc7dfa0410>], [<partitura.score.Part object at 0x7fcc90ec38d0>, <partitura.score.Part object at 0x7fcc7dd0c450>, <partitura.score.Part object at 0x7fcc7dcba090>, <partitura.score.Part object at 0x7fcc7de132d0>], [<partitura.score.Part object at 0x7fcc90f2bf50>, <partitura.score.Part object at 0x7fcc7dacd090>, <partitura.score.Part object at 0x7fcc7da29ad0>], [<partitura.score.Part object at 0x7fcc90a40210>, <partitura.score.Part object at 0x7fcc7d76d210>, <partitura.score.Part object at 0x7fcc7d6342d0>], [<partitura.score.Part object at 0x7fcc7d3c7210>, <partitura.score.Part object at 0x7fcc7d399090>, <partitura.score.Part object at 0x7fcc7d092450>], [<partitura.score.Part object at 0x7fcc7d36a6d0>, <partitura.score.Part object at 0x7fcc7ce32850>, <partitura.score.Part object at 0x7fcc7cd40350>], [<partitura.score.Part object at 0x7fcc7dc0f210>, <partitura.score.Part object at 0x7fcc90ed6610>], [<partitura.score.Part object at 0x7fcc7cc7c150>, <partitura.score.Part object at 0x7fcc7c7b6c10>, <partitura.score.Part object at 0x7fcc7c6a63d0>], [<partitura.score.Part object at 0x7fcc7c57b990>, <partitura.score.Part object at 0x7fcc7c257050>, <partitura.score.Part object at 0x7fcc7c409f10>, <partitura.score.Part object at 0x7fcc7c4a4550>], [<partitura.score.Part object at 0x7fcc90f12350>, <partitura.score.Part object at 0x7fc9ade7fc10>, <partitura.score.Part object at 0x7fc9add18090>], [<partitura.score.Part object at 0x7fc9adb41a90>, <partitura.score.Part object at 0x7fc9ada22050>, <partitura.score.Part object at 0x7fc9adb5cfd0>, <partitura.score.Part object at 0x7fc9ad8d11d0>], [<partitura.score.Part object at 0x7fc9ad77e690>, <partitura.score.Part object at 0x7fc9ad6c1550>, <partitura.score.Part object at 0x7fc9ad2e17d0>], [<partitura.score.Part object at 0x7fcc7c5b80d0>, <partitura.score.Part object at 0x7fc9acf8d0d0>, <partitura.score.Part object at 0x7fc9acf67350>, <partitura.score.Part object at 0x7fc9acefa7d0>], [<partitura.score.Part object at 0x7fcc7c5b52d0>, <partitura.score.Part object at 0x7fc9acc99e50>, <partitura.score.Part object at 0x7fc9acc1f190>, <partitura.score.Part object at 0x7fc9acb7ead0>], [<partitura.score.Part object at 0x7fc9ad784410>, <partitura.score.Part object at 0x7fc9ac847690>, <partitura.score.Part object at 0x7fc9ac7b8050>, <partitura.score.Part object at 0x7fc9ac6a9950>], [<partitura.score.Part object at 0x7fc9ac449510>, <partitura.score.Part object at 0x7fc9ac53fd10>, <partitura.score.Part object at 0x7fc9ac28dd90>], [<partitura.score.Part object at 0x7fc9abfef290>, <partitura.score.Part object at 0x7fc9abf34090>, <partitura.score.Part object at 0x7fcc7c559050>, <partitura.score.Part object at 0x7fc9ab9c9610>], [<partitura.score.Part object at 0x7fc9ac07e710>, <partitura.score.Part object at 0x7fc9ab724e10>, <partitura.score.Part object at 0x7fc9ab542690>], [<partitura.score.Part object at 0x7fc9ac542710>, <partitura.score.Part object at 0x7fc9ab3d5ad0>, <partitura.score.Part object at 0x7fc9ab2e7a10>, <partitura.score.Part object at 0x7fc9ab1963d0>, <partitura.score.Part object at 0x7fc9ab0c7610>], [<partitura.score.Part object at 0x7fc9adebcf10>, <partitura.score.Part object at 0x7fc9aaf54ed0>, <partitura.score.Part object at 0x7fc9aae530d0>, <partitura.score.Part object at 0x7fc9aadbf050>], [<partitura.score.Part object at 0x7fc9aab0ad10>, <partitura.score.Part object at 0x7fc9aaaa6090>, <partitura.score.Part object at 0x7fc9aa778110>, <partitura.score.Part object at 0x7fc9aa570050>], [<partitura.score.Part object at 0x7fc9ac074b50>, <partitura.score.Part object at 0x7fc9aa360ed0>, <partitura.score.Part object at 0x7fc9aa22df90>], [<partitura.score.Part object at 0x7fc9ac06c790>, <partitura.score.Part object at 0x7fc9a9f0c390>, <partitura.score.Part object at 0x7fc9a9e87310>, <partitura.score.Part object at 0x7fc9a9dce850>], [<partitura.score.Part object at 0x7fc9a9c951d0>, <partitura.score.Part object at 0x7fc9a9c34fd0>, <partitura.score.Part object at 0x7fc9a9b1d810>], [<partitura.score.Part object at 0x7fc9a97761d0>, <partitura.score.Part object at 0x7fc9a98458d0>, <partitura.score.Part object at 0x7fc9a95d8450>], [<partitura.score.Part object at 0x7fc9a98632d0>, <partitura.score.Part object at 0x7fc9a91c4050>, <partitura.score.Part object at 0x7fc9a91bc8d0>, <partitura.score.Part object at 0x7fc9a90b4550>], [<partitura.score.Part object at 0x7fc9a8effe90>, <partitura.score.Part object at 0x7fc9a8e05b10>, <partitura.score.Part object at 0x7fc9a8d6e090>], [<partitura.score.Part object at 0x7fc9ad7b3590>, <partitura.score.Part object at 0x7fc9a8a47b90>, <partitura.score.Part object at 0x7fc9a89ff290>, <partitura.score.Part object at 0x7fc9a88ce150>], [<partitura.score.Part object at 0x7fc9a873c610>, <partitura.score.Part object at 0x7fc9a8788210>, <partitura.score.Part object at 0x7fc9ac07ef90>, <partitura.score.Part object at 0x7fc9a8685610>], [<partitura.score.Part object at 0x7fc9a8411550>, <partitura.score.Part object at 0x7fc9a8328a90>, <partitura.score.Part object at 0x7fc9a83fda10>, <partitura.score.Part object at 0x7fc9a8226e90>], [<partitura.score.Part object at 0x7fc9a7f43090>, <partitura.score.Part object at 0x7fc9a7c43190>, <partitura.score.Part object at 0x7fc9a7a4ca10>], [<partitura.score.Part object at 0x7fc9a80efdd0>, <partitura.score.Part object at 0x7fc9a77aa9d0>, <partitura.score.Part object at 0x7fc9a7507a90>], [<partitura.score.Part object at 0x7fc9a840c410>, <partitura.score.Part object at 0x7fc9a873cc90>, <partitura.score.Part object at 0x7fc9a6fe9bd0>], [<partitura.score.Part object at 0x7fc9a6d37d50>, <partitura.score.Part object at 0x7fc9a6c82cd0>, <partitura.score.Part object at 0x7fc9a6b76410>], [<partitura.score.Part object at 0x7fc9a6840910>, <partitura.score.Part object at 0x7fc9a6802410>, <partitura.score.Part object at 0x7fc9a6566110>], [<partitura.score.Part object at 0x7fc9a667ded0>, <partitura.score.Part object at 0x7fc9a6e7ded0>, <partitura.score.Part object at 0x7fc9a633b090>], [<partitura.score.Part object at 0x7fc9a618f490>, <partitura.score.Part object at 0x7fc9a5e8a090>, <partitura.score.Part object at 0x7fc9a5d3a8d0>, <partitura.score.Part object at 0x7fc9a5b15d50>], [<partitura.score.Part object at 0x7fc9a6262810>, <partitura.score.Part object at 0x7fc9a8856cd0>, <partitura.score.Part object at 0x7fc9a55fb910>, <partitura.score.Part object at 0x7fc9a5450190>], [<partitura.score.Part object at 0x7fc9a62633d0>, <partitura.score.Part object at 0x7fc9a51fd490>, <partitura.score.Part object at 0x7fc9a4dbeb90>], [<partitura.score.Part object at 0x7fc9a52b2c10>, <partitura.score.Part object at 0x7fc9a4a0cfd0>, <partitura.score.Part object at 0x7fc9a49aead0>], [<partitura.score.Part object at 0x7fc9a72ccfd0>, <partitura.score.Part object at 0x7fc9a49888d0>, <partitura.score.Part object at 0x7fc9a4653b50>], [<partitura.score.Part object at 0x7fc9a44252d0>, <partitura.score.Part object at 0x7fc9a43bd090>, <partitura.score.Part object at 0x7fc9a4233090>], [<partitura.score.Part object at 0x7fc9a3e41550>, <partitura.score.Part object at 0x7fc9a3cd7f50>, <partitura.score.Part object at 0x7fc9a3dbc410>, <partitura.score.Part object at 0x7fc9a5230990>], [<partitura.score.Part object at 0x7fc9a44f9550>, <partitura.score.Part object at 0x7fc9a3fc40d0>, <partitura.score.Part object at 0x7fc9a3732210>, <partitura.score.Part object at 0x7fc9a3547090>], [<partitura.score.Part object at 0x7fc9a33033d0>, <partitura.score.Part object at 0x7fc9a33b80d0>, <partitura.score.Part object at 0x7fc9a310b3d0>]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dic with key:filename, val: part_obj  for chorales"
      ],
      "metadata": {
        "id": "6D9oTp_lNQbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if PATH_TO_DATA == \"AI-MA_project/pianoroll_88\":\n",
        "    path_parts = \"AI-MA_project/chorales_converted\"\n",
        "    part_dic = {}\n",
        "\n",
        "    #### create a list with all filenames in the right order ####\n",
        "    file_names_part = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.xml'): continue\n",
        "        file_names_part.append(filename[4:7])\n",
        "    #print(file_names_part)\n",
        "\n",
        "    #### create a list with all part objects in the right order ####\n",
        "    part_list = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.xml'): continue\n",
        "        fullname = os.path.join(path_parts, filename)\n",
        "        part = partitura.load_musicxml(fullname)\n",
        "        part_list.append(part)\n",
        "    #print(part_list)\n",
        "\n",
        "    #### create a dict with keys:filenames , values: part object ####\n",
        "    for i in range(len(file_names_part)):\n",
        "        part_dic[file_names_part[i]] = part_list[i]\n",
        "    \n",
        "    print(\"part_dic.keys()\",part_dic.keys())\n",
        "    print(\"part_dic.values()\",part_dic.values())"
      ],
      "metadata": {
        "id": "_4q58c16NjbE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate chorales"
      ],
      "metadata": {
        "id": "yphGmsr-NSV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate chorals"
      ],
      "metadata": {
        "id": "v4TJGKiUs086"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_one_choral(model, train_dataloader, part_dic,F1):\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    note_counter_0 = 0\n",
        "    note_counter_1 = 0\n",
        "    note_counter_2 = 0\n",
        "    note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match                                      \n",
        "            \n",
        "            #if idx > 40: # or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                note_array_3 = part_3.note_array\n",
        "\n",
        "                note_counter_0 += len(note_array_0)\n",
        "                note_counter_1 += len(note_array_1)\n",
        "                note_counter_2 += len(note_array_2)\n",
        "                note_counter_3 += len(note_array_3)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                   \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]                   \n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        \n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "                        else:\n",
        "                            accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                        counting = 0\n",
        "                        ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                        for j in range(len(total_predictions_dict[i])):\n",
        "                            if total_predictions_dict[i][j][0] == gt:\n",
        "                                counting +=1  \n",
        "                        count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                    acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                    print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                    \n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "                    acc_score_dict[\"3\"].append(acc_3)\n",
        "                \n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    print(\"f1_v0 , sample {}:\".format(idx),f1_v0)\n",
        "                    print(\"f1_v1 , sample {}:\".format(idx),f1_v1)\n",
        "                    print(\"f1_v2 , sample {}:\".format(idx),f1_v2)\n",
        "                    \n",
        "                    if len(part)==4:\n",
        "                        pred_3 = accordance_dict[\"3\"]\n",
        "                        truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                        f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                        f_score_dict[\"3\"].append(f1_v3)\n",
        "                        print(\"f1_v3 , sample {}:\".format(idx),f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])\n"
      ],
      "metadata": {
        "id": "UXr2DeiLSyLm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_one_choral(model,val_dataloader,part_dic,F1=False)\n",
        "    acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "ghAmXcXTTtD1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    plt.plot(acc_score_dict[\"0\"],'-o')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend(['Accuracy0'])\n",
        "    plt.title('Accuracy vs Epochs')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "J92mdaQG0MXZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate on F1 meassure"
      ],
      "metadata": {
        "id": "zYMy2JARxEBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    acc_0 , acc_1, acc_2, acc_3 = evaluate_one_choral(model,val_dataloader,part_dic,F1=True)\n",
        "    acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "RWxVG3XAYTcC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate fugues"
      ],
      "metadata": {
        "id": "TzTpXHznL02j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import statistics\n",
        "\n",
        "\n",
        "def evaluate_accuracy_for_all(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    note_counter_0 = 0\n",
        "    note_counter_1 = 0\n",
        "    note_counter_2 = 0\n",
        "    note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "                                      \n",
        "                print(\"nbr_voices:\",nbr_voices)\n",
        "            #if idx == 0 or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "\n",
        "                note_counter_0 += len(note_array_0)\n",
        "                note_counter_1 += len(note_array_1)\n",
        "                note_counter_2 += len(note_array_2)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "                if len(part)== 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = part_3.note_array\n",
        "                    note_counter_3 += len(note_array_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                   \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        \n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "                            \n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                      counting = 0\n",
        "                      ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                      for j in range(len(total_predictions_dict[i])):\n",
        "                          if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                      count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "\n",
        "                    if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    print(\"f1_v0 , sample {}:\".format(idx),f1_v0)\n",
        "                    print(\"f1_v1 , sample {}:\".format(idx),f1_v1)\n",
        "                    print(\"f1_v2 , sample {}:\".format(idx),f1_v2)\n",
        "                    if len(part)==4:\n",
        "                      pred_3 = accordance_dict[\"3\"]\n",
        "                      truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                      f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                      f_score_dict[\"3\"].append(f1_v3)\n",
        "                      print(\"f1_v3 , sample {}:\".format(idx),f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])\n",
        "        #return total_predictions_dict, total_truth_dict"
      ],
      "metadata": {
        "id": "0xbN5YU8nGT0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, train_dataloader, part_dic,F1):\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader): \n",
        "        print(\"nbr_voices:\",nbr_voices)\n",
        "\n",
        "test(model,val_dataloader,part_dic,F1=False)"
      ],
      "metadata": {
        "id": "p0HZ9d5TO_Aj",
        "outputId": "3ba11800-f2d1-427a-cf80-d5951d279071",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nbr_voices: tensor([3])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([3])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True:\n",
        "    dict_pred , dict_gt, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "20MP5Gk5kc2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5256aae-51b5-4d6c-fbb9-26be763832b5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nbr_voices: tensor([3])\n",
            "acc 0, sample 0: 0.7274881516587678\n",
            "acc 1, sample 0: 0.39293598233995586\n",
            "acc 2, sample 0: 1.0\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 1: 0.8238095238095238\n",
            "acc 1, sample 1: 0.6020942408376964\n",
            "acc 2, sample 1: 0.8481012658227848\n",
            "acc 3, sample 1: 0.0\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 2: 0.8014705882352942\n",
            "acc 1, sample 2: 0.5152224824355972\n",
            "acc 2, sample 2: 0.8178571428571428\n",
            "acc 3, sample 2: 0.0\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 3: 0.7254098360655737\n",
            "acc 1, sample 3: 0.743801652892562\n",
            "acc 2, sample 3: 0.8285714285714286\n",
            "acc 3, sample 3: 0.0\n",
            "nbr_voices: tensor([3])\n",
            "acc 0, sample 4: 0.7029702970297029\n",
            "acc 1, sample 4: 0.4150197628458498\n",
            "acc 2, sample 4: 0.9960629921259843\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 5: 0.7735849056603774\n",
            "acc 1, sample 5: 0.5220729366602687\n",
            "acc 2, sample 5: 0.8470588235294118\n",
            "acc 3, sample 5: 0.0\n",
            "nbr_voices: tensor([3])\n",
            "acc 0, sample 6: 0.760797342192691\n",
            "acc 1, sample 6: 0.40336134453781514\n",
            "acc 2, sample 6: 0.9973333333333333\n",
            "note counters: v0: 2282 v1: 2323 v2: 2236 v3: 1238\n",
            "total_predictions_dict dict_keys(['0', '1', '2', '3'])\n",
            "0.7593615206645615 0.5135012003642493 0.904997855177155 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "H9iWhIkMNjmp",
        "outputId": "f20a0f8f-7b9b-49be-90dc-01e27850f5d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7593615206645615, 0.5135012003642493, 0.904997855177155, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss3 * 1.5 = (0.8989219661919758, 0.7166572856993837, 0.8185917288474246, 0.0)"
      ],
      "metadata": {
        "id": "6nwnRTADQvQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 epoch, no loss modifier:\n",
        "#ACC:(0.9165209182020722,\n",
        "# 0.7864434689151618,\n",
        "# 0.8130949796045199,\n",
        "# 0.003652274754166715)\n",
        "\n",
        "# 20 epoch, no loss modifier:\n",
        "#(0.7962210840410273, 0.8669639629052727, 0.751302181991106, 0.0)\n",
        "\n",
        "# 20 ep, loss3 *1,5\n",
        "#(0.8721136343927623,\n",
        "# 0.8319586824413445,\n",
        "# 0.7563218924966578,\n",
        "# 0.09029535181592076)"
      ],
      "metadata": {
        "id": "6mCYnJLnHfYB"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate fugues F1 score"
      ],
      "metadata": {
        "id": "52P6em3ANb1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True:\n",
        "    f1_v0, f1_v1, f1_v2, f1_v3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=True)\n",
        "    print(f1_v0, f1_v1, f1_v2, f1_v3)"
      ],
      "metadata": {
        "id": "FLLmyO6o5vW5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "3c13289a-cf60-4365-b9fe-d71f1b347e89"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nbr_voices: tensor([3])\n",
            "f1_v0 , sample 0: 0.7477744807121661\n",
            "f1_v1 , sample 0: 0.7185289957567185\n",
            "f1_v2 , sample 0: 1.0\n",
            "nbr_voices: tensor([4])\n",
            "f1_v0 , sample 1: 0.8365650969529085\n",
            "f1_v1 , sample 1: 0.767741935483871\n",
            "f1_v2 , sample 1: 0.9153318077803204\n",
            "f1_v3 , sample 1: 1.0\n",
            "nbr_voices: tensor([4])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-580096e20ee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfugues\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf1_v0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_v1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_v3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_accuracy_for_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpart_dic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_v0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_v1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_v3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-da2541c71bea>\u001b[0m in \u001b[0;36mevaluate_accuracy_for_all\u001b[0;34m(model, train_dataloader, part_dic, F1)\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mmonophonic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonophonic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mground_truth_label_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mel_note_arr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-70ea2d067005>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentences, sentences_len, monophonic)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Compute the outputs from the linear units.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mscores_comb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmonophonic\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-70ea2d067005>\u001b[0m in \u001b[0;36mcompute_outputs\u001b[0;34m(self, sentences, sentences_len)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m                      \u001b[0;31m### squeeze output here before returning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-69b725fc1feb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mconcatenated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdouble_conv_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcatenated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    443\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 444\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "for gt, i in enumerate(dict_pred.keys()):\n",
        "  counting = 0\n",
        "  for j in range(len(dict_pred[i])):\n",
        "      if dict_pred[i][j][0] == gt:\n",
        "        print(dict_pred[i][j],dict_gt[i][j])\n",
        "        counting +=1\n",
        "\n",
        "      \n",
        "  count_dict_2[i].append(counting)\n",
        "\n",
        "print(count_dict_2[\"0\"],count_dict_2[\"1\"],count_dict_2[\"2\"],count_dict_2[\"3\"])\n",
        "\n",
        "if len(dict_pred.keys())==4:\n",
        "    print(\"accuracy:\",count_dict_2[\"0\"][0]/len(dict_pred[\"0\"]),count_dict_2[\"1\"][0]/len(dict_pred[\"1\"]),count_dict_2[\"2\"][0]/len(dict_pred[\"2\"]),count_dict_2[\"3\"][0]/len(dict_pred[\"3\"]))\n",
        "\n",
        "if len(dict_pred.keys())==3:\n",
        "    print(\"accuracy:\",count_dict_2[\"0\"][0]/len(dict_pred[\"0\"]),count_dict_2[\"1\"][0]/len(dict_pred[\"1\"]),count_dict_2[\"2\"][0]/len(dict_pred[\"2\"]))"
      ],
      "metadata": {
        "id": "VYBpV_hMfGGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### take 0 -> compare to truth of 0,1,2,3 -> overall voice\n",
        "\n",
        "count_list = []\n",
        "\n",
        "count_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "truth_dic = {'0': 0, '1': 1, '2': 2, '3': 3 }\n",
        "\n",
        "voice_entry_list = [\"0\", \"1\", \"2\", \"3\"]\n",
        "for voice_entry_one in voice_entry_list:\n",
        "    for voice_entry_two in voice_entry_list:\n",
        "        count_list = []\n",
        "        #print(\"voices:\",voice_entry_one,voice_entry_two)\n",
        "        for i in range(len(dict_pred[voice_entry_one])):\n",
        "            if dict_pred[voice_entry_one][i][0] == truth_dic[voice_entry_two]:      #dict_truth[voice_entry_two][i][0]:\n",
        "                count_list.append(1)\n",
        "            else:\n",
        "                count_list.append(0)\n",
        "        count_dict[voice_entry_one].append(count_list)\n",
        "\n",
        "dictionary_sum={}\n",
        "for i in voice_entry_list:\n",
        "    v0_match,v1_match,v2_match,v3_match = count_dict[i]\n",
        "    sum_v0 = np.sum(v0_match)\n",
        "    sum_v1 = np.sum(v1_match)\n",
        "    sum_v2 = np.sum(v2_match)\n",
        "    sum_v3 = np.sum(v3_match)\n",
        "    dictionary_sum[\"v0\"] = sum_v0\n",
        "    dictionary_sum[\"v1\"] = sum_v1\n",
        "    dictionary_sum[\"v2\"] = sum_v2\n",
        "    dictionary_sum[\"v3\"] = sum_v3\n",
        "\n",
        "    val_list = list(dictionary_sum.values())\n",
        "    \n",
        "    print(\"voice{} matches with\".format(i))\n",
        "    print(\"dict\",dictionary_sum)\n",
        "\n",
        "    max_sum = max(sum_v0,sum_v1,sum_v2,sum_v3)\n",
        "\n",
        "\n",
        "    print(\"max_sum\", val_list.index(max_sum) )\n",
        "\n",
        "    print(\"accuracy voice{}:\".format(i), max_sum/(sum_v0+sum_v1+sum_v2+sum_v3) )\n",
        "    print(\"________________\")\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "BoQcV_i038DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR MONOPHONIC F1\n",
        "\n",
        "# start with GT\n",
        "# look at first note in pred-> save note label\n",
        "# look at second note in pred-> if same note as before : SUCESS if it is not: FAIL\n",
        " # DO This for all 4 voices\n",
        " ## in GT there is always the same voice following -> would always be an array of 1\n",
        "\n",
        "## POLYPHONIC \n",
        "\n",
        "# prbl after 1 note there can be multiple diff voices .. chords"
      ],
      "metadata": {
        "id": "-bR7gcej90qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you have the ground truth on the different parts that you get when you import your score. Each part correspond to a voice. So if your note array contains all notes of all voices, you have for each note in your note array a number that is the ground truth voice (that you take from the part) and a number that is the predicted voice (that you take from the maximum vote)."
      ],
      "metadata": {
        "id": "Z5q305YzvjMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start time, duration , pitch to separate \n",
        "\n",
        "use the onset_beat and duration_beat\n",
        "\n",
        "multiply them according to the values set when producing the pianorolls \n",
        "\n",
        "-> get the position in the pianoroll\n",
        "\n",
        "time_div = 12\n",
        "\n"
      ],
      "metadata": {
        "id": "EmvxtyaVKG27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "A94mchm4LV6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy_v0\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v1\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v2\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v3\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Accuracy0','Accuracy1','Accuracy2','Accuracy3'])\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TgJDHaxAgYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend('Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxMs8GEfMvPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"train_loss\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqMcJT5aFL01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chew Method\n",
        "\n"
      ],
      "metadata": {
        "id": "fS4tzYkxr06Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_chew_pr (file_name, sentences):\n",
        "    path = \"AI-MA_project/chorales_converted/\"\n",
        "    fullname = os.path.join(path, \"chor\"+ file_name +\".xml\")\n",
        "    part = partitura.load_musicxml(fullname)\n",
        "        \n",
        "\n",
        "    ### apply chews method ### \n",
        "    chew_sep = partitura.musicanalysis.estimate_voices(part, monophonic_voices=True)\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_zero = np.where(chew_sep==1)\n",
        "    pos_one = np.where(chew_sep==2)\n",
        "    pos_two = np.where(chew_sep==3)\n",
        "    pos_three = np.where(chew_sep==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    part_zero = partitura.utils.ensure_notearray(part)[pos_zero]\n",
        "    part_one = partitura.utils.ensure_notearray(part)[pos_one]\n",
        "    part_two = partitura.utils.ensure_notearray(part)[pos_two]\n",
        "    part_three = partitura.utils.ensure_notearray(part)[pos_three]\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    pr_zero = partitura.utils.compute_pianoroll(part_zero, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(part_one, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(part_two, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    pr_three = partitura.utils.compute_pianoroll(part_three, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_three = pr_three.toarray()\n",
        "    \n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    #print(\"scores_comb.shape\",scores_comb.shape)\n",
        "    #print(\"sentences[:,None,:,:].shape\",sentences[:,None,:,:].shape)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "    \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "55-Dy39Cwg5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_chew(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    note_counter_0 = 0\n",
        "    note_counter_1 = 0\n",
        "    note_counter_2 = 0\n",
        "    note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx != 26 and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                note_array_3 = part_3.note_array\n",
        "\n",
        "                note_counter_0 += len(note_array_0)\n",
        "                note_counter_1 += len(note_array_1)\n",
        "                note_counter_2 += len(note_array_2)\n",
        "                note_counter_3 += len(note_array_3)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################\n",
        "\n",
        "                    \n",
        "                    \n",
        "                    prediction = calculate_chew_pr(file_name,voices[:,:,:,-1]) \n",
        "\n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                #print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                #print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                #print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                #print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "NVvLm9pdryYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sampel 26&27 dont work "
      ],
      "metadata": {
        "id": "yD0w6nJQ82iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_chew(model,val_dataloader,part_dic,F1=False)\n",
        "acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "UD11ziChvL_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# McLeod Method"
      ],
      "metadata": {
        "id": "9ejZujau-TXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mcleod_pr (file_name, sentences):\n",
        "    path = \"AI-MA_project/hmm_sep/\"\n",
        "    \n",
        "    fullname = os.path.join(path, \"chor\"+ file_name +\".mid\")\n",
        "        \n",
        "\n",
        "    ### apply chews method ### \n",
        "    part_hmm = partitura.load_score_midi(fullname,part_voice_assign_mode=2)\n",
        "    voice_info = partitura.utils.note_array_from_part(part_hmm)[\"voice\"]\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(voice_info==1)\n",
        "    pos_1 = np.where(voice_info==2)\n",
        "    pos_2 = np.where(voice_info==3)\n",
        "    pos_3 = np.where(voice_info==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    note_array_0= partitura.utils.ensure_notearray(part_hmm)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part_hmm)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part_hmm)[pos_2]\n",
        "    note_array_3 = partitura.utils.ensure_notearray(part_hmm)[pos_3]\n",
        "\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_three = pr_three.toarray()\n",
        "    \n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    #print(\"scores_comb.shape\",scores_comb.shape)\n",
        "    #print(\"sentences[:,None,:,:].shape\",sentences[:,None,:,:].shape)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "    \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "pQP8pq9a-S6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mc_leod(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    note_counter_0 = 0\n",
        "    note_counter_1 = 0\n",
        "    note_counter_2 = 0\n",
        "    note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx not in [16,17,18,27,28,32,44,45,48,49,50]: # and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                note_array_3 = part_3.note_array\n",
        "\n",
        "                note_counter_0 += len(note_array_0)\n",
        "                note_counter_1 += len(note_array_1)\n",
        "                note_counter_2 += len(note_array_2)\n",
        "                note_counter_3 += len(note_array_3)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################                   \n",
        "                    prediction = calculate_mcleod_pr(file_name,voices[:,:,:,-1]) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "2igkR5SI-Z1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "does not work for samples 16,17,18,27,28,32,44,45,48,49,50"
      ],
      "metadata": {
        "id": "g1JC5yeMBnnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_mc_leod(model,val_dataloader,part_dic,F1=False)\n",
        "acc_0 , acc_1, acc_2, acc_3 "
      ],
      "metadata": {
        "id": "5kSrFU8e-uol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old training loop - matrix and non matrix format"
      ],
      "metadata": {
        "id": "4olpdwzyG8dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        #accuracy_v0_sum = 0\n",
        "        #accuracy_v1_sum = 0\n",
        "        #accuracy_v2_sum = 0\n",
        "        #accuracy_v3_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4: \n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                    v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                    mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_pred_flat:\", v_pred_flat.shape)\n",
        "                    \"\"\"\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4:                   \n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                    v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                    mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                    print(\"old mask\", mask_ori.shape)\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_ori_flat\", v_ori_flat.shape)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)   \n",
        "                    print(\"acc\",acc)                    \n",
        "                    accuracy_sum += acc \n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                    acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                    for i in range(len(prediction[0,:])):\n",
        "                      acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                      accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    \n",
        "                    \"\"\"\n",
        "                    pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "\n",
        "                    acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                    acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                    acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                    if nbr_voices == 4:\n",
        "                        acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                    \n",
        "                    # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    if nbr_voices == 4:\n",
        "                        accuracy_v3_sum += acc_v3 / len(lens)\n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "            train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "            train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "            train_accuracy_v3 = accuracy_v3_sum / 18   ## bc only 18 pieces with len 3\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "            history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "            history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "            #if nbr_voices == 4:\n",
        "            history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3)) \n",
        "            #else:\n",
        "            #    print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2)) \n",
        "            \"\"\"\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_v0_sum = 0\n",
        "                accuracy_v1_sum = 0\n",
        "                accuracy_v2_sum = 0\n",
        "                accuracy_v3_sum = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                        truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                        acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                        for i in range(len(prediction[0,:])):\n",
        "                          acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                          val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                        #print(\"val_accuracy_sum_list[3]\",val_accuracy_sum_list[3])\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(train_dataloader)\n",
        "                    #val_acc_list[3] = val_acc_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_acc_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "                #history[\"val_acc_new\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        # Predict the model's output on a batch\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                            \n",
        "                        # compute the accuracy \n",
        "                        acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                        acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                        acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                        if nbr_voices == 4:\n",
        "                            acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                            \n",
        "                            \n",
        "                        # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                        accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                        accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                        accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                        if nbr_voices == 4:\n",
        "                            accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                    val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                    val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                    val_accuracy_v3 = accuracy_v3_sum / 18  ##len(val_dataloader). - bc 18 pieces only with voice 3\n",
        "\n",
        "\n",
        "                    val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    val_acc_list[3] = val_accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    val_acc_list[4] = val_accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                    \n",
        "\n",
        "\n",
        "                history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "                history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "                history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "                #if nbr_voices == 4:\n",
        "                history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "                #else:\n",
        "                #    print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2))\n",
        "\n",
        "\n",
        "                history[\"val_acc_new\"].append(val_acc_list)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "\n",
        "                        \"\"\"\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                        if nbr_voices == 4: \n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                        v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                        mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)                      \n",
        "                        if nbr_voices == 4:                   \n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                        v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                        mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)\n",
        "                        accuracy_sum += acc \n",
        "                        \"\"\"\n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "nfDV8MKGHE3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einfÃ¼hren damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "    \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "  \n",
        "\n",
        "    def __getitem__(self, idx):          \n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])    \n",
        "\n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "\n",
        "\n",
        "        return (v0, v1, v2, v3, v_all, length)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "x2lFUuw719EC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}