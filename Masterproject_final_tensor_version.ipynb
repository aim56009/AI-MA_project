{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject_final_tensor_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "SsyC2uB0KfaT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeb9777b-bae6-457b-cc21-0e04ebbf0720"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install partitura\n",
        "import partitura"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting partitura\n",
            "  Downloading partitura-0.4.0-py3-none-any.whl (218 kB)\n",
            "\u001b[K     |████████████████████████████████| 218 kB 28.9 MB/s \n",
            "\u001b[?25hCollecting mido\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.4.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from partitura) (4.2.6)\n",
            "Collecting lark-parser\n",
            "  Downloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 64.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.21.6)\n",
            "Collecting xmlschema\n",
            "  Downloading xmlschema-1.11.1-py3-none-any.whl (334 kB)\n",
            "\u001b[K     |████████████████████████████████| 334 kB 71.0 MB/s \n",
            "\u001b[?25hCollecting elementpath<3.0.0,>=2.5.0\n",
            "  Downloading elementpath-2.5.3-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 54.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: elementpath, xmlschema, mido, lark-parser, partitura\n",
            "Successfully installed elementpath-2.5.3 lark-parser-0.12.0 mido-1.2.10 partitura-0.4.0 xmlschema-1.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d618e4-69b9-41d6-9079-d6af3bf7b556"
      },
      "source": [
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI-MA_project'...\n",
            "remote: Enumerating objects: 5516, done.\u001b[K\n",
            "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
            "remote: Compressing objects: 100% (147/147), done.\u001b[K\n",
            "remote: Total 5516 (delta 63), reused 59 (delta 5), pack-reused 5364\u001b[K\n",
            "Receiving objects: 100% (5516/5516), 6.41 MiB | 18.69 MiB/s, done.\n",
            "Resolving deltas: 100% (4662/4662), done.\n",
            "Checking out files: 100% (5330/5330), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "batch_size = 1 \n",
        "PATH_TO_DATA = \"AI-MA_project/bach_pr_fugues\"\n",
        "#PATH_TO_DATA = \"AI-MA_project/pianoroll_88\"\n",
        "workers = 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1-u8zYd-gyo"
      },
      "source": [
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "    \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "  \n",
        "\n",
        "    def __getitem__(self, idx):          \n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])    \n",
        "\n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "\n",
        "\n",
        "        return (v0, v1, v2, v3, v_all, length)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_new(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        self.name_list = [\"1f01\",\"1f02\",\"1f03\",\"1f04\",\"1f05\",\"1f06\",\"1f07\",\"1f08\",\"1f09\",\"1f10\",\"1f11\",\"1f12\",\"1f13\",\"1f14\",\"1f15\",\"1f16\",\"1f17\",\"1f18\",\"1f19\",\"1f20\",\"1f21\",\"1f22\",\"1f23\",\"1f24\",\"2f01\",\"2f02\",\"2f03\",\"2f04\",\"2f05\",\"2f06\",\"2f07\",\"2f08\",\"2f09\",\"2f10\",\"2f11\",\"2f12\",\"2f13\",\"2f14\",\"2f15\",\"2f16\",\"2f17\",\"2f18\",\"2f19\",\"2f20\",\"2f21\",\"2f22\",\"2f23\",\"2f24\"]\n",
        "        self.name_list_voice_3 =  ['1f01', '1f05', '1f12', '1f14', '1f16', '1f17', '1f18', '1f23', '1f24', '2f02', '2f05', '2f07', '2f08', '2f09', '2f16', '2f17',  '2f22', '2f23']\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "\n",
        "                        file_names_list.append(name[-8:-4])\n",
        "\n",
        "                    if \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[49:53] + \".pkl\" in sorted(glob.glob(os.path.join(PATH_TO_DATA, \"voice_3\", \"*.pkl\"))):\n",
        "                        nbr_voices_list.append(4)\n",
        "                    else:\n",
        "                        nbr_voices_list.append(3)\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "\n",
        "\n",
        "            if iLabel == 3:  \n",
        "                voice_files = []\n",
        "                file_names_3 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[2], \"*.pkl\")))   \n",
        "                \n",
        "                ###### loop over all filnames in voices_2 and if an element there is not present in voices_3: append \"missing\" to the voice_files of label=3 => important bc. self.pr_dict[voice_3] has then len 42 and otherwise it would only have len 18  .. these \"missing\" el are not considered later in the dataloader (if len=3 is a diff case of get_idx)\n",
        "                for name in file_names_2:\n",
        "                    if name[45:49] in self.name_list_voice_3:\n",
        "                      correct_name_3 = \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[45:49] + \".pkl\"\n",
        "                      with open(correct_name_3 ,'rb') as f:  \n",
        "                            loaded_obj = pickle.load(f)  \n",
        "                            voice_files.append(loaded_obj)\n",
        "                    else:\n",
        "                      voice_files.append(\"missing\")\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                \n",
        "\n",
        "                \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                          loaded_obj = pickle.load(f)     \n",
        "                          voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 4:\n",
        "            for key, value in self.pr_dict.items():\n",
        "              out_list.append(self.pr_dict[key][idx])\n",
        "                              \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.tensor(out_list[3].T)\n",
        "            v_all = torch.tensor(out_list[4].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 3:\n",
        "\n",
        "            for key, value in self.pr_dict.items():\n",
        "                if key != \"voice_3\":\n",
        "                  out_list.append(self.pr_dict[key][idx]) \n",
        "            \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.zeros(v2.shape)\n",
        "            v_all = torch.tensor(out_list[3].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "            \n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        "
      ],
      "metadata": {
        "id": "3FxK6qr1FqIl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "#dataset = MusicDataset(PATH_TO_DATA)\n",
        "dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 1:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXsRYzQSUuQU",
        "outputId": "7f3210f3-958b-4652-cbcb-226d7e9cf48d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1f03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\n",
        "pianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\n",
        "pianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\n",
        "pianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\n",
        "pianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\n",
        "\n",
        "time_unit = \"beat\"\n",
        "time_div = 12\n",
        "piano_range = True"
      ],
      "metadata": {
        "id": "fR2HaA_BqeHw"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, figsize=(20, 10))\n",
        "ax.imshow(pianoroll_all, origin=\"lower\", cmap='gray', interpolation='nearest', aspect='auto')\n",
        "ax.set_xlabel(f'Time ({time_unit}s/{time_div})')\n",
        "ax.set_ylabel('Piano key' if piano_range else 'MIDI pitch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sRoyBJJVqX_u",
        "outputId": "c94890c5-cd44-4d14-eea0-4be1b2c0e8a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "  if i ==10:\n",
        "    all_voices, length, nbr_voices, _ = sample_batched\n",
        "    all_voices_pr = all_voices[0,:,:,-1].numpy()\n",
        "    \n",
        "    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\n",
        "    print(note_array.shape)\n",
        "    print(note_array[:10])\n",
        "    print(note_array.dtype.names)\n",
        "\n",
        "    #print(i,nbr_voices,all_voices.shape)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "b4QCaMEi3nw7",
        "outputId": "af56ed77-8970-4a28-c75b-bd3d8935595c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor i, sample_batched in enumerate(loader):\\n    all_voices, length, nbr_voices = sample_batched\\n    if nbr_voices ==3:\\n      print(i,nbr_voices,all_voices.shape)\\n    else:\\n      print(i,nbr_voices)\\n\\nfor i, sample_batched in enumerate(loader):\\n  if i ==10:\\n    all_voices, length, nbr_voices, _ = sample_batched\\n    all_voices_pr = all_voices[0,:,:,-1].numpy()\\n    \\n    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\\n    print(note_array.shape)\\n    print(note_array[:10])\\n    print(note_array.dtype.names)\\n\\n    #print(i,nbr_voices,all_voices.shape)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Music - Model\n"
      ],
      "metadata": {
        "id": "JNqxeacDwxNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UNET "
      ],
      "metadata": {
        "id": "QAIfIM69VHI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "XMdlm0_Vyyhc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_0 = []\n",
        "loss_1 = []\n",
        "loss_2 = []\n",
        "loss_3 = []\n",
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")                       # use weight parameters maybe take 1/88       \n",
        "\n",
        "    \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                      ### squeeze output here before returning                                       \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()\n",
        "\n",
        "\n",
        "\n",
        "        if nbr_voices==4:\n",
        "            loss = self.loss(score_0, v0) +  self.loss(score_1, v1) +  self.loss(score_2, v2) + 1.5* self.loss(score_3, v3) \n",
        "            \n",
        "            loss_0.append(self.loss(score_0, v0).cpu().detach().numpy())\n",
        "            loss_1.append(self.loss(score_1, v1).cpu().detach().numpy())\n",
        "            loss_2.append(self.loss(score_2, v2).cpu().detach().numpy())\n",
        "            loss_3.append(self.loss(score_3, v3).cpu().detach().numpy())\n",
        "            print(\"self.loss(score_0, v0)\",self.loss(score_0, v0).cpu().detach().numpy())\n",
        "            print(\"self.loss(score_0, v1)\",self.loss(score_1, v1).cpu().detach().numpy())\n",
        "            print(\"self.loss(score_0, v2)\",self.loss(score_2, v2).cpu().detach().numpy())\n",
        "            print(\"self.loss(score_2, v3)\",self.loss(score_3, v3).cpu().detach().numpy())\n",
        "            print(\"loss\",loss)      \n",
        "        else:\n",
        "            loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) \n",
        "        \n",
        "        return loss   #change also to matrix version\n",
        "        \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "            \n",
        "\n",
        "        else:\n",
        "            # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
        "            #predicted = scores_comb.argmax(dim=3)\n",
        "            #return np.squeeze(predicted.cpu().numpy())\n",
        "\n",
        "            sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "            prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            \n",
        "            mask_pred = np.squeeze(sentences)== 0\n",
        "            v_pred_argm[mask_pred] = -1\n",
        "\n",
        "            return v_pred_argm \n",
        "                       "
      ],
      "metadata": {
        "id": "CviiPTPOPW04"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"CNN\"\n",
        "lr = 0.0001  \n",
        "monophonic = True\n",
        "his = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "79cPe11WL6J0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4d3f7001-6a26-4390-bb1f-f4921eeb4dde"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"CNN\"\\nlr = 0.0001  \\nmonophonic = True\\nhis = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Define Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                                   ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "        \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]             \n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "                    ### before\n",
        "                    #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                    #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                    #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                    #for i in range(len(prediction[0,:])):\n",
        "                    #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                    #  accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                    #prediction = model.predict(voices, lens, monophonic)                    #for voice vise masking\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)         #for mixed voice masking        \n",
        "\n",
        "\n",
        "                    ## ground truth in shape 1280x88 -> mixed voice\n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    truth = v_ori_argm       \n",
        "\n",
        "                    # outsource accurcy to further down -> just a placeholder right now\n",
        "                    v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            #train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            #train_acc_list[3] = accuracy_sum_list[3] / 18                        ## bc only 18 pieces with len 3\n",
        "            #train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "            #history[\"train_loss\"].append(train_loss)\n",
        "            #history[\"train_acc\"].append(train_acc_list)\n",
        "            #print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "            \n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "                        voices = voices.to(device).float()\n",
        "                        ### before\n",
        "                        #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                        #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                        #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                        #for i in range(len(prediction[0,:])):\n",
        "                        #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                        #  val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    #val_acc_list[3] = val_accuracy_sum_list[3] / 18                         ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_accuracy_sum_list[4] / 2                          ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                #history[\"val_acc\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        #prediction = model.predict(voices, lens, monophonic)                #for voice vise masking\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)     # for masking with mixed voice\n",
        "\n",
        "\n",
        "\n",
        "                        ## ground truth in shape 1280x88 -> mixed voice\n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        truth = v_ori_argm       \n",
        "\n",
        "                        # outsource accurcy to further down -> just a placeholder right now\n",
        "                        v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "                    \n",
        "                history[\"val_acc\"].append(val_accuracy)\n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    # save the model\n",
        "    #torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"RNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ge8pY70uHxF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a6b8a592-0deb-4ead-ecd4-7f52795676fa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"RNN\"\\nmonophonic = True\\nhis = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= [\"CNN\",\"RNN\"]\n",
        "monophonic_list = [True,False]\n",
        "\n",
        "for net in network_type:\n",
        "    for monophonic in monophonic_list: \n",
        "        print(\"network set to:\",net,\"monophnic:\",monophonic)\n",
        "        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Bs6-iNEBu8o",
        "outputId": "22515c6e-639b-4758-e2b5-263369255f85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= [\"CNN\",\"RNN\"]\\nmonophonic_list = [True,False]\\n\\nfor net in network_type:\\n    for monophonic in monophonic_list: \\n        print(\"network set to:\",net,\"monophnic:\",monophonic)\\n        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "        \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "\n",
        "        \"\"\"\n",
        "        path_train, path_validation = sklearn.model_selection.train_test_split(PATH_TO_DATA, test_size=0.15, random_state=10,)\n",
        "\n",
        "        print(\"Train and validation lenghts: \", len(path_train), len(path_validation))\n",
        "        #train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "        train_dataset = MusicDataset_new(path_train)\n",
        "        validation_dataset = MusicDataset_new(path_validation) #MusicDataset(path_validation)\n",
        "\n",
        "        \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        \"\"\"\n",
        "        \n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicNetwork\n",
        "epochs = 20\n",
        "lr = 0.00001 # was 0.001\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Experiment"
      ],
      "metadata": {
        "id": "bdetlQP-LoRX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm"
      },
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_0,'-o')\n",
        "plt.plot(loss_1,'-o')\n",
        "plt.plot(loss_2,'-o')\n",
        "plt.plot(loss_3,'-o')\n",
        "plt.xlabel('sample')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['loss 0','loss 1','loss 2','loss 3'])\n",
        "plt.title('loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "UPfmfthBjSZw",
        "outputId": "c33e52d0-e509-4552-86f0-021852fb0bda"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy evalutaion F-scores"
      ],
      "metadata": {
        "id": "sJbWsH72N2Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. create folder with part object of all pieces \n",
        "2. load a piece from dataloader with true labels, the mixed piece and the part object \n",
        "3. create notearray from part object\n",
        "4. take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "\n",
        "Output: pianoroll\n",
        "\n",
        "1 note in notearray could be mulitple bins\n",
        "\n",
        "take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "note start at same time with different pitch -> different notes\n",
        "\n",
        "for each note array find corresponding matrix -> \n",
        "\n",
        "\n",
        "if note is only composed by 1 bin: save indx of vocie -> save it to note array\n",
        "\n",
        "if more than 1: look what are idx that compose this note -> majority note -> save it for the note array (if its 50/50 take it random -> count how often this happens) \n",
        "\n",
        "\n",
        "with idx : in note_array find which note corresponds to what voice"
      ],
      "metadata": {
        "id": "CFClch37N6nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader "
      ],
      "metadata": {
        "id": "afYHFVNMlMnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafecc17-5302-4b29-e704-67d75ad937cb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f0141d8d0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch20.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4TAhTQcpmx8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e446184-c41c-49ac-c519-c8929faf8ca4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MusicNetwork(\n",
              "  (rnn): GRU(88, 300, num_layers=2, batch_first=True)\n",
              "  (cnn): UNET(\n",
              "    (double_conv_downs): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up_trans): ModuleList(\n",
              "      (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (double_conv_ups): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (max_pool_2x2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (final_conv): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (top_layer_voice_0): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_1): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_2): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_3): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (loss): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dic with key:filename, val: part_obj"
      ],
      "metadata": {
        "id": "5RVmMv6Q9CJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_parts = \"AI-MA_project/bach_fugues\"\n",
        "part_dic = {}\n",
        "\n",
        "#### create a list with all filenames in the right order ####\n",
        "file_names_part = []\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    file_names_part.append(filename[3:7])\n",
        "#print(file_names_part)\n",
        "\n",
        "#### create a list with all part objects in the right order ####\n",
        "part_list = []\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    fullname = os.path.join(path_parts, filename)\n",
        "    part = partitura.load_score_midi(fullname)\n",
        "    part_list.append(part)\n",
        "#print(part_list)\n",
        "\n",
        "#### create a dict with keys:filenames , values: part object ####\n",
        "for i in range(len(file_names_part)):\n",
        "      part_dic[file_names_part[i]] = part_list[i]"
      ],
      "metadata": {
        "id": "_XYM_KWu2qkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "part_dic.keys(),part_dic.values()"
      ],
      "metadata": {
        "id": "Tt3uHTJY9Ojj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "geht gerade nur für monophonic True"
      ],
      "metadata": {
        "id": "v4TJGKiUs086"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import statistics\n",
        "\n",
        "\n",
        "def evaluate_accuracy_for_all(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    note_counter_0 = 0\n",
        "    note_counter_1 = 0\n",
        "    note_counter_2 = 0\n",
        "    note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "                                      \n",
        "            \n",
        "            #if idx == 0 or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "\n",
        "                note_counter_0 += len(note_array_0)\n",
        "                note_counter_1 += len(note_array_1)\n",
        "                note_counter_2 += len(note_array_2)\n",
        "\n",
        "\n",
        "                if len(part)== 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = part_3.note_array\n",
        "\n",
        "                    note_counter_3 += len(note_array_3)\n",
        "\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                    ground_truth_label_list = [0,1,2,3]              \n",
        "                    total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                    total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                    accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                \n",
        "                if len(part)== 3:\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "                    ground_truth_label_list = [0,1,2]\n",
        "                    total_predictions_dict = {'0': [], '1': [], '2': [] }\n",
        "                    total_truth_dict = {'0': [], '1': [], '2': [] }\n",
        "                    accordance_dict = {'0': [], '1': [], '2': []}\n",
        "\n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        \n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "                            \n",
        "\n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                      counting = 0\n",
        "                      for j in range(len(total_predictions_dict[i])):\n",
        "                          if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                      count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "\n",
        "                    if len(total_predictions_dict.keys())==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    if len(part)==4:\n",
        "                      pred_3 = accordance_dict[\"3\"]\n",
        "                      truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                      f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                      f_score_dict[\"3\"].append(f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])\n",
        "        #return total_predictions_dict, total_truth_dict"
      ],
      "metadata": {
        "id": "RWxVG3XAYTcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lower version trys to only intitialize dicts once not for every nbr of voices often"
      ],
      "metadata": {
        "id": "vAmMZksrnG8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import statistics\n",
        "\n",
        "\n",
        "def evaluate_accuracy_for_all(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    note_counter_0 = 0\n",
        "    note_counter_1 = 0\n",
        "    note_counter_2 = 0\n",
        "    note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "                                      \n",
        "            \n",
        "            #if idx == 0 or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "\n",
        "                note_counter_0 += len(note_array_0)\n",
        "                note_counter_1 += len(note_array_1)\n",
        "                note_counter_2 += len(note_array_2)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "                if len(part)== 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = part_3.note_array\n",
        "                    note_counter_3 += len(note_array_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                   \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        \n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "                            \n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                    print(\"total_predictions_dict.keys())\",total_predictions_dict.keys())\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                      counting = 0\n",
        "                      ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                      for j in range(len(total_predictions_dict[i])):\n",
        "                          if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                      count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "\n",
        "                    if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    if len(part)==4:\n",
        "                      pred_3 = accordance_dict[\"3\"]\n",
        "                      truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                      f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                      f_score_dict[\"3\"].append(f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])\n",
        "        #return total_predictions_dict, total_truth_dict"
      ],
      "metadata": {
        "id": "0xbN5YU8nGT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_pred , dict_gt, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=False)\n",
        "acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "20MP5Gk5kc2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 epoch, no loss modifier:\n",
        "#ACC:(0.9165209182020722,\n",
        "# 0.7864434689151618,\n",
        "# 0.8130949796045199,\n",
        "# 0.003652274754166715)\n",
        "\n",
        "# 20 epoch, no loss modifier:\n",
        "#(0.7962210840410273, 0.8669639629052727, 0.751302181991106, 0.0)\n",
        "\n",
        "# 20 ep, loss3 *1,5\n",
        "#(0.8721136343927623,\n",
        " 0.8319586824413445,\n",
        " 0.7563218924966578,\n",
        " 0.09029535181592076)"
      ],
      "metadata": {
        "id": "6mCYnJLnHfYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_for_all(model,train_dataloader,part_dic,F1=False)\n",
        "dict_pred , dict_gt, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=False)\n",
        "acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "ygMV28FhKVAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_v0, f1_v1, f1_v2, f1_v3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=True)\n",
        "print(f1_v0, f1_v1, f1_v2, f1_v3)"
      ],
      "metadata": {
        "id": "FLLmyO6o5vW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_pred.keys()"
      ],
      "metadata": {
        "id": "79s64i5_lDN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "for gt, i in enumerate(dict_pred.keys()):\n",
        "  counting = 0\n",
        "  for j in range(len(dict_pred[i])):\n",
        "      if dict_pred[i][j][0] == gt:\n",
        "        print(dict_pred[i][j],dict_gt[i][j])\n",
        "        counting +=1\n",
        "\n",
        "      \n",
        "  count_dict_2[i].append(counting)\n",
        "\n",
        "print(count_dict_2[\"0\"],count_dict_2[\"1\"],count_dict_2[\"2\"],count_dict_2[\"3\"])\n",
        "\n",
        "if len(dict_pred.keys())==4:\n",
        "    print(\"accuracy:\",count_dict_2[\"0\"][0]/len(dict_pred[\"0\"]),count_dict_2[\"1\"][0]/len(dict_pred[\"1\"]),count_dict_2[\"2\"][0]/len(dict_pred[\"2\"]),count_dict_2[\"3\"][0]/len(dict_pred[\"3\"]))\n",
        "\n",
        "if len(dict_pred.keys())==3:\n",
        "    print(\"accuracy:\",count_dict_2[\"0\"][0]/len(dict_pred[\"0\"]),count_dict_2[\"1\"][0]/len(dict_pred[\"1\"]),count_dict_2[\"2\"][0]/len(dict_pred[\"2\"]))"
      ],
      "metadata": {
        "id": "VYBpV_hMfGGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### take 0 -> compare to truth of 0,1,2,3 -> overall voice\n",
        "\n",
        "count_list = []\n",
        "\n",
        "count_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "truth_dic = {'0': 0, '1': 1, '2': 2, '3': 3 }\n",
        "\n",
        "voice_entry_list = [\"0\", \"1\", \"2\", \"3\"]\n",
        "for voice_entry_one in voice_entry_list:\n",
        "    for voice_entry_two in voice_entry_list:\n",
        "        count_list = []\n",
        "        #print(\"voices:\",voice_entry_one,voice_entry_two)\n",
        "        for i in range(len(dict_pred[voice_entry_one])):\n",
        "            if dict_pred[voice_entry_one][i][0] == truth_dic[voice_entry_two]:      #dict_truth[voice_entry_two][i][0]:\n",
        "                count_list.append(1)\n",
        "            else:\n",
        "                count_list.append(0)\n",
        "        count_dict[voice_entry_one].append(count_list)\n",
        "\n",
        "dictionary_sum={}\n",
        "for i in voice_entry_list:\n",
        "    v0_match,v1_match,v2_match,v3_match = count_dict[i]\n",
        "    sum_v0 = np.sum(v0_match)\n",
        "    sum_v1 = np.sum(v1_match)\n",
        "    sum_v2 = np.sum(v2_match)\n",
        "    sum_v3 = np.sum(v3_match)\n",
        "    dictionary_sum[\"v0\"] = sum_v0\n",
        "    dictionary_sum[\"v1\"] = sum_v1\n",
        "    dictionary_sum[\"v2\"] = sum_v2\n",
        "    dictionary_sum[\"v3\"] = sum_v3\n",
        "\n",
        "    val_list = list(dictionary_sum.values())\n",
        "    \n",
        "    print(\"voice{} matches with\".format(i))\n",
        "    print(\"dict\",dictionary_sum)\n",
        "\n",
        "    max_sum = max(sum_v0,sum_v1,sum_v2,sum_v3)\n",
        "\n",
        "\n",
        "    print(\"max_sum\", val_list.index(max_sum) )\n",
        "\n",
        "    print(\"accuracy voice{}:\".format(i), max_sum/(sum_v0+sum_v1+sum_v2+sum_v3) )\n",
        "    print(\"________________\")\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "BoQcV_i038DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR MONOPHONIC F1\n",
        "\n",
        "# start with GT\n",
        "# look at first note in pred-> save note label\n",
        "# look at second note in pred-> if same note as before : SUCESS if it is not: FAIL\n",
        " # DO This for all 4 voices\n",
        " ## in GT there is always the same voice following -> would always be an array of 1\n",
        "\n",
        "## POLYPHONIC \n",
        "\n",
        "# prbl after 1 note there can be multiple diff voices .. chords\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-bR7gcej90qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you have the ground truth on the different parts that you get when you import your score. Each part correspond to a voice. So if your note array contains all notes of all voices, you have for each note in your note array a number that is the ground truth voice (that you take from the part) and a number that is the predicted voice (that you take from the maximum vote)."
      ],
      "metadata": {
        "id": "Z5q305YzvjMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start time, duration , pitch to separate \n",
        "\n",
        "use the onset_beat and duration_beat\n",
        "\n",
        "multiply them according to the values set when producing the pianorolls \n",
        "\n",
        "-> get the position in the pianoroll\n",
        "\n",
        "time_div = 12\n",
        "\n"
      ],
      "metadata": {
        "id": "EmvxtyaVKG27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "A94mchm4LV6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy_v0\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v1\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v2\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v3\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Accuracy0','Accuracy1','Accuracy2','Accuracy3'])\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TgJDHaxAgYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend('Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxMs8GEfMvPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"train_loss\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqMcJT5aFL01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old training loop - matrix and non matrix format"
      ],
      "metadata": {
        "id": "4olpdwzyG8dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        #accuracy_v0_sum = 0\n",
        "        #accuracy_v1_sum = 0\n",
        "        #accuracy_v2_sum = 0\n",
        "        #accuracy_v3_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4: \n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                    v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                    mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_pred_flat:\", v_pred_flat.shape)\n",
        "                    \"\"\"\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4:                   \n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                    v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                    mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                    print(\"old mask\", mask_ori.shape)\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_ori_flat\", v_ori_flat.shape)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)   \n",
        "                    print(\"acc\",acc)                    \n",
        "                    accuracy_sum += acc \n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                    acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                    for i in range(len(prediction[0,:])):\n",
        "                      acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                      accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    \n",
        "                    \"\"\"\n",
        "                    pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "\n",
        "                    acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                    acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                    acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                    if nbr_voices == 4:\n",
        "                        acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                    \n",
        "                    # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    if nbr_voices == 4:\n",
        "                        accuracy_v3_sum += acc_v3 / len(lens)\n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "            train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "            train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "            train_accuracy_v3 = accuracy_v3_sum / 18   ## bc only 18 pieces with len 3\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "            history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "            history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "            #if nbr_voices == 4:\n",
        "            history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3)) \n",
        "            #else:\n",
        "            #    print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2)) \n",
        "            \"\"\"\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_v0_sum = 0\n",
        "                accuracy_v1_sum = 0\n",
        "                accuracy_v2_sum = 0\n",
        "                accuracy_v3_sum = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                        truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                        acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                        for i in range(len(prediction[0,:])):\n",
        "                          acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                          val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                        #print(\"val_accuracy_sum_list[3]\",val_accuracy_sum_list[3])\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(train_dataloader)\n",
        "                    #val_acc_list[3] = val_acc_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_acc_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "                #history[\"val_acc_new\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        # Predict the model's output on a batch\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                            \n",
        "                        # compute the accuracy \n",
        "                        acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                        acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                        acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                        if nbr_voices == 4:\n",
        "                            acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                            \n",
        "                            \n",
        "                        # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                        accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                        accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                        accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                        if nbr_voices == 4:\n",
        "                            accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                    val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                    val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                    val_accuracy_v3 = accuracy_v3_sum / 18  ##len(val_dataloader). - bc 18 pieces only with voice 3\n",
        "\n",
        "\n",
        "                    val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    val_acc_list[3] = val_accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    val_acc_list[4] = val_accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                    \n",
        "\n",
        "\n",
        "                history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "                history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "                history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "                #if nbr_voices == 4:\n",
        "                history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "                #else:\n",
        "                #    print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2))\n",
        "\n",
        "\n",
        "                history[\"val_acc_new\"].append(val_acc_list)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "\n",
        "                        \"\"\"\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                        if nbr_voices == 4: \n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                        v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                        mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)                      \n",
        "                        if nbr_voices == 4:                   \n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                        v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                        mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)\n",
        "                        accuracy_sum += acc \n",
        "                        \"\"\"\n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "nfDV8MKGHE3J"
      }
    }
  ]
}