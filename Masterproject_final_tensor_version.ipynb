{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject_final_tensor_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "SsyC2uB0KfaT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260a09ad-26f4-4f72-8146-fa44f81749f5"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install partitura\n",
        "! pip install git+https://github.com/CPJKU/partitura.git@develop\n",
        "import partitura\n",
        "import statistics"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/CPJKU/partitura.git@develop\n",
            "  Cloning https://github.com/CPJKU/partitura.git (to revision develop) to /tmp/pip-req-build-dya5ira8\n",
            "  Running command git clone -q https://github.com/CPJKU/partitura.git /tmp/pip-req-build-dya5ira8\n",
            "  Running command git checkout -b develop --track origin/develop\n",
            "  Switched to a new branch 'develop'\n",
            "  Branch 'develop' set up to track remote branch 'develop' from 'origin'.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.4.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (4.2.6)\n",
            "Requirement already satisfied: lark-parser in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (0.12.0)\n",
            "Requirement already satisfied: xmlschema in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.11.2)\n",
            "Requirement already satisfied: mido in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.2.10)\n",
            "Requirement already satisfied: elementpath<3.0.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from xmlschema->partitura==0.4.0) (2.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path_to_musicxml = \"AI-MA_project/chorales_converted/chor001.xml\"\n",
        "#part = partitura.load_musicxml(path_to_musicxml)\n",
        "#partitura.save_score_midi(part,\"chor001_5.mid\",5)"
      ],
      "metadata": {
        "id": "j6dq9ptQGmHB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader - Set the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "fugues = True\n",
        "\n",
        "if fugues == True:\n",
        "    PATH_TO_DATA = \"AI-MA_project/bach_pr_fugues\"\n",
        "else:\n",
        "    PATH_TO_DATA = \"AI-MA_project/pianoroll_88\"\n",
        "\n",
        "batch_size = 1 \n",
        "workers = 0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_new(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        self.name_list = [\"1f01\",\"1f02\",\"1f03\",\"1f04\",\"1f05\",\"1f06\",\"1f07\",\"1f08\",\"1f09\",\"1f10\",\"1f11\",\"1f12\",\"1f13\",\"1f14\",\"1f15\",\"1f16\",\"1f17\",\"1f18\",\"1f19\",\"1f20\",\"1f21\",\"1f22\",\"1f23\",\"1f24\",\"2f01\",\"2f02\",\"2f03\",\"2f04\",\"2f05\",\"2f06\",\"2f07\",\"2f08\",\"2f09\",\"2f10\",\"2f11\",\"2f12\",\"2f13\",\"2f14\",\"2f15\",\"2f16\",\"2f17\",\"2f18\",\"2f19\",\"2f20\",\"2f21\",\"2f22\",\"2f23\",\"2f24\"]\n",
        "        self.name_list_voice_3 =  ['1f01', '1f05', '1f12', '1f14', '1f16', '1f17', '1f18', '1f23', '1f24', '2f02', '2f05', '2f07', '2f08', '2f09', '2f16', '2f17',  '2f22', '2f23']\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einfÃ¼hren damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "\n",
        "                        file_names_list.append(name[-8:-4])\n",
        "\n",
        "                    if \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[49:53] + \".pkl\" in sorted(glob.glob(os.path.join(PATH_TO_DATA, \"voice_3\", \"*.pkl\"))):\n",
        "                        nbr_voices_list.append(4)\n",
        "                    else:\n",
        "                        nbr_voices_list.append(3)\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "\n",
        "\n",
        "            if iLabel == 3:  \n",
        "                voice_files = []\n",
        "                file_names_3 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[2], \"*.pkl\")))   \n",
        "                \n",
        "                ###### loop over all filnames in voices_2 and if an element there is not present in voices_3: append \"missing\" to the voice_files of label=3 => important bc. self.pr_dict[voice_3] has then len 42 and otherwise it would only have len 18  .. these \"missing\" el are not considered later in the dataloader (if len=3 is a diff case of get_idx)\n",
        "                for name in file_names_2:\n",
        "                    if name[45:49] in self.name_list_voice_3:\n",
        "                      correct_name_3 = \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[45:49] + \".pkl\"\n",
        "                      with open(correct_name_3 ,'rb') as f:  \n",
        "                            loaded_obj = pickle.load(f)  \n",
        "                            voice_files.append(loaded_obj)\n",
        "                    else:\n",
        "                      voice_files.append(\"missing\")\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                \n",
        "\n",
        "                \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                          loaded_obj = pickle.load(f)     \n",
        "                          voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 4:\n",
        "            for key, value in self.pr_dict.items():\n",
        "              out_list.append(self.pr_dict[key][idx])\n",
        "                              \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.tensor(out_list[3].T)\n",
        "            v_all = torch.tensor(out_list[4].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 3:\n",
        "\n",
        "            for key, value in self.pr_dict.items():\n",
        "                if key != \"voice_3\":\n",
        "                  out_list.append(self.pr_dict[key][idx]) \n",
        "            \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.zeros(v2.shape)\n",
        "            v_all = torch.tensor(out_list[3].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "            \n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        "
      ],
      "metadata": {
        "id": "3FxK6qr1FqIl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_chor(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einfÃ¼hren damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        file_names_list.append(name[-7:-4])      # e.g. name = AI-MA_project/pianoroll_88/voice_all/voice_all_001.pkl\n",
        "\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "              \n",
        "\n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])\n",
        "                            \n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "        file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "        voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "        \n",
        "        return (voices, length, 4, file_name)     # 4 bc nbr voices is always 4"
      ],
      "metadata": {
        "id": "3uQnok3VGngZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "if fugues == True:\n",
        "    dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "else:\n",
        "    dataset = MusicDataset_chor(PATH_TO_DATA)\n",
        "\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 1:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXsRYzQSUuQU",
        "outputId": "485835c9-fe99-4666-8fbd-3a17ad10b195"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1f03 tensor([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\n",
        "pianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\n",
        "pianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\n",
        "pianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\n",
        "pianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\n",
        "\n",
        "time_unit = \"beat\"\n",
        "time_div = 12\n",
        "piano_range = True"
      ],
      "metadata": {
        "id": "fR2HaA_BqeHw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, figsize=(20, 10))\n",
        "ax.imshow(pianoroll_all, origin=\"lower\", cmap='gray', interpolation='nearest', aspect='auto')\n",
        "ax.set_xlabel(f'Time ({time_unit}s/{time_div})')\n",
        "ax.set_ylabel('Piano key' if piano_range else 'MIDI pitch')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "sRoyBJJVqX_u",
        "outputId": "0f2de033-82eb-4a3a-c85a-286987a8487b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "  if i ==10:\n",
        "    all_voices, length, nbr_voices, _ = sample_batched\n",
        "    all_voices_pr = all_voices[0,:,:,-1].numpy()\n",
        "    \n",
        "    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\n",
        "    print(note_array.shape)\n",
        "    print(note_array[:10])\n",
        "    print(note_array.dtype.names)\n",
        "\n",
        "    #print(i,nbr_voices,all_voices.shape)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "b4QCaMEi3nw7",
        "outputId": "bcb2d68e-30d0-4130-854b-d4b68af74e29"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor i, sample_batched in enumerate(loader):\\n    all_voices, length, nbr_voices = sample_batched\\n    if nbr_voices ==3:\\n      print(i,nbr_voices,all_voices.shape)\\n    else:\\n      print(i,nbr_voices)\\n\\nfor i, sample_batched in enumerate(loader):\\n  if i ==10:\\n    all_voices, length, nbr_voices, _ = sample_batched\\n    all_voices_pr = all_voices[0,:,:,-1].numpy()\\n    \\n    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\\n    print(note_array.shape)\\n    print(note_array[:10])\\n    print(note_array.dtype.names)\\n\\n    #print(i,nbr_voices,all_voices.shape)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Music - Model\n"
      ],
      "metadata": {
        "id": "JNqxeacDwxNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UNET "
      ],
      "metadata": {
        "id": "QAIfIM69VHI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "XMdlm0_Vyyhc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_0 = []\n",
        "loss_1 = []\n",
        "loss_2 = []\n",
        "loss_3 = []\n",
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")                       # use weight parameters maybe take 1/88       \n",
        "\n",
        "    \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                      ### squeeze output here before returning                                       \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()    ### v3 is automatically tensor containing only 0 bc dataloader does this for pieces with 3 voices\n",
        "        \n",
        "\n",
        "        \"\"\"\n",
        "        if nbr_voices==4:\n",
        "            v3 = voices[:,:,:,3].squeeze()\n",
        "        else:\n",
        "            v3 = torch.zeros(v0.shape,  device=\"cuda\")\n",
        "        \"\"\"\n",
        "\n",
        "        loss = self.loss(score_0, v0) +  self.loss(score_1, v1) +  self.loss(score_2, v2) +  self.loss(score_3, v3) #*1.5\n",
        "            \n",
        "        loss_0.append(self.loss(score_0, v0).cpu().detach().numpy())\n",
        "        loss_1.append(self.loss(score_1, v1).cpu().detach().numpy())\n",
        "        loss_2.append(self.loss(score_2, v2).cpu().detach().numpy())\n",
        "        loss_3.append(self.loss(score_3, v3).cpu().detach().numpy())\n",
        "        print(\"self.loss(score_0, v0)\",self.loss(score_0, v0).cpu().detach().numpy())\n",
        "        print(\"self.loss(score_1, v1)\",self.loss(score_1, v1).cpu().detach().numpy())\n",
        "        print(\"self.loss(score_2, v2)\",self.loss(score_2, v2).cpu().detach().numpy())\n",
        "        print(\"self.loss(score_3, v3)\",self.loss(score_3, v3).cpu().detach().numpy())\n",
        "\n",
        "        print(\"loss\",loss) \n",
        "\n",
        "\n",
        "        return loss   #change also to matrix version\n",
        "        \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "            \n",
        "\n",
        "        else:\n",
        "            sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "            prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            \n",
        "            mask_pred = np.squeeze(sentences)== 0\n",
        "            v_pred_argm[mask_pred] = -1\n",
        "\n",
        "            return v_pred_argm \n",
        "                       "
      ],
      "metadata": {
        "id": "CviiPTPOPW04"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"CNN\"\n",
        "lr = 0.0001  \n",
        "monophonic = True\n",
        "his = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "79cPe11WL6J0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "de3d19ec-61e2-4115-9d7c-dba1f6547038"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"CNN\"\\nlr = 0.0001  \\nmonophonic = True\\nhis = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Define Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                                   ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "                \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]             \n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "                    ### before\n",
        "                    #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                    #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                    #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                    #for i in range(len(prediction[0,:])):\n",
        "                    #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                    #  accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                    #prediction = model.predict(voices, lens, monophonic)                    #for voice vise masking\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)         #for mixed voice masking        \n",
        "\n",
        "\n",
        "                    ## ground truth in shape 1280x88 -> mixed voice\n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    truth = v_ori_argm       \n",
        "\n",
        "                    # outsource accurcy to further down -> just a placeholder right now\n",
        "                    v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            #train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            #train_acc_list[3] = accuracy_sum_list[3] / 18                        ## bc only 18 pieces with len 3\n",
        "            #train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "            #history[\"train_loss\"].append(train_loss)\n",
        "            #history[\"train_acc\"].append(train_acc_list)\n",
        "            #print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "            \n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "                        voices = voices.to(device).float()\n",
        "                        ### before\n",
        "                        #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                        #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                        #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                        #for i in range(len(prediction[0,:])):\n",
        "                        #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                        #  val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    #val_acc_list[3] = val_accuracy_sum_list[3] / 18                         ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_accuracy_sum_list[4] / 2                          ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                #history[\"val_acc\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        #prediction = model.predict(voices, lens, monophonic)                #for voice vise masking\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)     # for masking with mixed voice\n",
        "\n",
        "\n",
        "\n",
        "                        ## ground truth in shape 1280x88 -> mixed voice\n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        truth = v_ori_argm       \n",
        "\n",
        "                        # outsource accurcy to further down -> just a placeholder right now\n",
        "                        v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "                    \n",
        "                history[\"val_acc\"].append(val_accuracy)\n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    # save the model\n",
        "    #torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"RNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ge8pY70uHxF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3bf3b0bf-9385-4ddf-8ae3-acd5fcb5a029"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"RNN\"\\nmonophonic = True\\nhis = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= [\"CNN\",\"RNN\"]\n",
        "monophonic_list = [True,False]\n",
        "\n",
        "for net in network_type:\n",
        "    for monophonic in monophonic_list: \n",
        "        print(\"network set to:\",net,\"monophnic:\",monophonic)\n",
        "        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Bs6-iNEBu8o",
        "outputId": "fbf9eb09-9338-4b6b-f0da-1683047ff190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= [\"CNN\",\"RNN\"]\\nmonophonic_list = [True,False]\\n\\nfor net in network_type:\\n    for monophonic in monophonic_list: \\n        print(\"network set to:\",net,\"monophnic:\",monophonic)\\n        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        if fugues == True:\n",
        "        ### uncomment for fugues ###\n",
        "            train_dataset = MusicDataset_new(PATH_TO_DATA) \n",
        "        ### uncomment for chorals ###\n",
        "        else:\n",
        "            train_dataset = MusicDataset_chor(PATH_TO_DATA) \n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        ### uncomment for fugues ###\n",
        "        if fugues == True:\n",
        "            dataset = MusicDataset_new(PATH_TO_DATA) \n",
        "        ### uncomment for chorals ###\n",
        "        else:\n",
        "            dataset = MusicDataset_chor(PATH_TO_DATA)\n",
        "        \n",
        "        \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicNetwork\n",
        "epochs = 10\n",
        "lr = 0.00001 # was 0.001\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Experiment"
      ],
      "metadata": {
        "id": "bdetlQP-LoRX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm",
        "outputId": "11b24c1e-f472-4ae7-ecd8-128cb06fbd27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataloader 34 val_dataloader 7\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "Training on device: cuda\n",
            "monophonic set to: True\n",
            "self.loss(score_0, v0) 4.6233964\n",
            "self.loss(score_1, v1) 3.9467375\n",
            "self.loss(score_2, v2) 3.3714893\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(11.9416, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.8276668\n",
            "self.loss(score_1, v1) 3.7656455\n",
            "self.loss(score_2, v2) 3.7133613\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(12.3067, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 3.4372828\n",
            "self.loss(score_1, v1) 3.667882\n",
            "self.loss(score_2, v2) 3.6251774\n",
            "self.loss(score_3, v3) 2.7982397\n",
            "loss tensor(13.5286, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.476224\n",
            "self.loss(score_1, v1) 3.3630383\n",
            "self.loss(score_2, v2) 3.1707904\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(11.0101, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 2.8546045\n",
            "self.loss(score_1, v1) 3.2004547\n",
            "self.loss(score_2, v2) 3.2963128\n",
            "self.loss(score_3, v3) 3.960617\n",
            "loss tensor(13.3120, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.66814\n",
            "self.loss(score_1, v1) 4.310036\n",
            "self.loss(score_2, v2) 4.026517\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(13.0047, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.5405087\n",
            "self.loss(score_1, v1) 3.833193\n",
            "self.loss(score_2, v2) 3.4664116\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(11.8401, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 5.219043\n",
            "self.loss(score_1, v1) 3.3173785\n",
            "self.loss(score_2, v2) 3.6489694\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(12.1854, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.0338593\n",
            "self.loss(score_1, v1) 3.591378\n",
            "self.loss(score_2, v2) 4.195462\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(11.8207, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.4408607\n",
            "self.loss(score_1, v1) 3.7191064\n",
            "self.loss(score_2, v2) 3.7598274\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(11.9198, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.1844153\n",
            "self.loss(score_1, v1) 4.068685\n",
            "self.loss(score_2, v2) 3.434776\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(11.6879, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 3.6065562\n",
            "self.loss(score_1, v1) 3.4620903\n",
            "self.loss(score_2, v2) 3.5068455\n",
            "self.loss(score_3, v3) 3.0739307\n",
            "loss tensor(13.6494, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.9977684\n",
            "self.loss(score_1, v1) 2.8264863\n",
            "self.loss(score_2, v2) 3.623507\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(11.4478, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.673854\n",
            "self.loss(score_1, v1) 3.9493728\n",
            "self.loss(score_2, v2) 4.234322\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(12.8575, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 3.802037\n",
            "self.loss(score_1, v1) 3.473559\n",
            "self.loss(score_2, v2) 2.0529103\n",
            "self.loss(score_3, v3) 3.2915053\n",
            "loss tensor(12.6200, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 3.801721\n",
            "self.loss(score_1, v1) 3.4514244\n",
            "self.loss(score_2, v2) 2.9769042\n",
            "self.loss(score_3, v3) 2.940027\n",
            "loss tensor(13.1701, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 3.8749774\n",
            "self.loss(score_1, v1) 4.000901\n",
            "self.loss(score_2, v2) 3.9795773\n",
            "self.loss(score_3, v3) 3.7117162\n",
            "loss tensor(15.5672, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 3.9712508\n",
            "self.loss(score_1, v1) 3.2469952\n",
            "self.loss(score_2, v2) 3.2458534\n",
            "self.loss(score_3, v3) 2.8015995\n",
            "loss tensor(13.2657, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.2940993\n",
            "self.loss(score_1, v1) 4.169145\n",
            "self.loss(score_2, v2) 3.4076338\n",
            "self.loss(score_3, v3) 1.2276484\n",
            "loss tensor(13.0985, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.7631664\n",
            "self.loss(score_1, v1) 3.343125\n",
            "self.loss(score_2, v2) 3.1947892\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(11.3011, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.2101984\n",
            "self.loss(score_1, v1) 3.7158082\n",
            "self.loss(score_2, v2) 3.6094563\n",
            "self.loss(score_3, v3) 3.3349748\n",
            "loss tensor(14.8704, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 3.841891\n",
            "self.loss(score_1, v1) 3.1690538\n",
            "self.loss(score_2, v2) 3.5599108\n",
            "self.loss(score_3, v3) 2.8335376\n",
            "loss tensor(13.4044, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 3.1422267\n",
            "self.loss(score_1, v1) 3.9929507\n",
            "self.loss(score_2, v2) 3.1424952\n",
            "self.loss(score_3, v3) 2.995435\n",
            "loss tensor(13.2731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.4519157\n",
            "self.loss(score_1, v1) 4.0129595\n",
            "self.loss(score_2, v2) 3.789024\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(12.2539, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.232759\n",
            "self.loss(score_1, v1) 3.7377186\n",
            "self.loss(score_2, v2) 3.5316317\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(11.5021, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 3.6642013\n",
            "self.loss(score_1, v1) 3.5824833\n",
            "self.loss(score_2, v2) 3.5459921\n",
            "self.loss(score_3, v3) 2.5791776\n",
            "loss tensor(13.3719, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.3519945\n",
            "self.loss(score_1, v1) 3.9518876\n",
            "self.loss(score_2, v2) 3.8065982\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(12.1105, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 3.8293107\n",
            "self.loss(score_1, v1) 3.9337647\n",
            "self.loss(score_2, v2) 3.822923\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(11.5860, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 3.1068628\n",
            "self.loss(score_1, v1) 3.8092015\n",
            "self.loss(score_2, v2) 3.875136\n",
            "self.loss(score_3, v3) 3.168494\n",
            "loss tensor(13.9597, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.6537066\n",
            "self.loss(score_1, v1) 3.4901044\n",
            "self.loss(score_2, v2) 3.5922964\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(11.7361, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.0230317\n",
            "self.loss(score_1, v1) 3.9276397\n",
            "self.loss(score_2, v2) 3.4728174\n",
            "self.loss(score_3, v3) 3.1820605\n",
            "loss tensor(14.6055, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.4343615\n",
            "self.loss(score_1, v1) 3.319304\n",
            "self.loss(score_2, v2) 2.9515557\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(10.7052, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 3.4883113\n",
            "self.loss(score_1, v1) 3.4488735\n",
            "self.loss(score_2, v2) 4.1338425\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(11.0710, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) 4.5594983\n",
            "self.loss(score_1, v1) 3.6152809\n",
            "self.loss(score_2, v2) 3.705335\n",
            "self.loss(score_3, v3) -0.0\n",
            "loss tensor(11.8801, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 12.584258360021254, Train Accuracy : 0.9739913634034588\n",
            " Validation Accuracy : 5.705149291598263\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(loss_0,'-o')\n",
        "plt.plot(loss_1,'-o')\n",
        "plt.plot(loss_2,'-o')\n",
        "plt.plot(loss_3,'-o')\n",
        "plt.xlabel('sample')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['loss 0','loss 1','loss 2','loss 3'])\n",
        "plt.title('loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "UPfmfthBjSZw",
        "outputId": "03cb39df-9707-47f6-beaf-8fc9a2cfad17"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy evalutaion F-scores"
      ],
      "metadata": {
        "id": "sJbWsH72N2Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. create folder with part object of all pieces \n",
        "2. load a piece from dataloader with true labels, the mixed piece and the part object \n",
        "3. create notearray from part object\n",
        "4. take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "\n",
        "Output: pianoroll\n",
        "\n",
        "1 note in notearray could be mulitple bins\n",
        "\n",
        "take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "note start at same time with different pitch -> different notes\n",
        "\n",
        "for each note array find corresponding matrix -> \n",
        "\n",
        "\n",
        "if note is only composed by 1 bin: save indx of vocie -> save it to note array\n",
        "\n",
        "if more than 1: look what are idx that compose this note -> majority note -> save it for the note array (if its 50/50 take it random -> count how often this happens) \n",
        "\n",
        "\n",
        "with idx : in note_array find which note corresponds to what voice"
      ],
      "metadata": {
        "id": "CFClch37N6nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader "
      ],
      "metadata": {
        "id": "afYHFVNMlMnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019eb6ae-7887-428b-a497-12ef8ac0443b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f6464017510>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## makes cell output nothing\n",
        "%%capture  \n",
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch10.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4TAhTQcpmx8m"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dic with key:filename, val: part_obj  for fugues"
      ],
      "metadata": {
        "id": "5RVmMv6Q9CJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if PATH_TO_DATA == \"AI-MA_project/bach_pr_fugues\":\n",
        "    path_parts = \"AI-MA_project/bach_fugues\"\n",
        "    part_dic = {}\n",
        "\n",
        "    #### create a list with all filenames in the right order ####\n",
        "    file_names_part = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.mid'): continue\n",
        "        file_names_part.append(filename[3:7])\n",
        "    #print(file_names_part)\n",
        "\n",
        "    #### create a list with all part objects in the right order ####\n",
        "    part_list = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.mid'): continue\n",
        "        fullname = os.path.join(path_parts, filename)\n",
        "        part = partitura.load_score_midi(fullname)\n",
        "        part_list.append(part)\n",
        "    #print(part_list)\n",
        "\n",
        "    #### create a dict with keys:filenames , values: part object ####\n",
        "    for i in range(len(file_names_part)):\n",
        "        part_dic[file_names_part[i]] = part_list[i]\n",
        "    \n",
        "    print(part_dic.keys(),part_dic.values())"
      ],
      "metadata": {
        "id": "_XYM_KWu2qkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dic with key:filename, val: part_obj  for chorales"
      ],
      "metadata": {
        "id": "6D9oTp_lNQbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if PATH_TO_DATA == \"AI-MA_project/pianoroll_88\":\n",
        "    path_parts = \"AI-MA_project/chorales_converted\"\n",
        "    part_dic = {}\n",
        "\n",
        "    #### create a list with all filenames in the right order ####\n",
        "    file_names_part = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.xml'): continue\n",
        "        file_names_part.append(filename[4:7])\n",
        "    #print(file_names_part)\n",
        "\n",
        "    #### create a list with all part objects in the right order ####\n",
        "    part_list = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.xml'): continue\n",
        "        fullname = os.path.join(path_parts, filename)\n",
        "        part = partitura.load_musicxml(fullname)\n",
        "        part_list.append(part)\n",
        "    #print(part_list)\n",
        "\n",
        "    #### create a dict with keys:filenames , values: part object ####\n",
        "    for i in range(len(file_names_part)):\n",
        "        part_dic[file_names_part[i]] = part_list[i]\n",
        "    \n",
        "    print(\"part_dic.keys()\",part_dic.keys())\n",
        "    print(\"part_dic.values()\",part_dic.values())"
      ],
      "metadata": {
        "id": "_4q58c16NjbE"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate chorales"
      ],
      "metadata": {
        "id": "yphGmsr-NSV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate chorals"
      ],
      "metadata": {
        "id": "v4TJGKiUs086"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_one_choral(model, train_dataloader, part_dic,F1):\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    note_counter_0 = 0\n",
        "    note_counter_1 = 0\n",
        "    note_counter_2 = 0\n",
        "    note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match                                      \n",
        "            \n",
        "            #if idx > 40: # or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                note_array_3 = part_3.note_array\n",
        "\n",
        "                note_counter_0 += len(note_array_0)\n",
        "                note_counter_1 += len(note_array_1)\n",
        "                note_counter_2 += len(note_array_2)\n",
        "                note_counter_3 += len(note_array_3)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                   \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]                   \n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        \n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "                        else:\n",
        "                            accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                        counting = 0\n",
        "                        ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                        for j in range(len(total_predictions_dict[i])):\n",
        "                            if total_predictions_dict[i][j][0] == gt:\n",
        "                                counting +=1  \n",
        "                        count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                    acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                    print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                    \n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "                    acc_score_dict[\"3\"].append(acc_3)\n",
        "                \n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    print(\"f1_v0 , sample {}:\".format(idx),f1_v0)\n",
        "                    print(\"f1_v1 , sample {}:\".format(idx),f1_v1)\n",
        "                    print(\"f1_v2 , sample {}:\".format(idx),f1_v2)\n",
        "                    \n",
        "                    if len(part)==4:\n",
        "                        pred_3 = accordance_dict[\"3\"]\n",
        "                        truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                        f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                        f_score_dict[\"3\"].append(f1_v3)\n",
        "                        print(\"f1_v3 , sample {}:\".format(idx),f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])\n"
      ],
      "metadata": {
        "id": "UXr2DeiLSyLm"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_one_choral(model,val_dataloader,part_dic,F1=False)\n",
        "    acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "ghAmXcXTTtD1"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    plt.plot(acc_score_dict[\"0\"],'-o')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend(['Accuracy0'])\n",
        "    plt.title('Accuracy vs Epochs')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "J92mdaQG0MXZ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate on F1 meassure"
      ],
      "metadata": {
        "id": "zYMy2JARxEBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    acc_0 , acc_1, acc_2, acc_3 = evaluate_one_choral(model,val_dataloader,part_dic,F1=True)\n",
        "    acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "RWxVG3XAYTcC"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate fugues"
      ],
      "metadata": {
        "id": "TzTpXHznL02j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import statistics\n",
        "\n",
        "\n",
        "def evaluate_accuracy_for_all(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    note_counter_0 = 0\n",
        "    note_counter_1 = 0\n",
        "    note_counter_2 = 0\n",
        "    note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "                                      \n",
        "                print(\"nbr_voices:\",nbr_voices)\n",
        "            #if idx == 0 or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "\n",
        "                note_counter_0 += len(note_array_0)\n",
        "                note_counter_1 += len(note_array_1)\n",
        "                note_counter_2 += len(note_array_2)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "                if len(part)== 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = part_3.note_array\n",
        "                    note_counter_3 += len(note_array_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                   \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        \n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "                            \n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                      counting = 0\n",
        "                      ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                      for j in range(len(total_predictions_dict[i])):\n",
        "                          if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                      count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "\n",
        "                    if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    print(\"f1_v0 , sample {}:\".format(idx),f1_v0)\n",
        "                    print(\"f1_v1 , sample {}:\".format(idx),f1_v1)\n",
        "                    print(\"f1_v2 , sample {}:\".format(idx),f1_v2)\n",
        "                    if len(part)==4:\n",
        "                      pred_3 = accordance_dict[\"3\"]\n",
        "                      truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                      f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                      f_score_dict[\"3\"].append(f1_v3)\n",
        "                      print(\"f1_v3 , sample {}:\".format(idx),f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])\n",
        "        #return total_predictions_dict, total_truth_dict"
      ],
      "metadata": {
        "id": "0xbN5YU8nGT0"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, train_dataloader, part_dic,F1):\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader): \n",
        "        print(\"nbr_voices:\",nbr_voices)\n",
        "\n",
        "test(model,val_dataloader,part_dic,F1=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0HZ9d5TO_Aj",
        "outputId": "89055155-d59f-4bb1-d796-1a503a1d86f3"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nbr_voices: tensor([3])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([3])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True:\n",
        "    dict_pred , dict_gt, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "20MP5Gk5kc2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681cfaaa-ffc1-4423-ed8f-a1862c439d0d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nbr_voices: tensor([3])\n",
            "acc 0, sample 0: 0.990521327014218\n",
            "acc 1, sample 0: 0.5960264900662252\n",
            "acc 2, sample 0: 0.9752380952380952\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 1: 1.0\n",
            "acc 1, sample 1: 0.49214659685863876\n",
            "acc 2, sample 1: 0.70042194092827\n",
            "acc 3, sample 1: 0.0\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 2: 0.9595588235294118\n",
            "acc 1, sample 2: 0.37236533957845436\n",
            "acc 2, sample 2: 0.5892857142857143\n",
            "acc 3, sample 2: 0.0\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 3: 0.930327868852459\n",
            "acc 1, sample 3: 0.6033057851239669\n",
            "acc 2, sample 3: 0.4714285714285714\n",
            "acc 3, sample 3: 0.09090909090909091\n",
            "nbr_voices: tensor([3])\n",
            "acc 0, sample 4: 0.9933993399339934\n",
            "acc 1, sample 4: 0.466403162055336\n",
            "acc 2, sample 4: 0.8976377952755905\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 5: 0.9735849056603774\n",
            "acc 1, sample 5: 0.3416506717850288\n",
            "acc 2, sample 5: 0.6564705882352941\n",
            "acc 3, sample 5: 0.0\n",
            "nbr_voices: tensor([3])\n",
            "acc 0, sample 6: 0.9867109634551495\n",
            "acc 1, sample 6: 0.5658263305322129\n",
            "acc 2, sample 6: 0.9653333333333334\n",
            "note counters: v0: 2282 v1: 2323 v2: 2236 v3: 1238\n",
            "total_predictions_dict dict_keys(['0', '1', '2', '3'])\n",
            "0.9763004612065156 0.4911034822856947 0.7508308626749813 0.022727272727272728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9iWhIkMNjmp",
        "outputId": "2757f211-95e4-43ca-82dd-f65b20350683"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9763004612065156,\n",
              " 0.4911034822856947,\n",
              " 0.7508308626749813,\n",
              " 0.022727272727272728)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss3 * 1.5 = (0.8989219661919758, 0.7166572856993837, 0.8185917288474246, 0.0)"
      ],
      "metadata": {
        "id": "6nwnRTADQvQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 epoch, no loss modifier:\n",
        "#ACC:(0.9165209182020722,\n",
        "# 0.7864434689151618,\n",
        "# 0.8130949796045199,\n",
        "# 0.003652274754166715)\n",
        "\n",
        "# 20 epoch, no loss modifier:\n",
        "#(0.7962210840410273, 0.8669639629052727, 0.751302181991106, 0.0)\n",
        "\n",
        "# 20 ep, loss3 *1,5\n",
        "#(0.8721136343927623,\n",
        "# 0.8319586824413445,\n",
        "# 0.7563218924966578,\n",
        "# 0.09029535181592076)"
      ],
      "metadata": {
        "id": "6mCYnJLnHfYB"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate fugues F1 score"
      ],
      "metadata": {
        "id": "52P6em3ANb1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True:\n",
        "    f1_v0, f1_v1, f1_v2, f1_v3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=True)\n",
        "    print(f1_v0, f1_v1, f1_v2, f1_v3)"
      ],
      "metadata": {
        "id": "FLLmyO6o5vW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### take 0 -> compare to truth of 0,1,2,3 -> overall voice\n",
        "\n",
        "count_list = []\n",
        "\n",
        "count_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "truth_dic = {'0': 0, '1': 1, '2': 2, '3': 3 }\n",
        "\n",
        "voice_entry_list = [\"0\", \"1\", \"2\", \"3\"]\n",
        "for voice_entry_one in voice_entry_list:\n",
        "    for voice_entry_two in voice_entry_list:\n",
        "        count_list = []\n",
        "        #print(\"voices:\",voice_entry_one,voice_entry_two)\n",
        "        for i in range(len(dict_pred[voice_entry_one])):\n",
        "            if dict_pred[voice_entry_one][i][0] == truth_dic[voice_entry_two]:      #dict_truth[voice_entry_two][i][0]:\n",
        "                count_list.append(1)\n",
        "            else:\n",
        "                count_list.append(0)\n",
        "        count_dict[voice_entry_one].append(count_list)\n",
        "\n",
        "dictionary_sum={}\n",
        "for i in voice_entry_list:\n",
        "    v0_match,v1_match,v2_match,v3_match = count_dict[i]\n",
        "    sum_v0 = np.sum(v0_match)\n",
        "    sum_v1 = np.sum(v1_match)\n",
        "    sum_v2 = np.sum(v2_match)\n",
        "    sum_v3 = np.sum(v3_match)\n",
        "    dictionary_sum[\"v0\"] = sum_v0\n",
        "    dictionary_sum[\"v1\"] = sum_v1\n",
        "    dictionary_sum[\"v2\"] = sum_v2\n",
        "    dictionary_sum[\"v3\"] = sum_v3\n",
        "\n",
        "    val_list = list(dictionary_sum.values())\n",
        "    \n",
        "    print(\"voice{} matches with\".format(i))\n",
        "    print(\"dict\",dictionary_sum)\n",
        "\n",
        "    max_sum = max(sum_v0,sum_v1,sum_v2,sum_v3)\n",
        "\n",
        "\n",
        "    print(\"max_sum\", val_list.index(max_sum) )\n",
        "\n",
        "    print(\"accuracy voice{}:\".format(i), max_sum/(sum_v0+sum_v1+sum_v2+sum_v3) )\n",
        "    print(\"________________\")\n",
        "    print(\" \")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BoQcV_i038DD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "6ec9bfe6-8aac-4ac7-c5d7-6da8321dda39"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n### take 0 -> compare to truth of 0,1,2,3 -> overall voice\\n\\ncount_list = []\\n\\ncount_dict = {\\'0\\': [], \\'1\\': [], \\'2\\': [], \\'3\\': [] }\\ntruth_dic = {\\'0\\': 0, \\'1\\': 1, \\'2\\': 2, \\'3\\': 3 }\\n\\nvoice_entry_list = [\"0\", \"1\", \"2\", \"3\"]\\nfor voice_entry_one in voice_entry_list:\\n    for voice_entry_two in voice_entry_list:\\n        count_list = []\\n        #print(\"voices:\",voice_entry_one,voice_entry_two)\\n        for i in range(len(dict_pred[voice_entry_one])):\\n            if dict_pred[voice_entry_one][i][0] == truth_dic[voice_entry_two]:      #dict_truth[voice_entry_two][i][0]:\\n                count_list.append(1)\\n            else:\\n                count_list.append(0)\\n        count_dict[voice_entry_one].append(count_list)\\n\\ndictionary_sum={}\\nfor i in voice_entry_list:\\n    v0_match,v1_match,v2_match,v3_match = count_dict[i]\\n    sum_v0 = np.sum(v0_match)\\n    sum_v1 = np.sum(v1_match)\\n    sum_v2 = np.sum(v2_match)\\n    sum_v3 = np.sum(v3_match)\\n    dictionary_sum[\"v0\"] = sum_v0\\n    dictionary_sum[\"v1\"] = sum_v1\\n    dictionary_sum[\"v2\"] = sum_v2\\n    dictionary_sum[\"v3\"] = sum_v3\\n\\n    val_list = list(dictionary_sum.values())\\n    \\n    print(\"voice{} matches with\".format(i))\\n    print(\"dict\",dictionary_sum)\\n\\n    max_sum = max(sum_v0,sum_v1,sum_v2,sum_v3)\\n\\n\\n    print(\"max_sum\", val_list.index(max_sum) )\\n\\n    print(\"accuracy voice{}:\".format(i), max_sum/(sum_v0+sum_v1+sum_v2+sum_v3) )\\n    print(\"________________\")\\n    print(\" \")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR MONOPHONIC F1\n",
        "\n",
        "# start with GT\n",
        "# look at first note in pred-> save note label\n",
        "# look at second note in pred-> if same note as before : SUCESS if it is not: FAIL\n",
        " # DO This for all 4 voices\n",
        " ## in GT there is always the same voice following -> would always be an array of 1\n",
        "\n",
        "## POLYPHONIC \n",
        "\n",
        "# prbl after 1 note there can be multiple diff voices .. chords"
      ],
      "metadata": {
        "id": "-bR7gcej90qh"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you have the ground truth on the different parts that you get when you import your score. Each part correspond to a voice. So if your note array contains all notes of all voices, you have for each note in your note array a number that is the ground truth voice (that you take from the part) and a number that is the predicted voice (that you take from the maximum vote)."
      ],
      "metadata": {
        "id": "Z5q305YzvjMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start time, duration , pitch to separate \n",
        "\n",
        "use the onset_beat and duration_beat\n",
        "\n",
        "multiply them according to the values set when producing the pianorolls \n",
        "\n",
        "-> get the position in the pianoroll\n",
        "\n",
        "time_div = 12\n",
        "\n"
      ],
      "metadata": {
        "id": "EmvxtyaVKG27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "A94mchm4LV6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy_v0\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v1\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v2\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v3\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Accuracy0','Accuracy1','Accuracy2','Accuracy3'])\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TgJDHaxAgYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend('Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxMs8GEfMvPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"train_loss\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqMcJT5aFL01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chew chorals\n",
        "\n"
      ],
      "metadata": {
        "id": "fS4tzYkxr06Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_chew_pr (file_name, sentences):\n",
        "    path = \"AI-MA_project/chorales_converted/\"\n",
        "    fullname = os.path.join(path, \"chor\"+ file_name +\".xml\")\n",
        "    part = partitura.load_musicxml(fullname)\n",
        "        \n",
        "\n",
        "    ### apply chews method ### \n",
        "    chew_sep = partitura.musicanalysis.estimate_voices(part, monophonic_voices=True)\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_zero = np.where(chew_sep==1)\n",
        "    pos_one = np.where(chew_sep==2)\n",
        "    pos_two = np.where(chew_sep==3)\n",
        "    pos_three = np.where(chew_sep==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    part_zero = partitura.utils.ensure_notearray(part)[pos_zero]\n",
        "    part_one = partitura.utils.ensure_notearray(part)[pos_one]\n",
        "    part_two = partitura.utils.ensure_notearray(part)[pos_two]\n",
        "    part_three = partitura.utils.ensure_notearray(part)[pos_three]\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    pr_zero = partitura.utils.compute_pianoroll(part_zero, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(part_one, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(part_two, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    pr_three = partitura.utils.compute_pianoroll(part_three, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_three = pr_three.toarray()\n",
        "    \n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    #print(\"scores_comb.shape\",scores_comb.shape)\n",
        "    #print(\"sentences[:,None,:,:].shape\",sentences[:,None,:,:].shape)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "    \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "55-Dy39Cwg5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_chew(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    note_counter_0 = 0\n",
        "    note_counter_1 = 0\n",
        "    note_counter_2 = 0\n",
        "    note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx != 26 and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                note_array_3 = part_3.note_array\n",
        "\n",
        "                note_counter_0 += len(note_array_0)\n",
        "                note_counter_1 += len(note_array_1)\n",
        "                note_counter_2 += len(note_array_2)\n",
        "                note_counter_3 += len(note_array_3)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################\n",
        "\n",
        "                    \n",
        "                    \n",
        "                    prediction = calculate_chew_pr(file_name,voices[:,:,:,-1]) \n",
        "\n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                #print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                #print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                #print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                #print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "NVvLm9pdryYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sampel 26&27 dont work "
      ],
      "metadata": {
        "id": "yD0w6nJQ82iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_chew(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "UD11ziChvL_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# McLeod chorals"
      ],
      "metadata": {
        "id": "9ejZujau-TXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mcleod_pr (file_name, sentences):\n",
        "    path = \"AI-MA_project/hmm_sep/\"\n",
        "    fullname = os.path.join(path, \"chor\"+ file_name +\".mid\")\n",
        "        \n",
        "\n",
        "    ### apply chews method ### \n",
        "    part_hmm = partitura.load_score_midi(fullname,part_voice_assign_mode=2)\n",
        "    voice_info = partitura.utils.note_array_from_part(part_hmm)[\"voice\"]\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(voice_info==1)\n",
        "    pos_1 = np.where(voice_info==2)\n",
        "    pos_2 = np.where(voice_info==3)\n",
        "    pos_3 = np.where(voice_info==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    note_array_0= partitura.utils.ensure_notearray(part_hmm)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part_hmm)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part_hmm)[pos_2]\n",
        "    note_array_3 = partitura.utils.ensure_notearray(part_hmm)[pos_3]\n",
        "\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_three = pr_three.toarray()\n",
        "    \n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    #print(\"scores_comb.shape\",scores_comb.shape)\n",
        "    #print(\"sentences[:,None,:,:].shape\",sentences[:,None,:,:].shape)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "    \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "pQP8pq9a-S6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mc_leod(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    note_counter_0 = 0\n",
        "    note_counter_1 = 0\n",
        "    note_counter_2 = 0\n",
        "    note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx not in [16,17,18,27,28,32,44,45,48,49,50]: # and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                note_array_3 = part_3.note_array\n",
        "\n",
        "                note_counter_0 += len(note_array_0)\n",
        "                note_counter_1 += len(note_array_1)\n",
        "                note_counter_2 += len(note_array_2)\n",
        "                note_counter_3 += len(note_array_3)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################                   \n",
        "                    prediction = calculate_mcleod_pr(file_name,voices[:,:,:,-1]) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "2igkR5SI-Z1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "does not work for samples 16,17,18,27,28,32,44,45,48,49,50"
      ],
      "metadata": {
        "id": "g1JC5yeMBnnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_mc_leod(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "oxRr270dC_mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chew fugues"
      ],
      "metadata": {
        "id": "uenr6Uavaic3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_chew_pr_fugue (file_name, sentences, nbr_voices):\n",
        "    path = \"AI-MA_project/bach_fugues/\"\n",
        "    fullname = os.path.join(path, \"wtc\"+ file_name +\".mid\")\n",
        "\n",
        "    part = partitura.load_score_midi(fullname)\n",
        "\n",
        "\n",
        "    ### apply chews method ### \n",
        "    chew_sep = partitura.musicanalysis.estimate_voices(part, monophonic_voices=True)\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(chew_sep==1)\n",
        "    pos_1 = np.where(chew_sep==2)\n",
        "    pos_2 = np.where(chew_sep==3)\n",
        "    if nbr_voices ==4:\n",
        "        pos_3 = np.where(chew_sep==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    \n",
        "    note_array_0 = partitura.utils.ensure_notearray(part)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part)[pos_2]\n",
        "    if nbr_voices==4:\n",
        "        note_array_3 = partitura.utils.ensure_notearray(part)[pos_3]\n",
        "\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    onset_beat_0 = note_array_0['onset_beat'][-1]\n",
        "    duration_beat_0 = note_array_0['duration_beat'][-1]\n",
        "    beat_0 = onset_beat_0 + duration_beat_0\n",
        "    \n",
        "    onset_beat_1 = note_array_1['onset_beat'][-1]\n",
        "    duration_beat_1 = note_array_1['duration_beat'][-1]\n",
        "    beat_1 = onset_beat_1 + duration_beat_1\n",
        "    \n",
        "    onset_beat_2 = note_array_2['onset_beat'][-1]\n",
        "    duration_beat_2 = note_array_2['duration_beat'][-1]\n",
        "    beat_2 = onset_beat_2 + duration_beat_2\n",
        "\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_0)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_1)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_2)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    if nbr_voices==4:\n",
        "        onset_beat_3 = note_array_3['onset_beat'][-1]\n",
        "        duration_beat_3 = note_array_3['duration_beat'][-1]\n",
        "        beat_3 = onset_beat_3 + duration_beat_3\n",
        "        pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_3)\n",
        "        pr_three = pr_three.toarray()\n",
        "    else:\n",
        "        pr_three = np.zeros(pr_two.shape)\n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0)) \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "vNsE3VVMa9Oo"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_chew_fugue( train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx != 26 and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                \n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "\n",
        "                if len(part) == 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################\n",
        "                    prediction = calculate_chew_pr_fugue(file_name,voices[:,:,:,-1],nbr_voices) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "C50fX7xjasD1"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True: \n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_chew_fugue(val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "yWz4I9b7asVR",
        "outputId": "e7fa8528-2493-44ca-b3a3-50c6e1edf224",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=119\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:419: UserWarning: pitch spelling\n",
            "  warnings.warn(\"pitch spelling\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: voice estimation\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: create_part\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add time sigs and measures\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: tie notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: find tuplets\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: done create_part\n",
            "  part_name=part_names.get(part_nr, None),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0, sample 0: 0.054502369668246446\n",
            "acc 1, sample 0: 0.891832229580574\n",
            "acc 2, sample 0: 0.0\n",
            "acc 0, sample 1: 0.0\n",
            "acc 1, sample 1: 0.21465968586387435\n",
            "acc 2, sample 1: 0.0\n",
            "acc 3, sample 1: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=72 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0, sample 2: 0.04044117647058824\n",
            "acc 1, sample 2: 0.2459016393442623\n",
            "acc 2, sample 2: 0.2714285714285714\n",
            "acc 3, sample 2: 0.08776595744680851\n",
            "acc 0, sample 3: 0.1557377049180328\n",
            "acc 1, sample 3: 0.4793388429752066\n",
            "acc 2, sample 3: 0.11428571428571428\n",
            "acc 3, sample 3: 0.33454545454545453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=77 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=75 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=80 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=73 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=78 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=68 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=71 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=59 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=56 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=58 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=60 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=61 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=60\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=55 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0, sample 4: 0.6402640264026402\n",
            "acc 1, sample 4: 0.42292490118577075\n",
            "acc 2, sample 4: 0.1141732283464567\n",
            "acc 0, sample 5: 0.3754716981132076\n",
            "acc 1, sample 5: 0.32437619961612285\n",
            "acc 2, sample 5: 0.27058823529411763\n",
            "acc 3, sample 5: 0.24550898203592814\n",
            "acc 0, sample 6: 0.08970099667774087\n",
            "acc 1, sample 6: 0.47058823529411764\n",
            "acc 2, sample 6: 0.0\n",
            "total_predictions_dict dict_keys(['0', '1', '2', '3'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.19373113889292232,\n",
              " 0.43566024769427547,\n",
              " 0.11006796419355143,\n",
              " 0.1669550985070478)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# McLeod fugues"
      ],
      "metadata": {
        "id": "xofnsEX4DAME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mcleod_pr (file_name, sentences,nbr_voices):\n",
        "    path = \"AI-MA_project/bach_fugues/\"\n",
        "    fullname = os.path.join(path, \"wtc\"+ file_name +\".mid\")\n",
        "    ### apply chews method ### \n",
        "    part_hmm = partitura.load_score_midi(fullname,part_voice_assign_mode=2)\n",
        "    voice_info = partitura.utils.note_array_from_part(part_hmm)[\"voice\"]\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(voice_info==1)\n",
        "    pos_1 = np.where(voice_info==2)\n",
        "    pos_2 = np.where(voice_info==3)\n",
        "    if nbr_voices ==4:\n",
        "        pos_3 = np.where(voice_info==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    note_array_0= partitura.utils.ensure_notearray(part_hmm)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part_hmm)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part_hmm)[pos_2]\n",
        "    if nbr_voices==4:\n",
        "        note_array_3 = partitura.utils.ensure_notearray(part_hmm)[pos_3]\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    onset_beat_0 = note_array_0['onset_beat'][-1]\n",
        "    duration_beat_0 = note_array_0['duration_beat'][-1]\n",
        "    beat_0 = onset_beat_0 + duration_beat_0\n",
        "    \n",
        "    onset_beat_1 = note_array_1['onset_beat'][-1]\n",
        "    duration_beat_1 = note_array_1['duration_beat'][-1]\n",
        "    beat_1 = onset_beat_1 + duration_beat_1\n",
        "    \n",
        "    onset_beat_2 = note_array_2['onset_beat'][-1]\n",
        "    duration_beat_2 = note_array_2['duration_beat'][-1]\n",
        "    beat_2 = onset_beat_2 + duration_beat_2\n",
        "\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_0)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_1)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_2)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    if nbr_voices==4:\n",
        "        onset_beat_3 = note_array_3['onset_beat'][-1]\n",
        "        duration_beat_3 = note_array_3['duration_beat'][-1]\n",
        "        beat_3 = onset_beat_3 + duration_beat_3\n",
        "        pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_3)\n",
        "        pr_three = pr_three.toarray()\n",
        "    else:\n",
        "        pr_three = np.zeros(pr_two.shape)\n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0)) \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "CU4iH-25aD5I"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mc_leod_fugues(train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            #if idx not in [16,17,18,27,28,32,44,45,48,49,50]: # and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                \n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "\n",
        "                if len(part) == 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "           \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "          \n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################                   \n",
        "                    prediction = calculate_mcleod_pr(file_name,voices[:,:,:,-1],nbr_voices) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "DgFfq9Q4Csei"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True: \n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_mc_leod_fugues(val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kSrFU8e-uol",
        "outputId": "4833365c-615f-4563-bffc-81deb16d2bed"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=119\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:419: UserWarning: pitch spelling\n",
            "  warnings.warn(\"pitch spelling\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: voice estimation\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: create_part\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add time sigs and measures\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: tie notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: find tuplets\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: done create_part\n",
            "  part_name=part_names.get(part_nr, None),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0, sample 0: 1.0\n",
            "acc 1, sample 0: 0.9911699779249448\n",
            "acc 2, sample 0: 0.9942857142857143\n",
            "acc 0, sample 1: 1.0\n",
            "acc 1, sample 1: 0.9633507853403142\n",
            "acc 2, sample 1: 1.0\n",
            "acc 3, sample 1: 0.9960474308300395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=72 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0, sample 2: 1.0\n",
            "acc 1, sample 2: 0.9976580796252927\n",
            "acc 2, sample 2: 0.9821428571428571\n",
            "acc 3, sample 2: 0.9867021276595744\n",
            "acc 0, sample 3: 1.0\n",
            "acc 1, sample 3: 1.0\n",
            "acc 2, sample 3: 1.0\n",
            "acc 3, sample 3: 0.9963636363636363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=77 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=75 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=80 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=73 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=78 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=68 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=71 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=59 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=56 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=58 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=60 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=61 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=60\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=55 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc 0, sample 4: 1.0\n",
            "acc 1, sample 4: 0.9920948616600791\n",
            "acc 2, sample 4: 0.9881889763779528\n",
            "acc 0, sample 5: 1.0\n",
            "acc 1, sample 5: 0.9788867562380038\n",
            "acc 2, sample 5: 0.9929411764705882\n",
            "acc 3, sample 5: 0.9940119760479041\n",
            "acc 0, sample 6: 1.0\n",
            "acc 1, sample 6: 0.9943977591036415\n",
            "acc 2, sample 6: 0.9893333333333333\n",
            "total_predictions_dict dict_keys(['0', '1', '2', '3'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 0.9882226028417537, 0.9924131510872065, 0.9932812927252886)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old training loop - matrix and non matrix format"
      ],
      "metadata": {
        "id": "4olpdwzyG8dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        #accuracy_v0_sum = 0\n",
        "        #accuracy_v1_sum = 0\n",
        "        #accuracy_v2_sum = 0\n",
        "        #accuracy_v3_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4: \n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                    v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                    mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_pred_flat:\", v_pred_flat.shape)\n",
        "                    \"\"\"\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4:                   \n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                    v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                    mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                    print(\"old mask\", mask_ori.shape)\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_ori_flat\", v_ori_flat.shape)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)   \n",
        "                    print(\"acc\",acc)                    \n",
        "                    accuracy_sum += acc \n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                    acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                    for i in range(len(prediction[0,:])):\n",
        "                      acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                      accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    \n",
        "                    \"\"\"\n",
        "                    pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "\n",
        "                    acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                    acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                    acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                    if nbr_voices == 4:\n",
        "                        acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                    \n",
        "                    # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    if nbr_voices == 4:\n",
        "                        accuracy_v3_sum += acc_v3 / len(lens)\n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "            train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "            train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "            train_accuracy_v3 = accuracy_v3_sum / 18   ## bc only 18 pieces with len 3\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "            history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "            history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "            #if nbr_voices == 4:\n",
        "            history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3)) \n",
        "            #else:\n",
        "            #    print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2)) \n",
        "            \"\"\"\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_v0_sum = 0\n",
        "                accuracy_v1_sum = 0\n",
        "                accuracy_v2_sum = 0\n",
        "                accuracy_v3_sum = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                        truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                        acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                        for i in range(len(prediction[0,:])):\n",
        "                          acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                          val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                        #print(\"val_accuracy_sum_list[3]\",val_accuracy_sum_list[3])\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(train_dataloader)\n",
        "                    #val_acc_list[3] = val_acc_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_acc_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "                #history[\"val_acc_new\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        # Predict the model's output on a batch\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                            \n",
        "                        # compute the accuracy \n",
        "                        acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                        acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                        acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                        if nbr_voices == 4:\n",
        "                            acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                            \n",
        "                            \n",
        "                        # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                        accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                        accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                        accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                        if nbr_voices == 4:\n",
        "                            accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                    val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                    val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                    val_accuracy_v3 = accuracy_v3_sum / 18  ##len(val_dataloader). - bc 18 pieces only with voice 3\n",
        "\n",
        "\n",
        "                    val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    val_acc_list[3] = val_accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    val_acc_list[4] = val_accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                    \n",
        "\n",
        "\n",
        "                history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "                history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "                history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "                #if nbr_voices == 4:\n",
        "                history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "                #else:\n",
        "                #    print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2))\n",
        "\n",
        "\n",
        "                history[\"val_acc_new\"].append(val_acc_list)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "\n",
        "                        \"\"\"\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                        if nbr_voices == 4: \n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                        v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                        mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)                      \n",
        "                        if nbr_voices == 4:                   \n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                        v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                        mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)\n",
        "                        accuracy_sum += acc \n",
        "                        \"\"\"\n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "nfDV8MKGHE3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einfÃ¼hren damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "    \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "  \n",
        "\n",
        "    def __getitem__(self, idx):          \n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])    \n",
        "\n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "\n",
        "\n",
        "        return (v0, v1, v2, v3, v_all, length)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "x2lFUuw719EC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}