{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject_final_tensor_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "SsyC2uB0KfaT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b3976f-6e5e-4409-b7b9-2827b02109ac"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install partitura\n",
        "! pip install git+https://github.com/CPJKU/partitura.git@develop\n",
        "import partitura\n",
        "import statistics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/CPJKU/partitura.git@develop\n",
            "  Cloning https://github.com/CPJKU/partitura.git (to revision develop) to /tmp/pip-req-build-gn9m89ky\n",
            "  Running command git clone -q https://github.com/CPJKU/partitura.git /tmp/pip-req-build-gn9m89ky\n",
            "  Running command git checkout -b develop --track origin/develop\n",
            "  Switched to a new branch 'develop'\n",
            "  Branch 'develop' set up to track remote branch 'develop' from 'origin'.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.4.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (4.2.6)\n",
            "Requirement already satisfied: lark-parser in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (0.12.0)\n",
            "Requirement already satisfied: xmlschema in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.11.2)\n",
            "Requirement already satisfied: mido in /usr/local/lib/python3.7/dist-packages (from partitura==0.4.0) (1.2.10)\n",
            "Requirement already satisfied: elementpath<3.0.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from xmlschema->partitura==0.4.0) (2.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path_to_musicxml = \"AI-MA_project/chorales_converted/chor001.xml\"\n",
        "#part = partitura.load_musicxml(path_to_musicxml)\n",
        "#partitura.save_score_midi(part,\"chor001_5.mid\",5)"
      ],
      "metadata": {
        "id": "j6dq9ptQGmHB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader - Set the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "fugues = True\n",
        "\n",
        "if fugues == True:\n",
        "    PATH_TO_DATA = \"AI-MA_project/bach_pr_fugues\"\n",
        "else:\n",
        "    PATH_TO_DATA = \"AI-MA_project/pianoroll_88\"\n",
        "\n",
        "batch_size = 1 \n",
        "workers = 0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_new(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        self.name_list = [\"1f01\",\"1f02\",\"1f03\",\"1f04\",\"1f05\",\"1f06\",\"1f07\",\"1f08\",\"1f09\",\"1f10\",\"1f11\",\"1f12\",\"1f13\",\"1f14\",\"1f15\",\"1f16\",\"1f17\",\"1f18\",\"1f19\",\"1f20\",\"1f21\",\"1f22\",\"1f23\",\"1f24\",\"2f01\",\"2f02\",\"2f03\",\"2f04\",\"2f05\",\"2f06\",\"2f07\",\"2f08\",\"2f09\",\"2f10\",\"2f11\",\"2f12\",\"2f13\",\"2f14\",\"2f15\",\"2f16\",\"2f17\",\"2f18\",\"2f19\",\"2f20\",\"2f21\",\"2f22\",\"2f23\",\"2f24\"]\n",
        "        self.name_list_voice_3 =  ['1f01', '1f05', '1f12', '1f14', '1f16', '1f17', '1f18', '1f23', '1f24', '2f02', '2f05', '2f07', '2f08', '2f09', '2f16', '2f17',  '2f22', '2f23']\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einfÃ¼hren damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "\n",
        "                        file_names_list.append(name[-8:-4])\n",
        "\n",
        "                    if \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[49:53] + \".pkl\" in sorted(glob.glob(os.path.join(PATH_TO_DATA, \"voice_3\", \"*.pkl\"))):\n",
        "                        nbr_voices_list.append(4)\n",
        "                    else:\n",
        "                        nbr_voices_list.append(3)\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "\n",
        "\n",
        "            if iLabel == 3:  \n",
        "                voice_files = []\n",
        "                file_names_3 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[2], \"*.pkl\")))   \n",
        "                \n",
        "                ###### loop over all filnames in voices_2 and if an element there is not present in voices_3: append \"missing\" to the voice_files of label=3 => important bc. self.pr_dict[voice_3] has then len 42 and otherwise it would only have len 18  .. these \"missing\" el are not considered later in the dataloader (if len=3 is a diff case of get_idx)\n",
        "                for name in file_names_2:\n",
        "                    if name[45:49] in self.name_list_voice_3:\n",
        "                      correct_name_3 = \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[45:49] + \".pkl\"\n",
        "                      with open(correct_name_3 ,'rb') as f:  \n",
        "                            loaded_obj = pickle.load(f)  \n",
        "                            voice_files.append(loaded_obj)\n",
        "                    else:\n",
        "                      voice_files.append(\"missing\")\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                \n",
        "\n",
        "                \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                          loaded_obj = pickle.load(f)     \n",
        "                          voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 4:\n",
        "            for key, value in self.pr_dict.items():\n",
        "              out_list.append(self.pr_dict[key][idx])\n",
        "                              \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.tensor(out_list[3].T)\n",
        "            v_all = torch.tensor(out_list[4].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 3:\n",
        "\n",
        "            for key, value in self.pr_dict.items():\n",
        "                if key != \"voice_3\":\n",
        "                  out_list.append(self.pr_dict[key][idx]) \n",
        "            \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.zeros(v2.shape)\n",
        "            v_all = torch.tensor(out_list[3].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "            \n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        "
      ],
      "metadata": {
        "id": "3FxK6qr1FqIl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_chor(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einfÃ¼hren damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        file_names_list.append(name[-7:-4])      # e.g. name = AI-MA_project/pianoroll_88/voice_all/voice_all_001.pkl\n",
        "\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "              \n",
        "\n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])\n",
        "                            \n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "        file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "        voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "        \n",
        "        return (voices, length, 4, file_name)     # 4 bc nbr voices is always 4"
      ],
      "metadata": {
        "id": "3uQnok3VGngZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "if fugues == True:\n",
        "    dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "else:\n",
        "    dataset = MusicDataset_chor(PATH_TO_DATA)\n",
        "\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    if i == 1:\n",
        "        all_voices, length, nbr_voices, file_name = sample_batched\n",
        "        print(file_name[0],nbr_voices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXsRYzQSUuQU",
        "outputId": "77623768-2c69-496e-f1f2-0f7e7f397933"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1f03 tensor([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "pianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\n",
        "pianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\n",
        "pianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\n",
        "pianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\n",
        "pianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\n",
        "\n",
        "time_unit = \"beat\"\n",
        "time_div = 12\n",
        "piano_range = True\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fR2HaA_BqeHw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "30983ea5-c6c3-42d0-980d-5056395053bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npianoroll_0 = all_voices.squeeze()[:,:,0].numpy()\\npianoroll_1 = all_voices.squeeze()[:,:,1].numpy()\\npianoroll_2 = all_voices.squeeze()[:,:,2].numpy()\\npianoroll_3 = all_voices.squeeze()[:,:,3].numpy()\\npianoroll_all = all_voices.squeeze()[:,:,-1].numpy()\\n\\ntime_unit = \"beat\"\\ntime_div = 12\\npiano_range = True\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "fig, ax = plt.subplots(1, figsize=(20, 10))\n",
        "ax.imshow(pianoroll_all, origin=\"lower\", cmap='gray', interpolation='nearest', aspect='auto')\n",
        "ax.set_xlabel(f'Time ({time_unit}s/{time_div})')\n",
        "ax.set_ylabel('Piano key' if piano_range else 'MIDI pitch')\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "sRoyBJJVqX_u",
        "outputId": "ea230012-5b08-4149-f130-191566aa03c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfig, ax = plt.subplots(1, figsize=(20, 10))\\nax.imshow(pianoroll_all, origin=\"lower\", cmap=\\'gray\\', interpolation=\\'nearest\\', aspect=\\'auto\\')\\nax.set_xlabel(f\\'Time ({time_unit}s/{time_div})\\')\\nax.set_ylabel(\\'Piano key\\' if piano_range else \\'MIDI pitch\\')\\nplt.show()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "  if i ==10:\n",
        "    all_voices, length, nbr_voices, _ = sample_batched\n",
        "    all_voices_pr = all_voices[0,:,:,-1].numpy()\n",
        "    \n",
        "    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\n",
        "    print(note_array.shape)\n",
        "    print(note_array[:10])\n",
        "    print(note_array.dtype.names)\n",
        "\n",
        "    #print(i,nbr_voices,all_voices.shape)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "b4QCaMEi3nw7",
        "outputId": "2b691536-0561-41ad-fb30-583bb049d817"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor i, sample_batched in enumerate(loader):\\n    all_voices, length, nbr_voices = sample_batched\\n    if nbr_voices ==3:\\n      print(i,nbr_voices,all_voices.shape)\\n    else:\\n      print(i,nbr_voices)\\n\\nfor i, sample_batched in enumerate(loader):\\n  if i ==10:\\n    all_voices, length, nbr_voices, _ = sample_batched\\n    all_voices_pr = all_voices[0,:,:,-1].numpy()\\n    \\n    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\\n    print(note_array.shape)\\n    print(note_array[:10])\\n    print(note_array.dtype.names)\\n\\n    #print(i,nbr_voices,all_voices.shape)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Music - Model\n"
      ],
      "metadata": {
        "id": "JNqxeacDwxNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UNET "
      ],
      "metadata": {
        "id": "QAIfIM69VHI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "XMdlm0_Vyyhc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_0 = []\n",
        "loss_1 = []\n",
        "loss_2 = []\n",
        "loss_3 = []\n",
        "loss_all = []\n",
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"sum\")                       # use weight parameters maybe take 1/88       \n",
        "\n",
        "    \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                      ### squeeze output here before returning                                       \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()    ### v3 is automatically tensor containing only 0 bc dataloader does this for pieces with 3 voices\n",
        "        \n",
        "        weight_v0 = v0.sum()\n",
        "        weight_v1 = v1.sum()\n",
        "        weight_v2 = v2.sum()\n",
        "        weight_v3 = v3.sum()\n",
        "\n",
        "        stack_tensors_gt = torch.stack([v0, v1, v2, v3], dim=0)\n",
        "        stack_gt = torch.argmax(stack_tensors_gt,axis=0)\n",
        "\n",
        "        weight_tensor = torch.stack([weight_v0,weight_v1,weight_v2,weight_v3])\n",
        "        criterion_weighted = nn.CrossEntropyLoss(weight=weight_tensor,reduction='mean',ignore_index=99)        \n",
        "        stack_pred = torch.stack([score_0, score_1, score_2, score_3], dim=0).softmax(dim=0)[None, :]\n",
        "        \n",
        "        #stack_gt.to(device)\n",
        "        #stack_pred.to(device)\n",
        "        mask_pred = torch.squeeze(voices[:,:,:,-1])== 0\n",
        "        stack_gt[mask_pred] = 99\n",
        "\n",
        "        loss = criterion_weighted(stack_pred, stack_gt[None,:])\n",
        "\n",
        "        print(\"loss:\",loss)\n",
        "        return loss   \n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        stack_pred = torch.stack([score_0, score_1, score_2, score_3], dim=0).softmax(dim=0)[None, :]     ######added softmax\n",
        "        stack_pred = stack_pred.view(1,4,-1)[:,:,voices[:,:,:,-1].bool().flatten()]\n",
        "\n",
        "\n",
        "        stack_tensors_gt = torch.stack([v0, v1, v2, v3], dim=0).cpu().detach().numpy()\n",
        "        stack_gt = torch.tensor(np.argmax(stack_tensors_gt,axis=0)) # [None, :]\n",
        "        gt_where_1 = stack_gt.view(1,-1)[:,voices[:,:,:,-1].bool().flatten()]\n",
        "\n",
        "        gt_where_1.to(device)\n",
        "        stack_pred.to(device)\n",
        "\n",
        "        nbr_relevant_idx = voices[:,:,:,-1].sum()\n",
        "        loss = self.loss(stack_pred, gt_where_1)/ nbr_relevant_idx\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "        \"\"\"\n",
        "        stack_pred = torch.stack([score_0, score_1, score_2, score_3], dim=0).softmax(dim=0)[None, :]     ######added softmax\n",
        "        \n",
        "        print(\"v0 == cuda\", v0.is_cuda)\n",
        "        stack_tensors_gt = torch.stack([v0, v1, v2, v3], dim=0)#.cpu().detach().numpy()\n",
        "        stack_gt = torch.tensor(torch.argmax(stack_tensors_gt,axis=0))[None, :]\n",
        "        \n",
        "\n",
        "        stack_gt.to(device)\n",
        "        stack_pred.to(device)\n",
        "\n",
        "        print(\"stack_pred == cuda\", stack_pred.is_cuda)\n",
        "        print(\"stack_gt == cuda\",stack_gt.is_cuda)\n",
        "\n",
        "        nbr_relevant_idx = voices[:,:,:,-1].sum()\n",
        "        loss = self.loss(stack_pred, stack_gt)/ nbr_relevant_idx\n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        #loss_all.append(self.loss(stack_pred, gt_where_1).cpu().detach().numpy())\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "\n",
        "        else:\n",
        "            sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "            prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            \n",
        "            mask_pred = np.squeeze(sentences)== 0\n",
        "            v_pred_argm[mask_pred] = -1\n",
        "\n",
        "            return v_pred_argm        "
      ],
      "metadata": {
        "id": "CviiPTPOPW04"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"CNN\"\n",
        "lr = 0.0001  \n",
        "monophonic = True\n",
        "his = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "79cPe11WL6J0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2a925d20-e70d-4c08-b64b-9a9d801007b0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"CNN\"\\nlr = 0.0001  \\nmonophonic = True\\nhis = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Define Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                                   ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "                \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            #if nbr_voices == 4:\n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]             \n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "                    ### before\n",
        "                    #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                    #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                    #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                    #for i in range(len(prediction[0,:])):\n",
        "                    #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                    #  accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                    #prediction = model.predict(voices, lens, monophonic)                    #for voice vise masking\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)         #for mixed voice masking        \n",
        "\n",
        "\n",
        "                    ## ground truth in shape 1280x88 -> mixed voice\n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    truth = v_ori_argm       \n",
        "\n",
        "                    # outsource accurcy to further down -> just a placeholder right now\n",
        "                    v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            #train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            #train_acc_list[3] = accuracy_sum_list[3] / 18                        ## bc only 18 pieces with len 3\n",
        "            #train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "            #history[\"train_loss\"].append(train_loss)\n",
        "            #history[\"train_acc\"].append(train_acc_list)\n",
        "            #print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "            \n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "                        voices = voices.to(device).float()\n",
        "                        ### before\n",
        "                        #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                        #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                        #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                        #for i in range(len(prediction[0,:])):\n",
        "                        #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                        #  val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    #val_acc_list[3] = val_accuracy_sum_list[3] / 18                         ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_accuracy_sum_list[4] / 2                          ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                #history[\"val_acc\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        #prediction = model.predict(voices, lens, monophonic)                #for voice vise masking\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)     # for masking with mixed voice\n",
        "\n",
        "\n",
        "\n",
        "                        ## ground truth in shape 1280x88 -> mixed voice\n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        truth = v_ori_argm       \n",
        "\n",
        "                        # outsource accurcy to further down -> just a placeholder right now\n",
        "                        v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "                    \n",
        "                history[\"val_acc\"].append(val_accuracy)\n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    # save the model\n",
        "    #torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"RNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ge8pY70uHxF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6b94fc6f-a986-45fe-ab9a-c98acaaf6715"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"RNN\"\\nmonophonic = True\\nhis = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= [\"CNN\",\"RNN\"]\n",
        "monophonic_list = [True,False]\n",
        "\n",
        "for net in network_type:\n",
        "    for monophonic in monophonic_list: \n",
        "        print(\"network set to:\",net,\"monophnic:\",monophonic)\n",
        "        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Bs6-iNEBu8o",
        "outputId": "f6dd35c0-d15a-446e-ecd8-9edbd53c3d1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= [\"CNN\",\"RNN\"]\\nmonophonic_list = [True,False]\\n\\nfor net in network_type:\\n    for monophonic in monophonic_list: \\n        print(\"network set to:\",net,\"monophnic:\",monophonic)\\n        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        if fugues == True:\n",
        "        ### uncomment for fugues ###\n",
        "            train_dataset = MusicDataset_new(PATH_TO_DATA) \n",
        "        ### uncomment for chorals ###\n",
        "        else:\n",
        "            train_dataset = MusicDataset_chor(PATH_TO_DATA) \n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        ### uncomment for fugues ###\n",
        "        if fugues == True:\n",
        "            dataset = MusicDataset_new(PATH_TO_DATA) \n",
        "        ### uncomment for chorals ###\n",
        "        else:\n",
        "            dataset = MusicDataset_chor(PATH_TO_DATA)\n",
        "        \n",
        "        \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicNetwork\n",
        "epochs = 20\n",
        "lr = 0.001 # was 0.001    #0.00001\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Experiment"
      ],
      "metadata": {
        "id": "bdetlQP-LoRX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm",
        "outputId": "45648cfe-a7e2-4ebd-c7d0-b1382f141217",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataloader 34 val_dataloader 7\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Training on device: cuda\n",
            "monophonic set to: True\n",
            "loss: tensor(1.3630, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2831, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.4773, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2330, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.4281, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2015, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1636, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1556, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1708, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2715, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1044, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3154, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0658, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0981, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3173, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2238, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2318, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1670, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0551, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9971, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3433, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2426, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2588, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1871, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0298, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1696, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9844, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0294, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3258, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9351, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1194, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0082, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0237, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8762, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 1.172249906203326, Train Accuracy : 0.9866245608817316\n",
            " Validation Accuracy : 5.780696110139511\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "loss: tensor(0.8825, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8808, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1828, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9829, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3279, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8890, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9196, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9092, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9193, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8512, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9188, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1616, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9326, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8026, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2230, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1277, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1224, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0960, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9575, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8925, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2260, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1992, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2474, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8555, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9344, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1148, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8822, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9172, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2401, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8385, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0878, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9501, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0091, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8255, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 1.0090511038022882, Train Accuracy : 0.9917044769747041\n",
            " Validation Accuracy : 5.8075303242208935\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "loss: tensor(0.8536, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8361, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1604, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8447, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3062, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8419, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8863, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8810, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8762, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8272, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9042, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1643, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9022, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7989, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2215, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0993, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0907, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0955, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9191, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9126, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0869, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1618, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2137, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8326, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8991, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0895, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8594, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9016, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2455, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8069, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1165, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8971, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9659, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8036, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.979468929416993, Train Accuracy : 0.9923657535222368\n",
            " Validation Accuracy : 5.811211059961282\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "loss: tensor(0.8201, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8068, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1441, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8331, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3040, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8074, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8535, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8581, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8680, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8154, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8828, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1516, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8800, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7838, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2188, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1105, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0948, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0813, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9003, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9449, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0428, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0825, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2094, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8269, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8872, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0918, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8338, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8782, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2451, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8119, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0883, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8647, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9586, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7991, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9641087002613965, Train Accuracy : 0.99271947643394\n",
            " Validation Accuracy : 5.813080750114965\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "loss: tensor(0.8020, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8023, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1563, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8227, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3107, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7947, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8364, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8603, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8622, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8229, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8757, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1427, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8719, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7794, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2131, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1117, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0975, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0776, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9060, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9426, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0433, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0893, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2083, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8202, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8782, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0805, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8343, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8736, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2460, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7938, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0964, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8564, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9629, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8017, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9609848295941072, Train Accuracy : 0.9928155864673575\n",
            " Validation Accuracy : 5.813554467935593\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "loss: tensor(0.7986, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7960, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1376, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8256, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3061, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7883, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8148, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8539, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8613, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8127, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8805, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1358, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8742, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7749, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2131, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1125, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0899, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0843, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8945, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9110, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0408, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0780, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2050, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8177, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8771, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0848, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8242, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8636, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2397, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8237, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0826, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8472, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9557, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7998, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9560470055131352, Train Accuracy : 0.9929567683012381\n",
            " Validation Accuracy : 5.813830743828963\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "loss: tensor(0.8043, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7948, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1479, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8181, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3003, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7962, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8348, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8614, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8635, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8263, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8750, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1541, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8725, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7826, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2154, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1069, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0859, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0783, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9008, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9094, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0365, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0624, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1999, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8186, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8710, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0931, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8245, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8641, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2481, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8156, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0714, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8568, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9344, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7933, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9564139474840725, Train Accuracy : 0.9929840604856562\n",
            " Validation Accuracy : 5.814461568304502\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "loss: tensor(0.7929, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7862, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1657, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8075, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3173, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7780, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8142, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8408, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8537, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8101, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8664, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1396, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8670, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7728, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2112, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1061, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0859, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0823, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8896, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8976, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0345, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0595, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1991, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8154, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8695, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0933, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8203, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8628, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2365, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8015, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0716, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8374, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9231, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7932, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.950075593064813, Train Accuracy : 0.9931535809384358\n",
            " Validation Accuracy : 5.815026678913928\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "loss: tensor(0.7929, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7850, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1779, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8058, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3136, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7785, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7939, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8379, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8590, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8229, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8637, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1304, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8845, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7743, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2136, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1288, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1156, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0885, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8814, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8873, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0375, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0460, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2035, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8402, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8833, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0857, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8312, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8762, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2523, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7950, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0760, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8483, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9414, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7994, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9544638903702006, Train Accuracy : 0.9930266896349063\n",
            " Validation Accuracy : 5.814677253616653\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "loss: tensor(0.7900, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7857, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1154, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8071, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.3084, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7745, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7904, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8430, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8510, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8101, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8686, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1300, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8709, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7703, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2134, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1096, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0841, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0848, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8841, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8945, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0264, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0502, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1725, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8191, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8622, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0738, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8191, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8680, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2403, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7929, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0739, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8306, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9363, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7948, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9454666481298559, Train Accuracy : 0.9932762927222569\n",
            " Validation Accuracy : 5.815913730691264\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.7899, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7890, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1031, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8051, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2989, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7807, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7971, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8304, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8554, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8086, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8657, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1238, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8554, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7702, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2108, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1015, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0852, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0925, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8682, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8333, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0372, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0525, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1888, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8051, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8529, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0724, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8118, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8554, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2350, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7910, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0683, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8176, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9181, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7926, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9400992761639988, Train Accuracy : 0.9933228142007996\n",
            " Validation Accuracy : 5.816332058989112\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.7863, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7868, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0965, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8078, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2966, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7786, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7903, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8291, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8546, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8072, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8609, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1223, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8499, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7703, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2090, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0994, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0846, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0917, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8621, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8292, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0348, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0517, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1886, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8039, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8526, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0713, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8113, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8551, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2334, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7911, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0648, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8158, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9134, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7920, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9380221033797544, Train Accuracy : 0.9933800554551607\n",
            " Validation Accuracy : 5.816658232842346\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.7838, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7874, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0951, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8098, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2958, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7764, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7865, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8285, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8547, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8073, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8599, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1218, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8482, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7702, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2093, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0995, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0843, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0914, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8608, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8266, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0342, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0514, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1886, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8044, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8531, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0708, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8107, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8564, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2335, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7884, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0649, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8101, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9101, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7908, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9371957954238442, Train Accuracy : 0.9934096408795748\n",
            " Validation Accuracy : 5.816795006530147\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.7852, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7846, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0944, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8049, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2953, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7756, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7842, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8281, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8536, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8064, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8585, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1211, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8475, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7697, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2063, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0987, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0834, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0919, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8578, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8261, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0328, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0503, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1881, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8022, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8495, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0699, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8132, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8475, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2257, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7977, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0569, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8330, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9048, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7903, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9363260374349707, Train Accuracy : 0.9934553624246675\n",
            " Validation Accuracy : 5.8170335511592\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.7830, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7842, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0929, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8013, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2949, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7764, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7854, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8278, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8533, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8076, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8585, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1204, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8458, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7696, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2092, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0962, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0828, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0901, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8606, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8234, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0324, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0508, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1886, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8054, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8532, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0702, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8101, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8555, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2333, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7865, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0643, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8042, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.9002, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7887, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.935487366774503, Train Accuracy : 0.9934592870749199\n",
            " Validation Accuracy : 5.817036877384352\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.7857, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7835, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0912, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8040, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2942, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7745, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7807, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8271, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8508, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8058, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8567, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1208, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8457, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7687, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2060, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0984, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0814, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0921, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8589, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8239, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0301, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0489, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1879, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8033, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8495, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0689, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8103, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8484, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2270, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7928, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0569, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8199, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8937, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7889, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9346031248569489, Train Accuracy : 0.993505463960007\n",
            " Validation Accuracy : 5.817339116347981\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.7784, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7895, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0873, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8007, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2937, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7751, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7805, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8277, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8505, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8067, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8559, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1191, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8420, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7689, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2082, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0960, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0809, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0900, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8586, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8221, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0290, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0479, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1884, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8052, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8516, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0687, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8086, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8533, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2312, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7851, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0637, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8007, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8880, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7878, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9335566846763387, Train Accuracy : 0.9935236408295409\n",
            " Validation Accuracy : 5.817381983491499\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.7834, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7812, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0875, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8009, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2938, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7738, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7757, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8281, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8479, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8050, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8540, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1205, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8447, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7682, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2023, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0956, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0747, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0896, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8544, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8240, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0251, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0425, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1843, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7967, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8360, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0668, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8172, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8372, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1824, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8505, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0797, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8660, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8880, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7881, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9342862963676453, Train Accuracy : 0.9936012200357379\n",
            " Validation Accuracy : 5.8177793801724595\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.7790, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7828, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0888, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7964, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2944, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7764, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7794, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8290, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8484, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8068, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8549, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1189, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8390, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7683, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2095, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0971, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0771, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0884, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8589, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8210, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0279, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0458, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1886, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8065, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8501, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0678, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8088, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8543, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2281, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7856, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0623, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8004, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8847, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7880, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9327443855650285, Train Accuracy : 0.9935447511811876\n",
            " Validation Accuracy : 5.8174924697354715\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "loss: tensor(0.7832, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7790, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0812, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7974, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2936, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7728, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7766, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8313, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8460, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8041, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8503, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1189, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8483, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7668, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.2014, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0999, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0698, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0876, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8579, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8208, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0165, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0336, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1729, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7918, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8260, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0481, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7940, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8320, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.1673, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8213, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(1.0447, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8344, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.8595, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "loss: tensor(0.7834, device='cuda:0', grad_fn=<NllLoss2DBackward0>)\n",
            "Train Loss: 0.9268461080158458, Train Accuracy : 0.9937774382530503\n",
            " Validation Accuracy : 5.81838638018938\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy evalutaion F-scores"
      ],
      "metadata": {
        "id": "sJbWsH72N2Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. create folder with part object of all pieces \n",
        "2. load a piece from dataloader with true labels, the mixed piece and the part object \n",
        "3. create notearray from part object\n",
        "4. take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "\n",
        "Output: pianoroll\n",
        "\n",
        "1 note in notearray could be mulitple bins\n",
        "\n",
        "take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "note start at same time with different pitch -> different notes\n",
        "\n",
        "for each note array find corresponding matrix -> \n",
        "\n",
        "\n",
        "if note is only composed by 1 bin: save indx of vocie -> save it to note array\n",
        "\n",
        "if more than 1: look what are idx that compose this note -> majority note -> save it for the note array (if its 50/50 take it random -> count how often this happens) \n",
        "\n",
        "\n",
        "with idx : in note_array find which note corresponds to what voice"
      ],
      "metadata": {
        "id": "CFClch37N6nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader "
      ],
      "metadata": {
        "id": "afYHFVNMlMnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44596f1-e201-46bf-8ee7-06761285035f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7ff63c62df50>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## makes cell output nothing\n",
        "%%capture  \n",
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch20.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4TAhTQcpmx8m"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dic with key:filename, val: part_obj  for fugues"
      ],
      "metadata": {
        "id": "5RVmMv6Q9CJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if PATH_TO_DATA == \"AI-MA_project/bach_pr_fugues\":\n",
        "    path_parts = \"AI-MA_project/bach_fugues\"\n",
        "    part_dic = {}\n",
        "\n",
        "    #### create a list with all filenames in the right order ####\n",
        "    file_names_part = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.mid'): continue\n",
        "        file_names_part.append(filename[3:7])\n",
        "    #print(file_names_part)\n",
        "\n",
        "    #### create a list with all part objects in the right order ####\n",
        "    part_list = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.mid'): continue\n",
        "        fullname = os.path.join(path_parts, filename)\n",
        "        part = partitura.load_score_midi(fullname)\n",
        "        part_list.append(part)\n",
        "    #print(part_list)\n",
        "\n",
        "    #### create a dict with keys:filenames , values: part object ####\n",
        "    for i in range(len(file_names_part)):\n",
        "        part_dic[file_names_part[i]] = part_list[i]\n",
        "    \n",
        "    print(part_dic.keys(),part_dic.values())"
      ],
      "metadata": {
        "id": "_XYM_KWu2qkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b11ff8-5c39-44f6-b52b-2f53e3586bb9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=76 velocity=64 time=30\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=78 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=119\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:419: UserWarning: pitch spelling\n",
            "  warnings.warn(\"pitch spelling\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: voice estimation\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: create_part\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: add time sigs and measures\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: tie notes\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: find tuplets\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:489: UserWarning: done create_part\n",
            "  part_name=part_names.get(part_nr, None),\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=419\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=72 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=43 velocity=64 time=360\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=299\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=60 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=77 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=75 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=80 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=73 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=68 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=71 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=59 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=56 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=58 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=61 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=60\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=55 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=389\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=2 note=49 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=67 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=66 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=0 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=359\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:360: UserWarning: ignoring MIDI message note_off channel=1 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['1f01', '1f02', '1f03', '1f04', '1f05', '1f06', '1f07', '1f08', '1f09', '1f10', '1f11', '1f12', '1f13', '1f14', '1f15', '1f16', '1f17', '1f18', '1f19', '1f20', '1f21', '1f22', '1f23', '1f24', '2f01', '2f02', '2f03', '2f04', '2f05', '2f06', '2f07', '2f08', '2f09', '2f10', '2f11', '2f12', '2f13', '2f14', '2f15', '2f16', '2f17', '2f18', '2f19', '2f20', '2f21', '2f22', '2f23', '2f24']) dict_values([[<partitura.score.Part object at 0x7ff3335b1390>, <partitura.score.Part object at 0x7ff33383fdd0>, <partitura.score.Part object at 0x7ff3333af610>, <partitura.score.Part object at 0x7ff332dd3ed0>], [<partitura.score.Part object at 0x7ff3335aac90>, <partitura.score.Part object at 0x7ff33287e8d0>, <partitura.score.Part object at 0x7ff3326ebe90>], [<partitura.score.Part object at 0x7ff33227a450>, <partitura.score.Part object at 0x7ff3321942d0>, <partitura.score.Part object at 0x7ff3315cc050>], [<partitura.score.Part object at 0x7ff330bcc8d0>, <partitura.score.Part object at 0x7ff330c7e990>, <partitura.score.Part object at 0x7ff3302c00d0>, <partitura.score.Part object at 0x7ff3300fc950>, <partitura.score.Part object at 0x7ff32fce0550>], [<partitura.score.Part object at 0x7ff32f7c5ad0>, <partitura.score.Part object at 0x7ff32f8741d0>, <partitura.score.Part object at 0x7ff32f713c90>, <partitura.score.Part object at 0x7ff32f438550>], [<partitura.score.Part object at 0x7ff332896210>, <partitura.score.Part object at 0x7ff334248250>, <partitura.score.Part object at 0x7ff32edf2090>], [<partitura.score.Part object at 0x7ff32e803810>, <partitura.score.Part object at 0x7ff32e23a090>, <partitura.score.Part object at 0x7ff32def2050>], [<partitura.score.Part object at 0x7ff32d4c6c90>, <partitura.score.Part object at 0x7ff32ce54e10>, <partitura.score.Part object at 0x7ff32caa5610>], [<partitura.score.Part object at 0x7ff332145750>, <partitura.score.Part object at 0x7ff3367ed650>, <partitura.score.Part object at 0x7ff3260dd3d0>], [<partitura.score.Part object at 0x7ff32e838a90>, <partitura.score.Part object at 0x7ff334b63e90>], [<partitura.score.Part object at 0x7ff63c168990>, <partitura.score.Part object at 0x7ff332891b90>, <partitura.score.Part object at 0x7ff32d6cab90>], [<partitura.score.Part object at 0x7ff3345ea390>, <partitura.score.Part object at 0x7ff3317ad0d0>, <partitura.score.Part object at 0x7ff3353a4710>, <partitura.score.Part object at 0x7ff32e5717d0>], [<partitura.score.Part object at 0x7ff6300b82d0>, <partitura.score.Part object at 0x7ff32e80c950>, <partitura.score.Part object at 0x7ff3328967d0>], [<partitura.score.Part object at 0x7ff327a3d650>, <partitura.score.Part object at 0x7ff337492e10>, <partitura.score.Part object at 0x7ff3274ec910>, <partitura.score.Part object at 0x7ff32873b6d0>], [<partitura.score.Part object at 0x7ff331cb7a50>, <partitura.score.Part object at 0x7ff328679810>, <partitura.score.Part object at 0x7ff32dc3c850>], [<partitura.score.Part object at 0x7ff3369be250>, <partitura.score.Part object at 0x7ff337ac6090>, <partitura.score.Part object at 0x7ff3301e5f50>, <partitura.score.Part object at 0x7ff335b31050>], [<partitura.score.Part object at 0x7ff333fe3b50>, <partitura.score.Part object at 0x7ff630973290>, <partitura.score.Part object at 0x7ff335d7ef10>, <partitura.score.Part object at 0x7ff332bd1510>], [<partitura.score.Part object at 0x7ff32e612e50>, <partitura.score.Part object at 0x7ff333ff7990>, <partitura.score.Part object at 0x7ff326be66d0>, <partitura.score.Part object at 0x7ff32e4eb190>], [<partitura.score.Part object at 0x7ff32e61cc90>, <partitura.score.Part object at 0x7ff3357e4f50>, <partitura.score.Part object at 0x7ff3278da550>], [<partitura.score.Part object at 0x7ff32e583b10>, <partitura.score.Part object at 0x7ff32e757050>, <partitura.score.Part object at 0x7ff333519390>, <partitura.score.Part object at 0x7ff326b54090>], [<partitura.score.Part object at 0x7ff630e6ea90>, <partitura.score.Part object at 0x7ff337eea1d0>, <partitura.score.Part object at 0x7ff631de8a10>], [<partitura.score.Part object at 0x7ff334a49210>, <partitura.score.Part object at 0x7ff32e5bff90>, <partitura.score.Part object at 0x7ff337ad7d90>, <partitura.score.Part object at 0x7ff32d34acd0>, <partitura.score.Part object at 0x7ff3313b3f10>], [<partitura.score.Part object at 0x7ff630569850>, <partitura.score.Part object at 0x7ff3372a4cd0>, <partitura.score.Part object at 0x7ff63c0cdb90>, <partitura.score.Part object at 0x7ff334a0a890>], [<partitura.score.Part object at 0x7ff32fe21950>, <partitura.score.Part object at 0x7ff327d50110>, <partitura.score.Part object at 0x7ff336359990>, <partitura.score.Part object at 0x7ff33477c590>], [<partitura.score.Part object at 0x7ff3353ae110>, <partitura.score.Part object at 0x7ff33136c790>, <partitura.score.Part object at 0x7ff33391e850>], [<partitura.score.Part object at 0x7ff630e700d0>, <partitura.score.Part object at 0x7ff3318d8c50>, <partitura.score.Part object at 0x7ff32f963410>, <partitura.score.Part object at 0x7ff335d0f150>], [<partitura.score.Part object at 0x7ff630cb2b10>, <partitura.score.Part object at 0x7ff3370f3ed0>, <partitura.score.Part object at 0x7ff32e8de210>], [<partitura.score.Part object at 0x7ff333e8b590>, <partitura.score.Part object at 0x7ff3360ab090>, <partitura.score.Part object at 0x7ff327280890>], [<partitura.score.Part object at 0x7ff32847c710>, <partitura.score.Part object at 0x7ff3317bffd0>, <partitura.score.Part object at 0x7ff32723a650>, <partitura.score.Part object at 0x7ff3282c62d0>], [<partitura.score.Part object at 0x7ff630cbdc10>, <partitura.score.Part object at 0x7ff331a29890>, <partitura.score.Part object at 0x7ff63045fbd0>], [<partitura.score.Part object at 0x7ff630840950>, <partitura.score.Part object at 0x7ff3336c5990>, <partitura.score.Part object at 0x7ff333d8a790>, <partitura.score.Part object at 0x7ff33175f090>], [<partitura.score.Part object at 0x7ff3334a3510>, <partitura.score.Part object at 0x7ff334ca3110>, <partitura.score.Part object at 0x7ff336a5f090>, <partitura.score.Part object at 0x7ff3322fb710>], [<partitura.score.Part object at 0x7ff33242db90>, <partitura.score.Part object at 0x7ff337c67f90>, <partitura.score.Part object at 0x7ff6303313d0>, <partitura.score.Part object at 0x7ff631649950>], [<partitura.score.Part object at 0x7ff332592a90>, <partitura.score.Part object at 0x7ff32e96ba90>, <partitura.score.Part object at 0x7ff3378a5dd0>], [<partitura.score.Part object at 0x7ff3325834d0>, <partitura.score.Part object at 0x7ff33412d4d0>, <partitura.score.Part object at 0x7ff3357a8b90>], [<partitura.score.Part object at 0x7ff32cdca610>, <partitura.score.Part object at 0x7ff32cdecf10>, <partitura.score.Part object at 0x7ff631afe090>], [<partitura.score.Part object at 0x7ff327e7c1d0>, <partitura.score.Part object at 0x7ff327160a10>, <partitura.score.Part object at 0x7ff32784c890>], [<partitura.score.Part object at 0x7ff331878dd0>, <partitura.score.Part object at 0x7ff334320a10>, <partitura.score.Part object at 0x7ff33685d050>], [<partitura.score.Part object at 0x7ff32f3db050>, <partitura.score.Part object at 0x7ff3348fa110>, <partitura.score.Part object at 0x7ff32f44e790>], [<partitura.score.Part object at 0x7ff333d4dad0>, <partitura.score.Part object at 0x7ff335536090>, <partitura.score.Part object at 0x7ff631d87050>, <partitura.score.Part object at 0x7ff630000350>], [<partitura.score.Part object at 0x7ff334c0fe50>, <partitura.score.Part object at 0x7ff32cb184d0>, <partitura.score.Part object at 0x7ff336b2e050>, <partitura.score.Part object at 0x7ff337432e50>], [<partitura.score.Part object at 0x7ff337b12150>, <partitura.score.Part object at 0x7ff337fb6bd0>, <partitura.score.Part object at 0x7ff3301376d0>], [<partitura.score.Part object at 0x7ff3313e0390>, <partitura.score.Part object at 0x7ff32fe87450>, <partitura.score.Part object at 0x7ff330990150>], [<partitura.score.Part object at 0x7ff631038750>, <partitura.score.Part object at 0x7ff630600650>, <partitura.score.Part object at 0x7ff331d672d0>], [<partitura.score.Part object at 0x7ff631019a10>, <partitura.score.Part object at 0x7ff631617250>, <partitura.score.Part object at 0x7ff326fed810>], [<partitura.score.Part object at 0x7ff32819cc10>, <partitura.score.Part object at 0x7ff3281afd10>, <partitura.score.Part object at 0x7ff631e1c810>, <partitura.score.Part object at 0x7ff6315f4510>], [<partitura.score.Part object at 0x7ff630d83ad0>, <partitura.score.Part object at 0x7ff630c12150>, <partitura.score.Part object at 0x7ff337bdf090>, <partitura.score.Part object at 0x7ff33772a390>], [<partitura.score.Part object at 0x7ff630d88d10>, <partitura.score.Part object at 0x7ff336fff0d0>, <partitura.score.Part object at 0x7ff336c72d10>]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create dic with key:filename, val: part_obj  for chorales"
      ],
      "metadata": {
        "id": "6D9oTp_lNQbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if PATH_TO_DATA == \"AI-MA_project/pianoroll_88\":\n",
        "    path_parts = \"AI-MA_project/chorales_converted\"\n",
        "    part_dic = {}\n",
        "\n",
        "    #### create a list with all filenames in the right order ####\n",
        "    file_names_part = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.xml'): continue\n",
        "        file_names_part.append(filename[4:7])\n",
        "    #print(file_names_part)\n",
        "\n",
        "    #### create a list with all part objects in the right order ####\n",
        "    part_list = []\n",
        "    for filename in sorted(os.listdir(path_parts)):\n",
        "        if not filename.endswith('.xml'): continue\n",
        "        fullname = os.path.join(path_parts, filename)\n",
        "        part = partitura.load_musicxml(fullname)\n",
        "        part_list.append(part)\n",
        "    #print(part_list)\n",
        "\n",
        "    #### create a dict with keys:filenames , values: part object ####\n",
        "    for i in range(len(file_names_part)):\n",
        "        part_dic[file_names_part[i]] = part_list[i]\n",
        "    \n",
        "    print(\"part_dic.keys()\",part_dic.keys())\n",
        "    print(\"part_dic.values()\",part_dic.values())"
      ],
      "metadata": {
        "id": "_4q58c16NjbE"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate chorales"
      ],
      "metadata": {
        "id": "yphGmsr-NSV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate chorals"
      ],
      "metadata": {
        "id": "v4TJGKiUs086"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_one_choral(model, train_dataloader, part_dic,F1):\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match                                      \n",
        "            \n",
        "            #if idx > 40: # or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "                part_3 = part[3]\n",
        "\n",
        "                #note_array_0 = part_0.note_array\n",
        "                #note_array_1 = part_1.note_array\n",
        "                #note_array_2 = part_2.note_array\n",
        "                #note_array_3 = part_3.note_array                \n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                   \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]                   \n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        \n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "                        else:\n",
        "                            accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                        counting = 0\n",
        "                        ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                        for j in range(len(total_predictions_dict[i])):\n",
        "                            if total_predictions_dict[i][j][0] == gt:\n",
        "                                counting +=1  \n",
        "                        count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                    acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                    print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                    \n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "                    acc_score_dict[\"3\"].append(acc_3)\n",
        "                \n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    print(\"f1_v0 , sample {}:\".format(idx),f1_v0)\n",
        "                    print(\"f1_v1 , sample {}:\".format(idx),f1_v1)\n",
        "                    print(\"f1_v2 , sample {}:\".format(idx),f1_v2)\n",
        "                    \n",
        "                    if len(part)==4:\n",
        "                        pred_3 = accordance_dict[\"3\"]\n",
        "                        truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                        f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                        f_score_dict[\"3\"].append(f1_v3)\n",
        "                        print(\"f1_v3 , sample {}:\".format(idx),f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "UXr2DeiLSyLm"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_one_choral(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "ghAmXcXTTtD1"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    plt.plot(acc_score_dict[\"0\"],'-o')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend(['Accuracy0'])\n",
        "    plt.title('Accuracy vs Epochs')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "J92mdaQG0MXZ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate on F1 meassure"
      ],
      "metadata": {
        "id": "zYMy2JARxEBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    acc_0 , acc_1, acc_2, acc_3 = evaluate_one_choral(model,val_dataloader,part_dic,F1=True)\n",
        "    acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "id": "RWxVG3XAYTcC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate fugues"
      ],
      "metadata": {
        "id": "TzTpXHznL02j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import statistics\n",
        "\n",
        "\n",
        "def evaluate_accuracy_for_all(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "                                      \n",
        "                print(\"nbr_voices:\",nbr_voices)\n",
        "            #if idx == 0 or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "                if len(part)== 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                   \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        \n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "                            \n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                      counting = 0\n",
        "                      ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                      for j in range(len(total_predictions_dict[i])):\n",
        "                          if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                      count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                    print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "\n",
        "                    if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    print(\"f1_v0 , sample {}:\".format(idx),f1_v0)\n",
        "                    print(\"f1_v1 , sample {}:\".format(idx),f1_v1)\n",
        "                    print(\"f1_v2 , sample {}:\".format(idx),f1_v2)\n",
        "                    if len(part)==4:\n",
        "                      pred_3 = accordance_dict[\"3\"]\n",
        "                      truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                      f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                      f_score_dict[\"3\"].append(f1_v3)\n",
        "                      print(\"f1_v3 , sample {}:\".format(idx),f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "        return total_predictions_dict, total_truth_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])\n",
        "        #return total_predictions_dict, total_truth_dict"
      ],
      "metadata": {
        "id": "0xbN5YU8nGT0"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, train_dataloader, part_dic,F1):\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader): \n",
        "        print(\"nbr_voices:\",nbr_voices)\n",
        "\n",
        "test(model,val_dataloader,part_dic,F1=False)"
      ],
      "metadata": {
        "id": "p0HZ9d5TO_Aj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26c22967-1eb2-4678-b8fe-8a630306f8a6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nbr_voices: tensor([3])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([3])\n",
            "nbr_voices: tensor([4])\n",
            "nbr_voices: tensor([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True:\n",
        "    dict_pred , dict_gt, acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "20MP5Gk5kc2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe50512-2e10-44b4-f2b1-e5e512f2a0be"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nbr_voices: tensor([3])\n",
            "acc 0, sample 0: 0.9383886255924171\n",
            "acc 1, sample 0: 0.869757174392936\n",
            "acc 2, sample 0: 0.9885714285714285\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 1: 0.9857142857142858\n",
            "acc 1, sample 1: 0.9267015706806283\n",
            "acc 2, sample 1: 0.7468354430379747\n",
            "acc 3, sample 1: 0.0\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 2: 0.9595588235294118\n",
            "acc 1, sample 2: 0.65807962529274\n",
            "acc 2, sample 2: 0.6642857142857143\n",
            "acc 3, sample 2: 0.0\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 3: 0.9385245901639344\n",
            "acc 1, sample 3: 0.7933884297520661\n",
            "acc 2, sample 3: 0.7142857142857143\n",
            "acc 3, sample 3: 0.0\n",
            "nbr_voices: tensor([3])\n",
            "acc 0, sample 4: 0.9471947194719472\n",
            "acc 1, sample 4: 0.7707509881422925\n",
            "acc 2, sample 4: 0.9803149606299213\n",
            "nbr_voices: tensor([4])\n",
            "acc 0, sample 5: 0.9452830188679245\n",
            "acc 1, sample 5: 0.7754318618042226\n",
            "acc 2, sample 5: 0.6705882352941176\n",
            "acc 3, sample 5: 0.0\n",
            "nbr_voices: tensor([3])\n",
            "acc 0, sample 6: 0.9568106312292359\n",
            "acc 1, sample 6: 0.7843137254901961\n",
            "acc 2, sample 6: 0.9893333333333333\n",
            "total_predictions_dict dict_keys(['0', '1', '2', '3'])\n",
            "0.9530678135098796 0.7969176250792974 0.8220306899197434 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### only train for nbr_voices == 4"
      ],
      "metadata": {
        "id": "qO3rzvsK2Hb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss3 * 1.5 = (0.8989219661919758, 0.7166572856993837, 0.8185917288474246, 0.0)"
      ],
      "metadata": {
        "id": "6nwnRTADQvQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 epoch, no loss modifier:\n",
        "#ACC:(0.9165209182020722,\n",
        "# 0.7864434689151618,\n",
        "# 0.8130949796045199,\n",
        "# 0.003652274754166715)\n",
        "\n",
        "# 20 epoch, no loss modifier:\n",
        "#(0.7962210840410273, 0.8669639629052727, 0.751302181991106, 0.0)\n",
        "\n",
        "# 20 ep, loss3 *1,5\n",
        "#(0.8721136343927623,\n",
        "# 0.8319586824413445,\n",
        "# 0.7563218924966578,\n",
        "# 0.09029535181592076)"
      ],
      "metadata": {
        "id": "6mCYnJLnHfYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluate fugues F1 score"
      ],
      "metadata": {
        "id": "52P6em3ANb1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True:\n",
        "    f1_v0, f1_v1, f1_v2, f1_v3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=True)\n",
        "    print(f1_v0, f1_v1, f1_v2, f1_v3)"
      ],
      "metadata": {
        "id": "FLLmyO6o5vW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### take 0 -> compare to truth of 0,1,2,3 -> overall voice\n",
        "\n",
        "count_list = []\n",
        "\n",
        "count_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "truth_dic = {'0': 0, '1': 1, '2': 2, '3': 3 }\n",
        "\n",
        "voice_entry_list = [\"0\", \"1\", \"2\", \"3\"]\n",
        "for voice_entry_one in voice_entry_list:\n",
        "    for voice_entry_two in voice_entry_list:\n",
        "        count_list = []\n",
        "        #print(\"voices:\",voice_entry_one,voice_entry_two)\n",
        "        for i in range(len(dict_pred[voice_entry_one])):\n",
        "            if dict_pred[voice_entry_one][i][0] == truth_dic[voice_entry_two]:      #dict_truth[voice_entry_two][i][0]:\n",
        "                count_list.append(1)\n",
        "            else:\n",
        "                count_list.append(0)\n",
        "        count_dict[voice_entry_one].append(count_list)\n",
        "\n",
        "dictionary_sum={}\n",
        "for i in voice_entry_list:\n",
        "    v0_match,v1_match,v2_match,v3_match = count_dict[i]\n",
        "    sum_v0 = np.sum(v0_match)\n",
        "    sum_v1 = np.sum(v1_match)\n",
        "    sum_v2 = np.sum(v2_match)\n",
        "    sum_v3 = np.sum(v3_match)\n",
        "    dictionary_sum[\"v0\"] = sum_v0\n",
        "    dictionary_sum[\"v1\"] = sum_v1\n",
        "    dictionary_sum[\"v2\"] = sum_v2\n",
        "    dictionary_sum[\"v3\"] = sum_v3\n",
        "\n",
        "    val_list = list(dictionary_sum.values())\n",
        "    \n",
        "    print(\"voice{} matches with\".format(i))\n",
        "    print(\"dict\",dictionary_sum)\n",
        "\n",
        "    max_sum = max(sum_v0,sum_v1,sum_v2,sum_v3)\n",
        "\n",
        "\n",
        "    print(\"max_sum\", val_list.index(max_sum) )\n",
        "\n",
        "    print(\"accuracy voice{}:\".format(i), max_sum/(sum_v0+sum_v1+sum_v2+sum_v3) )\n",
        "    print(\"________________\")\n",
        "    print(\" \")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BoQcV_i038DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR MONOPHONIC F1\n",
        "\n",
        "# start with GT\n",
        "# look at first note in pred-> save note label\n",
        "# look at second note in pred-> if same note as before : SUCESS if it is not: FAIL\n",
        " # DO This for all 4 voices\n",
        " ## in GT there is always the same voice following -> would always be an array of 1\n",
        "\n",
        "## POLYPHONIC \n",
        "\n",
        "# prbl after 1 note there can be multiple diff voices .. chords"
      ],
      "metadata": {
        "id": "-bR7gcej90qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you have the ground truth on the different parts that you get when you import your score. Each part correspond to a voice. So if your note array contains all notes of all voices, you have for each note in your note array a number that is the ground truth voice (that you take from the part) and a number that is the predicted voice (that you take from the maximum vote)."
      ],
      "metadata": {
        "id": "Z5q305YzvjMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start time, duration , pitch to separate \n",
        "\n",
        "use the onset_beat and duration_beat\n",
        "\n",
        "multiply them according to the values set when producing the pianorolls \n",
        "\n",
        "-> get the position in the pianoroll\n",
        "\n",
        "time_div = 12\n",
        "\n"
      ],
      "metadata": {
        "id": "EmvxtyaVKG27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "A94mchm4LV6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy_v0\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v1\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v2\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v3\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Accuracy0','Accuracy1','Accuracy2','Accuracy3'])\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TgJDHaxAgYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend('Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxMs8GEfMvPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"train_loss\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqMcJT5aFL01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chew chorals\n",
        "\n"
      ],
      "metadata": {
        "id": "fS4tzYkxr06Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_chew_pr (file_name, sentences):\n",
        "    path = \"AI-MA_project/chorales_converted/\"\n",
        "    fullname = os.path.join(path, \"chor\"+ file_name +\".xml\")\n",
        "    part = partitura.load_musicxml(fullname)\n",
        "        \n",
        "\n",
        "    ### apply chews method ### \n",
        "    chew_sep = partitura.musicanalysis.estimate_voices(part, monophonic_voices=True)\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_zero = np.where(chew_sep==1)\n",
        "    pos_one = np.where(chew_sep==2)\n",
        "    pos_two = np.where(chew_sep==3)\n",
        "    pos_three = np.where(chew_sep==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    part_zero = partitura.utils.ensure_notearray(part)[pos_zero]\n",
        "    part_one = partitura.utils.ensure_notearray(part)[pos_one]\n",
        "    part_two = partitura.utils.ensure_notearray(part)[pos_two]\n",
        "    part_three = partitura.utils.ensure_notearray(part)[pos_three]\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    pr_zero = partitura.utils.compute_pianoroll(part_zero, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(part_one, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(part_two, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    pr_three = partitura.utils.compute_pianoroll(part_three, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_three = pr_three.toarray()\n",
        "    \n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    #print(\"scores_comb.shape\",scores_comb.shape)\n",
        "    #print(\"sentences[:,None,:,:].shape\",sentences[:,None,:,:].shape)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "    \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "55-Dy39Cwg5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_chew(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    #note_counter_0 = 0\n",
        "    #note_counter_1 = 0\n",
        "    #note_counter_2 = 0\n",
        "    #note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx != 26 and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                #note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                #note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                #note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                #note_array_3 = part_3.note_array\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "                #note_counter_0 += len(note_array_0)\n",
        "                #note_counter_1 += len(note_array_1)\n",
        "                #note_counter_2 += len(note_array_2)\n",
        "                #note_counter_3 += len(note_array_3)\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################            \n",
        "                    prediction = calculate_chew_pr(file_name,voices[:,:,:,-1]) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    #print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "NVvLm9pdryYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sampel 26&27 dont work "
      ],
      "metadata": {
        "id": "yD0w6nJQ82iB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_chew(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "UD11ziChvL_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# McLeod chorals"
      ],
      "metadata": {
        "id": "9ejZujau-TXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mcleod_pr (file_name, sentences):\n",
        "    path = \"AI-MA_project/hmm_sep/\"\n",
        "    fullname = os.path.join(path, \"chor\"+ file_name +\".mid\")\n",
        "        \n",
        "\n",
        "    ### apply chews method ### \n",
        "    part_hmm = partitura.load_score_midi(fullname,part_voice_assign_mode=2)\n",
        "    voice_info = partitura.utils.note_array_from_part(part_hmm)[\"voice\"]\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(voice_info==1)\n",
        "    pos_1 = np.where(voice_info==2)\n",
        "    pos_2 = np.where(voice_info==3)\n",
        "    pos_3 = np.where(voice_info==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    note_array_0= partitura.utils.ensure_notearray(part_hmm)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part_hmm)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part_hmm)[pos_2]\n",
        "    note_array_3 = partitura.utils.ensure_notearray(part_hmm)[pos_3]\n",
        "\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True)\n",
        "    pr_three = pr_three.toarray()\n",
        "    \n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    #print(\"scores_comb.shape\",scores_comb.shape)\n",
        "    #print(\"sentences[:,None,:,:].shape\",sentences[:,None,:,:].shape)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "    \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "pQP8pq9a-S6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mc_leod(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx not in [16,17,18,27,28,32,44,45,48,49,50]: # and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                #note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                #note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                #note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                #note_array_3 = part_3.note_array\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################                   \n",
        "                    prediction = calculate_mcleod_pr(file_name,voices[:,:,:,-1]) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "                acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        "                print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "                acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "2igkR5SI-Z1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "does not work for samples 16,17,18,27,28,32,44,45,48,49,50"
      ],
      "metadata": {
        "id": "g1JC5yeMBnnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == False:\n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_mc_leod(model,val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "oxRr270dC_mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chew fugues"
      ],
      "metadata": {
        "id": "uenr6Uavaic3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_chew_pr_fugue (file_name, sentences, nbr_voices):\n",
        "    path = \"AI-MA_project/bach_fugues/\"\n",
        "    fullname = os.path.join(path, \"wtc\"+ file_name +\".mid\")\n",
        "\n",
        "    part = partitura.load_score_midi(fullname)\n",
        "\n",
        "\n",
        "    ### apply chews method ### \n",
        "    chew_sep = partitura.musicanalysis.estimate_voices(part, monophonic_voices=True)\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(chew_sep==1)\n",
        "    pos_1 = np.where(chew_sep==2)\n",
        "    pos_2 = np.where(chew_sep==3)\n",
        "    if nbr_voices ==4:\n",
        "        pos_3 = np.where(chew_sep==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    \n",
        "    note_array_0 = partitura.utils.ensure_notearray(part)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part)[pos_2]\n",
        "    if nbr_voices==4:\n",
        "        note_array_3 = partitura.utils.ensure_notearray(part)[pos_3]\n",
        "\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    onset_beat_0 = note_array_0['onset_beat'][-1]\n",
        "    duration_beat_0 = note_array_0['duration_beat'][-1]\n",
        "    beat_0 = onset_beat_0 + duration_beat_0\n",
        "    \n",
        "    onset_beat_1 = note_array_1['onset_beat'][-1]\n",
        "    duration_beat_1 = note_array_1['duration_beat'][-1]\n",
        "    beat_1 = onset_beat_1 + duration_beat_1\n",
        "    \n",
        "    onset_beat_2 = note_array_2['onset_beat'][-1]\n",
        "    duration_beat_2 = note_array_2['duration_beat'][-1]\n",
        "    beat_2 = onset_beat_2 + duration_beat_2\n",
        "\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_0)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_1)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_2)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    if nbr_voices==4:\n",
        "        onset_beat_3 = note_array_3['onset_beat'][-1]\n",
        "        duration_beat_3 = note_array_3['duration_beat'][-1]\n",
        "        beat_3 = onset_beat_3 + duration_beat_3\n",
        "        pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_3)\n",
        "        pr_three = pr_three.toarray()\n",
        "    else:\n",
        "        pr_three = np.zeros(pr_two.shape)\n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0)) \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "vNsE3VVMa9Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_chew_fugue( train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            if idx != 26 and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                \n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "\n",
        "                if len(part) == 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "\n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "            \n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################\n",
        "                    prediction = calculate_chew_pr_fugue(file_name,voices[:,:,:,-1],nbr_voices) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "C50fX7xjasD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True: \n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_chew_fugue(val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "yWz4I9b7asVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# McLeod fugues"
      ],
      "metadata": {
        "id": "xofnsEX4DAME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mcleod_pr (file_name, sentences,nbr_voices):\n",
        "    path = \"AI-MA_project/bach_fugues/\"\n",
        "    fullname = os.path.join(path, \"wtc\"+ file_name +\".mid\")\n",
        "    ### apply chews method ### \n",
        "    part_hmm = partitura.load_score_midi(fullname,part_voice_assign_mode=2)\n",
        "    voice_info = partitura.utils.note_array_from_part(part_hmm)[\"voice\"]\n",
        "\n",
        "    ### seperate the results -> e.g. pos_zero = all positions where chew prediction says voice 0 ####\n",
        "    pos_0 = np.where(voice_info==1)\n",
        "    pos_1 = np.where(voice_info==2)\n",
        "    pos_2 = np.where(voice_info==3)\n",
        "    if nbr_voices ==4:\n",
        "        pos_3 = np.where(voice_info==4)\n",
        "\n",
        "    ### create notearray object that contain only the corresponding voice ###\n",
        "    note_array_0= partitura.utils.ensure_notearray(part_hmm)[pos_0]\n",
        "    note_array_1 = partitura.utils.ensure_notearray(part_hmm)[pos_1]\n",
        "    note_array_2 = partitura.utils.ensure_notearray(part_hmm)[pos_2]\n",
        "    if nbr_voices==4:\n",
        "        note_array_3 = partitura.utils.ensure_notearray(part_hmm)[pos_3]\n",
        "\n",
        "    ### create pr representation of all voices ###\n",
        "    onset_beat_0 = note_array_0['onset_beat'][-1]\n",
        "    duration_beat_0 = note_array_0['duration_beat'][-1]\n",
        "    beat_0 = onset_beat_0 + duration_beat_0\n",
        "    \n",
        "    onset_beat_1 = note_array_1['onset_beat'][-1]\n",
        "    duration_beat_1 = note_array_1['duration_beat'][-1]\n",
        "    beat_1 = onset_beat_1 + duration_beat_1\n",
        "    \n",
        "    onset_beat_2 = note_array_2['onset_beat'][-1]\n",
        "    duration_beat_2 = note_array_2['duration_beat'][-1]\n",
        "    beat_2 = onset_beat_2 + duration_beat_2\n",
        "\n",
        "    pr_zero = partitura.utils.compute_pianoroll(note_array_0, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_0)\n",
        "    pr_zero = pr_zero.toarray()\n",
        "\n",
        "    pr_one = partitura.utils.compute_pianoroll(note_array_1, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_1)\n",
        "    pr_one = pr_one.toarray()\n",
        "\n",
        "    pr_two = partitura.utils.compute_pianoroll(note_array_2, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_2)\n",
        "    pr_two = pr_two.toarray()\n",
        "\n",
        "    if nbr_voices==4:\n",
        "        onset_beat_3 = note_array_3['onset_beat'][-1]\n",
        "        duration_beat_3 = note_array_3['duration_beat'][-1]\n",
        "        beat_3 = onset_beat_3 + duration_beat_3\n",
        "        pr_three = partitura.utils.compute_pianoroll(note_array_3, time_unit = \"beat\",time_div = 12,piano_range=True,remove_silence=False,end_time=beat_3)\n",
        "        pr_three = pr_three.toarray()\n",
        "    else:\n",
        "        pr_three = np.zeros(pr_two.shape)\n",
        "\n",
        "    scores_comb = np.stack([pr_zero, pr_one, pr_two, pr_three], axis=0)\n",
        "    scores_comb = np.swapaxes(scores_comb, 1, 2)\n",
        "    scores_comb = scores_comb[None,:,:,:]\n",
        "    scores_comb = torch.from_numpy(scores_comb)\n",
        "\n",
        "    sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "    prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0)) \n",
        "    mask_pred = np.squeeze(sentences)== 0\n",
        "    v_pred_argm[mask_pred] = -1\n",
        "\n",
        "    return v_pred_argm "
      ],
      "metadata": {
        "id": "CU4iH-25aD5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_mc_leod_fugues(train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match           \n",
        "            #if idx not in [16,17,18,27,28,32,44,45,48,49,50]: # and idx != 27: # or idx==2:                \n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                part_1 = part[1]\n",
        "                part_2 = part[2]\n",
        "\n",
        "                note_array_0 = partitura.utils.note_array_from_part(part_0)\n",
        "                note_array_1 = partitura.utils.note_array_from_part(part_1)\n",
        "                note_array_2 = partitura.utils.note_array_from_part(part_2)\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "\n",
        "\n",
        "                if len(part) == 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = partitura.utils.note_array_from_part(part_3)\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "\n",
        "           \n",
        "                \n",
        "                ground_truth_label_list = [0,1,2,3]              \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "          \n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):                    \n",
        "                    #### get only indices that are positive\n",
        "                    onset_beat = note_array[\"onset_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "\n",
        "                    if onset_beat[0] < 0:\n",
        "                        onset_beat -= onset_beat[0]  ### if 1st value of onset_beat is negative add the value of this entry to the whole entry (therefore -)\n",
        "\n",
        "                    duration_beat = note_array[\"duration_beat\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    \n",
        "                    pitch_list = note_array[\"pitch\"]#[note_array[\"onset_beat\"]>=0]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                    \n",
        "\n",
        "                    ################################### MODEL PREDICTION ###################################                   \n",
        "                    prediction = calculate_mcleod_pr(file_name,voices[:,:,:,-1],nbr_voices) \n",
        "                    label = ground_truth_label_list[el_note_arr]\n",
        "                \n",
        "\n",
        "                    for i in range(len(note_idx_start)):\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                        accordance_dict[str(label)].append(0)\n",
        "\n",
        "                count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                    counting = 0\n",
        "                    ### maybe insert if statement: if list_of_note_arrays == 4 oder sowas \n",
        "                    for j in range(len(total_predictions_dict[i])):\n",
        "                        if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                    count_dict_2[i].append(counting)\n",
        "\n",
        "                acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                print(\"acc 0, sample {}:\".format(idx),acc_0)\n",
        "                print(\"acc 1, sample {}:\".format(idx),acc_1)\n",
        "                print(\"acc 2, sample {}:\".format(idx),acc_2)\n",
        " \n",
        "                acc_score_dict[\"0\"].append(acc_0)\n",
        "                acc_score_dict[\"1\"].append(acc_1)\n",
        "                acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if len(list_of_note_arrays)==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"acc 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "    print(\"total_predictions_dict\",total_predictions_dict.keys())\n",
        "    return total_predictions_dict, acc_score_dict, statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])"
      ],
      "metadata": {
        "id": "DgFfq9Q4Csei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if fugues == True: \n",
        "    dict_pred , acc_score_dict, acc_0 , acc_1, acc_2, acc_3 = evaluate_mc_leod_fugues(val_dataloader,part_dic,F1=False)\n",
        "    print(acc_0 , acc_1, acc_2, acc_3)"
      ],
      "metadata": {
        "id": "5kSrFU8e-uol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old training loop - matrix and non matrix format"
      ],
      "metadata": {
        "id": "4olpdwzyG8dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        #accuracy_v0_sum = 0\n",
        "        #accuracy_v1_sum = 0\n",
        "        #accuracy_v2_sum = 0\n",
        "        #accuracy_v3_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4: \n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                    v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                    mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_pred_flat:\", v_pred_flat.shape)\n",
        "                    \"\"\"\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4:                   \n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                    v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                    mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                    print(\"old mask\", mask_ori.shape)\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_ori_flat\", v_ori_flat.shape)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)   \n",
        "                    print(\"acc\",acc)                    \n",
        "                    accuracy_sum += acc \n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                    acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                    for i in range(len(prediction[0,:])):\n",
        "                      acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                      accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    \n",
        "                    \"\"\"\n",
        "                    pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "\n",
        "                    acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                    acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                    acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                    if nbr_voices == 4:\n",
        "                        acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                    \n",
        "                    # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    if nbr_voices == 4:\n",
        "                        accuracy_v3_sum += acc_v3 / len(lens)\n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "            train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "            train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "            train_accuracy_v3 = accuracy_v3_sum / 18   ## bc only 18 pieces with len 3\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "            history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "            history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "            #if nbr_voices == 4:\n",
        "            history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3)) \n",
        "            #else:\n",
        "            #    print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2)) \n",
        "            \"\"\"\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_v0_sum = 0\n",
        "                accuracy_v1_sum = 0\n",
        "                accuracy_v2_sum = 0\n",
        "                accuracy_v3_sum = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                        truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                        acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                        for i in range(len(prediction[0,:])):\n",
        "                          acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                          val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                        #print(\"val_accuracy_sum_list[3]\",val_accuracy_sum_list[3])\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(train_dataloader)\n",
        "                    #val_acc_list[3] = val_acc_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_acc_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "                #history[\"val_acc_new\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        # Predict the model's output on a batch\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                            \n",
        "                        # compute the accuracy \n",
        "                        acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                        acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                        acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                        if nbr_voices == 4:\n",
        "                            acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                            \n",
        "                            \n",
        "                        # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                        accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                        accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                        accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                        if nbr_voices == 4:\n",
        "                            accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                    val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                    val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                    val_accuracy_v3 = accuracy_v3_sum / 18  ##len(val_dataloader). - bc 18 pieces only with voice 3\n",
        "\n",
        "\n",
        "                    val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    val_acc_list[3] = val_accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    val_acc_list[4] = val_accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                    \n",
        "\n",
        "\n",
        "                history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "                history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "                history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "                #if nbr_voices == 4:\n",
        "                history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "                #else:\n",
        "                #    print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2))\n",
        "\n",
        "\n",
        "                history[\"val_acc_new\"].append(val_acc_list)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "\n",
        "                        \"\"\"\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                        if nbr_voices == 4: \n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                        v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                        mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)                      \n",
        "                        if nbr_voices == 4:                   \n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                        v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                        mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)\n",
        "                        accuracy_sum += acc \n",
        "                        \"\"\"\n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "nfDV8MKGHE3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einfÃ¼hren damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "    \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "  \n",
        "\n",
        "    def __getitem__(self, idx):          \n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])    \n",
        "\n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "\n",
        "\n",
        "        return (v0, v1, v2, v3, v_all, length)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "x2lFUuw719EC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}