{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject_final_tensor_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "SsyC2uB0KfaT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff40a84-0a28-4fe0-f977-86b06a717142"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install partitura\n",
        "import partitura"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: partitura in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: mido in /usr/local/lib/python3.7/dist-packages (from partitura) (1.2.10)\n",
            "Requirement already satisfied: xmlschema in /usr/local/lib/python3.7/dist-packages (from partitura) (1.11.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from partitura) (4.2.6)\n",
            "Requirement already satisfied: lark-parser in /usr/local/lib/python3.7/dist-packages (from partitura) (0.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.21.6)\n",
            "Requirement already satisfied: elementpath<3.0.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from xmlschema->partitura) (2.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac6f07c6-1536-496f-e0fb-1965d52b991d"
      },
      "source": [
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AI-MA_project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "batch_size = 1 \n",
        "PATH_TO_DATA = \"AI-MA_project/bach_pr_fugues\"\n",
        "#PATH_TO_DATA = \"AI-MA_project/pianoroll_88\"\n",
        "workers = 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1-u8zYd-gyo"
      },
      "source": [
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "    \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "  \n",
        "\n",
        "    def __getitem__(self, idx):          \n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])    \n",
        "\n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "\n",
        "\n",
        "        return (v0, v1, v2, v3, v_all, length)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_new(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        self.name_list = [\"1f01\",\"1f02\",\"1f03\",\"1f04\",\"1f05\",\"1f06\",\"1f07\",\"1f08\",\"1f09\",\"1f10\",\"1f11\",\"1f12\",\"1f13\",\"1f14\",\"1f15\",\"1f16\",\"1f17\",\"1f18\",\"1f19\",\"1f20\",\"1f21\",\"1f22\",\"1f23\",\"1f24\",\"2f01\",\"2f02\",\"2f03\",\"2f04\",\"2f05\",\"2f06\",\"2f07\",\"2f08\",\"2f09\",\"2f10\",\"2f11\",\"2f12\",\"2f13\",\"2f14\",\"2f15\",\"2f16\",\"2f17\",\"2f18\",\"2f19\",\"2f20\",\"2f21\",\"2f22\",\"2f23\",\"2f24\"]\n",
        "        self.name_list_voice_3 =  ['1f01', '1f05', '1f12', '1f14', '1f16', '1f17', '1f18', '1f23', '1f24', '2f02', '2f05', '2f07', '2f08', '2f09', '2f16', '2f17',  '2f22', '2f23']\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "\n",
        "                        file_names_list.append(name[-8:-4])\n",
        "\n",
        "                    if \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[49:53] + \".pkl\" in sorted(glob.glob(os.path.join(PATH_TO_DATA, \"voice_3\", \"*.pkl\"))):\n",
        "                        nbr_voices_list.append(4)\n",
        "                    else:\n",
        "                        nbr_voices_list.append(3)\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "\n",
        "\n",
        "            if iLabel == 3:  \n",
        "                voice_files = []\n",
        "                file_names_3 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[2], \"*.pkl\")))   \n",
        "                \n",
        "                ###### loop over all filnames in voices_2 and if an element there is not present in voices_3: append \"missing\" to the voice_files of label=3 => important bc. self.pr_dict[voice_3] has then len 42 and otherwise it would only have len 18  .. these \"missing\" el are not considered later in the dataloader (if len=3 is a diff case of get_idx)\n",
        "                for name in file_names_2:\n",
        "                    if name[45:49] in self.name_list_voice_3:\n",
        "                      correct_name_3 = \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[45:49] + \".pkl\"\n",
        "                      with open(correct_name_3 ,'rb') as f:  \n",
        "                            loaded_obj = pickle.load(f)  \n",
        "                            voice_files.append(loaded_obj)\n",
        "                    else:\n",
        "                      voice_files.append(\"missing\")\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                \n",
        "\n",
        "                \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                          loaded_obj = pickle.load(f)     \n",
        "                          voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 4:\n",
        "            for key, value in self.pr_dict.items():\n",
        "              out_list.append(self.pr_dict[key][idx])\n",
        "                              \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.tensor(out_list[3].T)\n",
        "            v_all = torch.tensor(out_list[4].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 3:\n",
        "\n",
        "            for key, value in self.pr_dict.items():\n",
        "                if key != \"voice_3\":\n",
        "                  out_list.append(self.pr_dict[key][idx]) \n",
        "            \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.zeros(v2.shape)\n",
        "            v_all = torch.tensor(out_list[3].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "            \n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        "
      ],
      "metadata": {
        "id": "3FxK6qr1FqIl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "#dataset = MusicDataset(PATH_TO_DATA)\n",
        "dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices, file_name = sample_batched\n",
        "    print(file_name[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXsRYzQSUuQU",
        "outputId": "82ba4ddf-fccc-4093-f69d-9c0cf774cb99"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1f01\n",
            "1f03\n",
            "1f05\n",
            "1f06\n",
            "1f07\n",
            "1f08\n",
            "1f09\n",
            "1f11\n",
            "1f12\n",
            "1f13\n",
            "1f14\n",
            "1f16\n",
            "1f17\n",
            "1f18\n",
            "1f19\n",
            "1f21\n",
            "1f23\n",
            "1f24\n",
            "2f01\n",
            "2f02\n",
            "2f03\n",
            "2f04\n",
            "2f05\n",
            "2f06\n",
            "2f07\n",
            "2f08\n",
            "2f09\n",
            "2f11\n",
            "2f12\n",
            "2f13\n",
            "2f14\n",
            "2f15\n",
            "2f16\n",
            "2f17\n",
            "2f18\n",
            "2f19\n",
            "2f20\n",
            "2f21\n",
            "2f22\n",
            "2f23\n",
            "2f24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "  if i ==10:\n",
        "    all_voices, length, nbr_voices, _ = sample_batched\n",
        "    all_voices_pr = all_voices[0,:,:,-1].numpy()\n",
        "    \n",
        "    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\n",
        "    print(note_array.shape)\n",
        "    print(note_array[:10])\n",
        "    print(note_array.dtype.names)\n",
        "\n",
        "    #print(i,nbr_voices,all_voices.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4QCaMEi3nw7",
        "outputId": "617c99b4-b2ce-44c3-837b-7f018fc48c46"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2868,)\n",
            "[(0, 2.75, 0.08333334, 1) (1, 2.75, 0.08333334, 1)\n",
            " (2, 2.75, 0.08333334, 1) (3, 2.75, 0.08333334, 1)\n",
            " (4, 2.75, 0.08333334, 1) (5, 2.75, 0.08333334, 1)\n",
            " (6, 2.75, 0.08333334, 1) (7, 2.75, 0.08333334, 1)\n",
            " (8, 2.75, 0.08333334, 1) (9, 2.75, 0.08333334, 1)]\n",
            "('pitch', 'onset_beat', 'duration_beat', 'velocity')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Music - Model\n"
      ],
      "metadata": {
        "id": "JNqxeacDwxNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UNET "
      ],
      "metadata": {
        "id": "QAIfIM69VHI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "XMdlm0_Vyyhc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "          \n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                                                            \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()\n",
        "\n",
        "        if nbr_voices==4:\n",
        "            loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) + self.loss(score_3, v3)           \n",
        "        else:\n",
        "            loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) \n",
        "        \n",
        "        return loss\n",
        "        \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            #print(np.squeeze(sum)[:,150:160,50:60])\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "            \n",
        "\n",
        "        else:\n",
        "            # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
        "            #predicted = scores_comb.argmax(dim=3)\n",
        "            #return np.squeeze(predicted.cpu().numpy())\n",
        "\n",
        "            #### test ### \n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "\n",
        "            prediction = np.squeeze(sum.cpu().numpy())     # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            mask_pred = (prediction.sum(axis=0) == 0)\n",
        "            v_pred_argm[mask_pred] = -1          \n",
        "            return v_pred_argm \n",
        "                       "
      ],
      "metadata": {
        "id": "CviiPTPOPW04"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"RNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(2, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "79cPe11WL6J0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b205fd37-aef9-4dfc-c242-54b11aceeb4d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"RNN\"\\nmonophonic = True\\nhis = start_experiment(2, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Define Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                                   ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]             \n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "                    ### before\n",
        "                    #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                    #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                    #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                    #for i in range(len(prediction[0,:])):\n",
        "                    #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                    #  accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                                \n",
        "                    ## ground truth in shape 1280x88 -> mixed voice\n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    truth = v_ori_argm       \n",
        "\n",
        "                    # outsource accurcy to further down -> just a placeholder right now\n",
        "                    v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                        ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "                        voices = voices.to(device).float()\n",
        "                        ### before\n",
        "                        #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                        #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                        #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                        #for i in range(len(prediction[0,:])):\n",
        "                        #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                        #  val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    #val_acc_list[3] = val_accuracy_sum_list[3] / 18                         ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_accuracy_sum_list[4] / 2                          ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                #history[\"val_acc\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "   \n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                                \n",
        "                        ## ground truth in shape 1280x88 -> mixed voice\n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        truth = v_ori_argm       \n",
        "\n",
        "                        # outsource accurcy to further down -> just a placeholder right now\n",
        "                        v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "                    \n",
        "                history[\"val_acc\"].append(val_accuracy)\n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "    # save the model\n",
        "    #torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"RNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ge8pY70uHxF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bcc13bad-8108-42e2-8e85-1e1e9943c8b3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"RNN\"\\nmonophonic = True\\nhis = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= [\"CNN\",\"RNN\"]\n",
        "monophonic_list = [True,False]\n",
        "\n",
        "for net in network_type:\n",
        "    for monophonic in monophonic_list: \n",
        "        print(\"network set to:\",net,\"monophnic:\",monophonic)\n",
        "        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Bs6-iNEBu8o",
        "outputId": "aa1f3a96-8611-4d70-cf46-abfef58db6d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= [\"CNN\",\"RNN\"]\\nmonophonic_list = [True,False]\\n\\nfor net in network_type:\\n    for monophonic in monophonic_list: \\n        print(\"network set to:\",net,\"monophnic:\",monophonic)\\n        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        path_train, path_validation = sklearn.model_selection.train_test_split(PATH_TO_DATA, test_size=0.15, random_state=10,)\n",
        "\n",
        "        print(\"Train and validation lenghts: \", len(path_train), len(path_validation))\n",
        "        train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "        validation_dataset = MusicDataset_new(path_validation) #MusicDataset(path_validation)\n",
        "        \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicNetwork\n",
        "epochs = 5\n",
        "lr = 0.001  \n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train + valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Experiment"
      ],
      "metadata": {
        "id": "bdetlQP-LoRX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm",
        "outputId": "033c4883-da11-49e5-b1a9-d8663163f2ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and validation lenghts:  23 5\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Training on device: cuda\n",
            "monophonic set to: True\n",
            "Train Loss: 2.7900186311907884, Train Accuracy_0 : 0.0, Train Accuracy_1 : 0.0,Train Accuracy_2 : 0.0, Train Accuracy_3 : 0.0, Train Accuracy_4 : 0.0\n",
            " Validation Accuracy : 1.9718460926321675\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Train Loss: 1.106592924856558, Train Accuracy_0 : 0.0, Train Accuracy_1 : 0.0,Train Accuracy_2 : 0.0, Train Accuracy_3 : 0.0, Train Accuracy_4 : 0.0\n",
            " Validation Accuracy : 1.9720936584717985\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Train Loss: 0.9708107847992967, Train Accuracy_0 : 0.0, Train Accuracy_1 : 0.0,Train Accuracy_2 : 0.0, Train Accuracy_3 : 0.0, Train Accuracy_4 : 0.0\n",
            " Validation Accuracy : 1.9729498506329992\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Train Loss: 0.8589618031571551, Train Accuracy_0 : 0.0, Train Accuracy_1 : 0.0,Train Accuracy_2 : 0.0, Train Accuracy_3 : 0.0, Train Accuracy_4 : 0.0\n",
            " Validation Accuracy : 1.9750599347709037\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Train Loss: 0.8227710011528759, Train Accuracy_0 : 0.0, Train Accuracy_1 : 0.0,Train Accuracy_2 : 0.0, Train Accuracy_3 : 0.0, Train Accuracy_4 : 0.0\n",
            " Validation Accuracy : 1.9756729166966485\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy evalutaion F-scores"
      ],
      "metadata": {
        "id": "sJbWsH72N2Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. create folder with part object of all pieces \n",
        "2. load a piece from dataloader with true labels, the mixed piece and the part object \n",
        "3. create notearray from part object\n",
        "4. take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "\n",
        "Output: pianoroll\n",
        "\n",
        "1 note in notearray could be mulitple bins\n",
        "\n",
        "take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "note start at same time with different pitch -> different notes\n",
        "\n",
        "for each note array find corresponding matrix -> \n",
        "\n",
        "\n",
        "if note is only composed by 1 bin: save indx of vocie -> save it to note array\n",
        "\n",
        "if more than 1: look what are idx that compose this note -> majority note -> save it for the note array (if its 50/50 take it random -> count how often this happens) \n",
        "\n",
        "\n",
        "with idx : in note_array find which note corresponds to what voice"
      ],
      "metadata": {
        "id": "CFClch37N6nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "metadata": {
        "id": "afYHFVNMlMnJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch5.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4TAhTQcpmx8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9c8c46-07da-4d1b-8fb2-68f8a203bc9d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MusicNetwork(\n",
              "  (rnn): GRU(88, 300, num_layers=2, batch_first=True)\n",
              "  (cnn): UNET(\n",
              "    (double_conv_downs): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up_trans): ModuleList(\n",
              "      (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (double_conv_ups): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (max_pool_2x2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (final_conv): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (top_layer_voice_0): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_1): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_2): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_3): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (loss): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dic with key:filename, val: part_obj"
      ],
      "metadata": {
        "id": "5RVmMv6Q9CJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_parts = \"AI-MA_project/bach_fugues\"\n",
        "part_dic = {}\n",
        "\n",
        "#### create a list with all filenames in the right order ####\n",
        "file_names_part = []\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    file_names_part.append(filename[3:7])\n",
        "#print(file_names_part)\n",
        "\n",
        "#### create a list with all part objects in the right order ####\n",
        "part_list = []\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    fullname = os.path.join(path_parts, filename)\n",
        "    part = partitura.load_score_midi(fullname)\n",
        "    part_list.append(part)\n",
        "#print(part_list)\n",
        "\n",
        "#### create a dict with keys:filenames , values: part object ####\n",
        "for i in range(len(file_names_part)):\n",
        "      part_dic[file_names_part[i]] = part_list[i]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XYM_KWu2qkX",
        "outputId": "e350247b-5ae8-4eb1-8668-e548ea1b22fb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=76 velocity=64 time=30\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=78 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=119\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=419\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=72 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=43 velocity=64 time=360\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=299\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=60 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=77 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=75 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=80 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=73 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=68 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=71 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=59 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=56 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=58 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=61 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=60\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=55 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=389\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=2 note=49 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=67 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=66 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=359\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "part_dic.keys(),part_dic.values()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt3uHTJY9Ojj",
        "outputId": "ffe90e45-ec05-43ff-dc4b-51085c9571d0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dict_keys(['1f01', '1f02', '1f03', '1f04', '1f05', '1f06', '1f07', '1f08', '1f09', '1f10', '1f11', '1f12', '1f13', '1f14', '1f15', '1f16', '1f17', '1f18', '1f19', '1f20', '1f21', '1f22', '1f23', '1f24', '2f01', '2f02', '2f03', '2f04', '2f05', '2f06', '2f07', '2f08', '2f09', '2f10', '2f11', '2f12', '2f13', '2f14', '2f15', '2f16', '2f17', '2f18', '2f19', '2f20', '2f21', '2f22', '2f23', '2f24']),\n",
              " dict_values([[<partitura.score.Part object at 0x7f0d817005d0>, <partitura.score.Part object at 0x7f0d815df990>, <partitura.score.Part object at 0x7f0d8172ee50>, <partitura.score.Part object at 0x7f0d81450990>], [<partitura.score.Part object at 0x7f0d8132c850>, <partitura.score.Part object at 0x7f0d81211650>, <partitura.score.Part object at 0x7f0d81177250>], [<partitura.score.Part object at 0x7f0d80d328d0>, <partitura.score.Part object at 0x7f0d80e8bfd0>, <partitura.score.Part object at 0x7f0d816b2f10>], [<partitura.score.Part object at 0x7f0d80a7f650>, <partitura.score.Part object at 0x7f0d80a00890>, <partitura.score.Part object at 0x7f0d8081c9d0>, <partitura.score.Part object at 0x7f0d806de310>, <partitura.score.Part object at 0x7f0d805f3ed0>], [<partitura.score.Part object at 0x7f0d817af510>, <partitura.score.Part object at 0x7f0d80401b90>, <partitura.score.Part object at 0x7f0d8047a650>, <partitura.score.Part object at 0x7f0d802f9e50>], [<partitura.score.Part object at 0x7f0d817ada90>, <partitura.score.Part object at 0x7f0d8015ffd0>, <partitura.score.Part object at 0x7f0acff9b950>], [<partitura.score.Part object at 0x7f0acfe7d150>, <partitura.score.Part object at 0x7f0acfe7be10>, <partitura.score.Part object at 0x7f0acfbb8050>], [<partitura.score.Part object at 0x7f0d80adc8d0>, <partitura.score.Part object at 0x7f0acf765690>, <partitura.score.Part object at 0x7f0acf5b6c50>], [<partitura.score.Part object at 0x7f0acfa06190>, <partitura.score.Part object at 0x7f0acf38f510>, <partitura.score.Part object at 0x7f0acf26e050>], [<partitura.score.Part object at 0x7f0acf080d10>, <partitura.score.Part object at 0x7f0acefc8090>], [<partitura.score.Part object at 0x7f0acfa00fd0>, <partitura.score.Part object at 0x7f0aced9b590>, <partitura.score.Part object at 0x7f0acec31a10>], [<partitura.score.Part object at 0x7f0ace99b090>, <partitura.score.Part object at 0x7f0ace89be10>, <partitura.score.Part object at 0x7f0ace765090>, <partitura.score.Part object at 0x7f0d8eb81290>], [<partitura.score.Part object at 0x7f0d8eb49e50>, <partitura.score.Part object at 0x7f0ace3aa610>, <partitura.score.Part object at 0x7f0ace2ee950>], [<partitura.score.Part object at 0x7f0acdffe710>, <partitura.score.Part object at 0x7f0ace001f90>, <partitura.score.Part object at 0x7f0acdf8f290>, <partitura.score.Part object at 0x7f0acdef0050>], [<partitura.score.Part object at 0x7f0ace462190>, <partitura.score.Part object at 0x7f0acd9c0090>, <partitura.score.Part object at 0x7f0acd7cb310>], [<partitura.score.Part object at 0x7f0acd47bb90>, <partitura.score.Part object at 0x7f0acdd31ed0>, <partitura.score.Part object at 0x7f0acd428750>, <partitura.score.Part object at 0x7f0ace7650d0>], [<partitura.score.Part object at 0x7f0acd2c7450>, <partitura.score.Part object at 0x7f0acd1b07d0>, <partitura.score.Part object at 0x7f0acd2a99d0>, <partitura.score.Part object at 0x7f0accff5090>], [<partitura.score.Part object at 0x7f0ace08c990>, <partitura.score.Part object at 0x7f0accd29350>, <partitura.score.Part object at 0x7f0acce05110>, <partitura.score.Part object at 0x7f0accbcb390>], [<partitura.score.Part object at 0x7f0accafaf50>, <partitura.score.Part object at 0x7f0acca0ee90>, <partitura.score.Part object at 0x7f0acc760790>], [<partitura.score.Part object at 0x7f0acc4b2d50>, <partitura.score.Part object at 0x7f0acc420090>, <partitura.score.Part object at 0x7f0acbe9d090>, <partitura.score.Part object at 0x7f0acca1ddd0>], [<partitura.score.Part object at 0x7f0acc336250>, <partitura.score.Part object at 0x7f0acbb67950>, <partitura.score.Part object at 0x7f0acba161d0>], [<partitura.score.Part object at 0x7f0acd47f390>, <partitura.score.Part object at 0x7f0acb742710>, <partitura.score.Part object at 0x7f0acb6f44d0>, <partitura.score.Part object at 0x7f0acb6333d0>, <partitura.score.Part object at 0x7f0acb563610>], [<partitura.score.Part object at 0x7f0acc4c1f10>, <partitura.score.Part object at 0x7f0acb2fc050>, <partitura.score.Part object at 0x7f0acb2be790>, <partitura.score.Part object at 0x7f0acb190950>], [<partitura.score.Part object at 0x7f0acae404d0>, <partitura.score.Part object at 0x7f0acacd3090>, <partitura.score.Part object at 0x7f0acaaaf050>, <partitura.score.Part object at 0x7f0aca8ee290>], [<partitura.score.Part object at 0x7f0aca648d90>, <partitura.score.Part object at 0x7f0acade5d10>, <partitura.score.Part object at 0x7f0acb89eed0>], [<partitura.score.Part object at 0x7f0acc3312d0>, <partitura.score.Part object at 0x7f0aca3f3810>, <partitura.score.Part object at 0x7f0acb3d7f50>, <partitura.score.Part object at 0x7f0aca26bad0>], [<partitura.score.Part object at 0x7f0aca129490>, <partitura.score.Part object at 0x7f0ac9f8d210>, <partitura.score.Part object at 0x7f0ac9ea9050>], [<partitura.score.Part object at 0x7f0ac9c1e790>, <partitura.score.Part object at 0x7f0ac9ca7c50>, <partitura.score.Part object at 0x7f0ac9888710>], [<partitura.score.Part object at 0x7f0ac9aea590>, <partitura.score.Part object at 0x7f0ac94e4150>, <partitura.score.Part object at 0x7f0ac93ec750>, <partitura.score.Part object at 0x7f0ac92ed3d0>], [<partitura.score.Part object at 0x7f0ac9af0d50>, <partitura.score.Part object at 0x7f0ac953d390>, <partitura.score.Part object at 0x7f0ac8fe8c50>], [<partitura.score.Part object at 0x7f0ac9604190>, <partitura.score.Part object at 0x7f0ac8d12250>, <partitura.score.Part object at 0x7f0ac8cd0250>, <partitura.score.Part object at 0x7f0ac8c20190>], [<partitura.score.Part object at 0x7f0ac9c02290>, <partitura.score.Part object at 0x7f0ac8a40690>, <partitura.score.Part object at 0x7f0ac8aff090>, <partitura.score.Part object at 0x7f0ac8aad710>], [<partitura.score.Part object at 0x7f0acb49da90>, <partitura.score.Part object at 0x7f0ac86c4190>, <partitura.score.Part object at 0x7f0ac9aef250>, <partitura.score.Part object at 0x7f0ac85b3750>], [<partitura.score.Part object at 0x7f0ac8308910>, <partitura.score.Part object at 0x7f0ac8411d10>, <partitura.score.Part object at 0x7f0ac7fae710>], [<partitura.score.Part object at 0x7f0ac845aa90>, <partitura.score.Part object at 0x7f0ac7c80610>, <partitura.score.Part object at 0x7f0ac7a297d0>], [<partitura.score.Part object at 0x7f0ac77f8250>, <partitura.score.Part object at 0x7f0ac88b7f90>, <partitura.score.Part object at 0x7f0ac7554150>], [<partitura.score.Part object at 0x7f0ac71d42d0>, <partitura.score.Part object at 0x7f0ac71b8d50>, <partitura.score.Part object at 0x7f0ac7050f90>], [<partitura.score.Part object at 0x7f0ac6d3f490>, <partitura.score.Part object at 0x7f0ac6d27f50>, <partitura.score.Part object at 0x7f0ac69891d0>], [<partitura.score.Part object at 0x7f0ac8a72d50>, <partitura.score.Part object at 0x7f0ac6d48c10>, <partitura.score.Part object at 0x7f0ac680cd50>], [<partitura.score.Part object at 0x7f0ac63bf090>, <partitura.score.Part object at 0x7f0ac6662e50>, <partitura.score.Part object at 0x7f0ac6205050>, <partitura.score.Part object at 0x7f0ac6062490>], [<partitura.score.Part object at 0x7f0ac66fff10>, <partitura.score.Part object at 0x7f0ac5ded350>, <partitura.score.Part object at 0x7f0ac5be29d0>, <partitura.score.Part object at 0x7f0ac5abc250>], [<partitura.score.Part object at 0x7f0ac57b3550>, <partitura.score.Part object at 0x7f0ac554dc10>, <partitura.score.Part object at 0x7f0ac5407310>], [<partitura.score.Part object at 0x7f0ac8244710>, <partitura.score.Part object at 0x7f0ac50204d0>, <partitura.score.Part object at 0x7f0ac4f77cd0>], [<partitura.score.Part object at 0x7f0ac4d763d0>, <partitura.score.Part object at 0x7f0ac4c4f810>, <partitura.score.Part object at 0x7f0ac4b55090>], [<partitura.score.Part object at 0x7f0ac4979790>, <partitura.score.Part object at 0x7f0ac48609d0>, <partitura.score.Part object at 0x7f0ac4690050>], [<partitura.score.Part object at 0x7f0ac44b2e10>, <partitura.score.Part object at 0x7f0ac491efd0>, <partitura.score.Part object at 0x7f0ac65b5a50>, <partitura.score.Part object at 0x7f0ac41a7390>], [<partitura.score.Part object at 0x7f0ac3f79190>, <partitura.score.Part object at 0x7f0ac3f60b50>, <partitura.score.Part object at 0x7f0ac3cae050>, <partitura.score.Part object at 0x7f0ac3ac3110>], [<partitura.score.Part object at 0x7f0ac43cbe10>, <partitura.score.Part object at 0x7f0ac38c41d0>, <partitura.score.Part object at 0x7f0ac36d0090>]]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "geht gerade nur für monophonic True"
      ],
      "metadata": {
        "id": "v4TJGKiUs086"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_sum_list = [0 for i in range(5)]\n",
        "\n",
        "def evaluate_accuracy(model, train_dataloader, part_dic):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "            if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "              print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "            \n",
        "            # load correct part object\n",
        "            file_name = file_name[0]\n",
        "\n",
        "            part = part_dic[file_name]\n",
        "\n",
        "\n",
        "            part_zero = part[0]\n",
        "            part_one = part[1]\n",
        "            part_two = part[2]\n",
        "\n",
        "            if len(part) ==4:\n",
        "              part_three = part[3]\n",
        "              note_array_3 = part_three.note_array\n",
        "\n",
        "            note_array_0 = part_zero.note_array\n",
        "            note_array_1 = part_one.note_array\n",
        "            note_array_2 = part_two.note_array\n",
        "            \n",
        "            \n",
        "            print(\"note_array shapes for filename:\",file_name)\n",
        "            print(note_array_0.shape)\n",
        "            print(note_array_1.shape)\n",
        "            print(note_array_2.shape)\n",
        "            print(note_array_3.shape)\n",
        "\n",
        "            print(note_array_0[:5])\n",
        "            print(note_array_0.dtype.names)\n",
        "            \n",
        "            print(note_array_1[:5])\n",
        "            print(note_array_1.dtype.names)\n",
        "\n",
        "            print(note_array_2[:5])\n",
        "            print(note_array_2.dtype.names)\n",
        "\n",
        "            print(note_array_3[:5])\n",
        "            print(note_array_3.dtype.names)\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "            # do model prediction\n",
        "            model.eval()\n",
        "            voices = voices.to(device).float()\n",
        "            monophonic=True\n",
        "            with torch.no_grad():\n",
        "\n",
        "                prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "\n",
        "                \n",
        "                acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                \n",
        "                \n",
        "                for i in range(len(prediction[0,:])):\n",
        "                  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                  accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "                \n",
        "                #print(accuracy_sum_list)\n",
        "            #print(file_name)\n",
        "\n",
        "    \n",
        "    train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "    train_acc_list[3] = accuracy_sum_list[3] / 18                               ## bc only 18 pieces with len 3\n",
        "    train_acc_list[4] = accuracy_sum_list[4] / 2                                ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "    return train_acc_list"
      ],
      "metadata": {
        "id": "EH3o-VHpN4op"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate_accuracy(model,train_dataloader,part_dic)"
      ],
      "metadata": {
        "id": "oG0MpEImjGzV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_sum_list = [0 for i in range(5)]\n",
        "\n",
        "def evaluate_accuracy_for_one(model, train_dataloader, part_dic):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "\n",
        "\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "            if idx == 0:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[0]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "                part_3 = part[3]\n",
        "                note_array_3 = part_3.note_array\n",
        "\n",
        "                list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                ground_truth_label_list = [0,1,2,3]\n",
        "                \n",
        "                total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    #print(\"old list\",pitch_list)\n",
        "                    pitch_list = [pitch_list[i]-20 for i in range(len(pitch_list))]\n",
        "                    #print(\"new list\",pitch_list)\n",
        "\n",
        "                    note_idx_start = []\n",
        "                    note_idx_end = []\n",
        "\n",
        "                    for i in range(len(onset_beat)):\n",
        "                        onset = onset_beat[i]\n",
        "                        duration = duration_beat[i]\n",
        "\n",
        "                        note_range = 12 * (onset+duration)\n",
        "                        onset_pr = 12 * onset\n",
        "                        note_idx_start.append(onset_pr)\n",
        "                        note_idx_end.append(note_range)\n",
        "\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        print(\"prediction\",prediction.shape)\n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "                        print(\"truth label:\", label)\n",
        "\n",
        "                \n",
        "                    ## for voice 0##\n",
        "                    for i, j ,p in zip(note_idx_start, note_idx_end, pitch_list):\n",
        "                        pred_list = prediction[i:j,p]\n",
        "                        truth_list = [label for i in range(len(pred_list))]\n",
        "\n",
        "                        #print(\"truth:\",truth_list)\n",
        "                        #print(\"pred:\",pred_list)\n",
        "                        \n",
        "                        result = all(elem == pred_list[0] for elem in pred_list)\n",
        "                        if result == False:\n",
        "                            print(\"vote pred list:\", pred_list, p,i,j )\n",
        "\n",
        "                            major, major_idx = torch.mode(pred_list,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list = [major for i in pred_list]\n",
        "                            print(\"vote pred list:\", pred_list )\n",
        "\n",
        "                        total_predictions_dict[str(label)].append(pred_list)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "\n",
        "    return total_predictions_dict , total_truth_dict"
      ],
      "metadata": {
        "id": "KG7ZAlNZGUw7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_pred, dict_truth = evaluate_accuracy_for_one(model,train_dataloader,part_dic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc1KjJs3G4Wi",
        "outputId": "9577fe06-9535-44d9-8350-0527806c010c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction torch.Size([1290, 88])\n",
            "truth label: 0\n",
            "prediction torch.Size([1290, 88])\n",
            "truth label: 1\n",
            "prediction torch.Size([1290, 88])\n",
            "truth label: 2\n",
            "vote pred list: tensor([1, 2, 2]) 39 405 408\n",
            "vote pred list: [2, 2, 2]\n",
            "prediction torch.Size([1290, 88])\n",
            "truth label: 3\n",
            "vote pred list: tensor([-1, -1, -1,  2,  2,  2]) 34 966 972\n",
            "vote pred list: [-1, -1, -1, -1, -1, -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "voice_entry = \"0\"\n",
        "for i in range(len(dict_pred[voice_entry])):\n",
        "    #print(dict_pred[voice_entry][i] == dict_truth[voice_entry][i])\n",
        "    print(\"pred\",dict_pred[voice_entry][i])\n",
        "    print(\"truth\",dict_truth[voice_entry][i])"
      ],
      "metadata": {
        "id": "BoQcV_i038DD",
        "outputId": "241a49a8-36e4-4589-a98d-f6319dcf9877",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
            "        -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
            "        -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
            "        -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([0, 0, 0])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([0, 0, 0])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
            "        -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1])\n",
            "truth [0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "pred tensor([-1, -1, -1])\n",
            "truth [0, 0, 0]\n",
            "pred tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
            "        -1, -1, -1, -1, -1, -1])\n",
            "truth [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "you have the ground truth on the different parts that you get when you import your score. Each part correspond to a voice. So if your note array contains all notes of all voices, you have for each note in your note array a number that is the ground truth voice (that you take from the part) and a number that is the predicted voice (that you take from the maximum vote)."
      ],
      "metadata": {
        "id": "Z5q305YzvjMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start time, duration , pitch to separate \n",
        "\n",
        "use the onset_beat and duration_beat\n",
        "\n",
        "multiply them according to the values set when producing the pianorolls \n",
        "\n",
        "-> get the position in the pianoroll\n",
        "\n",
        "time_div = 12\n",
        "\n"
      ],
      "metadata": {
        "id": "EmvxtyaVKG27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "A94mchm4LV6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy_v0\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v1\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v2\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v3\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Accuracy0','Accuracy1','Accuracy2','Accuracy3'])\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TgJDHaxAgYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend('Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxMs8GEfMvPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"train_loss\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqMcJT5aFL01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old training loop - matrix and non matrix format"
      ],
      "metadata": {
        "id": "4olpdwzyG8dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        #accuracy_v0_sum = 0\n",
        "        #accuracy_v1_sum = 0\n",
        "        #accuracy_v2_sum = 0\n",
        "        #accuracy_v3_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4: \n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                    v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                    mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_pred_flat:\", v_pred_flat.shape)\n",
        "                    \"\"\"\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4:                   \n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                    v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                    mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                    print(\"old mask\", mask_ori.shape)\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_ori_flat\", v_ori_flat.shape)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)   \n",
        "                    print(\"acc\",acc)                    \n",
        "                    accuracy_sum += acc \n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                    acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                    for i in range(len(prediction[0,:])):\n",
        "                      acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                      accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    \n",
        "                    \"\"\"\n",
        "                    pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "\n",
        "                    acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                    acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                    acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                    if nbr_voices == 4:\n",
        "                        acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                    \n",
        "                    # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    if nbr_voices == 4:\n",
        "                        accuracy_v3_sum += acc_v3 / len(lens)\n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "            train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "            train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "            train_accuracy_v3 = accuracy_v3_sum / 18   ## bc only 18 pieces with len 3\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "            history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "            history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "            #if nbr_voices == 4:\n",
        "            history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3)) \n",
        "            #else:\n",
        "            #    print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2)) \n",
        "            \"\"\"\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_v0_sum = 0\n",
        "                accuracy_v1_sum = 0\n",
        "                accuracy_v2_sum = 0\n",
        "                accuracy_v3_sum = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                        truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                        acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                        for i in range(len(prediction[0,:])):\n",
        "                          acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                          val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                        #print(\"val_accuracy_sum_list[3]\",val_accuracy_sum_list[3])\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(train_dataloader)\n",
        "                    #val_acc_list[3] = val_acc_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_acc_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "                #history[\"val_acc_new\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        # Predict the model's output on a batch\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                            \n",
        "                        # compute the accuracy \n",
        "                        acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                        acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                        acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                        if nbr_voices == 4:\n",
        "                            acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                            \n",
        "                            \n",
        "                        # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                        accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                        accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                        accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                        if nbr_voices == 4:\n",
        "                            accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                    val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                    val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                    val_accuracy_v3 = accuracy_v3_sum / 18  ##len(val_dataloader). - bc 18 pieces only with voice 3\n",
        "\n",
        "\n",
        "                    val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    val_acc_list[3] = val_accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    val_acc_list[4] = val_accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                    \n",
        "\n",
        "\n",
        "                history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "                history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "                history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "                #if nbr_voices == 4:\n",
        "                history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "                #else:\n",
        "                #    print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2))\n",
        "\n",
        "\n",
        "                history[\"val_acc_new\"].append(val_acc_list)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "\n",
        "                        \"\"\"\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                        if nbr_voices == 4: \n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                        v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                        mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)                      \n",
        "                        if nbr_voices == 4:                   \n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                        v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                        mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)\n",
        "                        accuracy_sum += acc \n",
        "                        \"\"\"\n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "nfDV8MKGHE3J"
      }
    }
  ]
}