{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject_final_tensor_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "SsyC2uB0KfaT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59284d7f-f960-4340-cf7e-999461220ca9"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import torchvision.transforms.functional as TF \n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install partitura\n",
        "import partitura"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: partitura in /usr/local/lib/python3.7/dist-packages (0.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.4.1)\n",
            "Requirement already satisfied: xmlschema in /usr/local/lib/python3.7/dist-packages (from partitura) (1.11.1)\n",
            "Requirement already satisfied: mido in /usr/local/lib/python3.7/dist-packages (from partitura) (1.2.10)\n",
            "Requirement already satisfied: lark-parser in /usr/local/lib/python3.7/dist-packages (from partitura) (0.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from partitura) (1.21.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from partitura) (4.2.6)\n",
            "Requirement already satisfied: elementpath<3.0.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from xmlschema->partitura) (2.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d52a53-a34f-4027-bb0e-2f3b70b5f899"
      },
      "source": [
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AI-MA_project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "batch_size = 1 \n",
        "PATH_TO_DATA = \"AI-MA_project/bach_pr_fugues\"\n",
        "#PATH_TO_DATA = \"AI-MA_project/pianoroll_88\"\n",
        "workers = 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1-u8zYd-gyo"
      },
      "source": [
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "    \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "  \n",
        "\n",
        "    def __getitem__(self, idx):          \n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])    \n",
        "\n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "\n",
        "\n",
        "        return (v0, v1, v2, v3, v_all, length)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset_new(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        self.name_list = [\"1f01\",\"1f02\",\"1f03\",\"1f04\",\"1f05\",\"1f06\",\"1f07\",\"1f08\",\"1f09\",\"1f10\",\"1f11\",\"1f12\",\"1f13\",\"1f14\",\"1f15\",\"1f16\",\"1f17\",\"1f18\",\"1f19\",\"1f20\",\"1f21\",\"1f22\",\"1f23\",\"1f24\",\"2f01\",\"2f02\",\"2f03\",\"2f04\",\"2f05\",\"2f06\",\"2f07\",\"2f08\",\"2f09\",\"2f10\",\"2f11\",\"2f12\",\"2f13\",\"2f14\",\"2f15\",\"2f16\",\"2f17\",\"2f18\",\"2f19\",\"2f20\",\"2f21\",\"2f22\",\"2f23\",\"2f24\"]\n",
        "        self.name_list_voice_3 =  ['1f01', '1f05', '1f12', '1f14', '1f16', '1f17', '1f18', '1f23', '1f24', '2f02', '2f05', '2f07', '2f08', '2f09', '2f16', '2f17',  '2f22', '2f23']\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "        len_list = []\n",
        "        nbr_voices_list = []\n",
        "        file_names_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: ### normal sollte es egal sein wenn voice_4 bei manchen nicht existiert - wenn nicht condition einführen damit das funktioniert\n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))\n",
        "\n",
        "                        file_names_list.append(name[-8:-4])\n",
        "\n",
        "                    if \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[49:53] + \".pkl\" in sorted(glob.glob(os.path.join(PATH_TO_DATA, \"voice_3\", \"*.pkl\"))):\n",
        "                        nbr_voices_list.append(4)\n",
        "                    else:\n",
        "                        nbr_voices_list.append(3)\n",
        "                        \n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "                self.pr_dict[\"nbr_voices\"] = nbr_voices_list\n",
        "                self.pr_dict[\"name\"] = file_names_list\n",
        "                #print(self.pr_dict[\"name\"])\n",
        "\n",
        "\n",
        "            if iLabel == 3:  \n",
        "                voice_files = []\n",
        "                file_names_3 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                file_names_2 = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[2], \"*.pkl\")))   \n",
        "                \n",
        "                ###### loop over all filnames in voices_2 and if an element there is not present in voices_3: append \"missing\" to the voice_files of label=3 => important bc. self.pr_dict[voice_3] has then len 42 and otherwise it would only have len 18  .. these \"missing\" el are not considered later in the dataloader (if len=3 is a diff case of get_idx)\n",
        "                for name in file_names_2:\n",
        "                    if name[45:49] in self.name_list_voice_3:\n",
        "                      correct_name_3 = \"AI-MA_project/bach_pr_fugues/voice_3/voice_3_\" + name[45:49] + \".pkl\"\n",
        "                      with open(correct_name_3 ,'rb') as f:  \n",
        "                            loaded_obj = pickle.load(f)  \n",
        "                            voice_files.append(loaded_obj)\n",
        "                    else:\n",
        "                      voice_files.append(\"missing\")\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                \n",
        "\n",
        "                \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\"))) \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                          loaded_obj = pickle.load(f)     \n",
        "                          voice_files.append(loaded_obj)\n",
        "\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):      \n",
        "        out_list = []\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 4:\n",
        "            for key, value in self.pr_dict.items():\n",
        "              out_list.append(self.pr_dict[key][idx])\n",
        "                              \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.tensor(out_list[3].T)\n",
        "            v_all = torch.tensor(out_list[4].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "\n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        \n",
        "        if self.pr_dict[\"nbr_voices\"][idx] == 3:\n",
        "\n",
        "            for key, value in self.pr_dict.items():\n",
        "                if key != \"voice_3\":\n",
        "                  out_list.append(self.pr_dict[key][idx]) \n",
        "            \n",
        "            v0 = torch.tensor(out_list[0].T)\n",
        "            v1 = torch.tensor(out_list[1].T)\n",
        "            v2 = torch.tensor(out_list[2].T)\n",
        "            v3 = torch.zeros(v2.shape)\n",
        "            v_all = torch.tensor(out_list[3].T) \n",
        "            length = self.pr_dict[\"length\"][idx]\n",
        "            nbr_voices = self.pr_dict[\"nbr_voices\"][idx]\n",
        "            file_name = self.pr_dict[\"name\"][idx]\n",
        "            \n",
        "            voices = torch.stack([v0, v1, v2, v3, v_all], dim=2)\n",
        "\n",
        "            \n",
        "            return (voices, length, nbr_voices, file_name)\n",
        "        "
      ],
      "metadata": {
        "id": "3FxK6qr1FqIl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "#dataset = MusicDataset(PATH_TO_DATA)\n",
        "dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\"\"\"\n",
        "#for i, sample_batched in enumerate(loader):\n",
        "#    all_voices, length, nbr_voices, file_name = sample_batched\n",
        "#    print(file_name[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "LXsRYzQSUuQU",
        "outputId": "b09fc12b-b728-4651-8d3e-b72bf46bc5fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor i, sample_batched in enumerate(loader):\\n    all_voices, length, nbr_voices = sample_batched\\n    if nbr_voices ==3:\\n      print(i,nbr_voices,all_voices.shape)\\n    else:\\n      print(i,nbr_voices)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for i, sample_batched in enumerate(loader):\n",
        "    all_voices, length, nbr_voices = sample_batched\n",
        "    if nbr_voices ==3:\n",
        "      print(i,nbr_voices,all_voices.shape)\n",
        "    else:\n",
        "      print(i,nbr_voices)\n",
        "\n",
        "for i, sample_batched in enumerate(loader):\n",
        "  if i ==10:\n",
        "    all_voices, length, nbr_voices, _ = sample_batched\n",
        "    all_voices_pr = all_voices[0,:,:,-1].numpy()\n",
        "    \n",
        "    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\n",
        "    print(note_array.shape)\n",
        "    print(note_array[:10])\n",
        "    print(note_array.dtype.names)\n",
        "\n",
        "    #print(i,nbr_voices,all_voices.shape)\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "b4QCaMEi3nw7",
        "outputId": "f205128f-6765-45c8-9241-f6e19731be38"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfor i, sample_batched in enumerate(loader):\\n    all_voices, length, nbr_voices = sample_batched\\n    if nbr_voices ==3:\\n      print(i,nbr_voices,all_voices.shape)\\n    else:\\n      print(i,nbr_voices)\\n\\nfor i, sample_batched in enumerate(loader):\\n  if i ==10:\\n    all_voices, length, nbr_voices, _ = sample_batched\\n    all_voices_pr = all_voices[0,:,:,-1].numpy()\\n    \\n    note_array = partitura.utils.pianoroll_to_notearray(all_voices[0,:,:,-1].numpy(), time_div=12, time_unit='beat')\\n    print(note_array.shape)\\n    print(note_array[:10])\\n    print(note_array.dtype.names)\\n\\n    #print(i,nbr_voices,all_voices.shape)\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Music - Model\n"
      ],
      "metadata": {
        "id": "JNqxeacDwxNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define UNET "
      ],
      "metadata": {
        "id": "QAIfIM69VHI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNET(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels=1, classes=1):\n",
        "        super(UNET, self).__init__()\n",
        "        self.layers = [in_channels, 64, 128, 256, 512, 1024]\n",
        "        \n",
        "        self.double_conv_downs = nn.ModuleList([self.__double_conv(layer, layer_n) for layer, layer_n in zip(self.layers[:-1], self.layers[1:])])\n",
        "        \n",
        "        self.up_trans = nn.ModuleList([nn.ConvTranspose2d(layer, layer_n, kernel_size=2, stride=2) for layer, layer_n in zip(self.layers[::-1][:-2], self.layers[::-1][1:-1])])\n",
        "            \n",
        "        self.double_conv_ups = nn.ModuleList([self.__double_conv(layer, layer//2) for layer in self.layers[::-1][:-2]])\n",
        "        \n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.final_conv = nn.Conv2d(64, classes, kernel_size=1)\n",
        "\n",
        "        \n",
        "    def __double_conv(self, in_channels, out_channels):\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        return conv\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # down layers\n",
        "        concat_layers = []\n",
        "        \n",
        "        for down in self.double_conv_downs:\n",
        "            x = down(x)\n",
        "            if down != self.double_conv_downs[-1]:\n",
        "                concat_layers.append(x)\n",
        "                x = self.max_pool_2x2(x)\n",
        "        \n",
        "        concat_layers = concat_layers[::-1]\n",
        "        \n",
        "        # up layers\n",
        "        for up_trans, double_conv_up, concat_layer  in zip(self.up_trans, self.double_conv_ups, concat_layers):\n",
        "            x = up_trans(x)\n",
        "            if x.shape != concat_layer.shape:\n",
        "                x = TF.resize(x, concat_layer.shape[2:])\n",
        "            \n",
        "            concatenated = torch.cat((concat_layer, x), dim=1)\n",
        "            x = double_conv_up(concatenated)\n",
        "            \n",
        "        x = self.final_conv(x)\n",
        "        \n",
        "        return x "
      ],
      "metadata": {
        "id": "XMdlm0_Vyyhc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, network_type,output_dim=88, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                 \n",
        "        super(MusicNetwork, self).__init__()\n",
        "\n",
        "        self.network_type = network_type\n",
        "        self.n_out = output_dim\n",
        "        input_dim = output_dim \n",
        "        rnn_cell = nn.GRU\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "        self.cnn = UNET(in_channels=1, classes=4)\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")                       # use weight parameters maybe take 1/88       \n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        if self.network_type == \"RNN\":\n",
        "          rnn_out ,_= self.rnn(sentences)     \n",
        "          out_0 = self.top_layer_voice_0(rnn_out)\n",
        "          out_1 = self.top_layer_voice_1(rnn_out)\n",
        "          out_2 = self.top_layer_voice_2(rnn_out)\n",
        "          out_3 = self.top_layer_voice_3(rnn_out)\n",
        "\n",
        "          return torch.stack([out_0, out_1, out_2, out_3], dim=1)\n",
        "\n",
        "        else: \n",
        "          sentences = sentences[:,None]\n",
        "          out = self.cnn(sentences)\n",
        "          return out                      ### squeeze output here before returning                                       \n",
        "        \n",
        "\n",
        "    def forward(self, voices, sentences_len, nbr_voices):            \n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_comb = self.compute_outputs(voices[:,:,:,-1], sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_comb[:,0,:,:].view(-1, self.n_out)\n",
        "        score_1  = scores_comb[:,1,:,:].view(-1, self.n_out)\n",
        "        score_2  = scores_comb[:,2,:,:].view(-1, self.n_out)\n",
        "        score_3  = scores_comb[:,3,:,:].view(-1, self.n_out)\n",
        "\n",
        "\n",
        "        v0 = voices[:,:,:,0].squeeze()\n",
        "        v1 = voices[:,:,:,1].squeeze()\n",
        "        v2 = voices[:,:,:,2].squeeze()\n",
        "        v3 = voices[:,:,:,3].squeeze()\n",
        "\n",
        "\n",
        "\n",
        "        if nbr_voices==4:\n",
        "            loss = self.loss(score_0, v0) +  self.loss(score_1, v1) +  self.loss(score_2, v2) + self.loss(score_3, v3)  \n",
        "            print(\"self.loss(score_0, v0)\",self.loss(score_0, v0))\n",
        "            print(\"self.loss(score_0, v1)\",self.loss(score_1, v1))\n",
        "            print(\"self.loss(score_0, v2)\",self.loss(score_2, v2)) \n",
        "            print(\"self.loss(score_2, v3)\",self.loss(score_3, v3)) \n",
        "            print(\"loss\",loss)      \n",
        "        else:\n",
        "            loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) \n",
        "        \n",
        "        return loss   #change also to matrix version\n",
        "        \n",
        "\n",
        "\n",
        "    def predict(self, sentences, sentences_len,monophonic=True):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "\n",
        "        scores_comb = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        if monophonic==False:\n",
        "            sum = scores_comb * sentences[:,None,:,:]\n",
        "            return np.squeeze(sum.cpu().numpy())\n",
        "            \n",
        "\n",
        "        else:\n",
        "            # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
        "            #predicted = scores_comb.argmax(dim=3)\n",
        "            #return np.squeeze(predicted.cpu().numpy())\n",
        "\n",
        "            sum_tensor = scores_comb * sentences[:,None,:,:]\n",
        "            prediction = np.squeeze(sum_tensor.cpu().numpy())                # prediction is of shape 4,T,88 and contains a probability for the result to belong to one of the 4 voices -> taking argmax: gives the voice with the highes probability\n",
        "            v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "            \n",
        "            mask_pred = np.squeeze(sentences)== 0\n",
        "            v_pred_argm[mask_pred] = -1\n",
        "\n",
        "            return v_pred_argm \n",
        "                       "
      ],
      "metadata": {
        "id": "CviiPTPOPW04"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"CNN\"\n",
        "lr = 0.0001  \n",
        "monophonic = True\n",
        "his = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "79cPe11WL6J0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7718d7b5-2d08-412e-afa3-e16d0ffe5e17"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"CNN\"\\nlr = 0.0001  \\nmonophonic = True\\nhis = start_experiment(10, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Define Training Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay,network_type, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 88\n",
        "    model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader,monophonic, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                                   ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "        \n",
        "        for idx, (voices, lens, nbr_voices, _) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]             \n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "                    ### before\n",
        "                    #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                    #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                    #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                    #for i in range(len(prediction[0,:])):\n",
        "                    #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                    #  accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                    #prediction = model.predict(voices, lens, monophonic)                    #for voice vise masking\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)         #for mixed voice masking        \n",
        "\n",
        "\n",
        "                    ## ground truth in shape 1280x88 -> mixed voice\n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    truth = v_ori_argm       \n",
        "\n",
        "                    # outsource accurcy to further down -> just a placeholder right now\n",
        "                    v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            #train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            #train_acc_list[3] = accuracy_sum_list[3] / 18                        ## bc only 18 pieces with len 3\n",
        "            #train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "            #history[\"train_loss\"].append(train_loss)\n",
        "            #history[\"train_acc\"].append(train_acc_list)\n",
        "            #print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "            \n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "                        voices = voices.to(device).float()\n",
        "                        ### before\n",
        "                        #prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        #prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "                        #truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "                        #acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "                        #for i in range(len(prediction[0,:])):\n",
        "                        #  acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                        #  val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    #val_acc_list[3] = val_accuracy_sum_list[3] / 18                         ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_accuracy_sum_list[4] / 2                          ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                #history[\"val_acc\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        #prediction = model.predict(voices, lens, monophonic)                #for voice vise masking\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)     # for masking with mixed voice\n",
        "\n",
        "\n",
        "\n",
        "                        ## ground truth in shape 1280x88 -> mixed voice\n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        truth = v_ori_argm       \n",
        "\n",
        "                        # outsource accurcy to further down -> just a placeholder right now\n",
        "                        v_pred_flat = torch.flatten(prediction, start_dim=0, end_dim=-1)\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "                    \n",
        "                history[\"val_acc\"].append(val_accuracy)\n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices, _ in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "        \n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        \n",
        "    # save the model\n",
        "    #torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "    torch.save({'model_state_dict': model.state_dict()}, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= \"RNN\"\n",
        "monophonic = True\n",
        "his = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ge8pY70uHxF9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "52dcc4ac-c154-4eea-bebc-2fe9debcac21"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= \"RNN\"\\nmonophonic = True\\nhis = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "network_type= [\"CNN\",\"RNN\"]\n",
        "monophonic_list = [True,False]\n",
        "\n",
        "for net in network_type:\n",
        "    for monophonic in monophonic_list: \n",
        "        print(\"network set to:\",net,\"monophnic:\",monophonic)\n",
        "        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Bs6-iNEBu8o",
        "outputId": "34077fcc-385f-4385-d84f-3539a7b25c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnetwork_type= [\"CNN\",\"RNN\"]\\nmonophonic_list = [True,False]\\n\\nfor net in network_type:\\n    for monophonic in monophonic_list: \\n        print(\"network set to:\",net,\"monophnic:\",monophonic)\\n        start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, net, learn_all)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "def start_experiment( epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay,network_type, learn_all):\n",
        "    \n",
        "    trainer = partial(train,epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, network_type)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        dataset = MusicDataset_new(PATH_TO_DATA)\n",
        "        \n",
        "        train_dataset, validation_dataset = sklearn.model_selection.train_test_split(dataset, test_size=0.15, random_state=10,)\n",
        "\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        print(\"train_dataloader\",len(train_dataloader),\"val_dataloader\",len(val_dataloader))\n",
        "\n",
        "        \"\"\"\n",
        "        path_train, path_validation = sklearn.model_selection.train_test_split(PATH_TO_DATA, test_size=0.15, random_state=10,)\n",
        "\n",
        "        print(\"Train and validation lenghts: \", len(path_train), len(path_validation))\n",
        "        #train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "        train_dataset = MusicDataset_new(path_train)\n",
        "        validation_dataset = MusicDataset_new(path_validation) #MusicDataset(path_validation)\n",
        "\n",
        "        \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        \"\"\"\n",
        "        \n",
        "        _, history = trainer(train_dataloader, val_dataloader)\n",
        "\n",
        "    return history, val_dataloader"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicNetwork\n",
        "epochs = 30\n",
        "lr = 0.00001 # was 0.001\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 \n",
        "device = None                 #if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"           # False -> uses train and valid set\n",
        "network_type= \"CNN\"\n",
        "\n",
        "monophonic = True"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Experiment"
      ],
      "metadata": {
        "id": "bdetlQP-LoRX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm",
        "outputId": "16db5b8b-7349-411b-8267-7b9986e2c125",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "his, val_dataloader = start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, network_type, learn_all)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataloader 34 val_dataloader 7\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Training on device: cuda\n",
            "monophonic set to: True\n",
            "self.loss(score_0, v0) tensor(3.0182, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(4.0355, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.7619, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.9739, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(13.7895, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.4881, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.4917, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.4196, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(4.1820, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(13.5815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(3.0644, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.6725, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.5842, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.1807, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(13.5018, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(3.1789, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.6771, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.0380, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.4047, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(12.2987, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(3.1671, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.6233, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.9695, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.0298, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(12.7897, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(3.2346, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(4.1658, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.8967, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.8080, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(15.1051, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(3.2863, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.3909, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.2526, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.8785, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(12.8084, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(3.5302, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(4.3755, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.3841, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.2609, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(12.5507, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(3.5187, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.8225, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.6621, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.3637, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(14.3670, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(3.1408, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.2873, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.5128, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.8848, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(12.8257, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.4995, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(4.0881, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.9648, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.0739, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(12.6263, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.9409, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.6230, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.4502, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.6108, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(12.6250, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.4620, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.8387, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.7106, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.1966, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(13.2079, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(3.2427, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.9457, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.3216, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.2352, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(13.7452, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 12.02981003592996, Train Accuracy : 0.9770820356515779\n",
            " Validation Accuracy : 5.7228890629776235\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(2.5796, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.5155, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.2638, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.8270, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(12.1859, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.1221, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.0491, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.9987, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(4.0005, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(12.1704, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.6224, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.2375, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.1811, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.0915, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(12.1325, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.7014, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.2087, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.7999, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.3149, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(11.0249, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.6905, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.1659, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.6293, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.9555, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(11.4413, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.7592, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.6341, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.4643, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.7488, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(13.6064, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.7808, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.9676, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.8795, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.8173, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(11.4453, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.9796, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.8448, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.9769, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.2393, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(11.0406, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.9876, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.4018, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.2683, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.2988, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(12.9565, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.6361, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.8749, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.1092, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.8374, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(11.4577, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.0560, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.4918, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.5969, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.0270, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(11.1718, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.4353, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.1515, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.0428, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.5739, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(11.2034, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.0332, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.3455, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.2803, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.1545, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(11.8135, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.7181, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.4763, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.9460, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.1799, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(12.3203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 10.488053013296689, Train Accuracy : 0.9783374193172408\n",
            " Validation Accuracy : 5.729373571178972\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(2.0866, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.0303, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.8426, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.7792, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(10.7388, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.7001, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.6284, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.6465, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.9254, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(10.9004, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.0948, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.8210, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.8150, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.0301, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(10.7609, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.1486, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.7570, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.5779, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.2458, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(9.7293, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.1222, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.7333, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.3017, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.8957, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(10.0528, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.1674, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.1439, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(3.0320, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.6773, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(12.0204, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.1746, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.5608, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.5132, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.7595, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(10.0081, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.3415, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.3565, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.5753, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.2103, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(9.4837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.3543, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.0352, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.8978, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.2397, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(11.5271, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.0363, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.5018, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.7181, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.7766, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(10.0329, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.5447, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.9706, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.2684, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.9531, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(9.7368, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.8644, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.7297, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.6670, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.5164, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(9.7775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.5748, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.8748, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.9298, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.0898, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(10.4692, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(2.1622, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(3.0026, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.6128, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.1103, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(10.8880, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 8.961766958236694, Train Accuracy : 0.9785774793832317\n",
            " Validation Accuracy : 5.731064941674574\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(1.5702, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.5778, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.4807, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.7144, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(9.3431, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.2644, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.2163, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.3767, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.8397, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(9.6972, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.6028, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.4013, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.5278, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.9649, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(9.4968, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.6569, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.3215, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.4194, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.1721, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(8.5699, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.6012, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.3138, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.0355, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.8303, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(8.7808, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.6154, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.6671, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.6654, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.5964, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(10.5442, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.6030, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.1726, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.1861, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.6932, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(8.6548, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.7579, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.9190, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.1967, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.1820, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(8.0556, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.7662, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.6974, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.5271, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.1642, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(10.1550, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.4845, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.1459, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.3582, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.7054, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(8.6940, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.0789, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.4901, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.9685, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.8769, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(8.4145, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.3703, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.3154, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.3362, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.4459, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(8.4678, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.1947, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.4175, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.6631, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.0017, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(9.2770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.6770, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.5193, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.3201, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.0151, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(9.5316, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 7.550247697269215, Train Accuracy : 0.9819898856322534\n",
            " Validation Accuracy : 5.754217135661913\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(1.1347, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.1360, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.1405, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.6164, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(8.0276, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.9082, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.8127, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.1665, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.7045, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(8.5918, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.2134, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.9823, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.2862, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.8379, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(8.3197, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.2605, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.9149, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.3049, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.0217, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(7.5020, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.1788, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.9051, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.8035, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.6917, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(7.5791, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.1615, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.1877, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.2919, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.4306, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(9.0717, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.1379, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.7908, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.8600, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.5556, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(7.3443, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.2762, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.5062, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.7815, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.1253, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(6.6892, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.2902, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.3392, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.1207, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.0096, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(8.7597, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.0491, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.7808, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.9806, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.5590, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(7.3694, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.7157, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.0092, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.6596, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.7103, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(7.0948, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.9999, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.8869, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.0070, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.2938, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(7.1877, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.9000, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.9742, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.4494, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.7984, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(8.1220, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.2889, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.0194, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.0308, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.8104, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(8.1494, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 6.211855018840117, Train Accuracy : 0.9891416854604164\n",
            " Validation Accuracy : 5.793638296387288\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(0.8122, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.6642, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.8375, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.4089, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(6.7228, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.6555, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.4101, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.0442, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.4226, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(7.5324, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.9184, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.5605, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.0815, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.5942, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(7.1547, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.9599, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.5277, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.2255, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.7444, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(6.4574, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.8749, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.4810, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.6183, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.4464, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(6.4206, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.8438, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.6668, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.9191, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.1368, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(7.5666, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.8210, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.3618, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.5567, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.3235, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(6.0630, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.9223, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(2.0368, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.3775, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.0251, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(5.3616, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.9722, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.9218, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.7343, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.7709, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(7.3993, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.7520, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.3943, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.6282, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.3277, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(6.1022, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4785, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.5290, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.3739, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.4346, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(5.8160, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.7418, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.4644, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.7147, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.0599, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(5.9808, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.6883, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.5899, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.3289, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.4966, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(7.1037, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(1.0178, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.5440, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.8411, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.5135, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(6.9164, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 4.949352173244252, Train Accuracy : 0.9918447689973211\n",
            " Validation Accuracy : 5.807748328345654\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(0.5917, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.2349, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.6314, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.1331, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(5.5911, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4753, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.0581, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.9760, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(3.0462, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(6.5555, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.7047, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.1843, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.9035, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.3038, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(6.0963, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.7563, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.2035, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.1627, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.4194, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(5.5419, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.6722, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.1072, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.5022, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.1633, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(5.4450, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.6396, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.1864, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.6336, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.8005, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(6.2602, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.6174, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.9566, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.3253, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.0566, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.9559, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.6971, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.5695, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.0755, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.9109, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.2529, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.7708, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.5071, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.4394, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.5062, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(6.2235, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.5591, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.0352, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.3636, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.0715, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(5.0295, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3297, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.0923, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.1467, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.1283, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.6970, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.5661, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.0903, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.5007, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.8044, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.9615, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.5302, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.2836, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(2.2308, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.1700, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(6.2147, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.7946, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.1489, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.6920, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.1930, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(5.8286, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 3.911485693034004, Train Accuracy : 0.992882597728587\n",
            " Validation Accuracy : 5.813007905698204\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(0.4226, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.9179, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.4271, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.8406, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.6081, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3339, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.7919, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.8299, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.6233, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(5.5790, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.5266, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.9083, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.6677, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.9972, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(5.0997, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.5850, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.9870, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.9873, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.0675, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.6268, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4956, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.8577, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.3417, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.8539, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.5490, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4656, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.8666, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.3671, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.4422, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(5.1415, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4531, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.6754, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.0734, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.7695, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.9713, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.5229, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.1917, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.8421, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7868, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.3434, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.6053, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.1745, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.1985, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.2167, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(5.1950, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4073, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.7774, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.1261, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.7963, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.1071, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2173, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.7858, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.9108, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.8012, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.7152, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4299, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.8434, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.2922, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.5324, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.0980, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3982, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(1.0622, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.9963, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.8289, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(5.2857, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.6095, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.8956, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.4693, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.8542, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.8286, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 3.0740665057126213, Train Accuracy : 0.993564682225352\n",
            " Validation Accuracy : 5.816475339311914\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(0.2920, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.7114, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.1220, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.5501, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.6755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2317, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.5975, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.5362, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.1927, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.5581, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4011, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.7064, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.3683, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.6904, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.1662, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4675, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.7789, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7287, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.7025, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.6776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3689, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.6527, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.1378, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.5319, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.6913, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3506, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.6298, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.1281, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(2.0632, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.1716, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3574, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4677, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.8432, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.4716, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.1399, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4074, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.8848, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6609, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6566, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.6097, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4990, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.8916, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.0146, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.9005, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.3057, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3149, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.5797, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.9346, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.5060, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.3352, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1585, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.5362, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7192, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.4556, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.8694, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3521, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.6372, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.1033, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.2464, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.3390, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3079, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.7845, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.6083, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.5001, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(4.2008, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4879, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.6992, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.1782, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.5403, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.9056, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 2.386414622559267, Train Accuracy : 0.9942479560863947\n",
            " Validation Accuracy : 5.819810222522302\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(0.2167, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.5019, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.8170, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.2873, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.8229, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1763, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4192, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.2299, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.8049, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.6303, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3279, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.5467, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.0853, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.4367, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.3967, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4072, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.5906, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5581, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.3743, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.9302, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2901, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4734, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.9872, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.2490, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.9998, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2871, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4544, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.9732, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.7183, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.4330, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2889, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3317, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7165, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.2049, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.5420, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3357, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.6469, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5337, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5420, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0582, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4274, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.7022, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.8805, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.6295, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.6395, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2537, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4438, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7690, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.2720, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.7385, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1229, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3650, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5885, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.1804, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2568, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3056, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4689, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.9481, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.0262, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.7488, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2456, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.5401, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.2097, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.2828, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.2783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.4040, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.5425, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.9540, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.3078, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.2084, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 1.8667334546061123, Train Accuracy : 0.9947821761796\n",
            " Validation Accuracy : 5.822642416647191\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(0.1695, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3542, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6590, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.0797, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2623, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1346, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2703, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.9202, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.5528, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.8780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2919, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4213, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.9695, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.1935, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.8762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3628, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4810, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4542, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.1205, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.4185, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2345, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3355, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.8109, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.0385, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.4193, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2422, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3545, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.8624, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.4560, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.9151, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2343, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2569, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6162, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.0024, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.1098, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2789, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.5180, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4413, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4647, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7030, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3741, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.5604, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7765, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.4161, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(3.1270, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2124, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3517, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6485, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.0824, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2950, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0971, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3118, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4733, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.9920, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.8742, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2728, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3602, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.8107, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.8630, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.3067, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2035, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4294, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.0390, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.0851, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.7571, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3259, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4534, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7881, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.1316, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.6992, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 1.5144695587017958, Train Accuracy : 0.9951255758774044\n",
            " Validation Accuracy : 5.824028131880463\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(0.1419, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2607, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6231, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.8818, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.9074, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1119, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2069, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.8149, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.3108, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.4445, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2439, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3523, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7221, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.1070, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.4253, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3176, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4437, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4531, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.9114, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.1258, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2073, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2927, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7952, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.8524, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.1476, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2138, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2904, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7939, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.2310, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.5291, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2015, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1888, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5279, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.8508, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7690, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2398, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4260, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3974, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3967, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4600, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3335, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4616, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7174, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.2435, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.7561, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1764, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2976, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5910, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.9294, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.9945, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0798, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2404, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5174, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.8028, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6404, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2825, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4130, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6239, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.8257, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.1451, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2052, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3712, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.8745, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.9523, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.4032, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3050, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4759, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.9330, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.9343, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.6482, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 1.3079939037561417, Train Accuracy : 0.9951623819217306\n",
            " Validation Accuracy : 5.82461151038063\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(0.1229, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2605, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7577, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6987, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.8397, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0959, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2486, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.9100, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.0345, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2890, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2099, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3056, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7281, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.8872, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.1308, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3806, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4305, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3508, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.8774, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0394, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1870, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2600, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7086, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7352, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.8907, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1880, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2649, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7087, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.0694, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2310, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1692, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1786, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6188, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6903, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6570, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2064, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3591, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3556, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3520, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3062, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4001, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6653, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(1.0898, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.4614, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1551, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2500, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6178, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7748, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7977, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0712, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2692, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6005, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6340, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2100, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3911, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6803, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6579, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.9392, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1738, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2962, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6090, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.9057, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.9847, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.3091, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.5047, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7653, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.8413, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.4204, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 1.2012032305493074, Train Accuracy : 0.9950481206045986\n",
            " Validation Accuracy : 5.823178105080623\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(0.1081, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2103, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7053, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6021, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6258, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0826, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2226, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.1924, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.8904, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.3880, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1803, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2697, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7799, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7244, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.9543, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2701, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.5147, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5872, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6039, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.9759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1710, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3400, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.9260, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5763, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0132, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1791, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3459, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.8675, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.8598, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2524, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1996, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1669, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4968, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6161, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4795, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1901, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3419, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3384, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2996, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1700, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2892, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3580, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6048, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.9861, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2381, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1487, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2026, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4881, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7123, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5517, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0683, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2304, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3797, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6093, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2878, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1923, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4416, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6032, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6215, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.8586, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1322, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3815, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.8430, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7263, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2227, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3275, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6111, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7435, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.9048, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 1.1365371802273918, Train Accuracy : 0.9948242374349201\n",
            " Validation Accuracy : 5.821798366373576\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "self.loss(score_0, v0) tensor(0.1109, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2976, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.1144, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3909, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.9138, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0773, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1847, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.1003, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7023, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0646, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1592, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2916, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5335, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7813, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7656, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2773, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3985, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2489, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6293, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5540, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1327, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2508, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5478, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5922, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5234, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1591, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2627, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6688, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.8335, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.9242, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1425, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1655, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6340, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5184, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4603, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1779, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3100, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4189, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2397, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1465, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2714, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3835, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.8486, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7612, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2647, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1228, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1655, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6438, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5430, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0562, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1571, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3821, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4824, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1899, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3232, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5970, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5218, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6319, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1282, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3110, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5191, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7435, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7018, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1945, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3159, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4732, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7396, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7232, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 1.0197159630410813, Train Accuracy : 0.995077473571604\n",
            " Validation Accuracy : 5.822909539566614\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.1081, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2106, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5733, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4560, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3480, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0997, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2519, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7943, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6479, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7938, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1736, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4393, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(1.1477, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4906, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.2511, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2280, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.4102, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3703, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4587, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4672, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1335, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2106, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5364, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5061, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3866, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1446, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2268, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5850, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7623, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7188, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1513, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1763, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5069, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4854, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3199, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1623, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2976, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3387, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2306, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0293, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2598, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3497, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6839, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7426, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(2.0361, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1469, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2588, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.7832, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4661, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6548, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0735, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2272, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6025, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3535, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2567, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1792, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2385, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6476, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4113, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1269, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2269, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5558, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6283, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5379, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1845, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3011, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4658, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6975, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6489, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.9273757329758476, Train Accuracy : 0.9949253226246192\n",
            " Validation Accuracy : 5.823361474076994\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0768, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2024, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3739, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5474, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2004, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0620, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1343, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4557, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7936, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4455, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1480, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2279, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6366, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5857, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5982, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2623, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3089, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2256, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5412, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3381, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1234, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1469, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3628, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5556, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1887, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1315, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2292, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4771, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.8258, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6636, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1555, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1141, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3444, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5139, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1279, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1418, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2701, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2635, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2583, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9337, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2408, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3044, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5737, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7665, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.8854, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1307, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1667, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5546, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4944, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3465, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0604, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1528, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4201, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3912, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0245, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1790, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2138, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6136, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4170, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4233, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1216, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2138, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5282, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6288, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4924, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1821, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2996, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4615, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6858, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6290, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.8248234031831517, Train Accuracy : 0.9954550949468342\n",
            " Validation Accuracy : 5.826060164991369\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0758, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1698, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3646, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5260, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1362, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0627, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1282, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4614, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7683, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4207, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1487, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2227, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6764, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5558, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6036, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2510, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.3011, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2379, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5106, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3006, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1210, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1399, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3747, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5328, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1685, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1317, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2069, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4726, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7935, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6046, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1480, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1118, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3469, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4985, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1052, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1431, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2585, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2657, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2492, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9165, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2425, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2963, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5894, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7405, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.8687, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1301, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1699, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5857, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4758, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3615, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0628, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1565, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4516, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3705, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0415, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1787, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2066, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6287, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4038, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4179, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1198, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2096, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5206, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6187, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4687, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1818, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2956, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4532, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6758, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6064, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.8027688164044829, Train Accuracy : 0.9955253476521582\n",
            " Validation Accuracy : 5.826401981482023\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0755, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1672, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3546, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5212, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1184, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0625, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1262, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4457, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7666, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4010, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1477, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2125, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6411, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5549, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5563, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2504, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2919, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2300, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5063, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1207, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1360, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3608, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5297, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1471, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1310, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2044, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4565, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7883, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5802, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1472, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1080, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3341, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4941, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0833, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1416, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2560, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2584, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2475, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9036, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2404, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2899, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5761, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7332, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.8396, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1283, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1621, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5639, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4721, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3266, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0620, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1498, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4317, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3677, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0112, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1780, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2005, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6129, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4000, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3915, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1186, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2043, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5016, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6158, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4403, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1804, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2918, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4417, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6700, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5839, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.7866803477792179, Train Accuracy : 0.9955682999738036\n",
            " Validation Accuracy : 5.826611846619701\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0747, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1629, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3447, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5156, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0978, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0619, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1235, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4333, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7597, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1468, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2067, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6237, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5498, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5269, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2482, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2860, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2257, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4979, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2578, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1195, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1329, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3524, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5234, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1282, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1300, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2004, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4457, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7790, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5550, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1439, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1052, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3268, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4872, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0632, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1403, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2528, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2534, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2446, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8910, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2385, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2851, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5676, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7243, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.8155, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1261, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1573, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5516, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4659, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3009, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0612, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1453, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4195, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3619, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9880, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1768, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1959, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6014, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3947, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3689, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1172, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1998, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4875, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6097, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4141, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1788, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2868, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4314, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6626, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5596, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.7726141556220896, Train Accuracy : 0.9956024856943584\n",
            " Validation Accuracy : 5.82678197942437\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0738, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1589, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3356, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5093, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0612, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1211, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4224, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7513, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3561, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1457, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2016, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.6058, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5448, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4978, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1184, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1302, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3441, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5172, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1100, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1288, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1972, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4354, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7697, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5312, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1410, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1028, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3202, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4803, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0442, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1386, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2500, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2484, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2417, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2363, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2811, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5584, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7158, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7916, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1239, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1529, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5381, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4602, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2751, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0602, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1410, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4058, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3569, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9639, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1756, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1919, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5886, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3898, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3459, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1158, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1956, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4746, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6031, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3891, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1770, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2819, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4219, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6547, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5355, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.7591416713069467, Train Accuracy : 0.9956496789473214\n",
            " Validation Accuracy : 5.827011686202277\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0729, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1553, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3272, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5027, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0580, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0604, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1190, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4125, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7420, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3339, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1445, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1972, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5900, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5393, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4709, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2442, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2753, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2159, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4826, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2179, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1171, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1279, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3366, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5106, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0922, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1275, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1942, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4260, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7597, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5074, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1379, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1007, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3143, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4728, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0257, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1369, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2474, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2437, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2387, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8667, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2340, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2775, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5495, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7072, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7681, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1217, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1489, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5251, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4544, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2501, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0592, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1371, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3925, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3516, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9405, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1743, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1882, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5760, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3847, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3233, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1143, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1918, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4630, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5956, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3648, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1752, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2771, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4130, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6462, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5115, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.7461594842812594, Train Accuracy : 0.9957053123750065\n",
            " Validation Accuracy : 5.827285638871374\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0718, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1517, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3191, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4955, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0382, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0595, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1170, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4034, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7319, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3118, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1432, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1933, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5754, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5333, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4452, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2421, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2704, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2112, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4747, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1984, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1157, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1258, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3296, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5036, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1260, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1913, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4171, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7492, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4835, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1349, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.0987, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3088, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4651, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0075, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1350, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2449, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2390, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2356, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8546, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2315, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2741, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5407, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6984, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7448, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1195, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1451, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5121, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4484, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2251, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0582, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1335, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3795, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3463, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9175, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1729, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1848, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5634, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3796, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3008, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1127, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1883, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4524, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5875, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3409, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1732, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2724, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4047, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6372, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4875, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.7334840345908614, Train Accuracy : 0.995769637161058\n",
            " Validation Accuracy : 5.8276064019869\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0707, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1483, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3115, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4879, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0184, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0586, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1151, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3948, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7211, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2897, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1419, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1897, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5617, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5270, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4203, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2399, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2657, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2067, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4666, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1790, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1143, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1238, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3229, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4963, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0573, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1245, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1886, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4086, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7381, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4597, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1319, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.0970, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3036, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4571, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9896, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1331, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2426, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2344, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2325, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8425, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2291, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2709, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5321, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6894, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.7215, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1173, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1416, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4995, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4423, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2008, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0571, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1302, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3670, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3408, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8950, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1715, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1817, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5509, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3744, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1112, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1849, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4425, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5788, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3173, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1713, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2678, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3968, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6276, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4635, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.7210612454835106, Train Accuracy : 0.995845298393574\n",
            " Validation Accuracy : 5.827995485265726\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0697, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1448, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3041, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4799, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9985, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0577, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1133, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3866, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7098, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2675, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1405, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1864, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5487, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5203, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3959, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2377, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2612, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2022, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4585, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1597, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1129, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1219, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3165, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4887, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0400, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1229, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1860, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4003, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7266, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4358, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1291, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.0953, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2986, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4489, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9719, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1312, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2403, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2298, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2292, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8305, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2266, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2679, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5236, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6802, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6982, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1152, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1383, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4872, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4361, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1768, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0560, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1271, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3548, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3352, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1701, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1788, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5383, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3691, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2562, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1097, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1816, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4329, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5697, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2938, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1694, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2631, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3892, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6177, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4394, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.7088286337607047, Train Accuracy : 0.995944490628293\n",
            " Validation Accuracy : 5.828492205178228\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0686, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1415, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2970, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4716, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0568, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1116, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3787, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6982, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2454, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1391, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1833, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5364, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5133, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3721, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2355, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2568, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.1978, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4503, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1404, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1115, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1200, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3102, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4810, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0227, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1214, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1835, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3923, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7147, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4118, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1263, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.0936, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2937, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4406, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9543, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1293, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2380, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2253, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2259, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8185, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2241, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2649, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5151, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6709, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1131, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1352, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4750, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4298, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1531, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0549, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1242, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3430, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3294, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8515, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1686, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1760, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5258, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3636, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2341, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1082, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1784, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4236, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5602, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2704, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1675, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2586, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3818, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6075, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.4154, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.6967564251493005, Train Accuracy : 0.9960565949496738\n",
            " Validation Accuracy : 5.829054512507162\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0676, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1381, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2901, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4631, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9588, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0560, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1099, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3711, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6862, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2232, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1377, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1803, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5248, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5060, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3488, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2331, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2525, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.1935, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4419, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1210, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1101, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1182, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3040, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4730, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0053, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1198, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1810, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3844, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.7024, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3877, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1236, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.0921, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2889, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4322, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9368, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1274, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2359, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2208, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2225, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8066, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2217, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2620, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5067, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6615, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6520, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1111, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1322, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4630, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4233, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1297, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0539, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1214, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3316, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3235, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8303, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1671, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1735, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5132, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3581, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2120, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1067, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1754, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4146, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5504, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2471, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1655, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2541, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3748, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5970, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3914, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.6848093797178829, Train Accuracy : 0.9961734585656357\n",
            " Validation Accuracy : 5.829645619768752\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0665, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1347, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2834, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4542, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9388, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0551, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1083, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3638, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6738, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2010, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1363, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1775, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5139, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4984, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3260, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2306, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2484, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.1892, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4334, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1016, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1087, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1165, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2980, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4647, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9879, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1182, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1786, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3767, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6897, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3633, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1210, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.0906, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2843, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4235, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9193, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1256, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2337, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2165, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2190, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7948, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2192, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2592, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4985, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6520, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6289, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1092, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1294, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4514, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4167, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1067, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0528, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1187, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3206, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3174, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8096, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1656, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1711, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5009, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3525, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1901, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1052, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1724, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4058, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5404, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2238, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1636, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2498, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3679, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5862, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3674, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.6729895301601466, Train Accuracy : 0.9963036096698673\n",
            " Validation Accuracy : 5.830309413188783\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0655, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1314, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2768, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4452, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9189, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0542, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1066, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3566, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6613, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1348, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1747, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.5036, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4905, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3037, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2281, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2446, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.1850, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4248, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0825, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1073, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1147, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2921, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4564, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9705, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1166, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1761, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3692, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6768, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3387, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1185, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.0891, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2797, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4147, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9019, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1237, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2315, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2122, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2155, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2168, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2564, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4903, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6423, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.6058, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1073, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1266, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4399, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4100, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0518, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1161, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3098, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3113, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7890, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1641, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1688, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4888, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3468, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1684, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1037, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1695, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3972, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5301, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2006, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1617, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2455, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3612, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5752, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3435, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.6612954854088671, Train Accuracy : 0.9964247125618786\n",
            " Validation Accuracy : 5.830941964733967\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "self.loss(score_0, v0) tensor(0.0645, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1282, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2704, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4359, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8991, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0534, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1051, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3496, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6486, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1566, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1334, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1721, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4936, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4825, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.2815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2255, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2409, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.1808, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4162, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0634, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1059, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1130, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2863, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4478, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.9530, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1151, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1738, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3617, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6635, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3140, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1161, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.0876, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2750, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4059, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.8845, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1219, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2294, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2079, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.2120, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7711, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.2144, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2537, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4822, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.6325, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.5828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1055, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1239, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4286, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.4032, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.0612, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.0508, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1137, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.2994, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3050, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(0.7688, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1626, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1666, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.4766, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.3410, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1468, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1023, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.1667, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3887, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5196, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.1774, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "self.loss(score_0, v0) tensor(0.1599, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v1) tensor(0.2413, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_0, v2) tensor(0.3546, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "self.loss(score_2, v3) tensor(0.5639, device='cuda:0', grad_fn=<DivBackward1>)\n",
            "loss tensor(1.3196, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Train Loss: 0.6497065727325047, Train Accuracy : 0.9965528079108674\n",
            " Validation Accuracy : 5.831602312400599\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy evalutaion F-scores"
      ],
      "metadata": {
        "id": "sJbWsH72N2Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. create folder with part object of all pieces \n",
        "2. load a piece from dataloader with true labels, the mixed piece and the part object \n",
        "3. create notearray from part object\n",
        "4. take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "\n",
        "Output: pianoroll\n",
        "\n",
        "1 note in notearray could be mulitple bins\n",
        "\n",
        "take 1 note from notearrray - input -> find corresponding frame by looking at time and pitch\n",
        "\n",
        "note start at same time with different pitch -> different notes\n",
        "\n",
        "for each note array find corresponding matrix -> \n",
        "\n",
        "\n",
        "if note is only composed by 1 bin: save indx of vocie -> save it to note array\n",
        "\n",
        "if more than 1: look what are idx that compose this note -> majority note -> save it for the note array (if its 50/50 take it random -> count how often this happens) \n",
        "\n",
        "\n",
        "with idx : in note_array find which note corresponds to what voice"
      ],
      "metadata": {
        "id": "CFClch37N6nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MusicDataset_new(PATH_TO_DATA) #MusicDataset(PATH_TO_DATA)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "val_dataloader "
      ],
      "metadata": {
        "id": "afYHFVNMlMnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d1d847-02b1-4d01-af26-34933571ebd0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f6f09c7b250>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 88\n",
        "model = MusicNetwork(network_type, output_dim, hidden_dim, rnn_depth, cell_type)  \n",
        "checkpoint = torch.load(\"./AI-MA_project/model_temp_epoch30.pkl\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "4TAhTQcpmx8m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d0395e-3661-473a-81f4-fa92cc04d02f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MusicNetwork(\n",
              "  (rnn): GRU(88, 300, num_layers=2, batch_first=True)\n",
              "  (cnn): UNET(\n",
              "    (double_conv_downs): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (up_trans): ModuleList(\n",
              "      (0): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (double_conv_ups): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (4): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (max_pool_2x2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (final_conv): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (top_layer_voice_0): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_1): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_2): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (top_layer_voice_3): Linear(in_features=300, out_features=88, bias=True)\n",
              "  (loss): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dic with key:filename, val: part_obj"
      ],
      "metadata": {
        "id": "5RVmMv6Q9CJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_parts = \"AI-MA_project/bach_fugues\"\n",
        "part_dic = {}\n",
        "\n",
        "#### create a list with all filenames in the right order ####\n",
        "file_names_part = []\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    file_names_part.append(filename[3:7])\n",
        "#print(file_names_part)\n",
        "\n",
        "#### create a list with all part objects in the right order ####\n",
        "part_list = []\n",
        "for filename in sorted(os.listdir(path_parts)):\n",
        "    if not filename.endswith('.mid'): continue\n",
        "    fullname = os.path.join(path_parts, filename)\n",
        "    part = partitura.load_score_midi(fullname)\n",
        "    part_list.append(part)\n",
        "#print(part_list)\n",
        "\n",
        "#### create a dict with keys:filenames , values: part object ####\n",
        "for i in range(len(file_names_part)):\n",
        "      part_dic[file_names_part[i]] = part_list[i]"
      ],
      "metadata": {
        "id": "_XYM_KWu2qkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f75116-70aa-4b26-a98c-f63105fa66a0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=76 velocity=64 time=30\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=78 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=119\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=419\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=72 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=43 velocity=64 time=360\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=299\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=60 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=77 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=75 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=80 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=73 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=70 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=68 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=71 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=59 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=56 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=58 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=61 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=63 velocity=64 time=60\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=55 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=389\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=2 note=49 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=69 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=67 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=66 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=0 note=64 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_on channel=0 note=0 velocity=0 time=359\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n",
            "/usr/local/lib/python3.7/dist-packages/partitura/io/importmidi.py:345: UserWarning: ignoring MIDI message note_off channel=1 note=65 velocity=64 time=0\n",
            "  warnings.warn(\"ignoring MIDI message %s\" % msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "part_dic.keys(),part_dic.values()"
      ],
      "metadata": {
        "id": "Tt3uHTJY9Ojj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb27710-7e1e-4b72-9ab6-e716189bc0fe"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dict_keys(['1f01', '1f02', '1f03', '1f04', '1f05', '1f06', '1f07', '1f08', '1f09', '1f10', '1f11', '1f12', '1f13', '1f14', '1f15', '1f16', '1f17', '1f18', '1f19', '1f20', '1f21', '1f22', '1f23', '1f24', '2f01', '2f02', '2f03', '2f04', '2f05', '2f06', '2f07', '2f08', '2f09', '2f10', '2f11', '2f12', '2f13', '2f14', '2f15', '2f16', '2f17', '2f18', '2f19', '2f20', '2f21', '2f22', '2f23', '2f24']),\n",
              " dict_values([[<partitura.score.Part object at 0x7f6f0295aa10>, <partitura.score.Part object at 0x7f6f028a8610>, <partitura.score.Part object at 0x7f6f02723d50>, <partitura.score.Part object at 0x7f6f02677510>], [<partitura.score.Part object at 0x7f6f0254f3d0>, <partitura.score.Part object at 0x7f6f024f7710>, <partitura.score.Part object at 0x7f6f02406610>], [<partitura.score.Part object at 0x7f6f02258b50>, <partitura.score.Part object at 0x7f6f0219b3d0>, <partitura.score.Part object at 0x7f6f01e2b110>], [<partitura.score.Part object at 0x7f6f02973ed0>, <partitura.score.Part object at 0x7f6f019a9a50>, <partitura.score.Part object at 0x7f6f0191a050>, <partitura.score.Part object at 0x7f6f017d8810>, <partitura.score.Part object at 0x7f6f0167c510>], [<partitura.score.Part object at 0x7f6f02247990>, <partitura.score.Part object at 0x7f6f0145d510>, <partitura.score.Part object at 0x7f6f014daad0>, <partitura.score.Part object at 0x7f6f01389390>], [<partitura.score.Part object at 0x7f6f01b4a250>, <partitura.score.Part object at 0x7f6f02258e50>, <partitura.score.Part object at 0x7f6f01068310>], [<partitura.score.Part object at 0x7f6f00e66b10>, <partitura.score.Part object at 0x7f6f00f1af90>, <partitura.score.Part object at 0x7f6f00d1e710>], [<partitura.score.Part object at 0x7f6f008d6410>, <partitura.score.Part object at 0x7f6f00a2b550>, <partitura.score.Part object at 0x7f6f00776b10>], [<partitura.score.Part object at 0x7f6f004e2990>, <partitura.score.Part object at 0x7f6f01b4ad50>, <partitura.score.Part object at 0x7f6f003b3650>], [<partitura.score.Part object at 0x7f6f00261590>, <partitura.score.Part object at 0x7f6f00226f10>], [<partitura.score.Part object at 0x7f6c1bee6890>, <partitura.score.Part object at 0x7f6c1bde8390>, <partitura.score.Part object at 0x7f6c1bd06a10>], [<partitura.score.Part object at 0x7f6c1b93c050>, <partitura.score.Part object at 0x7f6c1bb02050>, <partitura.score.Part object at 0x7f6c1b7f1390>, <partitura.score.Part object at 0x7f6c1b60b050>], [<partitura.score.Part object at 0x7f6f004fc6d0>, <partitura.score.Part object at 0x7f6c1b45f090>, <partitura.score.Part object at 0x7f6c1b3528d0>], [<partitura.score.Part object at 0x7f6f00254690>, <partitura.score.Part object at 0x7f6c1b0bbb10>, <partitura.score.Part object at 0x7f6c1b1ba490>, <partitura.score.Part object at 0x7f6c1af5b050>], [<partitura.score.Part object at 0x7f6f004fb090>, <partitura.score.Part object at 0x7f6c1aa75850>, <partitura.score.Part object at 0x7f6c1a80c050>], [<partitura.score.Part object at 0x7f6c1bef43d0>, <partitura.score.Part object at 0x7f6c1add7f90>, <partitura.score.Part object at 0x7f6c1a400050>, <partitura.score.Part object at 0x7f6f005755d0>], [<partitura.score.Part object at 0x7f6c1a547c90>, <partitura.score.Part object at 0x7f6c1a2a3090>, <partitura.score.Part object at 0x7f6c1a3b9250>, <partitura.score.Part object at 0x7f6c1a0c8810>], [<partitura.score.Part object at 0x7f6c19f7e210>, <partitura.score.Part object at 0x7f6c19e7eb90>, <partitura.score.Part object at 0x7f6c19f76350>, <partitura.score.Part object at 0x7f6c19d1abd0>], [<partitura.score.Part object at 0x7f6c19f08790>, <partitura.score.Part object at 0x7f6c1a638350>, <partitura.score.Part object at 0x7f6c198fa610>], [<partitura.score.Part object at 0x7f6c1945ec90>, <partitura.score.Part object at 0x7f6c195ce0d0>, <partitura.score.Part object at 0x7f6c1907e290>, <partitura.score.Part object at 0x7f6c19f00990>], [<partitura.score.Part object at 0x7f6c19aefc10>, <partitura.score.Part object at 0x7f6c18c4a350>, <partitura.score.Part object at 0x7f6c18b79b90>], [<partitura.score.Part object at 0x7f6c188fccd0>, <partitura.score.Part object at 0x7f6c1888e090>, <partitura.score.Part object at 0x7f6c187c5e10>, <partitura.score.Part object at 0x7f6c1877de50>, <partitura.score.Part object at 0x7f6c186bc090>], [<partitura.score.Part object at 0x7f6c18d1b890>, <partitura.score.Part object at 0x7f6c18422310>, <partitura.score.Part object at 0x7f6c1856f9d0>, <partitura.score.Part object at 0x7f6c183358d0>], [<partitura.score.Part object at 0x7f6c17e6b650>, <partitura.score.Part object at 0x7f6c1812e050>, <partitura.score.Part object at 0x7f6c17c65650>, <partitura.score.Part object at 0x7f6c17a5a1d0>], [<partitura.score.Part object at 0x7f6c177c9c90>, <partitura.score.Part object at 0x7f6c181ef250>, <partitura.score.Part object at 0x7f6c18002690>], [<partitura.score.Part object at 0x7f6c17875bd0>, <partitura.score.Part object at 0x7f6c174d4290>, <partitura.score.Part object at 0x7f6c174721d0>, <partitura.score.Part object at 0x7f6c173b8710>], [<partitura.score.Part object at 0x7f6c18176f90>, <partitura.score.Part object at 0x7f6c1714fe10>, <partitura.score.Part object at 0x7f6c16fdb610>], [<partitura.score.Part object at 0x7f6c16dc6e90>, <partitura.score.Part object at 0x7f6c16dbefd0>, <partitura.score.Part object at 0x7f6c16a52050>], [<partitura.score.Part object at 0x7f6c166fed10>, <partitura.score.Part object at 0x7f6c166a2990>, <partitura.score.Part object at 0x7f6c165f6d10>, <partitura.score.Part object at 0x7f6c164f7990>], [<partitura.score.Part object at 0x7f6c162cc250>, <partitura.score.Part object at 0x7f6c172a7f50>, <partitura.score.Part object at 0x7f6c1619e290>], [<partitura.score.Part object at 0x7f6c15fc27d0>, <partitura.score.Part object at 0x7f6c15ffb0d0>, <partitura.score.Part object at 0x7f6c15ffd890>, <partitura.score.Part object at 0x7f6c15e2d750>], [<partitura.score.Part object at 0x7f6c15bdeb10>, <partitura.score.Part object at 0x7f6c15c88710>, <partitura.score.Part object at 0x7f6c15a19510>, <partitura.score.Part object at 0x7f6c15b4a910>], [<partitura.score.Part object at 0x7f6c15cf69d0>, <partitura.score.Part object at 0x7f6c17243050>, <partitura.score.Part object at 0x7f6c15869750>, <partitura.score.Part object at 0x7f6c157becd0>], [<partitura.score.Part object at 0x7f6c154cee50>, <partitura.score.Part object at 0x7f6c15565cd0>, <partitura.score.Part object at 0x7f6c15172650>], [<partitura.score.Part object at 0x7f6c14e41990>, <partitura.score.Part object at 0x7f6c14e5f590>, <partitura.score.Part object at 0x7f6c14b9bb10>], [<partitura.score.Part object at 0x7f6c148c4610>, <partitura.score.Part object at 0x7f6c149f2a90>, <partitura.score.Part object at 0x7f6c14734510>], [<partitura.score.Part object at 0x7f6c14378690>, <partitura.score.Part object at 0x7f6c142c5150>, <partitura.score.Part object at 0x7f6c141e3390>], [<partitura.score.Part object at 0x7f6c13ebe850>, <partitura.score.Part object at 0x7f6c13d62fd0>, <partitura.score.Part object at 0x7f6c13b02590>], [<partitura.score.Part object at 0x7f6c14e42b90>, <partitura.score.Part object at 0x7f6c13f2bf90>, <partitura.score.Part object at 0x7f6c14e75f10>], [<partitura.score.Part object at 0x7f6c138628d0>, <partitura.score.Part object at 0x7f6c1375b9d0>, <partitura.score.Part object at 0x7f6c1342e050>, <partitura.score.Part object at 0x7f6c1320d310>], [<partitura.score.Part object at 0x7f6c14939d10>, <partitura.score.Part object at 0x7f6c12dcb6d0>, <partitura.score.Part object at 0x7f6c12cb3790>, <partitura.score.Part object at 0x7f6c12b08150>], [<partitura.score.Part object at 0x7f6c14937350>, <partitura.score.Part object at 0x7f6c125f2590>, <partitura.score.Part object at 0x7f6c12398090>], [<partitura.score.Part object at 0x7f6c1207dc90>, <partitura.score.Part object at 0x7f6c12033110>, <partitura.score.Part object at 0x7f6c11f942d0>], [<partitura.score.Part object at 0x7f6c1296e8d0>, <partitura.score.Part object at 0x7f6c11e27cd0>, <partitura.score.Part object at 0x7f6c11c6a450>], [<partitura.score.Part object at 0x7f6c11a71b90>, <partitura.score.Part object at 0x7f6c11a42450>, <partitura.score.Part object at 0x7f6c11800110>], [<partitura.score.Part object at 0x7f6c112fde10>, <partitura.score.Part object at 0x7f6c11244b50>, <partitura.score.Part object at 0x7f6c11320490>, <partitura.score.Part object at 0x7f6c162ccc10>], [<partitura.score.Part object at 0x7f6c1302e9d0>, <partitura.score.Part object at 0x7f6c10e73110>, <partitura.score.Part object at 0x7f6c10ce3090>, <partitura.score.Part object at 0x7f6c10b76050>], [<partitura.score.Part object at 0x7f6c112f40d0>, <partitura.score.Part object at 0x7f6c107eb210>, <partitura.score.Part object at 0x7f6c106b3bd0>]]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "geht gerade nur für monophonic True"
      ],
      "metadata": {
        "id": "v4TJGKiUs086"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import statistics\n",
        "\n",
        "\n",
        "def evaluate_accuracy_for_all(model, train_dataloader, part_dic,F1):\n",
        "    #print(\"part_dic:\",part_dic)\n",
        "\n",
        "    f_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    acc_score_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "    note_counter_0 = 0\n",
        "    note_counter_1 = 0\n",
        "    note_counter_2 = 0\n",
        "    note_counter_3 = 0\n",
        "    for idx, (voices, lens, nbr_voices, file_name) in enumerate(train_dataloader):\n",
        "            #check if elements match\n",
        "                                      \n",
        "            \n",
        "            #if idx == 0 or idx==2:\n",
        "                if nbr_voices[0]!=len(part_dic[file_name[0]]):\n",
        "                  print(\"ERROR: nbr_voices from part DOES NOT MATCH data loader:\" ) \n",
        "                \n",
        "                # load correct part object\n",
        "                file_name = file_name[0]\n",
        "                part = part_dic[file_name]\n",
        "                part_0 = part[0]\n",
        "                note_array_0 = part_0.note_array\n",
        "                part_1 = part[1]\n",
        "                note_array_1 = part_1.note_array\n",
        "                part_2 = part[2]\n",
        "                note_array_2 = part_2.note_array\n",
        "\n",
        "                note_counter_0 += len(note_array_0)\n",
        "                note_counter_1 += len(note_array_1)\n",
        "                note_counter_2 += len(note_array_2)\n",
        "\n",
        "\n",
        "                if len(part)== 4:\n",
        "                    part_3 = part[3]\n",
        "                    note_array_3 = part_3.note_array\n",
        "\n",
        "                    note_counter_3 += len(note_array_3)\n",
        "\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2,note_array_3]\n",
        "                    ground_truth_label_list = [0,1,2,3]              \n",
        "                    total_predictions_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                    total_truth_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                    accordance_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "                \n",
        "                if len(part)== 3:\n",
        "                    list_of_note_arrays = [note_array_0,note_array_1,note_array_2]\n",
        "                    ground_truth_label_list = [0,1,2]\n",
        "                    total_predictions_dict = {'0': [], '1': [], '2': [] }\n",
        "                    total_truth_dict = {'0': [], '1': [], '2': [] }\n",
        "                    accordance_dict = {'0': [], '1': [], '2': []}\n",
        "\n",
        "\n",
        "                for el_note_arr, note_array in enumerate(list_of_note_arrays):\n",
        "                    onset_beat = note_array[\"onset_beat\"]\n",
        "                    duration_beat = note_array[\"duration_beat\"]\n",
        "                    pitch_list = note_array[\"pitch\"]\n",
        "                    pitch_list = pitch_list - 21             \n",
        "                    note_idx_start = 12 * onset_beat\n",
        "                    note_idx_end = 12 * (onset_beat+duration_beat)\n",
        "\n",
        "                    ### round every entry up to next integer for the starting idx ###\n",
        "                    note_idx_start = [int(np.ceil(num)) for num in note_idx_start]                      # do this fur whole np array np.ceil(note_idx_start)\n",
        "                    ### round every entry down to next integer for the ending idx###\n",
        "                    note_idx_end = [int(np.floor(num)) for num in note_idx_end]\n",
        "\n",
        "                               \n",
        "                    # do model prediction\n",
        "                    model.eval()\n",
        "                    voices = voices.to(device).float()\n",
        "                    monophonic=True\n",
        "                    with torch.no_grad():\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        label = ground_truth_label_list[el_note_arr]\n",
        "\n",
        "                \n",
        "                    for i in range(len(note_idx_start)):\n",
        "\n",
        "                        start_first = note_idx_start[i]\n",
        "                        end_first =  note_idx_end[i]\n",
        "                        pitch_first = pitch_list[i]\n",
        "                        pred_list_first = prediction[start_first:end_first,pitch_first]\n",
        "                        \n",
        "                        if i < len(note_idx_start)-1:\n",
        "                            start_second = note_idx_start[i+1]\n",
        "                            end_second =  note_idx_end[i+1]\n",
        "                            pitch_second = pitch_list[i+1]\n",
        "                            pred_list_second = prediction[start_second:end_second,pitch_second]\n",
        "\n",
        "\n",
        "                        truth_list = [label for i in range(len(pred_list_first))]\n",
        "\n",
        "                    \n",
        "                        result = all(elem == pred_list_first[0] for elem in pred_list_first)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result == False:\n",
        "                            major, major_idx = torch.mode(pred_list_first,0)\n",
        "                            major = major.numpy().tolist()\n",
        "                            pred_list_first = [major for i in pred_list_first]\n",
        "\n",
        "                        result_second = all(elem == pred_list_second[0] for elem in pred_list_second)\n",
        "                        # do majority vote if not all predictions are for same voice\n",
        "                        if result_second == False:\n",
        "                            major_, major_idx = torch.mode(pred_list_second,0)\n",
        "                            major_ = major_.numpy().tolist()\n",
        "                            pred_list_second = [major_ for i in pred_list_second]\n",
        "                        \n",
        "                        total_predictions_dict[str(label)].append(pred_list_first)\n",
        "                        total_truth_dict[str(label)].append(truth_list)\n",
        "                            \n",
        "\n",
        "                        if F1 == True:\n",
        "                            if pred_list_first[0] == pred_list_second[0]:   #the list might have diff lenghts as diff notes have diff lengths, so is ito oke to just take first elemet\n",
        "                                accordance_dict[str(label)].append(1)\n",
        "                            else:\n",
        "                                accordance_dict[str(label)].append(0)\n",
        "\n",
        "                if F1 == False:\n",
        "                    count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "                    for gt, i in enumerate(total_predictions_dict.keys()):\n",
        "                      counting = 0\n",
        "                      for j in range(len(total_predictions_dict[i])):\n",
        "                          if total_predictions_dict[i][j][0] == gt:\n",
        "                            counting +=1  \n",
        "                      count_dict_2[i].append(counting)\n",
        "\n",
        "                    acc_0 = count_dict_2[\"0\"][0]/len(total_predictions_dict[\"0\"])\n",
        "                    acc_1 = count_dict_2[\"1\"][0]/len(total_predictions_dict[\"1\"])\n",
        "                    acc_2 = count_dict_2[\"2\"][0]/len(total_predictions_dict[\"2\"])\n",
        "\n",
        "                    print(\"accuracy 0, sample {}:\".format(idx),acc_0)\n",
        "                    print(\"accuracy 1, sample {}:\".format(idx),acc_1)\n",
        "                    print(\"accuracy 2, sample {}:\".format(idx),acc_2)\n",
        "\n",
        "                    if len(total_predictions_dict.keys())==4:\n",
        "                        acc_3 = count_dict_2[\"3\"][0]/len(total_predictions_dict[\"3\"])\n",
        "                        print(\"accuracy 3, sample {}:\".format(idx),acc_3)\n",
        "                        acc_score_dict[\"3\"].append(acc_3)\n",
        "\n",
        "\n",
        "                    acc_score_dict[\"0\"].append(acc_0)\n",
        "                    acc_score_dict[\"1\"].append(acc_1)\n",
        "                    acc_score_dict[\"2\"].append(acc_2)\n",
        "\n",
        "                if F1 == True:\n",
        "                    pred_0 = accordance_dict[\"0\"]\n",
        "                    pred_1 = accordance_dict[\"1\"]\n",
        "                    pred_2 = accordance_dict[\"2\"]                   \n",
        "                    truth_0 = [1 for i in range(len(accordance_dict[\"0\"]))]\n",
        "                    truth_1 = [1 for i in range(len(accordance_dict[\"1\"]))]\n",
        "                    truth_2 = [1 for i in range(len(accordance_dict[\"2\"]))]                  \n",
        "                    f1_v0 = sklearn.metrics.f1_score(truth_0, pred_0)\n",
        "                    f1_v1 = sklearn.metrics.f1_score(truth_1, pred_1)\n",
        "                    f1_v2 = sklearn.metrics.f1_score(truth_2, pred_2)                \n",
        "                    f_score_dict[\"0\"].append(f1_v0)\n",
        "                    f_score_dict[\"1\"].append(f1_v1)\n",
        "                    f_score_dict[\"2\"].append(f1_v2)\n",
        "                    if len(part)==4:\n",
        "                      pred_3 = accordance_dict[\"3\"]\n",
        "                      truth_3 = [1 for i in range(len(accordance_dict[\"3\"]))]\n",
        "                      f1_v3 = sklearn.metrics.f1_score(truth_3, pred_3)\n",
        "                      f_score_dict[\"3\"].append(f1_v3)\n",
        "    \n",
        "    if F1 == True:\n",
        "        return statistics.mean(f_score_dict[\"0\"]), statistics.mean(f_score_dict[\"1\"]), statistics.mean(f_score_dict[\"2\"]),statistics.mean(f_score_dict[\"3\"])\n",
        "    \n",
        "    if F1 == False:\n",
        "        print(\"note counters:\",\"v0:\",note_counter_0,\"v1:\",note_counter_1,\"v2:\",note_counter_2,\"v3:\",note_counter_3)\n",
        "        return statistics.mean(acc_score_dict[\"0\"]), statistics.mean(acc_score_dict[\"1\"]), statistics.mean(acc_score_dict[\"2\"]),statistics.mean(acc_score_dict[\"3\"])\n",
        "        #return total_predictions_dict, total_truth_dict"
      ],
      "metadata": {
        "id": "RWxVG3XAYTcC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_for_all(model,train_dataloader,part_dic,F1=False)\n",
        "\n",
        "acc_0 , acc_1, acc_2, acc_3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=False)\n",
        "acc_0 , acc_1, acc_2, acc_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygMV28FhKVAW",
        "outputId": "114d2993-cf20-4c6d-a7e0-93033f1d2fb9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0, sample 0: 0.9052132701421801\n",
            "accuracy 1, sample 0: 0.8233995584988962\n",
            "accuracy 2, sample 0: 0.979047619047619\n",
            "accuracy 0, sample 1: 0.9619047619047619\n",
            "accuracy 1, sample 1: 0.8952879581151832\n",
            "accuracy 2, sample 1: 0.729957805907173\n",
            "accuracy 3, sample 1: 0.02766798418972332\n",
            "accuracy 0, sample 2: 0.9117647058823529\n",
            "accuracy 1, sample 2: 0.6557377049180327\n",
            "accuracy 2, sample 2: 0.6892857142857143\n",
            "accuracy 3, sample 2: 0.05319148936170213\n",
            "accuracy 0, sample 3: 0.8934426229508197\n",
            "accuracy 1, sample 3: 0.8016528925619835\n",
            "accuracy 2, sample 3: 0.6\n",
            "accuracy 3, sample 3: 0.04\n",
            "accuracy 0, sample 4: 0.8943894389438944\n",
            "accuracy 1, sample 4: 0.7707509881422925\n",
            "accuracy 2, sample 4: 0.9448818897637795\n",
            "accuracy 0, sample 5: 0.9320754716981132\n",
            "accuracy 1, sample 5: 0.7428023032629558\n",
            "accuracy 2, sample 5: 0.6094117647058823\n",
            "accuracy 3, sample 5: 0.03293413173652695\n",
            "accuracy 0, sample 6: 0.9269102990033222\n",
            "accuracy 1, sample 6: 0.9243697478991597\n",
            "accuracy 2, sample 6: 0.9706666666666667\n",
            "note counters: v0: 2282 v1: 2323 v2: 2236 v3: 1238\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9179572243607778,\n",
              " 0.8020001647712148,\n",
              " 0.7890359229109764,\n",
              " 0.038448401321988096)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_v0, f1_v1, f1_v2, f1_v3 = evaluate_accuracy_for_all(model,val_dataloader,part_dic,F1=True)\n",
        "print(f1_v0, f1_v1, f1_v2, f1_v3)"
      ],
      "metadata": {
        "id": "FLLmyO6o5vW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_dict_2 = {'0': [], '1': [], '2': [], '3': [] }\n",
        "\n",
        "for gt, i in enumerate(dict_pred.keys()):\n",
        "  counting = 0\n",
        "  for j in range(len(dict_pred[i])):\n",
        "      if dict_pred[i][j][0] == gt:\n",
        "        counting +=1\n",
        "\n",
        "      \n",
        "  count_dict_2[i].append(counting)\n",
        "\n",
        "print(count_dict_2[\"0\"],count_dict_2[\"1\"],count_dict_2[\"2\"],count_dict_2[\"3\"])\n",
        "\n",
        "if len(dict_pred.keys())==4:\n",
        "    print(\"accuracy:\",count_dict_2[\"0\"][0]/len(dict_pred[\"0\"]),count_dict_2[\"1\"][0]/len(dict_pred[\"1\"]),count_dict_2[\"2\"][0]/len(dict_pred[\"2\"]),count_dict_2[\"3\"][0]/len(dict_pred[\"3\"]))\n",
        "\n",
        "if len(dict_pred.keys())==3:\n",
        "    print(\"accuracy:\",count_dict_2[\"0\"][0]/len(dict_pred[\"0\"]),count_dict_2[\"1\"][0]/len(dict_pred[\"1\"]),count_dict_2[\"2\"][0]/len(dict_pred[\"2\"]))"
      ],
      "metadata": {
        "id": "VYBpV_hMfGGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### take 0 -> compare to truth of 0,1,2,3 -> overall voice\n",
        "\n",
        "count_list = []\n",
        "\n",
        "count_dict = {'0': [], '1': [], '2': [], '3': [] }\n",
        "truth_dic = {'0': 0, '1': 1, '2': 2, '3': 3 }\n",
        "\n",
        "voice_entry_list = [\"0\", \"1\", \"2\", \"3\"]\n",
        "for voice_entry_one in voice_entry_list:\n",
        "    for voice_entry_two in voice_entry_list:\n",
        "        count_list = []\n",
        "        #print(\"voices:\",voice_entry_one,voice_entry_two)\n",
        "        for i in range(len(dict_pred[voice_entry_one])):\n",
        "            if dict_pred[voice_entry_one][i][0] == truth_dic[voice_entry_two]:      #dict_truth[voice_entry_two][i][0]:\n",
        "                count_list.append(1)\n",
        "            else:\n",
        "                count_list.append(0)\n",
        "        count_dict[voice_entry_one].append(count_list)\n",
        "\n",
        "dictionary_sum={}\n",
        "for i in voice_entry_list:\n",
        "    v0_match,v1_match,v2_match,v3_match = count_dict[i]\n",
        "    sum_v0 = np.sum(v0_match)\n",
        "    sum_v1 = np.sum(v1_match)\n",
        "    sum_v2 = np.sum(v2_match)\n",
        "    sum_v3 = np.sum(v3_match)\n",
        "    dictionary_sum[\"v0\"] = sum_v0\n",
        "    dictionary_sum[\"v1\"] = sum_v1\n",
        "    dictionary_sum[\"v2\"] = sum_v2\n",
        "    dictionary_sum[\"v3\"] = sum_v3\n",
        "\n",
        "    val_list = list(dictionary_sum.values())\n",
        "    \n",
        "    print(\"voice{} matches with\".format(i))\n",
        "    print(\"dict\",dictionary_sum)\n",
        "\n",
        "    max_sum = max(sum_v0,sum_v1,sum_v2,sum_v3)\n",
        "\n",
        "\n",
        "    print(\"max_sum\", val_list.index(max_sum) )\n",
        "\n",
        "    print(\"accuracy voice{}:\".format(i), max_sum/(sum_v0+sum_v1+sum_v2+sum_v3) )\n",
        "    print(\"________________\")\n",
        "    print(\" \")"
      ],
      "metadata": {
        "id": "BoQcV_i038DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR MONOPHONIC F1\n",
        "\n",
        "# start with GT\n",
        "# look at first note in pred-> save note label\n",
        "# look at second note in pred-> if same note as before : SUCESS if it is not: FAIL\n",
        " # DO This for all 4 voices\n",
        " ## in GT there is always the same voice following -> would always be an array of 1\n",
        "\n",
        "## POLYPHONIC \n",
        "\n",
        "# prbl after 1 note there can be multiple diff voices .. chords\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-bR7gcej90qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you have the ground truth on the different parts that you get when you import your score. Each part correspond to a voice. So if your note array contains all notes of all voices, you have for each note in your note array a number that is the ground truth voice (that you take from the part) and a number that is the predicted voice (that you take from the maximum vote)."
      ],
      "metadata": {
        "id": "Z5q305YzvjMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start time, duration , pitch to separate \n",
        "\n",
        "use the onset_beat and duration_beat\n",
        "\n",
        "multiply them according to the values set when producing the pianorolls \n",
        "\n",
        "-> get the position in the pianoroll\n",
        "\n",
        "time_div = 12\n",
        "\n"
      ],
      "metadata": {
        "id": "EmvxtyaVKG27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "A94mchm4LV6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy_v0\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v1\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v2\"],'-o')\n",
        "plt.plot(his[\"val_accuracy_v3\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['Accuracy0','Accuracy1','Accuracy2','Accuracy3'])\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TgJDHaxAgYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"val_accuracy\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend('Accuracy')\n",
        "plt.title('Accuracy vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OxMs8GEfMvPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(his[\"train_loss\"],'-o')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Loss'])\n",
        "plt.title('Loss vs Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqMcJT5aFL01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Old training loop - matrix and non matrix format"
      ],
      "metadata": {
        "id": "4olpdwzyG8dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "def training_loop(model,optimizer, train_dataloader, monophonic, epochs=50, val_dataloader=None, device=None, scheduler=None):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    print(\"monophonic set to:\",monophonic)\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        #accuracy_v0_sum = 0\n",
        "        #accuracy_v1_sum = 0\n",
        "        #accuracy_v2_sum = 0\n",
        "        #accuracy_v3_sum = 0\n",
        "\n",
        "        accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "        val_accuracy_sum_list = [0 for i in range(5)]                               ########## FIXED FOR 5 voices MAX right now - b.c. FUGUES have max 5 voices\n",
        "\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        accuracy_sum = 0\n",
        "        \n",
        "\n",
        "        for idx, (voices, lens, nbr_voices) in enumerate(train_dataloader):  \n",
        "            \n",
        "            voices = voices.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(voices, lens, nbr_voices)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()    \n",
        "\n",
        "            if monophonic == False:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                    v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                    mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                    \n",
        "                    single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                    v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                    mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                    accuracy_sum += acc \n",
        "\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4: \n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                    v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                    mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                    v_pred_argm[mask_pred] = -1\n",
        "                    v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_pred_flat:\", v_pred_flat.shape)\n",
        "                    \"\"\"\n",
        "                    \"\"\"\n",
        "                    if nbr_voices == 4:                   \n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                    if nbr_voices ==3:\n",
        "                        v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                    v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                    mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                    print(\"old mask\", mask_ori.shape)\n",
        "                    v_ori_argm[mask_ori] = -1\n",
        "                    v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                    print(\"v_ori_flat\", v_ori_flat.shape)\n",
        "                    acc = accuracy_score(v_pred_flat,v_ori_flat)   \n",
        "                    print(\"acc\",acc)                    \n",
        "                    accuracy_sum += acc \n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "            if monophonic == True:\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                    prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                    truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                    acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                    for i in range(len(prediction[0,:])):\n",
        "                      acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                      accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "                    \n",
        "                    \"\"\"\n",
        "                    pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "\n",
        "                    acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                    acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                    acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                    if nbr_voices == 4:\n",
        "                        acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                    \n",
        "                    # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    if nbr_voices == 4:\n",
        "                        accuracy_v3_sum += acc_v3 / len(lens)\n",
        "                    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "        if monophonic == True:\n",
        "\n",
        "            train_acc_list = np.array(accuracy_sum_list) / len(train_dataloader)\n",
        "            train_acc_list[3] = accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "            train_acc_list[4] = accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_acc\"].append(train_acc_list)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}, Train Accuracy_4 : {}\".format(train_loss, train_acc_list[0], train_acc_list[1], train_acc_list[2], train_acc_list[3],train_acc_list[4])) \n",
        "\n",
        "\n",
        "            \"\"\"\n",
        "            train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "            train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "            train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "            train_accuracy_v3 = accuracy_v3_sum / 18   ## bc only 18 pieces with len 3\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "            history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "            history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "            #if nbr_voices == 4:\n",
        "            history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "            print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3)) \n",
        "            #else:\n",
        "            #    print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2)) \n",
        "            \"\"\"\n",
        "\n",
        "        if monophonic == False:\n",
        "            train_accuracy = accuracy_sum / len(train_dataloader)\n",
        "\n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_accuracy\"].append(train_accuracy)\n",
        "            print(\"Train Loss: {}, Train Accuracy : {}\".format(train_loss, train_accuracy)) \n",
        "\n",
        "\n",
        "        if monophonic == True:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_v0_sum = 0\n",
        "                accuracy_v1_sum = 0\n",
        "                accuracy_v2_sum = 0\n",
        "                accuracy_v3_sum = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)  \n",
        "                        prediction = torch.swapaxes(torch.tensor(prediction), 0, 1)\n",
        "\n",
        "                        truth = np.squeeze(voices[:,:,:,:-1]).argmax(dim=1).cpu()\n",
        "\n",
        "                        acc_list = [0 for i in range(len(prediction[0,:]))]\n",
        "\n",
        "                        for i in range(len(prediction[0,:])):\n",
        "                          acc_list[i] = accuracy_score(prediction[:,i], truth[:,i])\n",
        "                          val_accuracy_sum_list[i] += acc_list[i]/len(lens)\n",
        "\n",
        "\n",
        "                        #print(\"val_accuracy_sum_list[3]\",val_accuracy_sum_list[3])\n",
        "                    #val_acc_list = np.array(val_accuracy_sum_list) / len(train_dataloader)\n",
        "                    #val_acc_list[3] = val_acc_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    #val_acc_list[4] = val_acc_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "\n",
        "\n",
        "                #history[\"val_acc_new\"].append(val_acc_list)\n",
        "                #print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "\n",
        "                        # Predict the model's output on a batch\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                            \n",
        "                        # compute the accuracy \n",
        "                        acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(voices[:,:,:,0]).argmax(dim=1).cpu())\n",
        "                        acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(voices[:,:,:,1]).argmax(dim=1).cpu())\n",
        "                        acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(voices[:,:,:,2]).argmax(dim=1).cpu())\n",
        "                        if nbr_voices == 4:\n",
        "                            acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(voices[:,:,:,3]).argmax(dim=1).cpu())\n",
        "                            \n",
        "                            \n",
        "                        # normalize according to the number of sequences in the batch (atm len(lens)==1)\n",
        "                        accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                        accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                        accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                        if nbr_voices == 4:\n",
        "                            accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                    val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                    val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                    val_accuracy_v3 = accuracy_v3_sum / 18  ##len(val_dataloader). - bc 18 pieces only with voice 3\n",
        "\n",
        "\n",
        "                    val_acc_list = np.array(val_accuracy_sum_list) / len(val_dataloader)\n",
        "                    val_acc_list[3] = val_accuracy_sum_list[3] / 18                       ## bc only 18 pieces with len 3\n",
        "                    val_acc_list[4] = val_accuracy_sum_list[4] / 2                         ##### CHECK IF 2 or 3 pieces with len 4\n",
        "                    \n",
        "\n",
        "\n",
        "                history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "                history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "                history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "                #if nbr_voices == 4:\n",
        "                history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "                #else:\n",
        "                #    print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2))\n",
        "\n",
        "\n",
        "                history[\"val_acc_new\"].append(val_acc_list)\n",
        "                print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}, Validation Accuracy_4 : {}\".format(val_acc_list[0], val_acc_list[1], val_acc_list[2], val_acc_list[3],val_acc_list[4]))\n",
        "\n",
        "\n",
        "        if monophonic == False:\n",
        "            if val_dataloader is not None:\n",
        "                # Evaluate on the validation set\n",
        "                model.eval()\n",
        "                accuracy_sum = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for voices, lens, nbr_voices in val_dataloader:\n",
        "\n",
        "                        voices = voices.to(device).float()\n",
        "                        \n",
        "                        # Predict the model's output on a batch\n",
        "\n",
        "                        prediction = model.predict(voices[:,:,:,-1], lens, monophonic)                      \n",
        "\n",
        "                        v_pred_argm = torch.tensor(np.argmax(prediction,axis=0))\n",
        "                        mask_pred = (prediction.sum(axis=0) == 0)\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)\n",
        "                        \n",
        "                        single_voices = voices[:,:,:,:-1]\n",
        "\n",
        "                        v_ori_argm = torch.argmax(np.squeeze(single_voices,axis=0).cpu(),axis=2)\n",
        "                        mask_ori = ((np.squeeze(single_voices,axis=0).cpu()).sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)  \n",
        "                        accuracy_sum += acc \n",
        "\n",
        "                        \"\"\"\n",
        "                        pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(voices[:,:,:,-1], lens,monophonic)\n",
        "                        if nbr_voices == 4: \n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2), torch.tensor(pred_v3)], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_pred_comb = torch.stack([torch.tensor(pred_v0), torch.tensor(pred_v1), torch.tensor(pred_v2)], dim=2)\n",
        "                        v_pred_argm = v_pred_comb.argmax(dim=2)\n",
        "                        mask_pred = (v_pred_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_pred_argm[mask_pred] = -1\n",
        "                        v_pred_flat = torch.flatten(v_pred_argm, start_dim=0, end_dim=-1)                      \n",
        "                        if nbr_voices == 4:                   \n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu(), np.squeeze(voices[:,:,:,3]).cpu()], dim=2)\n",
        "                        if nbr_voices ==3:\n",
        "                            v_ori_comb = torch.stack([np.squeeze(voices[:,:,:,0]).cpu(), np.squeeze(voices[:,:,:,1]).cpu(), np.squeeze(voices[:,:,:,2]).cpu()], dim=2)\n",
        "                        v_ori_argm = v_ori_comb.argmax(dim=2)\n",
        "                        mask_ori = (v_ori_comb.sum(axis=2) == 0).numpy()\n",
        "                        v_ori_argm[mask_ori] = -1\n",
        "                        v_ori_flat = torch.flatten(v_ori_argm, start_dim=0, end_dim=-1)\n",
        "\n",
        "                        acc = accuracy_score(v_pred_flat,v_ori_flat)\n",
        "                        accuracy_sum += acc \n",
        "                        \"\"\"\n",
        "                        \n",
        "                    # normalize according to the number of batches\n",
        "                    val_accuracy = accuracy_sum / len(val_dataloader)\n",
        "\n",
        "                history[\"val_accuracy\"].append(val_accuracy)  \n",
        "                print(\" Validation Accuracy : {}\".format(val_accuracy))\n",
        "\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "nfDV8MKGHE3J"
      }
    }
  ]
}