{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2"
      },
      "source": [
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77uea5NROiFC"
      },
      "source": [
        "Maybe later: \n",
        "\n",
        "in dataloader:  pad_colate ----> consider only unpaded data for loss in training\n",
        "\n",
        " pad to longest seq in batch ..- better sort after len before loading batches\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "batch_size = 1      # needs pading for > 1\n",
        "PATH_TO_DATA = \"AI-MA_project/pianoroll\"\n",
        "workers = 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew5g9Q-3PAAr"
      },
      "source": [
        "# DATALOADER V1\n",
        "\"\"\"class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, labels, n, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "        # load all files like below  (maybe do dicts instead of 1 big array)\n",
        "        # make list of dict.  -voice_0 ..voice_all, length (-6 in total)\n",
        "\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            file_names = sorted(glob.glob(os.path.join(data_dir, labels[iLabel], \"*.pkl\")))    \n",
        "\n",
        "            if iLabel == 0:\n",
        "                self.data = np.array(file_names[:n])\n",
        "                self.labels = np.array(np.repeat(labels[iLabel], len(self.data)))\n",
        "                \n",
        "            else:\n",
        "                self.data = np.append(self.data, file_names[:n])\n",
        "                self.labels = np.append(self.labels, np.repeat(labels[iLabel], len(self.data)))\n",
        "                \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):    ###maybe later revert sorted idx to file name idx\n",
        "        print(self.data[idx])\n",
        "\n",
        "        with open(self.data[idx]  ,'rb') as f:\n",
        "          loaded_obj = pickle.load(f)\n",
        "    \n",
        "        pr_load = torch.tensor(loaded_obj.T)      # transpose np array first\n",
        "\n",
        "\n",
        "        sample = {'pr': pr_load.float(), 'label': self.labels[idx]}     # also output the len \n",
        "        return sample\n",
        "\"\"\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1-u8zYd-gyo"
      },
      "source": [
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, labels, n, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "                                                                                # load all files like below  (maybe do dicts instead of 1 big array)\n",
        "                                                                                # make list of dict.  -voice_0 ..voice_all, length (-6 in total)\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        #loaded_obj = loaded_obj.T\n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))                             # for later: sort it after len(rows) when loading batches - to not have batches with len 1 and len 100 and then need to pad 99 zeros\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "            \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)                              # for later: sort it after len(rows) when loading batches - to not have batches with len 1 and len 100 and then need to pad 99 zeros\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                                                    \n",
        "        \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):                                                 ###maybe later revert sorted idx to file name idx\n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])\n",
        "          \n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "\n",
        "        length = self.pr_dict[\"length\"]\n",
        "\n",
        "        # v0.shape =torch.Size([816, 128])  , when doing v0, v1, v2, v3, v_all  = real , print(v0.shape) it is torch.Size([1, 756, 128])  - normal?\n",
        "\n",
        "        return v0, v1, v2, v3, v_all, length"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "#labels = [\"voice_all\",\"voice_0\"]\n",
        "dataset = MusicDataset(PATH_TO_DATA, labels, 368)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1u2sxCLaZH6"
      },
      "source": [
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM0SS-uLDL0C"
      },
      "source": [
        "### just for testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHbTh6ZkdFOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59750053-da91-45b0-9491-2680bde3c5dd"
      },
      "source": [
        "for batch_idx, real in enumerate(loader):\n",
        "  v0, v1, v2, v3, v_all, length  = real\n",
        "print(length)\n",
        "  #print(v0.shape, v1.shape, v2.shape, v3.shape, v_all.shape)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([756]), tensor([624]), tensor([480]), tensor([480]), tensor([816]), tensor([384]), tensor([1332]), tensor([960]), tensor([576]), tensor([624]), tensor([1344]), tensor([576]), tensor([768]), tensor([672]), tensor([588]), tensor([816]), tensor([648]), tensor([624]), tensor([432]), tensor([576]), tensor([576]), tensor([720]), tensor([576]), tensor([576]), tensor([576]), tensor([480]), tensor([480]), tensor([384]), tensor([624]), tensor([564]), tensor([480]), tensor([576]), tensor([420]), tensor([624]), tensor([384]), tensor([672]), tensor([576]), tensor([480]), tensor([576]), tensor([384]), tensor([672]), tensor([432]), tensor([384]), tensor([624]), tensor([384]), tensor([576]), tensor([480]), tensor([588]), tensor([576]), tensor([480]), tensor([720]), tensor([576]), tensor([480]), tensor([480]), tensor([780]), tensor([396]), tensor([912]), tensor([528]), tensor([576]), tensor([768]), tensor([432]), tensor([576]), tensor([624]), tensor([480]), tensor([672]), tensor([624]), tensor([432]), tensor([1152]), tensor([960]), tensor([624]), tensor([396]), tensor([528]), tensor([576]), tensor([528]), tensor([672]), tensor([576]), tensor([528]), tensor([864]), tensor([576]), tensor([816]), tensor([876]), tensor([768]), tensor([672]), tensor([768]), tensor([672]), tensor([756]), tensor([576]), tensor([576]), tensor([468]), tensor([1296]), tensor([528]), tensor([588]), tensor([480]), tensor([576]), tensor([624]), tensor([720]), tensor([576]), tensor([576]), tensor([624]), tensor([480]), tensor([792]), tensor([576]), tensor([432]), tensor([528]), tensor([768]), tensor([1008]), tensor([576]), tensor([1008]), tensor([576]), tensor([528]), tensor([432]), tensor([816]), tensor([576]), tensor([576]), tensor([1296]), tensor([576]), tensor([528]), tensor([864]), tensor([576]), tensor([768]), tensor([672]), tensor([576]), tensor([528]), tensor([480]), tensor([624]), tensor([444]), tensor([576]), tensor([576]), tensor([600]), tensor([480]), tensor([1920]), tensor([672]), tensor([768]), tensor([384]), tensor([996]), tensor([624]), tensor([624]), tensor([576]), tensor([528]), tensor([768]), tensor([1164]), tensor([612]), tensor([480]), tensor([432]), tensor([672]), tensor([480]), tensor([384]), tensor([432]), tensor([624]), tensor([576]), tensor([384]), tensor([864]), tensor([384]), tensor([384]), tensor([768]), tensor([480]), tensor([492]), tensor([576]), tensor([576]), tensor([432]), tensor([576]), tensor([528]), tensor([624]), tensor([768]), tensor([432]), tensor([768]), tensor([384]), tensor([576]), tensor([672]), tensor([648]), tensor([528]), tensor([432]), tensor([576]), tensor([480]), tensor([576]), tensor([816]), tensor([384]), tensor([528]), tensor([480]), tensor([480]), tensor([588]), tensor([384]), tensor([528]), tensor([384]), tensor([576]), tensor([384]), tensor([576]), tensor([576]), tensor([576]), tensor([444]), tensor([576]), tensor([672]), tensor([576]), tensor([1416]), tensor([816]), tensor([756]), tensor([720]), tensor([864]), tensor([1296]), tensor([756]), tensor([432]), tensor([2316]), tensor([720]), tensor([576]), tensor([792]), tensor([576]), tensor([720]), tensor([768]), tensor([672]), tensor([528]), tensor([1296]), tensor([1296]), tensor([960]), tensor([576]), tensor([744]), tensor([528]), tensor([1152]), tensor([480]), tensor([648]), tensor([732]), tensor([384]), tensor([480]), tensor([480]), tensor([912]), tensor([288]), tensor([864]), tensor([480]), tensor([684]), tensor([540]), tensor([576]), tensor([1368]), tensor([816]), tensor([384]), tensor([768]), tensor([480]), tensor([672]), tensor([384]), tensor([1212]), tensor([528]), tensor([720]), tensor([768]), tensor([384]), tensor([624]), tensor([384]), tensor([480]), tensor([480]), tensor([528]), tensor([480]), tensor([1272]), tensor([816]), tensor([624]), tensor([768]), tensor([624]), tensor([588]), tensor([768]), tensor([1296]), tensor([480]), tensor([588]), tensor([528]), tensor([624]), tensor([624]), tensor([720]), tensor([528]), tensor([588]), tensor([912]), tensor([672]), tensor([576]), tensor([576]), tensor([768]), tensor([576]), tensor([624]), tensor([588]), tensor([480]), tensor([912]), tensor([672]), tensor([480]), tensor([972]), tensor([576]), tensor([624]), tensor([576]), tensor([480]), tensor([576]), tensor([480]), tensor([480]), tensor([576]), tensor([480]), tensor([768]), tensor([576]), tensor([480]), tensor([528]), tensor([384]), tensor([1296]), tensor([768]), tensor([972]), tensor([624]), tensor([480]), tensor([480]), tensor([756]), tensor([480]), tensor([576]), tensor([672]), tensor([864]), tensor([816]), tensor([384]), tensor([816]), tensor([384]), tensor([576]), tensor([768]), tensor([480]), tensor([576]), tensor([576]), tensor([684]), tensor([576]), tensor([612]), tensor([816]), tensor([528]), tensor([528]), tensor([720]), tensor([672]), tensor([624]), tensor([576]), tensor([480]), tensor([864]), tensor([480]), tensor([480]), tensor([576]), tensor([576]), tensor([768]), tensor([864]), tensor([576]), tensor([480]), tensor([480]), tensor([624]), tensor([384]), tensor([432]), tensor([576]), tensor([816]), tensor([480]), tensor([792]), tensor([384]), tensor([576]), tensor([720]), tensor([480]), tensor([624]), tensor([576]), tensor([576]), tensor([720]), tensor([768]), tensor([480]), tensor([480]), tensor([576]), tensor([624]), tensor([624]), tensor([1056]), tensor([924]), tensor([528]), tensor([576]), tensor([480]), tensor([576]), tensor([576]), tensor([576]), tensor([576]), tensor([576]), tensor([684]), tensor([576]), tensor([624]), tensor([576])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScrB4vSsHJh1",
        "outputId": "c23428ba-eeae-4008-dc51-d931793adb79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "v0.shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 576, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iybzKxeDjxe"
      },
      "source": [
        "# Model - RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAv7D01XO4Of"
      },
      "source": [
        "#https://github.com/fosfrancesco/pkspell/blob/main/src/models/models.py"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1oi5Ey7Fzpb"
      },
      "source": [
        "class MusicRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim, input_dim=17, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):\n",
        "        super(MusicRNN, self).__init__()\n",
        "\n",
        "        self.n_out = output_dim\n",
        "        \n",
        "        \n",
        "        rnn_cell = nn.GRU\n",
        "\n",
        "        self.rnn = rnn_cell(\n",
        "            input_size=input_dim,\n",
        "            hidden_size= hidden_dim,\n",
        "            num_layers=rnn_depth)\n",
        "\n",
        "\n",
        "        # Output layers. The input will be two times\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "\n",
        "        # Loss function that we will use during training.\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "\n",
        "        # pack padded seq\n",
        "        rnn_out = self.rnn(sentences)\n",
        "        # pad packed seq\n",
        "\n",
        "        out_0 = self.top_layer_voice_0(rnn_out)\n",
        "        out_1 = self.top_layer_voice_1(rnn_out)\n",
        "        out_2 = self.top_layer_voice_2(rnn_out)\n",
        "        out_3 = self.top_layer_voice_3(rnn_out)\n",
        "        \n",
        "\n",
        "        return out_0, out_1, out_2, out_3\n",
        "\n",
        "    def forward(self, sentences, v0, v1, v2, v3, sentences_len):\n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_0, scores_1, scores_2, scores_3 = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_0.view(-1, self.n_out)\n",
        "        score_1  = scores_1.view(-1, self.n_out)\n",
        "        score_2  = scores_2.view(-1, self.n_out)\n",
        "        score_3  = scores_3.view(-1, self.n_out)\n",
        "\n",
        "        # dont take padded len -> loss(score_0[:sentence_len], v0)      -> therefore store real len of voices  - before view\n",
        "\n",
        "        v0 = v0.view(-1)\n",
        "        v1 = v1.view(-1)\n",
        "        v2 = v2.view(-1)\n",
        "        v3 = v3.view(-1)\n",
        " \n",
        "        \n",
        "        loss = self.loss_pitch(score_0, v0) + self.loss_pitch(score_1, v1) + self.loss_pitch(score_2, v2) + self.loss_pitch(score_3, v3)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def predict(self, sentences, sentences_len):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_0, scores_1, scores_2, scores_3 = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
        "        predicted_0 = scores_0.argmax(dim=2)\n",
        "        predicted_1 = scores_1.argmax(dim=2)\n",
        "        predicted_2 = scores_2.argmax(dim=2)\n",
        "        predicted_3 = scores_3.argmax(dim=2)       # prbl distr over 0-not there, 1-there   - argmax -> take higher probl of those 2\n",
        "        \n",
        "        ##  predicted_X should be enough for BS = 1;  \n",
        "        return (\n",
        "            [\n",
        "                predicted_0[: int(l), i].cpu().numpy()\n",
        "                for i, l in enumerate(sentences_len)\n",
        "            ],\n",
        "            [\n",
        "                predicted_1[: int(l), i].cpu().numpy()\n",
        "                for i, l in enumerate(sentences_len)\n",
        "            ],\n",
        "            [\n",
        "                predicted_2[: int(l), i].cpu().numpy()\n",
        "                for i, l in enumerate(sentences_len)\n",
        "            ],\n",
        "            [\n",
        "                predicted_3[: int(l), i].cpu().numpy()\n",
        "                for i, l in enumerate(sentences_len)\n",
        "            ],\n",
        "        )\n",
        "\n"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay, train_dataloader, val_dataloader=None):\n",
        "  \n",
        "    \n",
        "    model = MusicRNN(output_dim, input_dim, hidden_dim, rnn_depth, cell_type)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITD9mYRfb9p-"
      },
      "source": [
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "#sys.path.append(str(Path(__file__).resolve().parents[2]))"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, epochs=50, val_dataloader=None, device=None, scheduler=None,):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    # Move model to GPU if needed\n",
        "    model = model.to(device)\n",
        "\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        accuracy_pitch_sum = 0\n",
        "        accuracy_ks_sum = 0\n",
        "        model.train()\n",
        "\n",
        "        for idx, (seqs, pitches, keysignatures, lens,) in enumerate(train_dataloader):  # seqs, pitches, keysignatures, lens are batches\n",
        "\n",
        "            seqs, pitches, keysignatures = (seqs.to(device), pitches.to(device),keysignatures.to(device),)\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(seqs, pitches, keysignatures, lens)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                predicted_pitch, predicted_ks = model.predict(seqs, lens)\n",
        "                for i, (p, k) in enumerate(zip(predicted_pitch, predicted_ks)):\n",
        "                    # compute the accuracy without considering the padding\n",
        "                    acc_pitch = accuracy_score(p, pitches[:, i][: len(p)].cpu())\n",
        "                    acc_ks = accuracy_score(k, keysignatures[:, i][: len(k)].cpu())\n",
        "                    # normalize according to the number of sequences in the batch\n",
        "                    accuracy_pitch_sum += acc_pitch / len(lens)\n",
        "                    accuracy_ks_sum += acc_ks / len(lens)\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "        # normalize according to the number of batches\n",
        "        train_accuracy_pitch = accuracy_pitch_sum / len(train_dataloader)\n",
        "        train_accuracy_ks = accuracy_ks_sum / len(train_dataloader)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_accuracy_pitch\"].append(train_accuracy_pitch)\n",
        "        history[\"train_accuracy_ks\"].append(train_accuracy_ks)\n",
        "        \n",
        "        print(\"Train Loss: {}, Train Accuracy (Pitch): {}, Train Accuracy (KS): {}\".format(\n",
        "                train_loss, train_accuracy_pitch, train_accuracy_ks))\n",
        "\n",
        "        if val_dataloader is not None:\n",
        "            # Evaluate on the validation set\n",
        "            model.eval()\n",
        "            accuracy_pitch_sum = 0\n",
        "            accuracy_ks_sum = 0\n",
        "            with torch.no_grad():\n",
        "                for seqs, pitches, keysignatures, lens in val_dataloader:\n",
        "                    # Predict the model's output on a batch\n",
        "                    predicted_pitch, predicted_ks = model.predict(seqs.to(device), lens)\n",
        "                    # Update the lists that will be used to compute the accuracy\n",
        "                    for i, (p, k) in enumerate(zip(predicted_pitch, predicted_ks)):\n",
        "                        # compute the accuracy without considering the padding\n",
        "                        acc_pitch = accuracy_score(p, pitches[:, i][: len(p)].cpu())\n",
        "                        acc_ks = accuracy_score(k, keysignatures[:, i][: len(k)].cpu())\n",
        "                        # normalize according to the number of sequences in the batch\n",
        "                        accuracy_pitch_sum += acc_pitch / len(lens)\n",
        "                        accuracy_ks_sum += acc_ks / len(lens)\n",
        "\n",
        "                # normalize according to the number of batches\n",
        "                val_accuracy_pitch = accuracy_pitch_sum / len(val_dataloader)\n",
        "                val_accuracy_ks = accuracy_ks_sum / len(val_dataloader)\n",
        "\n",
        "            history[\"val_accuracy_pitch\"].append(val_accuracy_pitch)\n",
        "            history[\"val_accuracy_ks\"].append(val_accuracy_ks)\n",
        "            \n",
        "            print(\"Validation Accuracy (Pitch): {}, Validation Accuracy (KS): {}\".format(\n",
        "                    val_accuracy_pitch, val_accuracy_ks))\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./models/temp/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "def start_experiment(\n",
        "    model,\n",
        "    epochs,\n",
        "    lr,\n",
        "    hidden_dim,\n",
        "    bs,\n",
        "    momentum,\n",
        "    hidden_dim2,\n",
        "    rnn_depth,\n",
        "    device,\n",
        "    dropout,\n",
        "    dropout2,\n",
        "    cell,\n",
        "    decay,\n",
        "    optimizer,\n",
        "    learn_all,\n",
        "    bidirectional,\n",
        "    mode,\n",
        "    augmentation):\n",
        "    \n",
        "    print(\"Loading the augmented ASAP dataset\")\n",
        "    # load the asap datasets with ks\n",
        "    with open(Path(\"./data/processed/asap_augmented.pkl\"), \"rb\") as fid:\n",
        "        full_list_of_dict_dataset = pickle.load(fid)\n",
        "\n",
        "    paths = list(set([e[\"original_path\"] for e in full_list_of_dict_dataset]))\n",
        "    list_of_dict_dataset = keep_best_transpositions(full_list_of_dict_dataset)\n",
        "\n",
        "    # remove pieces from asap that are in Musedata\n",
        "    paths = [p for p in paths if p != \"Bach/Prelude/bwv_865/xml_score.musicxml\"]\n",
        "    # remove mozart Fantasie because of incoherent key signature\n",
        "    paths = [p for p in paths if p != \"Mozart/Fantasie_475/xml_score.musicxml\"]\n",
        "    paths = sorted(paths)\n",
        "\n",
        "    from functools import partial\n",
        "\n",
        "    trainer = partial(\n",
        "        train_pkspell,\n",
        "        model,\n",
        "        epochs,\n",
        "        lr,\n",
        "        hidden_dim,\n",
        "        momentum,\n",
        "        hidden_dim2,\n",
        "        rnn_depth,\n",
        "        device,\n",
        "        dropout,\n",
        "        dropout2,\n",
        "        cell,\n",
        "        decay,\n",
        "        optimizer,\n",
        "        bidirectional,\n",
        "        mode)\n",
        "\n",
        "    def train_dataloader(ds):\n",
        "        return DataLoader(ds, batch_size=bs, shuffle=True, collate_fn=pad_collate, num_workers=2)\n",
        "\n",
        "    def val_dataloader(ds):\n",
        "        return DataLoader(ds, batch_size=1, shuffle=False, collate_fn=pad_collate, num_workers=1)\n",
        "\n",
        "    if learn_all:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = PSDataset(\n",
        "            list_of_dict_dataset,\n",
        "            paths,\n",
        "            transform_pc,\n",
        "            transform_tpc,\n",
        "            transform_key,\n",
        "            augment_dataset=augmentation,\n",
        "            sort=True,\n",
        "            truncate=None)\n",
        "        \n",
        "        _, history = trainer(train_dataloader(train_dataset),)\n",
        "    else:\n",
        "        paths = np.array(paths)\n",
        "        # Divide train and validation set\n",
        "        path_train, path_validation = sklearn.model_selection.train_test_split(\n",
        "            paths, test_size=0.15, random_state=seed,\n",
        "        )\n",
        "        print(\"Train and validation lenghts: \", len(path_train), len(path_validation))\n",
        "        train_dataset = PSDataset(\n",
        "            list_of_dict_dataset,\n",
        "            path_train,\n",
        "            transform_pc,\n",
        "            transform_tpc,\n",
        "            transform_key,\n",
        "            augment_dataset=augmentation,\n",
        "            sort=True,\n",
        "            truncate=None) \n",
        "        \n",
        "        validation_dataset = PSDataset(\n",
        "            list_of_dict_dataset,\n",
        "            path_validation,\n",
        "            transform_pc,\n",
        "            transform_tpc,\n",
        "            transform_key,\n",
        "            augment_dataset=False)\n",
        "        \n",
        "        _, history = trainer(\n",
        "            train_dataloader(train_dataset), val_dataloader(validation_dataset))"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}