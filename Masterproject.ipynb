{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4526d5f9-60fd-4ef7-fa85-68ad1fe5e3e1"
      },
      "source": [
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AI-MA_project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77uea5NROiFC"
      },
      "source": [
        "Maybe later: \n",
        "\n",
        "in dataloader:  pad_colate ----> consider only unpaded data for loss in training\n",
        "\n",
        " pad to longest seq in batch ..- better sort after len before loading batches\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "batch_size = 1      # needs pading for > 1\n",
        "PATH_TO_DATA = \"AI-MA_project/pianoroll\"\n",
        "workers = 0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1-u8zYd-gyo"
      },
      "source": [
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "                                                                                # load all files like below  (maybe do dicts instead of 1 big array)\n",
        "                                                                                # make list of dict.  -voice_0 ..voice_all, length (-6 in total)\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        #loaded_obj = loaded_obj.T\n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))                             # for later: sort it after len(rows) when loading batches - to not have batches with len 1 and len 100 and then need to pad 99 zeros\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "            \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)                              # for later: sort it after len(rows) when loading batches - to not have batches with len 1 and len 100 and then need to pad 99 zeros\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                                        \n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):                                                 ###maybe later revert sorted idx to file name idx\n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])                  ##maybe shorten not saving it in list first\n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "\n",
        "        length = self.pr_dict[\"length\"][idx]\n",
        "\n",
        "        # v0.shape =torch.Size([816, 128])  , when doing v0, v1, v2, v3, v_all  = real , print(v0.shape) it is torch.Size([1, 756, 128])  - normal?\n",
        "\n",
        "        return (v0, v1, v2, v3, v_all, length)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "dataset = MusicDataset(PATH_TO_DATA)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM0SS-uLDL0C"
      },
      "source": [
        "### just for testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHbTh6ZkdFOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454fb05a-6485-49c0-85ff-e4f2b952e950"
      },
      "source": [
        "for batch_idx, real in enumerate(loader):\n",
        "  v0, v1, v2, v3, v_all, length  = real\n",
        "print(v0.shape)\n",
        "  #print(v0.shape, v1.shape, v2.shape, v3.shape, v_all.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 576, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iybzKxeDjxe"
      },
      "source": [
        "# Model - RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAv7D01XO4Of"
      },
      "source": [
        "#https://github.com/fosfrancesco/pkspell/blob/main/src/models/models.py"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1oi5Ey7Fzpb"
      },
      "source": [
        "class MusicRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim=128, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                  #need to know input/output dim first\n",
        "        super(MusicRNN, self).__init__()\n",
        "\n",
        "        self.n_out = output_dim\n",
        "\n",
        "        input_dim = output_dim \n",
        "        \n",
        "        \n",
        "        rnn_cell = nn.GRU\n",
        "\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth, batch_first=True)\n",
        "\n",
        "\n",
        "        # Output layers. The input will be two times\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "\n",
        "        # Loss function that we will use during training.\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "\n",
        "        # pack padded seq\n",
        "        rnn_out ,_= self.rnn(sentences)\n",
        "        # pad packed seq\n",
        "\n",
        "        out_0 = self.top_layer_voice_0(rnn_out)\n",
        "        out_1 = self.top_layer_voice_1(rnn_out)\n",
        "        out_2 = self.top_layer_voice_2(rnn_out)\n",
        "        out_3 = self.top_layer_voice_3(rnn_out)\n",
        "        \n",
        "        #print(\"out_0.shape:\",out_0.shape)\n",
        "        #print(\"out_1.shape:\",out_1.shape)\n",
        "        #print(\"out_2.shape:\",out_2.shape)\n",
        "        #print(\"out_3.shape:\",out_3.shape)\n",
        "\n",
        "        return out_0, out_1, out_2, out_3\n",
        "\n",
        "    def forward(self, sentences, v0, v1, v2, v3, sentences_len):                # maybe change sent -> v_all\n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_0, scores_1, scores_2, scores_3 = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_0.view(-1, self.n_out)\n",
        "        score_1  = scores_1.view(-1, self.n_out)\n",
        "        score_2  = scores_2.view(-1, self.n_out)\n",
        "        score_3  = scores_3.view(-1, self.n_out)\n",
        "\n",
        "        # dont take padded len -> loss(score_0[:sentence_len], v0)      -> therefore store real len of voices  - before view\n",
        "\n",
        "        #### check in documentation of loss if this makes sense -- concerning the shapes\n",
        "\n",
        "\n",
        "\n",
        "        v0 = v0.squeeze()\n",
        "        v1 = v1.squeeze()\n",
        "        v2 = v2.squeeze()\n",
        "        v3 = v3.squeeze()\n",
        "\n",
        " \n",
        "        \n",
        "        loss = self.loss(score_0, v0) + self.loss(score_1, v1) + self.loss(score_2, v2) + self.loss(score_3, v3)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def predict(self, sentences, sentences_len):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_0, scores_1, scores_2, scores_3 = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
        "        predicted_0 = scores_0.argmax(dim=2)\n",
        "        predicted_1 = scores_1.argmax(dim=2)\n",
        "        predicted_2 = scores_2.argmax(dim=2)\n",
        "        predicted_3 = scores_3.argmax(dim=2)       # prbl distr over 0-not there, 1-there   - argmax -> take higher probl of those 2\n",
        "        \n",
        "        ##  predicted_X should be enough for BS = 1;  \n",
        "\n",
        "        #print(\"simple pred:\",predicted_0)\n",
        "        #print(\"indexing pred:\",[predicted_0[: int(l), i].cpu().numpy() for i, l in enumerate(sentences_len)])\n",
        "        #print(\"test:\",[predicted_0[: int(l), i].cpu().numpy() for i, l in enumerate(sentences_len)])\n",
        "        \n",
        "        # ([predicted_0[: int(l), i].cpu().numpy() for i, l in enumerate(sentences_len)],\n",
        "        #        [predicted_1[: int(l), i].cpu().numpy() for i, l in enumerate(sentences_len)],\n",
        "        #        [predicted_2[: int(l), i].cpu().numpy() for i, l in enumerate(sentences_len)],\n",
        "        #        [predicted_3[: int(l), i].cpu().numpy() for i, l in enumerate(sentences_len)])\n",
        "        return np.squeeze(predicted_0.cpu().numpy()), np.squeeze(predicted_1.cpu().numpy()), np.squeeze(predicted_2.cpu().numpy()), np.squeeze(predicted_3.cpu().numpy())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHgEe10xprjf"
      },
      "source": [
        "#start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, learn_all)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITD9mYRfb9p-"
      },
      "source": [
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay, train_dataloader, val_dataloader=None):\n",
        "    \n",
        "    output_dim = 128                                                            ### think about how to get the correct input=output size\n",
        "    model = MusicRNN(output_dim, hidden_dim, rnn_depth, cell_type)              \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxwrIPxjsIWw"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, epochs=50, val_dataloader=None, device=None, scheduler=None,):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    # Move model to GPU if needed\n",
        "    model = model.to(device)\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        accuracy_v0_sum = 0\n",
        "        accuracy_v1_sum = 0\n",
        "        accuracy_v2_sum = 0\n",
        "        accuracy_v3_sum = 0\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "        \n",
        "\n",
        "        for idx, (v0, v1, v2, v3, v_all, lens) in enumerate(train_dataloader):  \n",
        "\n",
        "            v0, v1, v2, v3, v_all = (v0.to(device).float(), v1.to(device).float(), v2.to(device).float(), v3.to(device).float(), v_all.to(device).float())\n",
        "            \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(v_all, v0, v1, v2, v3, lens)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()\n",
        "\n",
        "            print(len(lens),lens)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(v_all, lens)\n",
        "                \n",
        "\n",
        "                # compute the accuracy without considering the padding\n",
        "               \n",
        "                acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(v0).argmax(dim=1).cpu())\n",
        "                acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(v1).argmax(dim=1).cpu())\n",
        "                acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(v2).argmax(dim=1).cpu())\n",
        "                acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(v3).argmax(dim=1).cpu())\n",
        "                # normalize according to the number of sequences in the batch\n",
        "                accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "\n",
        "        # normalize according to the number of batches\n",
        "\n",
        "        train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "        train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "        train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "        train_accuracy_v3 = accuracy_v3_sum / len(train_dataloader)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "        history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "        history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "        history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "        \n",
        "        print(\"Train Loss: {}, Train Accuracy_0 : {}, Train Accuracy_1 : {},Train Accuracy_2 : {}, Train Accuracy_3 : {}\".format(train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3)) \n",
        "\n",
        "\n",
        "        if val_dataloader is not None:\n",
        "            # Evaluate on the validation set\n",
        "            model.eval()\n",
        "            accuracy_v0_sum = 0\n",
        "            accuracy_v1_sum = 0\n",
        "            accuracy_v2_sum = 0\n",
        "            accuracy_v3_sum = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for v0, v1, v2, v3, v_all, lens in val_dataloader:\n",
        "\n",
        "                    v0, v1, v2, v3, v_all = (v0.to(device).float(), v1.to(device).float(), v2.to(device).float(), v3.to(device).float(), v_all.to(device).float())\n",
        "\n",
        "                    # Predict the model's output on a batch\n",
        "                    pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(v_all, lens)\n",
        "                        \n",
        "                        # compute the accuracy without considering the padding\n",
        "                    acc_v0 = accuracy_score(torch.tensor(pred_v0), np.squeeze(v0).argmax(dim=1).cpu())\n",
        "                    acc_v1 = accuracy_score(torch.tensor(pred_v1), np.squeeze(v1).argmax(dim=1).cpu())\n",
        "                    acc_v2 = accuracy_score(torch.tensor(pred_v2), np.squeeze(v2).argmax(dim=1).cpu())\n",
        "                    acc_v3 = accuracy_score(torch.tensor(pred_v3), np.squeeze(v3).argmax(dim=1).cpu())\n",
        "                        \n",
        "                        \n",
        "                        # normalize according to the number of sequences in the batch\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                # normalize according to the number of batches\n",
        "                val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                val_accuracy_v3 = accuracy_v3_sum / len(val_dataloader)\n",
        "\n",
        "            history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "            history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "            history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "            history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "            \n",
        "            print(\" Validation Accuracy_0 : {}, Validation Accuracy_1 : {}, Validation Accuracy_2 : {}, Validation Accuracy_3 : {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNzPG7eeIEoO",
        "outputId": "cd617bcb-261a-48a1-8109-d21157419aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, learn_all)\n",
        "\n",
        "#print(next(model.parameters()).is_cuda)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-bc96b26f9aec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(next(model.parameters()).is_cuda)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'start_experiment' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "def start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay, learn_all):\n",
        "    \n",
        "\n",
        "    trainer = partial(train, epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay)\n",
        "\n",
        "    if learn_all == True:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset(PATH_TO_DATA)\n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "                \n",
        "        _, history = trainer(train_dataloader)\n",
        "    \n",
        "    else:\n",
        "        # Divide train and validation set\n",
        "        path_train, path_validation = sklearn.model_selection.train_test_split(PATH_TO_DATA, test_size=0.15, random_state=10,)\n",
        "\n",
        "        print(\"Train and validation lenghts: \", len(path_train), len(path_validation))\n",
        "        train_dataset = MusicDataset(path_train)\n",
        "        validation_dataset = MusicDataset(path_validation)\n",
        "        \n",
        "        train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        val_dataloader = torch.utils.data.DataLoader(validation_dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "\n",
        "        _, history = trainer(train_dataloader, val_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicRNN\n",
        "epochs = 10\n",
        "lr = 0.001  # was 0.01\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 2 # was 1\n",
        "device = None #torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")            ---- if None:  choses device automatically\n",
        "cell_type = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"False\"      # change later to True to see if learning works"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm"
      },
      "source": [
        "start_experiment(epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell_type, decay, learn_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B1TRcSBn0w2"
      },
      "source": [
        "# seems like there is still something wrong bc the accuracy doesnt change"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zH4kncO4O5-k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}