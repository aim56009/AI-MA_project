{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import glob\n",
        "import os"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4775233-2937-45c2-f091-de2ed55cd1a5"
      },
      "source": [
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AI-MA_project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpz1MOIOgtk"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77uea5NROiFC"
      },
      "source": [
        "Maybe later: \n",
        "\n",
        "in dataloader:  pad_colate ----> consider only unpaded data for loss in training\n",
        "\n",
        " pad to longest seq in batch ..- better sort after len before loading batches\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygWXNSrCbRQi"
      },
      "source": [
        "batch_size = 1      # needs pading for > 1\n",
        "PATH_TO_DATA = \"AI-MA_project/pianoroll\"\n",
        "workers = 0"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew5g9Q-3PAAr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "7b344f7b-4c4d-4ea5-b833-e4dd8b82b7d1"
      },
      "source": [
        "# DATALOADER V1\n",
        "\"\"\"class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, labels, n, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "        # load all files like below  (maybe do dicts instead of 1 big array)\n",
        "        # make list of dict.  -voice_0 ..voice_all, length (-6 in total)\n",
        "\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            file_names = sorted(glob.glob(os.path.join(data_dir, labels[iLabel], \"*.pkl\")))    \n",
        "\n",
        "            if iLabel == 0:\n",
        "                self.data = np.array(file_names[:n])\n",
        "                self.labels = np.array(np.repeat(labels[iLabel], len(self.data)))\n",
        "                \n",
        "            else:\n",
        "                self.data = np.append(self.data, file_names[:n])\n",
        "                self.labels = np.append(self.labels, np.repeat(labels[iLabel], len(self.data)))\n",
        "                \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):    ###maybe later revert sorted idx to file name idx\n",
        "        print(self.data[idx])\n",
        "\n",
        "        with open(self.data[idx]  ,'rb') as f:\n",
        "          loaded_obj = pickle.load(f)\n",
        "    \n",
        "        pr_load = torch.tensor(loaded_obj.T)      # transpose np array first\n",
        "\n",
        "\n",
        "        sample = {'pr': pr_load.float(), 'label': self.labels[idx]}     # also output the len \n",
        "        return sample\n",
        "\"\"\""
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'class MusicDataset(Dataset):\\n\\n    def __init__(self, data_dir, labels, n, transforms=None):\\n        self.transforms = transforms\\n        self.data_dir = data_dir\\n        # load all files like below  (maybe do dicts instead of 1 big array)\\n        # make list of dict.  -voice_0 ..voice_all, length (-6 in total)\\n\\n\\n        for iLabel in range(len(labels)):\\n            file_names = sorted(glob.glob(os.path.join(data_dir, labels[iLabel], \"*.pkl\")))    \\n\\n            if iLabel == 0:\\n                self.data = np.array(file_names[:n])\\n                self.labels = np.array(np.repeat(labels[iLabel], len(self.data)))\\n                \\n            else:\\n                self.data = np.append(self.data, file_names[:n])\\n                self.labels = np.append(self.labels, np.repeat(labels[iLabel], len(self.data)))\\n                \\n\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, idx):    ###maybe later revert sorted idx to file name idx\\n        print(self.data[idx])\\n\\n        with open(self.data[idx]  ,\\'rb\\') as f:\\n          loaded_obj = pickle.load(f)\\n    \\n        pr_load = torch.tensor(loaded_obj.T)      # transpose np array first\\n\\n\\n        sample = {\\'pr\\': pr_load.float(), \\'label\\': self.labels[idx]}     # also output the len \\n        return sample\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1-u8zYd-gyo"
      },
      "source": [
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        labels = [\"voice_0\", \"voice_1\", \"voice_2\", \"voice_3\", \"voice_all\"]\n",
        "        self.labels = labels\n",
        "        self.pr_dict = {}\n",
        "                                                                                # load all files like below  (maybe do dicts instead of 1 big array)\n",
        "                                                                                # make list of dict.  -voice_0 ..voice_all, length (-6 in total)\n",
        "        len_list = []\n",
        "\n",
        "        for iLabel in range(len(labels)):\n",
        "            \n",
        "            if iLabel == 4:   \n",
        "                \n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)  \n",
        "                        #loaded_obj = loaded_obj.T\n",
        "                        voice_files.append(loaded_obj) \n",
        "                        len_list.append(len(loaded_obj.T))                             # for later: sort it after len(rows) when loading batches - to not have batches with len 1 and len 100 and then need to pad 99 zeros\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                self.pr_dict[\"length\"] = len_list\n",
        "            \n",
        "            else:\n",
        "                voice_files = []\n",
        "                file_names = sorted(glob.glob(os.path.join(PATH_TO_DATA, self.labels[iLabel], \"*.pkl\")))   \n",
        "                for name in file_names:\n",
        "                    with open(name ,'rb') as f: \n",
        "                        loaded_obj = pickle.load(f)     \n",
        "                        voice_files.append(loaded_obj)                              # for later: sort it after len(rows) when loading batches - to not have batches with len 1 and len 100 and then need to pad 99 zeros\n",
        "                self.pr_dict[self.labels[iLabel]] = voice_files \n",
        "                                                    \n",
        "        \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "      \n",
        "        return len(self.pr_dict[self.labels[0]])\n",
        "\n",
        "    def __getitem__(self, idx):                                                 ###maybe later revert sorted idx to file name idx\n",
        "\n",
        "        out_list = []\n",
        "        for key, value in self.pr_dict.items():\n",
        "            out_list.append(self.pr_dict[key][idx])\n",
        "          \n",
        "        v0 = torch.tensor(out_list[0].T)\n",
        "        v1 = torch.tensor(out_list[1].T)\n",
        "        v2 = torch.tensor(out_list[2].T)\n",
        "        v3 = torch.tensor(out_list[3].T)\n",
        "        v_all = torch.tensor(out_list[4].T) \n",
        "\n",
        "        length = self.pr_dict[\"length\"]\n",
        "\n",
        "        # v0.shape =torch.Size([816, 128])  , when doing v0, v1, v2, v3, v_all  = real , print(v0.shape) it is torch.Size([1, 756, 128])  - normal?\n",
        "\n",
        "        return v0, v1, v2, v3, v_all, length"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oTPIsBwPAJg"
      },
      "source": [
        "dataset = MusicDataset(PATH_TO_DATA, labels)\n",
        "loader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM0SS-uLDL0C"
      },
      "source": [
        "### just for testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHbTh6ZkdFOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c544c93-6d21-42c6-c3f7-ae7314245a10"
      },
      "source": [
        "for batch_idx, real in enumerate(loader):\n",
        "  v0, v1, v2, v3, v_all, length  = real\n",
        "print(length)\n",
        "  #print(v0.shape, v1.shape, v2.shape, v3.shape, v_all.shape)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([756]), tensor([624]), tensor([480]), tensor([480]), tensor([816]), tensor([384]), tensor([1332]), tensor([960]), tensor([576]), tensor([624]), tensor([1344]), tensor([576]), tensor([768]), tensor([672]), tensor([588]), tensor([816]), tensor([648]), tensor([624]), tensor([432]), tensor([576]), tensor([576]), tensor([720]), tensor([576]), tensor([576]), tensor([576]), tensor([480]), tensor([480]), tensor([384]), tensor([624]), tensor([564]), tensor([480]), tensor([576]), tensor([420]), tensor([624]), tensor([384]), tensor([672]), tensor([576]), tensor([480]), tensor([576]), tensor([384]), tensor([672]), tensor([432]), tensor([384]), tensor([624]), tensor([384]), tensor([576]), tensor([480]), tensor([588]), tensor([576]), tensor([480]), tensor([720]), tensor([576]), tensor([480]), tensor([480]), tensor([780]), tensor([396]), tensor([912]), tensor([528]), tensor([576]), tensor([768]), tensor([432]), tensor([576]), tensor([624]), tensor([480]), tensor([672]), tensor([624]), tensor([432]), tensor([1152]), tensor([960]), tensor([624]), tensor([396]), tensor([528]), tensor([576]), tensor([528]), tensor([672]), tensor([576]), tensor([528]), tensor([864]), tensor([576]), tensor([816]), tensor([876]), tensor([768]), tensor([672]), tensor([768]), tensor([672]), tensor([756]), tensor([576]), tensor([576]), tensor([468]), tensor([1296]), tensor([528]), tensor([588]), tensor([480]), tensor([576]), tensor([624]), tensor([720]), tensor([576]), tensor([576]), tensor([624]), tensor([480]), tensor([792]), tensor([576]), tensor([432]), tensor([528]), tensor([768]), tensor([1008]), tensor([576]), tensor([1008]), tensor([576]), tensor([528]), tensor([432]), tensor([816]), tensor([576]), tensor([576]), tensor([1296]), tensor([576]), tensor([528]), tensor([864]), tensor([576]), tensor([768]), tensor([672]), tensor([576]), tensor([528]), tensor([480]), tensor([624]), tensor([444]), tensor([576]), tensor([576]), tensor([600]), tensor([480]), tensor([1920]), tensor([672]), tensor([768]), tensor([384]), tensor([996]), tensor([624]), tensor([624]), tensor([576]), tensor([528]), tensor([768]), tensor([1164]), tensor([612]), tensor([480]), tensor([432]), tensor([672]), tensor([480]), tensor([384]), tensor([432]), tensor([624]), tensor([576]), tensor([384]), tensor([864]), tensor([384]), tensor([384]), tensor([768]), tensor([480]), tensor([492]), tensor([576]), tensor([576]), tensor([432]), tensor([576]), tensor([528]), tensor([624]), tensor([768]), tensor([432]), tensor([768]), tensor([384]), tensor([576]), tensor([672]), tensor([648]), tensor([528]), tensor([432]), tensor([576]), tensor([480]), tensor([576]), tensor([816]), tensor([384]), tensor([528]), tensor([480]), tensor([480]), tensor([588]), tensor([384]), tensor([528]), tensor([384]), tensor([576]), tensor([384]), tensor([576]), tensor([576]), tensor([576]), tensor([444]), tensor([576]), tensor([672]), tensor([576]), tensor([1416]), tensor([816]), tensor([756]), tensor([720]), tensor([864]), tensor([1296]), tensor([756]), tensor([432]), tensor([2316]), tensor([720]), tensor([576]), tensor([792]), tensor([576]), tensor([720]), tensor([768]), tensor([672]), tensor([528]), tensor([1296]), tensor([1296]), tensor([960]), tensor([576]), tensor([744]), tensor([528]), tensor([1152]), tensor([480]), tensor([648]), tensor([732]), tensor([384]), tensor([480]), tensor([480]), tensor([912]), tensor([288]), tensor([864]), tensor([480]), tensor([684]), tensor([540]), tensor([576]), tensor([1368]), tensor([816]), tensor([384]), tensor([768]), tensor([480]), tensor([672]), tensor([384]), tensor([1212]), tensor([528]), tensor([720]), tensor([768]), tensor([384]), tensor([624]), tensor([384]), tensor([480]), tensor([480]), tensor([528]), tensor([480]), tensor([1272]), tensor([816]), tensor([624]), tensor([768]), tensor([624]), tensor([588]), tensor([768]), tensor([1296]), tensor([480]), tensor([588]), tensor([528]), tensor([624]), tensor([624]), tensor([720]), tensor([528]), tensor([588]), tensor([912]), tensor([672]), tensor([576]), tensor([576]), tensor([768]), tensor([576]), tensor([624]), tensor([588]), tensor([480]), tensor([912]), tensor([672]), tensor([480]), tensor([972]), tensor([576]), tensor([624]), tensor([576]), tensor([480]), tensor([576]), tensor([480]), tensor([480]), tensor([576]), tensor([480]), tensor([768]), tensor([576]), tensor([480]), tensor([528]), tensor([384]), tensor([1296]), tensor([768]), tensor([972]), tensor([624]), tensor([480]), tensor([480]), tensor([756]), tensor([480]), tensor([576]), tensor([672]), tensor([864]), tensor([816]), tensor([384]), tensor([816]), tensor([384]), tensor([576]), tensor([768]), tensor([480]), tensor([576]), tensor([576]), tensor([684]), tensor([576]), tensor([612]), tensor([816]), tensor([528]), tensor([528]), tensor([720]), tensor([672]), tensor([624]), tensor([576]), tensor([480]), tensor([864]), tensor([480]), tensor([480]), tensor([576]), tensor([576]), tensor([768]), tensor([864]), tensor([576]), tensor([480]), tensor([480]), tensor([624]), tensor([384]), tensor([432]), tensor([576]), tensor([816]), tensor([480]), tensor([792]), tensor([384]), tensor([576]), tensor([720]), tensor([480]), tensor([624]), tensor([576]), tensor([576]), tensor([720]), tensor([768]), tensor([480]), tensor([480]), tensor([576]), tensor([624]), tensor([624]), tensor([1056]), tensor([924]), tensor([528]), tensor([576]), tensor([480]), tensor([576]), tensor([576]), tensor([576]), tensor([576]), tensor([576]), tensor([684]), tensor([576]), tensor([624]), tensor([576])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScrB4vSsHJh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c991e209-fc2c-4561-e58f-20249fde15af"
      },
      "source": [
        "v0.shape"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 576, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iybzKxeDjxe"
      },
      "source": [
        "# Model - RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAv7D01XO4Of"
      },
      "source": [
        "#https://github.com/fosfrancesco/pkspell/blob/main/src/models/models.py"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1oi5Ey7Fzpb"
      },
      "source": [
        "class MusicRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim=120, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):                  #need to know input/output dim first\n",
        "        super(MusicRNN, self).__init__()\n",
        "\n",
        "        self.n_out = output_dim\n",
        "\n",
        "        input_dim = output_dim \n",
        "        \n",
        "        \n",
        "        rnn_cell = nn.GRU\n",
        "\n",
        "        self.rnn = rnn_cell(input_size=input_dim, hidden_size=hidden_dim, num_layers=rnn_depth)\n",
        "\n",
        "\n",
        "        # Output layers. The input will be two times\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "\n",
        "        # Loss function that we will use during training.\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "\n",
        "        # pack padded seq\n",
        "        rnn_out = self.rnn(sentences)\n",
        "        # pad packed seq\n",
        "\n",
        "        out_0 = self.top_layer_voice_0(rnn_out)\n",
        "        out_1 = self.top_layer_voice_1(rnn_out)\n",
        "        out_2 = self.top_layer_voice_2(rnn_out)\n",
        "        out_3 = self.top_layer_voice_3(rnn_out)\n",
        "        \n",
        "\n",
        "        return out_0, out_1, out_2, out_3\n",
        "\n",
        "    def forward(self, sentences, v0, v1, v2, v3, sentences_len):\n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_0, scores_1, scores_2, scores_3 = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_0.view(-1, self.n_out)\n",
        "        score_1  = scores_1.view(-1, self.n_out)\n",
        "        score_2  = scores_2.view(-1, self.n_out)\n",
        "        score_3  = scores_3.view(-1, self.n_out)\n",
        "\n",
        "        # dont take padded len -> loss(score_0[:sentence_len], v0)      -> therefore store real len of voices  - before view\n",
        "\n",
        "        v0 = v0.view(-1)\n",
        "        v1 = v1.view(-1)\n",
        "        v2 = v2.view(-1)\n",
        "        v3 = v3.view(-1)\n",
        " \n",
        "        \n",
        "        loss = self.loss_pitch(score_0, v0) + self.loss_pitch(score_1, v1) + self.loss_pitch(score_2, v2) + self.loss_pitch(score_3, v3)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def predict(self, sentences, sentences_len):\n",
        "\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_0, scores_1, scores_2, scores_3 = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
        "        predicted_0 = scores_0.argmax(dim=2)\n",
        "        predicted_1 = scores_1.argmax(dim=2)\n",
        "        predicted_2 = scores_2.argmax(dim=2)\n",
        "        predicted_3 = scores_3.argmax(dim=2)       # prbl distr over 0-not there, 1-there   - argmax -> take higher probl of those 2\n",
        "        \n",
        "        ##  predicted_X should be enough for BS = 1;  \n",
        "        return (\n",
        "            [\n",
        "                predicted_0[: int(l), i].cpu().numpy()\n",
        "                for i, l in enumerate(sentences_len)\n",
        "            ],\n",
        "            [\n",
        "                predicted_1[: int(l), i].cpu().numpy()\n",
        "                for i, l in enumerate(sentences_len)\n",
        "            ],\n",
        "            [\n",
        "                predicted_2[: int(l), i].cpu().numpy()\n",
        "                for i, l in enumerate(sentences_len)\n",
        "            ],\n",
        "            [\n",
        "                predicted_3[: int(l), i].cpu().numpy()\n",
        "                for i, l in enumerate(sentences_len)\n",
        "            ],\n",
        "        )\n",
        "\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07I2QbRDbUlA"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHESuQEQbVRB"
      },
      "source": [
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "def train(epochs, lr, hidden_dim, momentum, rnn_depth, device, rnn_cell, weight_decay, train_dataloader, val_dataloader=None):\n",
        "  \n",
        "    model = MusicRNN(output_dim, input_dim, hidden_dim, rnn_depth, cell_type)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimizer, [epochs // 2], gamma=0.1, verbose=True)\n",
        "\n",
        "    history = training_loop(model, optimizer, train_dataloader, epochs=epochs, val_dataloader=val_dataloader, device=device, scheduler=scheduler)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITD9mYRfb9p-"
      },
      "source": [
        "import random\n",
        "import click\n",
        "import sklearn\n",
        "import sklearn.model_selection\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "#sys.path.append(str(Path(__file__).resolve().parents[2]))"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG_ONds0bkt-"
      },
      "source": [
        "def training_loop(model,optimizer, train_dataloader, epochs=50, val_dataloader=None, device=None, scheduler=None,):\n",
        "    if device is None:\n",
        "        device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "        print(f\"Training on device: {device}\")\n",
        "\n",
        "    # Move model to GPU if needed\n",
        "    model = model.to(device)\n",
        "\n",
        "    history = defaultdict(list)\n",
        "\n",
        "    for i_epoch in range(1, epochs + 1):\n",
        "        loss_sum = 0\n",
        "        accuracy_v0_sum = 0\n",
        "        accuracy_v1_sum = 0\n",
        "        accuracy_v2_sum = 0\n",
        "        accuracy_v3_sum = 0\n",
        "        accuracy_v_all_sum = 0\n",
        "        model.train()\n",
        "\n",
        "        for idx, (v0, v1, v2, v3, v_all, lens,) in enumerate(train_dataloader):  \n",
        "\n",
        "            v0, v1, v2, v3, v_all = (v0.to(device), v1.to(device), v2.to(device), v3.to(device), v_all.to(device))\n",
        "            optimizer.zero_grad()\n",
        "            loss = model.forward(v_all, len)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_sum += loss.item()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred_v0, pred_v1, pred_v2, pred_v3 = model.predict(v_all, lens)\n",
        "\n",
        "                for i, (p, k, l, m) in enumerate(zip(pred_v0, pred_v1, pred_v2, pred_v3)):\n",
        "                    # compute the accuracy without considering the padding\n",
        "                    acc_v0 = accuracy_score(p, v0[:, i][: len(p)].cpu())\n",
        "                    acc_v1 = accuracy_score(k, v1[:, i][: len(k)].cpu())\n",
        "                    acc_v2 = accuracy_score(l, v2[:, i][: len(m)].cpu())\n",
        "                    acc_v3 = accuracy_score(m, v3[:, i][: len(l)].cpu())\n",
        "                    # normalize according to the number of sequences in the batch\n",
        "                    accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                    accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                    accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                    accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "        train_loss = loss_sum / len(train_dataloader)\n",
        "        # normalize according to the number of batches\n",
        "        train_accuracy_v0 = accuracy_v0_sum / len(train_dataloader)\n",
        "        train_accuracy_v1 = accuracy_v1_sum / len(train_dataloader)\n",
        "        train_accuracy_v2 = accuracy_v2_sum / len(train_dataloader)\n",
        "        train_accuracy_v3 = accuracy_v3_sum / len(train_dataloader)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_accuracy_v0\"].append(train_accuracy_v0)\n",
        "        history[\"train_accuracy_v1\"].append(train_accuracy_v1)\n",
        "        history[\"train_accuracy_v2\"].append(train_accuracy_v2)\n",
        "        history[\"train_accuracy_v3\"].append(train_accuracy_v3)\n",
        "        \n",
        "        print(\"Train Loss: {}, Train Accuracy (Pitch): {}, Train Accuracy (KS): {}\".format(\n",
        "                train_loss, train_accuracy_v0, train_accuracy_v1, train_accuracy_v2, train_accuracy_v3))\n",
        "\n",
        "        if val_dataloader is not None:\n",
        "            # Evaluate on the validation set\n",
        "            model.eval()\n",
        "            accuracy_v0_sum = 0\n",
        "            accuracy_v1_sum = 0\n",
        "            accuracy_v2_sum = 0\n",
        "            accuracy_v3_sum = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for v0, v1, v2, v3, v_all, lens in val_dataloader:\n",
        "                    # Predict the model's output on a batch\n",
        "                    predicted_v0, predicted_v1, predicted_v2, predicted_v3 = model.predict(v_all.to(device), lens)\n",
        "                    # Update the lists that will be used to compute the accuracy\n",
        "                    for i, (p, k, m ,l) in enumerate(zip(predicted_v0, predicted_v1, predicted_v2, predicted_v3)):\n",
        "                        # compute the accuracy without considering the padding\n",
        "                        acc_v0 = accuracy_score(p, v0[:, i][: len(p)].cpu())\n",
        "                        acc_v1 = accuracy_score(k, v1[:, i][: len(k)].cpu())\n",
        "                        acc_v2 = accuracy_score(m, v2[:, i][: len(m)].cpu())\n",
        "                        acc_v3 = accuracy_score(l, v3[:, i][: len(l)].cpu())\n",
        "                        \n",
        "                        # normalize according to the number of sequences in the batch\n",
        "                        accuracy_v0_sum += acc_v0 / len(lens)\n",
        "                        accuracy_v1_sum += acc_v1 / len(lens)\n",
        "                        accuracy_v2_sum += acc_v2 / len(lens)\n",
        "                        accuracy_v3_sum += acc_v3 / len(lens)\n",
        "\n",
        "                # normalize according to the number of batches\n",
        "                val_accuracy_v0 = accuracy_v0_sum / len(val_dataloader)\n",
        "                val_accuracy_v1 = accuracy_v1_sum / len(val_dataloader)\n",
        "                val_accuracy_v2 = accuracy_v2_sum / len(val_dataloader)\n",
        "                val_accuracy_v3 = accuracy_v3_sum / len(val_dataloader)\n",
        "\n",
        "            history[\"val_accuracy_v0\"].append(val_accuracy_v0)\n",
        "            history[\"val_accuracy_v1\"].append(val_accuracy_v1)\n",
        "            history[\"val_accuracy_v2\"].append(val_accuracy_v2)\n",
        "            history[\"val_accuracy_v3\"].append(val_accuracy_v3)\n",
        "            \n",
        "            print(\"Validation Accuracy (Pitch): {}, Validation Accuracy (KS): {}\".format(val_accuracy_v0, val_accuracy_v1, val_accuracy_v2, val_accuracy_v3))\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # save the model\n",
        "        torch.save(model, Path(\"./AI-MA_project/model_temp_epoch{}.pkl\".format(i_epoch)))\n",
        "\n",
        "    return history"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNzPG7eeIEoO",
        "outputId": "3d9dd6dd-ae00-4203-f3da-4732d54a6258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI-MA_project  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sBoQnA6bo71"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "def start_experiment(model, epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay, optimizer, learn_all):\n",
        "    \n",
        "\n",
        "    trainer = partial(train, model, epochs, lr, hidden_dim, momentum, rnn_depth, device, cell, decay, optimizer)\n",
        "\n",
        "    if learn_all:\n",
        "        print(\"Learning from full dataset\")\n",
        "        train_dataset = MusicDataset(PATH_TO_DATA)\n",
        "        train_dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size, shuffle=False, num_workers=workers, drop_last=True)\n",
        "        \n",
        "        _, history = trainer(train_dataloader(train_dataset),)\n",
        "    \n",
        "    else:\n",
        "        paths = np.array(paths)\n",
        "        # Divide train and validation set\n",
        "        path_train, path_validation = sklearn.model_selection.train_test_split(PATH_TO_DATA, test_size=0.15, random_state=seed,)\n",
        "\n",
        "        print(\"Train and validation lenghts: \", len(path_train), len(path_validation))\n",
        "        train_dataset = MusicDataset(path_train)\n",
        "        validation_dataset = MusicDataset(path_validation)\n",
        "\n",
        "        _, history = trainer(\n",
        "            train_dataloader(train_dataset), val_dataloader(validation_dataset))"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgtn-a7bMTf7"
      },
      "source": [
        "# Hyperparameter choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNI9b6jKLpOX"
      },
      "source": [
        "model = MusicRNN\n",
        "epochs = 1\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "decay = 1e-4\n",
        "hidden_dim = 300\n",
        "bs = 1\n",
        "rnn_depth = 1\n",
        "device = \"GPU\"\n",
        "cell = \"GRU\"\n",
        "optimizer = \"Adam\"\n",
        "learn_all = \"True\"      # change later to false to see if learning works"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1LTlFJddpwm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "e7fe3a4a-96a8-499d-a731-5634cd65eee6"
      },
      "source": [
        "start_experiment(model, epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay, optimizer, learn_all)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning from full dataset\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-34d6aee39734>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-83-d904989a5a48>\u001b[0m in \u001b[0;36mstart_experiment\u001b[0;34m(model, epochs, lr, hidden_dim, bs, momentum, rnn_depth, device, cell, decay, optimizer, learn_all)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhbqDDmPLkrH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}