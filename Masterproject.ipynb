{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masterproject.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlfIzMCqaiGaiMvhn3BKRm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aim56009/AI-MA_project/blob/main/Masterproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUwCmeIW8i1"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsFs8dyqXBx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a79fee43-f436-4f70-d185-c6201e88b69d"
      },
      "source": [
        "!git clone https://github.com/aim56009/AI-MA_project.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI-MA_project'...\n",
            "remote: Enumerating objects: 1809, done.\u001b[K\n",
            "remote: Counting objects: 100% (1809/1809), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 1809 (delta 1756), reused 1780 (delta 1747), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1809/1809), 3.86 MiB | 12.46 MiB/s, done.\n",
            "Resolving deltas: 100% (1756/1756), done.\n",
            "Checking out files: 100% (1846/1846), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdW3DWVIByCc"
      },
      "source": [
        "PATH_TO_DATA = \"AI-MA_project/pianoroll\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riVVYUR4B7kp",
        "outputId": "75e8bed4-f292-4e3b-d386-e077962310df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with open(PATH_TO_DATA + \"/voice_0/voice_0_001.pkl\",'rb') as f:\n",
        "    loaded_obj = pickle.load(f)\n",
        "    print(loaded_obj.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 756)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te-I51m3CEPZ",
        "outputId": "7232ba60-9920-455b-dc09-b02670c89869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loaded_obj"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iybzKxeDjxe"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1oi5Ey7Fzpb"
      },
      "source": [
        "class PKSpell_single(nn.Module):\n",
        "\n",
        "    def __init__(self, pitch_to_ix, output_dim, input_dim=17, hidden_dim=300, rnn_depth=1, cell_type=\"GRU\"):\n",
        "        super(PKSpell_single, self).__init__()\n",
        "\n",
        "        self.n_out = output_dim\n",
        "        \n",
        "        \n",
        "        rnn_cell = nn.GRU\n",
        "\n",
        "        self.rnn = rnn_cell(\n",
        "            input_size=input_dim,\n",
        "            hidden_size= hidden_dim,\n",
        "            num_layers=rnn_depth)\n",
        "\n",
        "\n",
        "        # Output layers. The input will be two times\n",
        "        self.top_layer_voice_0 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_1 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_2 = nn.Linear(hidden_dim, self.n_out)\n",
        "        self.top_layer_voice_3 = nn.Linear(hidden_dim, self.n_out)\n",
        "\n",
        "        # Loss function that we will use during training.\n",
        "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "\n",
        "    def compute_outputs(self, sentences, sentences_len):\n",
        "        \n",
        "        rnn_out = self.rnn(sentences)\n",
        "        \n",
        "        out_0 = self.top_layer_voice_0(rnn_out)\n",
        "        out_1 = self.top_layer_voice_1(rnn_out)\n",
        "        out_2 = self.top_layer_voice_2(rnn_out)\n",
        "        out_3 = self.top_layer_voice_3(rnn_out)\n",
        "        \n",
        "\n",
        "        return out_0, out_1, out_2, out_3\n",
        "\n",
        "    def forward(self, sentences, v0, v1, v2, v3, sentences_len):\n",
        "        # First computes the predictions, and then the loss function.\n",
        "\n",
        "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
        "        scores_0, scores_1, scores_2, scores_3 = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        # Flatten the outputs and the labels, to compute the loss.\n",
        "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
        "        score_0  = scores_0.view(-1, self.n_out)\n",
        "        score_1  = scores_1.view(-1, self.n_out)\n",
        "        score_2  = scores_2.view(-1, self.n_out)\n",
        "        score_3  = scores_3.view(-1, self.n_out)\n",
        "\n",
        "\n",
        "        v0 = v0.view(-1)\n",
        "        v1 = v1.view(-1)\n",
        "        v2 = v2.view(-1)\n",
        "        v3 = v3.view(-1)\n",
        "\n",
        "        \n",
        "        loss = self.loss_pitch(score_0, v0) + self.loss_pitch(score_1, v1) + self.loss_pitch(score_2, v2) + self.loss_pitch(score_3, v3)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def predict(self, sentences, sentences_len):\n",
        "        # Compute the outputs from the linear units.\n",
        "        scores_0, scores_1, scores_2, scores_3 = self.compute_outputs(sentences, sentences_len)\n",
        "\n",
        "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
        "        predicted_0 = scores_0.argmax(dim=2)\n",
        "        predicted_1 = scores_1.argmax(dim=2)\n",
        "        predicted_2 = scores_2.argmax(dim=2)\n",
        "        predicted_3 = scores_3.argmax(dim=2)\n",
        "\n",
        "        return (\n",
        "            [\n",
        "                predicted_0[: int(l), i].cpu().numpy()\n",
        "                for i, l in enumerate(sentences_len)\n",
        "            ],\n",
        "            [\n",
        "                predicted_1[: int(l), i].cpu().numpy()\n",
        "                for i, l in enumerate(sentences_len)\n",
        "            ],\n",
        "            [\n",
        "                predicted_2[: int(l), i].cpu().numpy()\n",
        "                for i, l in enumerate(sentences_len)\n",
        "            ],\n",
        "            [\n",
        "                predicted_3[: int(l), i].cpu().numpy()\n",
        "                for i, l in enumerate(sentences_len)\n",
        "            ],\n",
        "        )\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0pQkEihHFx3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}